// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -triple kvx-kalray-cos -emit-llvm -disable-free -disable-llvm-verifier -discard-value-names -O2 -o - %s | FileCheck %s
#include <kvxtcaintrin.h>
// CHECK-LABEL: @foo(
// CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds <256 x i1>, <256 x i1>* [[TMP0:%.*]], i64 1
// CHECK-NEXT:    [[TMP3:%.*]] = load <256 x i1>, <256 x i1>* [[TMP2]], align 32, [[TBAA2:!tbaa !.*]]
// CHECK-NEXT:    store <256 x i1> [[TMP3]], <256 x i1>* [[TMP0]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void foo(tca256_t *v){
	v[0] = v[1];
}

// CHECK-LABEL: @movefobv(
// CHECK-NEXT:    [[TMP2:%.*]] = load <256 x i1>, <256 x i1>* [[TMP0:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <4 x i64> @llvm.kvx.xmovefo(<256 x i1> [[TMP2]]) [[ATTR5:#.*]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <32 x i8>
// CHECK-NEXT:    ret <32 x i8> [[TMP4]]
//
char32 movefobv(tca256_t *v){
    return __builtin_kvx_movefobv(v);
}

// CHECK-LABEL: @movefohx(
// CHECK-NEXT:    [[TMP2:%.*]] = load <256 x i1>, <256 x i1>* [[TMP0:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <4 x i64> @llvm.kvx.xmovefo(<256 x i1> [[TMP2]]) [[ATTR5]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <16 x i16>
// CHECK-NEXT:    ret <16 x i16> [[TMP4]]
//
short16 movefohx(tca256_t *v){
    return __builtin_kvx_movefohx(v);
}

// CHECK-LABEL: @movefowo(
// CHECK-NEXT:    [[TMP2:%.*]] = load <256 x i1>, <256 x i1>* [[TMP0:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <4 x i64> @llvm.kvx.xmovefo(<256 x i1> [[TMP2]]) [[ATTR5]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <8 x i32>
// CHECK-NEXT:    ret <8 x i32> [[TMP4]]
//
int8 movefowo(__kvx_x256 *v){
   return __builtin_kvx_movefowo(v);
}

// CHECK-LABEL: @movefodq(
// CHECK-NEXT:    [[TMP2:%.*]] = load <256 x i1>, <256 x i1>* [[TMP0:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <4 x i64> @llvm.kvx.xmovefo(<256 x i1> [[TMP2]]) [[ATTR5]]
// CHECK-NEXT:    ret <4 x i64> [[TMP3]]
//
long4 movefodq(__kvx_x256 *v){
   return __builtin_kvx_movefodq(v);
}

// CHECK-LABEL: @movefofhx(
// CHECK-NEXT:    [[TMP2:%.*]] = load <256 x i1>, <256 x i1>* [[TMP0:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <4 x i64> @llvm.kvx.xmovefo(<256 x i1> [[TMP2]]) [[ATTR5]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <16 x half>
// CHECK-NEXT:    ret <16 x half> [[TMP4]]
//
half16 movefofhx(tca256_t *v){
    return __builtin_kvx_movefofhx(v);
}

// CHECK-LABEL: @movefofwo(
// CHECK-NEXT:    [[TMP2:%.*]] = load <256 x i1>, <256 x i1>* [[TMP0:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <4 x i64> @llvm.kvx.xmovefo(<256 x i1> [[TMP2]]) [[ATTR5]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <8 x float>
// CHECK-NEXT:    ret <8 x float> [[TMP4]]
//
float8 movefofwo(__kvx_x256 *v){
   return __builtin_kvx_movefofwo(v);
}

// CHECK-LABEL: @movefofdq(
// CHECK-NEXT:    [[TMP2:%.*]] = load <256 x i1>, <256 x i1>* [[TMP0:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <4 x i64> @llvm.kvx.xmovefo(<256 x i1> [[TMP2]]) [[ATTR5]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <4 x double>
// CHECK-NEXT:    ret <4 x double> [[TMP4]]
//
double4 movefofdq(__kvx_x256 *v){
   return __builtin_kvx_movefofdq(v);
}

// CHECK-LABEL: @alignobv(
// CHECK-NEXT:    [[TMP4:%.*]] = load <256 x i1>, <256 x i1>* [[TMP0:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP5:%.*]] = load <256 x i1>, <256 x i1>* [[TMP1:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP6:%.*]] = tail call <4 x i64> @llvm.kvx.alignov(<256 x i1> [[TMP4]], <256 x i1> [[TMP5]], i64 [[TMP2:%.*]]) [[ATTR5]]
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <4 x i64> [[TMP6]] to <32 x i8>
// CHECK-NEXT:    ret <32 x i8> [[TMP7]]
//
char32 alignobv(__kvx_x256 *v0, __kvx_x256 *v1, unsigned long byteshift){
    return __builtin_kvx_alignobv(v0, v1, byteshift);
}

// CHECK-LABEL: @alignohx(
// CHECK-NEXT:    [[TMP4:%.*]] = load <256 x i1>, <256 x i1>* [[TMP0:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP5:%.*]] = load <256 x i1>, <256 x i1>* [[TMP1:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP6:%.*]] = tail call <4 x i64> @llvm.kvx.alignov(<256 x i1> [[TMP4]], <256 x i1> [[TMP5]], i64 [[TMP2:%.*]]) [[ATTR5]]
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <4 x i64> [[TMP6]] to <16 x i16>
// CHECK-NEXT:    ret <16 x i16> [[TMP7]]
//
short16 alignohx(__kvx_x256 *v0, __kvx_x256 *v1, unsigned long byteshift){
    return __builtin_kvx_alignohx(v0, v1, byteshift);
}

// CHECK-LABEL: @alignowo(
// CHECK-NEXT:    [[TMP4:%.*]] = load <256 x i1>, <256 x i1>* [[TMP0:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP5:%.*]] = load <256 x i1>, <256 x i1>* [[TMP1:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP6:%.*]] = tail call <4 x i64> @llvm.kvx.alignov(<256 x i1> [[TMP4]], <256 x i1> [[TMP5]], i64 [[TMP2:%.*]]) [[ATTR5]]
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <4 x i64> [[TMP6]] to <8 x i32>
// CHECK-NEXT:    ret <8 x i32> [[TMP7]]
//
int8 alignowo(__kvx_x256 *v0, __kvx_x256 *v1, unsigned long byteshift){
    return __builtin_kvx_alignowo(v0, v1, byteshift);
}

// CHECK-LABEL: @alignodq(
// CHECK-NEXT:    [[TMP4:%.*]] = load <256 x i1>, <256 x i1>* [[TMP0:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP5:%.*]] = load <256 x i1>, <256 x i1>* [[TMP1:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP6:%.*]] = tail call <4 x i64> @llvm.kvx.alignov(<256 x i1> [[TMP4]], <256 x i1> [[TMP5]], i64 [[TMP2:%.*]]) [[ATTR5]]
// CHECK-NEXT:    ret <4 x i64> [[TMP6]]
//
long4 alignodq(__kvx_x256 *v0, __kvx_x256 *v1, unsigned long byteshift){
    return __builtin_kvx_alignodq(v0, v1, byteshift);
}

// CHECK-LABEL: @alignofhx(
// CHECK-NEXT:    [[TMP4:%.*]] = load <256 x i1>, <256 x i1>* [[TMP0:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP5:%.*]] = load <256 x i1>, <256 x i1>* [[TMP1:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP6:%.*]] = tail call <4 x i64> @llvm.kvx.alignov(<256 x i1> [[TMP4]], <256 x i1> [[TMP5]], i64 [[TMP2:%.*]]) [[ATTR5]]
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <4 x i64> [[TMP6]] to <16 x half>
// CHECK-NEXT:    ret <16 x half> [[TMP7]]
//
half16 alignofhx(__kvx_x256 *v0, __kvx_x256 *v1, unsigned long byteshift){
    return __builtin_kvx_alignofhx(v0, v1, byteshift);
}

// CHECK-LABEL: @alignofwo(
// CHECK-NEXT:    [[TMP4:%.*]] = load <256 x i1>, <256 x i1>* [[TMP0:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP5:%.*]] = load <256 x i1>, <256 x i1>* [[TMP1:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP6:%.*]] = tail call <4 x i64> @llvm.kvx.alignov(<256 x i1> [[TMP4]], <256 x i1> [[TMP5]], i64 [[TMP2:%.*]]) [[ATTR5]]
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <4 x i64> [[TMP6]] to <8 x float>
// CHECK-NEXT:    ret <8 x float> [[TMP7]]
//
float8 alignofwo(__kvx_x256 *v0, __kvx_x256 *v1, unsigned long byteshift){
    return __builtin_kvx_alignofwo(v0, v1, byteshift);
}

// CHECK-LABEL: @alignofdq(
// CHECK-NEXT:    [[TMP4:%.*]] = load <256 x i1>, <256 x i1>* [[TMP0:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP5:%.*]] = load <256 x i1>, <256 x i1>* [[TMP1:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP6:%.*]] = tail call <4 x i64> @llvm.kvx.alignov(<256 x i1> [[TMP4]], <256 x i1> [[TMP5]], i64 [[TMP2:%.*]]) [[ATTR5]]
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <4 x i64> [[TMP6]] to <4 x double>
// CHECK-NEXT:    ret <4 x double> [[TMP7]]
//
double4 alignofdq(__kvx_x256 *v0, __kvx_x256 *v1, unsigned long byteshift){
    return __builtin_kvx_alignofdq(v0, v1, byteshift);
}

// CHECK-LABEL: @movetobv(
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i8> [[TMP0:%.*]] to <4 x i64>
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <256 x i1> @llvm.kvx.xmoveoto(<4 x i64> [[TMP3]]) [[ATTR5]]
// CHECK-NEXT:    store <256 x i1> [[TMP4]], <256 x i1>* [[TMP1:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret <256 x i1>* [[TMP1]]
//
__kvx_x256 * movetobv(char32 bv, __kvx_x256 *v){
    return __builtin_kvx_movetobv(bv, v);
}

// CHECK-LABEL: @movetohx(
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <16 x i16> [[TMP0:%.*]] to <4 x i64>
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <256 x i1> @llvm.kvx.xmoveoto(<4 x i64> [[TMP3]]) [[ATTR5]]
// CHECK-NEXT:    store <256 x i1> [[TMP4]], <256 x i1>* [[TMP1:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret <256 x i1>* [[TMP1]]
//
__kvx_x256 *movetohx(short16 hx, __kvx_x256 *v){
    return __builtin_kvx_movetohx(hx, v);
}

// CHECK-LABEL: @movetowo(
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x i32> [[TMP0:%.*]] to <4 x i64>
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <256 x i1> @llvm.kvx.xmoveoto(<4 x i64> [[TMP3]]) [[ATTR5]]
// CHECK-NEXT:    store <256 x i1> [[TMP4]], <256 x i1>* [[TMP1:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret <256 x i1>* [[TMP1]]
//
__kvx_x256 *movetowo(int8 wo, __kvx_x256 *v){
    return __builtin_kvx_movetowo(wo, v);
}

// CHECK-LABEL: @movetodq(
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <256 x i1> @llvm.kvx.xmoveoto(<4 x i64> [[TMP0:%.*]]) [[ATTR5]]
// CHECK-NEXT:    store <256 x i1> [[TMP3]], <256 x i1>* [[TMP1:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret <256 x i1>* [[TMP1]]
//
__kvx_x256 *movetodq(long4 dq, __kvx_x256 *v){
    return __builtin_kvx_movetodq(dq, v);
}

// CHECK-LABEL: @movetofhx(
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <16 x half> [[TMP0:%.*]] to <4 x i64>
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <256 x i1> @llvm.kvx.xmoveoto(<4 x i64> [[TMP3]]) [[ATTR5]]
// CHECK-NEXT:    store <256 x i1> [[TMP4]], <256 x i1>* [[TMP1:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret <256 x i1>* [[TMP1]]
//
__kvx_x256 *movetofhx(half16 fhx, __kvx_x256 *v){
    return __builtin_kvx_movetofhx(fhx, v);
}

// CHECK-LABEL: @movetofo(
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x float> [[TMP0:%.*]] to <4 x i64>
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <256 x i1> @llvm.kvx.xmoveoto(<4 x i64> [[TMP3]]) [[ATTR5]]
// CHECK-NEXT:    store <256 x i1> [[TMP4]], <256 x i1>* [[TMP1:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret <256 x i1>* [[TMP1]]
//
__kvx_x256 *movetofo(float8 fo, __kvx_x256 *v){
    return __builtin_kvx_movetofo(fo, v);
}

// CHECK-LABEL: @movetofdq(
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <4 x double> [[TMP0:%.*]] to <4 x i64>
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <256 x i1> @llvm.kvx.xmoveoto(<4 x i64> [[TMP3]]) [[ATTR5]]
// CHECK-NEXT:    store <256 x i1> [[TMP4]], <256 x i1>* [[TMP1:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret <256 x i1>* [[TMP1]]
//
__kvx_x256 *movetofdq(double4 fdq, __kvx_x256 *v){
    return __builtin_kvx_movetofdq(fdq, v);
}

// CHECK-LABEL: @swapvobv(
// CHECK-NEXT:    [[TMP3:%.*]] = load <256 x i1>, <256 x i1>* [[TMP1:%.*]], align 32
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <32 x i8>* [[TMP0:%.*]] to <4 x i64>*
// CHECK-NEXT:    [[TMP5:%.*]] = load <4 x i64>, <4 x i64>* [[TMP4]], align 32
// CHECK-NEXT:    [[TMP6:%.*]] = tail call { <4 x i64>, <256 x i1> } @llvm.kvx.xswapvo(<4 x i64> [[TMP5]], <256 x i1> [[TMP3]]) [[ATTR5]]
// CHECK-NEXT:    [[TMP7:%.*]] = extractvalue { <4 x i64>, <256 x i1> } [[TMP6]], 1
// CHECK-NEXT:    store <256 x i1> [[TMP7]], <256 x i1>* [[TMP1]], align 32
// CHECK-NEXT:    [[TMP8:%.*]] = extractvalue { <4 x i64>, <256 x i1> } [[TMP6]], 0
// CHECK-NEXT:    store <4 x i64> [[TMP8]], <4 x i64>* [[TMP4]], align 32
// CHECK-NEXT:    ret void
//
void swapvobv(char32 *v0, __kvx_x256 *v1){
    __builtin_kvx_swapvobv(v0, v1);
}

// CHECK-LABEL: @swapvohx(
// CHECK-NEXT:    [[TMP3:%.*]] = load <256 x i1>, <256 x i1>* [[TMP1:%.*]], align 32
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <16 x i16>* [[TMP0:%.*]] to <4 x i64>*
// CHECK-NEXT:    [[TMP5:%.*]] = load <4 x i64>, <4 x i64>* [[TMP4]], align 32
// CHECK-NEXT:    [[TMP6:%.*]] = tail call { <4 x i64>, <256 x i1> } @llvm.kvx.xswapvo(<4 x i64> [[TMP5]], <256 x i1> [[TMP3]]) [[ATTR5]]
// CHECK-NEXT:    [[TMP7:%.*]] = extractvalue { <4 x i64>, <256 x i1> } [[TMP6]], 1
// CHECK-NEXT:    store <256 x i1> [[TMP7]], <256 x i1>* [[TMP1]], align 32
// CHECK-NEXT:    [[TMP8:%.*]] = extractvalue { <4 x i64>, <256 x i1> } [[TMP6]], 0
// CHECK-NEXT:    store <4 x i64> [[TMP8]], <4 x i64>* [[TMP4]], align 32
// CHECK-NEXT:    ret void
//
void swapvohx(short16 *v0, __kvx_x256 *v1){
    __builtin_kvx_swapvohx(v0, v1);
}

// CHECK-LABEL: @swapvowo(
// CHECK-NEXT:    [[TMP3:%.*]] = load <256 x i1>, <256 x i1>* [[TMP1:%.*]], align 32
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32>* [[TMP0:%.*]] to <4 x i64>*
// CHECK-NEXT:    [[TMP5:%.*]] = load <4 x i64>, <4 x i64>* [[TMP4]], align 32
// CHECK-NEXT:    [[TMP6:%.*]] = tail call { <4 x i64>, <256 x i1> } @llvm.kvx.xswapvo(<4 x i64> [[TMP5]], <256 x i1> [[TMP3]]) [[ATTR5]]
// CHECK-NEXT:    [[TMP7:%.*]] = extractvalue { <4 x i64>, <256 x i1> } [[TMP6]], 1
// CHECK-NEXT:    store <256 x i1> [[TMP7]], <256 x i1>* [[TMP1]], align 32
// CHECK-NEXT:    [[TMP8:%.*]] = extractvalue { <4 x i64>, <256 x i1> } [[TMP6]], 0
// CHECK-NEXT:    store <4 x i64> [[TMP8]], <4 x i64>* [[TMP4]], align 32
// CHECK-NEXT:    ret void
//
void swapvowo(int8 *v0, __kvx_x256 *v1){
    __builtin_kvx_swapvowo(v0, v1);
}

// CHECK-LABEL: @swapvodq(
// CHECK-NEXT:    [[TMP3:%.*]] = load <256 x i1>, <256 x i1>* [[TMP1:%.*]], align 32
// CHECK-NEXT:    [[TMP4:%.*]] = load <4 x i64>, <4 x i64>* [[TMP0:%.*]], align 32
// CHECK-NEXT:    [[TMP5:%.*]] = tail call { <4 x i64>, <256 x i1> } @llvm.kvx.xswapvo(<4 x i64> [[TMP4]], <256 x i1> [[TMP3]]) [[ATTR5]]
// CHECK-NEXT:    [[TMP6:%.*]] = extractvalue { <4 x i64>, <256 x i1> } [[TMP5]], 1
// CHECK-NEXT:    store <256 x i1> [[TMP6]], <256 x i1>* [[TMP1]], align 32
// CHECK-NEXT:    [[TMP7:%.*]] = extractvalue { <4 x i64>, <256 x i1> } [[TMP5]], 0
// CHECK-NEXT:    store <4 x i64> [[TMP7]], <4 x i64>* [[TMP0]], align 32
// CHECK-NEXT:    ret void
//
void swapvodq(long4 *v0, __kvx_x256 *v1){
    __builtin_kvx_swapvodq(v0, v1);
}

// CHECK-LABEL: @swapvofhx(
// CHECK-NEXT:    [[TMP3:%.*]] = load <256 x i1>, <256 x i1>* [[TMP1:%.*]], align 32
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <16 x half>* [[TMP0:%.*]] to <4 x i64>*
// CHECK-NEXT:    [[TMP5:%.*]] = load <4 x i64>, <4 x i64>* [[TMP4]], align 32
// CHECK-NEXT:    [[TMP6:%.*]] = tail call { <4 x i64>, <256 x i1> } @llvm.kvx.xswapvo(<4 x i64> [[TMP5]], <256 x i1> [[TMP3]]) [[ATTR5]]
// CHECK-NEXT:    [[TMP7:%.*]] = extractvalue { <4 x i64>, <256 x i1> } [[TMP6]], 1
// CHECK-NEXT:    store <256 x i1> [[TMP7]], <256 x i1>* [[TMP1]], align 32
// CHECK-NEXT:    [[TMP8:%.*]] = extractvalue { <4 x i64>, <256 x i1> } [[TMP6]], 0
// CHECK-NEXT:    store <4 x i64> [[TMP8]], <4 x i64>* [[TMP4]], align 32
// CHECK-NEXT:    ret void
//
void swapvofhx(half16 *v0, __kvx_x256 *v1){
    __builtin_kvx_swapvofhx(v0, v1);
}

// CHECK-LABEL: @swapvofwo(
// CHECK-NEXT:    [[TMP3:%.*]] = load <256 x i1>, <256 x i1>* [[TMP1:%.*]], align 32
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <8 x float>* [[TMP0:%.*]] to <4 x i64>*
// CHECK-NEXT:    [[TMP5:%.*]] = load <4 x i64>, <4 x i64>* [[TMP4]], align 32
// CHECK-NEXT:    [[TMP6:%.*]] = tail call { <4 x i64>, <256 x i1> } @llvm.kvx.xswapvo(<4 x i64> [[TMP5]], <256 x i1> [[TMP3]]) [[ATTR5]]
// CHECK-NEXT:    [[TMP7:%.*]] = extractvalue { <4 x i64>, <256 x i1> } [[TMP6]], 1
// CHECK-NEXT:    store <256 x i1> [[TMP7]], <256 x i1>* [[TMP1]], align 32
// CHECK-NEXT:    [[TMP8:%.*]] = extractvalue { <4 x i64>, <256 x i1> } [[TMP6]], 0
// CHECK-NEXT:    store <4 x i64> [[TMP8]], <4 x i64>* [[TMP4]], align 32
// CHECK-NEXT:    ret void
//
void swapvofwo(float8 *v0, __kvx_x256 *v1){
    __builtin_kvx_swapvofwo(v0, v1);
}

// CHECK-LABEL: @swapvofdq(
// CHECK-NEXT:    [[TMP3:%.*]] = load <256 x i1>, <256 x i1>* [[TMP1:%.*]], align 32
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <4 x double>* [[TMP0:%.*]] to <4 x i64>*
// CHECK-NEXT:    [[TMP5:%.*]] = load <4 x i64>, <4 x i64>* [[TMP4]], align 32
// CHECK-NEXT:    [[TMP6:%.*]] = tail call { <4 x i64>, <256 x i1> } @llvm.kvx.xswapvo(<4 x i64> [[TMP5]], <256 x i1> [[TMP3]]) [[ATTR5]]
// CHECK-NEXT:    [[TMP7:%.*]] = extractvalue { <4 x i64>, <256 x i1> } [[TMP6]], 1
// CHECK-NEXT:    store <256 x i1> [[TMP7]], <256 x i1>* [[TMP1]], align 32
// CHECK-NEXT:    [[TMP8:%.*]] = extractvalue { <4 x i64>, <256 x i1> } [[TMP6]], 0
// CHECK-NEXT:    store <4 x i64> [[TMP8]], <4 x i64>* [[TMP4]], align 32
// CHECK-NEXT:    ret void
//
void swapvofdq(double4 *v0, __kvx_x256 *v1){
    __builtin_kvx_swapvofdq(v0, v1);
}
