// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang -target kvx-kalray-cos -march=kv3-2 %s -O3 -S -o - -emit-llvm | FileCheck %s

// Generated from base/loadc.C

typedef int __attribute__((__vector_size__(2 * sizeof(int)))) v2i32;
typedef int __attribute__((__vector_size__(4 * sizeof(int)))) v4i32;
typedef int __attribute__((__vector_size__(8 * sizeof(int)))) v8i32;

// CHECK-LABEL: @loadc256_mt(
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP0:%.*]] to <4 x i64>
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i8* [[TMP1:%.*]] to <4 x i64>*
// CHECK-NEXT:    [[TMP6:%.*]] = tail call <4 x i64> @llvm.kvx.loadc.u.v4i64(<4 x i64> [[TMP4]], <4 x i64>* [[TMP5]], i32 256, i64 [[TMP2:%.*]], i32 0, i32 -1, i32 4)
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <4 x i64> [[TMP6]] to <8 x i32>
// CHECK-NEXT:    ret <8 x i32> [[TMP7]]
//
v8i32 loadc256_mt(v8i32 a, void * ptr, unsigned long cond) {return __builtin_kvx_loadc256(a, ptr, cond, ".mt");}

// CHECK-LABEL: @loadc256_mf(
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP0:%.*]] to <4 x i64>
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i8* [[TMP1:%.*]] to <4 x i64>*
// CHECK-NEXT:    [[TMP6:%.*]] = tail call <4 x i64> @llvm.kvx.loadc.u.v4i64(<4 x i64> [[TMP4]], <4 x i64>* [[TMP5]], i32 256, i64 [[TMP2:%.*]], i32 0, i32 -1, i32 5)
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <4 x i64> [[TMP6]] to <8 x i32>
// CHECK-NEXT:    ret <8 x i32> [[TMP7]]
//
v8i32 loadc256_mf(v8i32 a, void * ptr, unsigned long cond) {return __builtin_kvx_loadc256(a, ptr, cond, ".mf");}

// CHECK-LABEL: @loadc256_mtc(
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP0:%.*]] to <4 x i64>
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i8* [[TMP1:%.*]] to <4 x i64>*
// CHECK-NEXT:    [[TMP6:%.*]] = tail call <4 x i64> @llvm.kvx.loadc.u.v4i64(<4 x i64> [[TMP4]], <4 x i64>* [[TMP5]], i32 256, i64 [[TMP2:%.*]], i32 0, i32 -1, i32 6)
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <4 x i64> [[TMP6]] to <8 x i32>
// CHECK-NEXT:    ret <8 x i32> [[TMP7]]
//
v8i32 loadc256_mtc(v8i32 a, void * ptr, unsigned long cond) {return __builtin_kvx_loadc256(a, ptr, cond, ".mtc");}

// CHECK-LABEL: @loadc256_mfc(
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP0:%.*]] to <4 x i64>
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i8* [[TMP1:%.*]] to <4 x i64>*
// CHECK-NEXT:    [[TMP6:%.*]] = tail call <4 x i64> @llvm.kvx.loadc.u.v4i64(<4 x i64> [[TMP4]], <4 x i64>* [[TMP5]], i32 256, i64 [[TMP2:%.*]], i32 0, i32 -1, i32 7)
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <4 x i64> [[TMP6]] to <8 x i32>
// CHECK-NEXT:    ret <8 x i32> [[TMP7]]
//
v8i32 loadc256_mfc(v8i32 a, void * ptr, unsigned long cond) {return __builtin_kvx_loadc256(a, ptr, cond, ".mfc");}

// CHECK-LABEL: @loadc128_mt(
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <4 x i32> [[TMP0:%.*]] to <2 x i64>
// CHECK-NEXT:    [[TMP5:%.*]] = shufflevector <2 x i64> [[TMP4]], <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast i8* [[TMP1:%.*]] to <4 x i64>*
// CHECK-NEXT:    [[TMP7:%.*]] = tail call <4 x i64> @llvm.kvx.loadc.u.v4i64(<4 x i64> [[TMP5]], <4 x i64>* [[TMP6]], i32 256, i64 [[TMP2:%.*]], i32 0, i32 -1, i32 4)
// CHECK-NEXT:    [[TMP8:%.*]] = shufflevector <4 x i64> [[TMP7]], <4 x i64> undef, <2 x i32> <i32 0, i32 1>
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast <2 x i64> [[TMP8]] to <4 x i32>
// CHECK-NEXT:    ret <4 x i32> [[TMP9]]
//
v4i32 loadc128_mt(v4i32 a, void * ptr, unsigned long cond) {return __builtin_kvx_loadc128(a, ptr, cond, ".mt");}

// CHECK-LABEL: @loadc128_mf(
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <4 x i32> [[TMP0:%.*]] to <2 x i64>
// CHECK-NEXT:    [[TMP5:%.*]] = shufflevector <2 x i64> [[TMP4]], <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast i8* [[TMP1:%.*]] to <4 x i64>*
// CHECK-NEXT:    [[TMP7:%.*]] = tail call <4 x i64> @llvm.kvx.loadc.u.v4i64(<4 x i64> [[TMP5]], <4 x i64>* [[TMP6]], i32 256, i64 [[TMP2:%.*]], i32 0, i32 -1, i32 5)
// CHECK-NEXT:    [[TMP8:%.*]] = shufflevector <4 x i64> [[TMP7]], <4 x i64> undef, <2 x i32> <i32 0, i32 1>
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast <2 x i64> [[TMP8]] to <4 x i32>
// CHECK-NEXT:    ret <4 x i32> [[TMP9]]
//
v4i32 loadc128_mf(v4i32 a, void * ptr, unsigned long cond) {return __builtin_kvx_loadc128(a, ptr, cond, ".mf");}

// CHECK-LABEL: @loadc128_mtc(
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <4 x i32> [[TMP0:%.*]] to <2 x i64>
// CHECK-NEXT:    [[TMP5:%.*]] = shufflevector <2 x i64> [[TMP4]], <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast i8* [[TMP1:%.*]] to <4 x i64>*
// CHECK-NEXT:    [[TMP7:%.*]] = tail call <4 x i64> @llvm.kvx.loadc.u.v4i64(<4 x i64> [[TMP5]], <4 x i64>* [[TMP6]], i32 256, i64 [[TMP2:%.*]], i32 0, i32 -1, i32 6)
// CHECK-NEXT:    [[TMP8:%.*]] = shufflevector <4 x i64> [[TMP7]], <4 x i64> undef, <2 x i32> <i32 0, i32 1>
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast <2 x i64> [[TMP8]] to <4 x i32>
// CHECK-NEXT:    ret <4 x i32> [[TMP9]]
//
v4i32 loadc128_mtc(v4i32 a, void * ptr, unsigned long cond) {return __builtin_kvx_loadc128(a, ptr, cond, ".mtc");}

// CHECK-LABEL: @loadc128_mfc(
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <4 x i32> [[TMP0:%.*]] to <2 x i64>
// CHECK-NEXT:    [[TMP5:%.*]] = shufflevector <2 x i64> [[TMP4]], <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast i8* [[TMP1:%.*]] to <4 x i64>*
// CHECK-NEXT:    [[TMP7:%.*]] = tail call <4 x i64> @llvm.kvx.loadc.u.v4i64(<4 x i64> [[TMP5]], <4 x i64>* [[TMP6]], i32 256, i64 [[TMP2:%.*]], i32 0, i32 -1, i32 7)
// CHECK-NEXT:    [[TMP8:%.*]] = shufflevector <4 x i64> [[TMP7]], <4 x i64> undef, <2 x i32> <i32 0, i32 1>
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast <2 x i64> [[TMP8]] to <4 x i32>
// CHECK-NEXT:    ret <4 x i32> [[TMP9]]
//
v4i32 loadc128_mfc(v4i32 a, void * ptr, unsigned long cond) {return __builtin_kvx_loadc128(a, ptr, cond, ".mfc");}

// CHECK-LABEL: @loadc64_mt(
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <2 x i32> [[TMP0:%.*]] to <1 x i64>
// CHECK-NEXT:    [[TMP5:%.*]] = shufflevector <1 x i64> [[TMP4]], <1 x i64> undef, <4 x i32> <i32 0, i32 undef, i32 undef, i32 undef>
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast i8* [[TMP1:%.*]] to <4 x i64>*
// CHECK-NEXT:    [[TMP7:%.*]] = tail call <4 x i64> @llvm.kvx.loadc.u.v4i64(<4 x i64> [[TMP5]], <4 x i64>* [[TMP6]], i32 256, i64 [[TMP2:%.*]], i32 0, i32 -1, i32 4)
// CHECK-NEXT:    [[TMP8:%.*]] = shufflevector <4 x i64> [[TMP7]], <4 x i64> undef, <1 x i32> zeroinitializer
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast <1 x i64> [[TMP8]] to <2 x i32>
// CHECK-NEXT:    ret <2 x i32> [[TMP9]]
//
v2i32 loadc64_mt(v2i32 a, void * ptr, unsigned long cond) {return __builtin_kvx_loadc64(a, ptr, cond, ".mt");}

// CHECK-LABEL: @loadc64_mf(
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <2 x i32> [[TMP0:%.*]] to <1 x i64>
// CHECK-NEXT:    [[TMP5:%.*]] = shufflevector <1 x i64> [[TMP4]], <1 x i64> undef, <4 x i32> <i32 0, i32 undef, i32 undef, i32 undef>
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast i8* [[TMP1:%.*]] to <4 x i64>*
// CHECK-NEXT:    [[TMP7:%.*]] = tail call <4 x i64> @llvm.kvx.loadc.u.v4i64(<4 x i64> [[TMP5]], <4 x i64>* [[TMP6]], i32 256, i64 [[TMP2:%.*]], i32 0, i32 -1, i32 5)
// CHECK-NEXT:    [[TMP8:%.*]] = shufflevector <4 x i64> [[TMP7]], <4 x i64> undef, <1 x i32> zeroinitializer
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast <1 x i64> [[TMP8]] to <2 x i32>
// CHECK-NEXT:    ret <2 x i32> [[TMP9]]
//
v2i32 loadc64_mf(v2i32 a, void * ptr, unsigned long cond) {return __builtin_kvx_loadc64(a, ptr, cond, ".mf");}

// CHECK-LABEL: @loadc64_mtc(
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <2 x i32> [[TMP0:%.*]] to <1 x i64>
// CHECK-NEXT:    [[TMP5:%.*]] = shufflevector <1 x i64> [[TMP4]], <1 x i64> undef, <4 x i32> <i32 0, i32 undef, i32 undef, i32 undef>
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast i8* [[TMP1:%.*]] to <4 x i64>*
// CHECK-NEXT:    [[TMP7:%.*]] = tail call <4 x i64> @llvm.kvx.loadc.u.v4i64(<4 x i64> [[TMP5]], <4 x i64>* [[TMP6]], i32 256, i64 [[TMP2:%.*]], i32 0, i32 -1, i32 6)
// CHECK-NEXT:    [[TMP8:%.*]] = shufflevector <4 x i64> [[TMP7]], <4 x i64> undef, <1 x i32> zeroinitializer
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast <1 x i64> [[TMP8]] to <2 x i32>
// CHECK-NEXT:    ret <2 x i32> [[TMP9]]
//
v2i32 loadc64_mtc(v2i32 a, void * ptr, unsigned long cond) {return __builtin_kvx_loadc64(a, ptr, cond, ".mtc");}

// CHECK-LABEL: @loadc64_mfc(
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <2 x i32> [[TMP0:%.*]] to <1 x i64>
// CHECK-NEXT:    [[TMP5:%.*]] = shufflevector <1 x i64> [[TMP4]], <1 x i64> undef, <4 x i32> <i32 0, i32 undef, i32 undef, i32 undef>
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast i8* [[TMP1:%.*]] to <4 x i64>*
// CHECK-NEXT:    [[TMP7:%.*]] = tail call <4 x i64> @llvm.kvx.loadc.u.v4i64(<4 x i64> [[TMP5]], <4 x i64>* [[TMP6]], i32 256, i64 [[TMP2:%.*]], i32 0, i32 -1, i32 7)
// CHECK-NEXT:    [[TMP8:%.*]] = shufflevector <4 x i64> [[TMP7]], <4 x i64> undef, <1 x i32> zeroinitializer
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast <1 x i64> [[TMP8]] to <2 x i32>
// CHECK-NEXT:    ret <2 x i32> [[TMP9]]
//
v2i32 loadc64_mfc(v2i32 a, void * ptr, unsigned long cond) {return __builtin_kvx_loadc64(a, ptr, cond, ".mfc");}

