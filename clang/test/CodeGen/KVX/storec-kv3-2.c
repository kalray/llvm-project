// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang -target kvx-kalray-cos -march=kv3-2 %s -O3 -S -o - -emit-llvm | FileCheck %s

// Generated from base/storec.C

typedef int __attribute__((__vector_size__(2 * sizeof(int)))) v2i32;
typedef int __attribute__((__vector_size__(4 * sizeof(int)))) v4i32;
typedef int __attribute__((__vector_size__(8 * sizeof(int)))) v8i32;
typedef short __attribute__((__vector_size__(4 * sizeof(short)))) v4i16;
typedef short __attribute__((__vector_size__(8 * sizeof(short)))) v8i16;
typedef short __attribute__((__vector_size__(16 * sizeof(short)))) v16i16;

// CHECK-LABEL: @storec256_mt(
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP0:%.*]] to <4 x i64>
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i8* [[TMP1:%.*]] to <4 x i64>*
// CHECK-NEXT:    tail call void (<4 x i64>, <4 x i64>*, i32, i64, i32, i32, ...) @llvm.kvx.storec.v4i64(<4 x i64> [[TMP4]], <4 x i64>* [[TMP5]], i32 256, i64 [[TMP2:%.*]], i32 -1, i32 4)
// CHECK-NEXT:    ret void
//
void storec256_mt(v8i32 a, void *ptr, unsigned long cond) { __builtin_kvx_storec256(a, ptr, cond, ".mt"); }

// CHECK-LABEL: @storec256_mf(
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP0:%.*]] to <4 x i64>
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i8* [[TMP1:%.*]] to <4 x i64>*
// CHECK-NEXT:    tail call void (<4 x i64>, <4 x i64>*, i32, i64, i32, i32, ...) @llvm.kvx.storec.v4i64(<4 x i64> [[TMP4]], <4 x i64>* [[TMP5]], i32 256, i64 [[TMP2:%.*]], i32 -1, i32 5)
// CHECK-NEXT:    ret void
//
void storec256_mf(v8i32 a, void *ptr, unsigned long cond) { __builtin_kvx_storec256(a, ptr, cond, ".mf"); }

// CHECK-LABEL: @storec256_mtc(
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP0:%.*]] to <4 x i64>
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i8* [[TMP1:%.*]] to <4 x i64>*
// CHECK-NEXT:    tail call void (<4 x i64>, <4 x i64>*, i32, i64, i32, i32, ...) @llvm.kvx.storec.v4i64(<4 x i64> [[TMP4]], <4 x i64>* [[TMP5]], i32 256, i64 [[TMP2:%.*]], i32 -1, i32 6)
// CHECK-NEXT:    ret void
//
void storec256_mtc(v8i32 a, void *ptr, unsigned long cond) { __builtin_kvx_storec256(a, ptr, cond, ".mtc"); }

// CHECK-LABEL: @storec256_mfc(
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP0:%.*]] to <4 x i64>
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i8* [[TMP1:%.*]] to <4 x i64>*
// CHECK-NEXT:    tail call void (<4 x i64>, <4 x i64>*, i32, i64, i32, i32, ...) @llvm.kvx.storec.v4i64(<4 x i64> [[TMP4]], <4 x i64>* [[TMP5]], i32 256, i64 [[TMP2:%.*]], i32 -1, i32 7)
// CHECK-NEXT:    ret void
//
void storec256_mfc(v8i32 a, void *ptr, unsigned long cond) { __builtin_kvx_storec256(a, ptr, cond, ".mfc"); }

// CHECK-LABEL: @storec128_mt(
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <4 x i32> [[TMP0:%.*]] to <2 x i64>
// CHECK-NEXT:    [[TMP5:%.*]] = and i64 [[TMP2:%.*]], 65535
// CHECK-NEXT:    [[TMP6:%.*]] = shufflevector <2 x i64> [[TMP4]], <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast i8* [[TMP1:%.*]] to <4 x i64>*
// CHECK-NEXT:    tail call void (<4 x i64>, <4 x i64>*, i32, i64, i32, i32, ...) @llvm.kvx.storec.v4i64(<4 x i64> [[TMP6]], <4 x i64>* [[TMP7]], i32 256, i64 [[TMP5]], i32 -1, i32 4)
// CHECK-NEXT:    ret void
//
void storec128_mt(v4i32 a, void *ptr, unsigned long cond) { __builtin_kvx_storec128(a, ptr, cond, ".mt"); }

// CHECK-LABEL: @storec128_mf(
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <4 x i32> [[TMP0:%.*]] to <2 x i64>
// CHECK-NEXT:    [[TMP5:%.*]] = or i64 [[TMP2:%.*]], -65536
// CHECK-NEXT:    [[TMP6:%.*]] = shufflevector <2 x i64> [[TMP4]], <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast i8* [[TMP1:%.*]] to <4 x i64>*
// CHECK-NEXT:    tail call void (<4 x i64>, <4 x i64>*, i32, i64, i32, i32, ...) @llvm.kvx.storec.v4i64(<4 x i64> [[TMP6]], <4 x i64>* [[TMP7]], i32 256, i64 [[TMP5]], i32 -1, i32 5)
// CHECK-NEXT:    ret void
//
void storec128_mf(v4i32 a, void *ptr, unsigned long cond) { __builtin_kvx_storec128(a, ptr, cond, ".mf"); }

// CHECK-LABEL: @storec64_mt(
// CHECK-NEXT:    [[TMP4:%.*]] = and i64 [[TMP2:%.*]], 255
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <2 x i32> [[TMP0:%.*]] to <1 x i64>
// CHECK-NEXT:    [[TMP6:%.*]] = shufflevector <1 x i64> [[TMP5]], <1 x i64> undef, <4 x i32> <i32 0, i32 undef, i32 undef, i32 undef>
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast i8* [[TMP1:%.*]] to <4 x i64>*
// CHECK-NEXT:    tail call void (<4 x i64>, <4 x i64>*, i32, i64, i32, i32, ...) @llvm.kvx.storec.v4i64(<4 x i64> [[TMP6]], <4 x i64>* [[TMP7]], i32 256, i64 [[TMP4]], i32 -1, i32 4)
// CHECK-NEXT:    ret void
//
void storec64_mt(v2i32 a, void *ptr, unsigned long cond) { __builtin_kvx_storec64(a, ptr, cond, ".mt"); }

// CHECK-LABEL: @storec64_mf(
// CHECK-NEXT:    [[TMP4:%.*]] = or i64 [[TMP2:%.*]], -256
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <2 x i32> [[TMP0:%.*]] to <1 x i64>
// CHECK-NEXT:    [[TMP6:%.*]] = shufflevector <1 x i64> [[TMP5]], <1 x i64> undef, <4 x i32> <i32 0, i32 undef, i32 undef, i32 undef>
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast i8* [[TMP1:%.*]] to <4 x i64>*
// CHECK-NEXT:    tail call void (<4 x i64>, <4 x i64>*, i32, i64, i32, i32, ...) @llvm.kvx.storec.v4i64(<4 x i64> [[TMP6]], <4 x i64>* [[TMP7]], i32 256, i64 [[TMP4]], i32 -1, i32 5)
// CHECK-NEXT:    ret void
//
void storec64_mf(v2i32 a, void *ptr, unsigned long cond) { __builtin_kvx_storec64(a, ptr, cond, ".mf"); }

