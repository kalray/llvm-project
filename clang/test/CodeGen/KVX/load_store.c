// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -triple kvx-kalray-cos -O1 -S -emit-llvm -o - %s | FileCheck %s
typedef double __attribute__((__vector_size__(32))) v4f64_t;
typedef long int __attribute__((__vector_size__(32))) v4i64_t;

// 4 x double vector store
// CHECK-LABEL: @so_4xdouble_ri(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <4 x double>, <4 x double>* [[F:%.*]], i64 1
// CHECK-NEXT:    store volatile <4 x double> [[V:%.*]], <4 x double>* [[ARRAYIDX]], align 32, [[TBAA2:!tbaa !.*]]
// CHECK-NEXT:    ret void
//
void so_4xdouble_ri(v4f64_t v, volatile v4f64_t *f) {
  f[1] = v;
}

// CHECK-LABEL: @so_4xdouble_rr(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[IDXPROM:%.*]] = sext i32 [[C:%.*]] to i64
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <4 x double>, <4 x double>* [[F:%.*]], i64 [[IDXPROM]]
// CHECK-NEXT:    store volatile <4 x double> [[V:%.*]], <4 x double>* [[ARRAYIDX]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void so_4xdouble_rr(v4f64_t v, volatile v4f64_t *f, int c) {
  f[c] = v;
}

// 4 x double vector load
// CHECK-LABEL: @lo_4xdouble_ri(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <4 x double>, <4 x double>* [[F:%.*]], i64 -1
// CHECK-NEXT:    [[TMP0:%.*]] = load volatile <4 x double>, <4 x double>* [[ARRAYIDX]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void lo_4xdouble_ri(volatile v4f64_t *f) {
  v4f64_t l = f[-1];
}

// CHECK-LABEL: @lo_4xdouble_rr(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[IDXPROM:%.*]] = sext i32 [[C:%.*]] to i64
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <4 x i64>, <4 x i64>* [[F:%.*]], i64 [[IDXPROM]]
// CHECK-NEXT:    [[TMP0:%.*]] = load volatile <4 x i64>, <4 x i64>* [[ARRAYIDX]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void lo_4xdouble_rr(volatile v4i64_t *f, int c) {
  v4i64_t l = f[c];
}

// 4 x double vector load - preload
// CHECK-LABEL: @lo_4xdouble_ri_p(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <4 x double>, <4 x double> addrspace(257)* [[F:%.*]], i64 -1
// CHECK-NEXT:    [[TMP0:%.*]] = load volatile <4 x double>, <4 x double> addrspace(257)* [[ARRAYIDX]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void lo_4xdouble_ri_p(__preload volatile v4f64_t *f) {
  v4f64_t l = f[-1];
}

// CHECK-LABEL: @lo_4xdouble_rr_p(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[IDXPROM:%.*]] = sext i32 [[C:%.*]] to i64
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <4 x double>, <4 x double> addrspace(257)* [[F:%.*]], i64 [[IDXPROM]]
// CHECK-NEXT:    [[TMP0:%.*]] = load volatile <4 x double>, <4 x double> addrspace(257)* [[ARRAYIDX]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void lo_4xdouble_rr_p(__preload volatile v4f64_t *f, int c) {
  v4f64_t l = f[c];
}

// 4 x double vector load - bypass
// CHECK-LABEL: @lo_4xdouble_ri_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <4 x double>, <4 x double> addrspace(256)* [[F:%.*]], i64 -1
// CHECK-NEXT:    [[TMP0:%.*]] = load volatile <4 x double>, <4 x double> addrspace(256)* [[ARRAYIDX]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void lo_4xdouble_ri_b(__bypass volatile v4f64_t *f) {
  v4f64_t l = f[-1];
}

// CHECK-LABEL: @lo_4xdouble_rr_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[IDXPROM:%.*]] = sext i32 [[C:%.*]] to i64
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <4 x double>, <4 x double> addrspace(256)* [[F:%.*]], i64 [[IDXPROM]]
// CHECK-NEXT:    [[TMP0:%.*]] = load volatile <4 x double>, <4 x double> addrspace(256)* [[ARRAYIDX]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void lo_4xdouble_rr_b(__bypass volatile v4f64_t *f, int c) {
  v4f64_t l = f[c];
}

// 4 x double vector load - speculative
// CHECK-LABEL: @lo_4xdouble_ri_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <4 x double>, <4 x double> addrspace(258)* [[F:%.*]], i64 -1
// CHECK-NEXT:    [[TMP0:%.*]] = load volatile <4 x double>, <4 x double> addrspace(258)* [[ARRAYIDX]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void lo_4xdouble_ri_s(__speculative volatile v4f64_t *f) {
  v4f64_t l = f[-1];
}

// CHECK-LABEL: @lo_4xdouble_rr_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[IDXPROM:%.*]] = sext i32 [[C:%.*]] to i64
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <4 x double>, <4 x double> addrspace(258)* [[F:%.*]], i64 [[IDXPROM]]
// CHECK-NEXT:    [[TMP0:%.*]] = load volatile <4 x double>, <4 x double> addrspace(258)* [[ARRAYIDX]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void lo_4xdouble_rr_s(__speculative volatile v4f64_t *f, int c) {
  v4f64_t l = f[c];
}

// 4 x double vector store
// CHECK-LABEL: @so_4xi64_ri(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <4 x i64>, <4 x i64>* [[F:%.*]], i64 1
// CHECK-NEXT:    store volatile <4 x i64> [[V:%.*]], <4 x i64>* [[ARRAYIDX]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void so_4xi64_ri(v4i64_t v, volatile v4i64_t *f) {
  f[1] = v;
}

// CHECK-LABEL: @so_4xi64_rr(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[IDXPROM:%.*]] = sext i32 [[C:%.*]] to i64
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <4 x i64>, <4 x i64>* [[F:%.*]], i64 [[IDXPROM]]
// CHECK-NEXT:    store volatile <4 x i64> [[V:%.*]], <4 x i64>* [[ARRAYIDX]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void so_4xi64_rr(v4i64_t v, volatile v4i64_t *f, int c) {
  f[c] = v;
}

// 4 x i64 vector load
// CHECK-LABEL: @lo_4xi64_ri(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <4 x i64>, <4 x i64>* [[F:%.*]], i64 -1
// CHECK-NEXT:    [[TMP0:%.*]] = load volatile <4 x i64>, <4 x i64>* [[ARRAYIDX]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void lo_4xi64_ri(volatile v4i64_t *f) {
  v4i64_t l = f[-1];
}

// CHECK-LABEL: @lo_4xi64_rr(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[IDXPROM:%.*]] = sext i32 [[C:%.*]] to i64
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <4 x i64>, <4 x i64>* [[F:%.*]], i64 [[IDXPROM]]
// CHECK-NEXT:    [[TMP0:%.*]] = load volatile <4 x i64>, <4 x i64>* [[ARRAYIDX]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void lo_4xi64_rr(volatile v4i64_t *f, int c) {
  v4i64_t l = f[c];
}

// 4 x i64 vector load - preload
// CHECK-LABEL: @lo_4xi64_ri_p(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <4 x i64>, <4 x i64> addrspace(257)* [[F:%.*]], i64 -1
// CHECK-NEXT:    [[TMP0:%.*]] = load volatile <4 x i64>, <4 x i64> addrspace(257)* [[ARRAYIDX]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void lo_4xi64_ri_p(__preload volatile v4i64_t *f) {
  v4i64_t l = f[-1];
}

// CHECK-LABEL: @lo_4xi64_rr_p(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[IDXPROM:%.*]] = sext i32 [[C:%.*]] to i64
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <4 x i64>, <4 x i64> addrspace(257)* [[F:%.*]], i64 [[IDXPROM]]
// CHECK-NEXT:    [[TMP0:%.*]] = load volatile <4 x i64>, <4 x i64> addrspace(257)* [[ARRAYIDX]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void lo_4xi64_rr_p(__preload volatile v4i64_t *f, int c) {
  v4i64_t l = f[c];
}

// 4 x i64 vector load - bypass
// CHECK-LABEL: @lo_4xi64_ri_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <4 x i64>, <4 x i64> addrspace(256)* [[F:%.*]], i64 -1
// CHECK-NEXT:    [[TMP0:%.*]] = load volatile <4 x i64>, <4 x i64> addrspace(256)* [[ARRAYIDX]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void lo_4xi64_ri_b(__bypass volatile v4i64_t *f) {
  v4i64_t l = f[-1];
}

// CHECK-LABEL: @lo_4xi64_rr_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[IDXPROM:%.*]] = sext i32 [[C:%.*]] to i64
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <4 x i64>, <4 x i64> addrspace(256)* [[F:%.*]], i64 [[IDXPROM]]
// CHECK-NEXT:    [[TMP0:%.*]] = load volatile <4 x i64>, <4 x i64> addrspace(256)* [[ARRAYIDX]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void lo_4xi64_rr_b(__bypass volatile v4i64_t *f, int c) {
  v4i64_t l = f[c];
}

// 4 x i64 vector load - speculative
// CHECK-LABEL: @lo_4xi64_ri_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <4 x i64>, <4 x i64> addrspace(258)* [[F:%.*]], i64 -1
// CHECK-NEXT:    [[TMP0:%.*]] = load volatile <4 x i64>, <4 x i64> addrspace(258)* [[ARRAYIDX]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void lo_4xi64_ri_s(__speculative volatile v4i64_t *f) {
  v4i64_t l = f[-1];
}

// CHECK-LABEL: @lo_4xi64_rr_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[IDXPROM:%.*]] = sext i32 [[C:%.*]] to i64
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <4 x i64>, <4 x i64> addrspace(258)* [[F:%.*]], i64 [[IDXPROM]]
// CHECK-NEXT:    [[TMP0:%.*]] = load volatile <4 x i64>, <4 x i64> addrspace(258)* [[ARRAYIDX]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void lo_4xi64_rr_s(__speculative volatile v4i64_t *f, int c) {
  v4i64_t l = f[c];
}
