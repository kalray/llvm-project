// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang -S --target=kvx-kalray-cos -emit-llvm %s -o - -O1 | FileCheck %s
//---------------------------------------------------------------------------//
// GPR to TCA registers copy
//---------------------------------------------------------------------------//
typedef char __attribute__((__vector_size__(8))) v8i8_t;
typedef char __attribute__((__vector_size__(16))) v16i8_t;
// CHECK-LABEL: @test_movetobvhi_v8i8(
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x i8> [[TMP0:%.*]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <8 x i8> [[TMP1:%.*]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = call <2 x i64> @llvm.kvx.movetohi(i64 [[TMP3]], i64 [[TMP4]])
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <2 x i64> [[TMP5]] to <16 x i8>
// CHECK-NEXT:    ret <16 x i8> [[TMP6]]
//
v16i8_t test_movetobvhi_v8i8(v8i8_t a, v8i8_t b) {
  return __builtin_kvx_movetohi((long)a, (long)b);
}

// CHECK-LABEL: @test_movetobvlo_v8i8(
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x i8> [[TMP0:%.*]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <8 x i8> [[TMP1:%.*]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = call <2 x i64> @llvm.kvx.movetolo(i64 [[TMP3]], i64 [[TMP4]])
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <2 x i64> [[TMP5]] to <16 x i8>
// CHECK-NEXT:    ret <16 x i8> [[TMP6]]
//
v16i8_t test_movetobvlo_v8i8(v8i8_t a, v8i8_t b) {
  return __builtin_kvx_movetolo((long)a, (long)b);
}

typedef short __attribute__((__vector_size__(8))) v4i16_t;
typedef short __attribute__((__vector_size__(16))) v8i16_t;
// CHECK-LABEL: @test_movetobvhi_v4i16(
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <4 x i16> [[TMP0:%.*]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <4 x i16> [[TMP1:%.*]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = call <2 x i64> @llvm.kvx.movetohi(i64 [[TMP3]], i64 [[TMP4]])
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <2 x i64> [[TMP5]] to <8 x i16>
// CHECK-NEXT:    ret <8 x i16> [[TMP6]]
//
v8i16_t test_movetobvhi_v4i16(v4i16_t a, v4i16_t b) {
  return __builtin_kvx_movetohi((long)a, (long)b);
}

// CHECK-LABEL: @test_movetobvlo_v4i16(
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <4 x i16> [[TMP0:%.*]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <4 x i16> [[TMP1:%.*]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = call <2 x i64> @llvm.kvx.movetolo(i64 [[TMP3]], i64 [[TMP4]])
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <2 x i64> [[TMP5]] to <8 x i16>
// CHECK-NEXT:    ret <8 x i16> [[TMP6]]
//
v8i16_t test_movetobvlo_v4i16(v4i16_t a, v4i16_t b) {
  return __builtin_kvx_movetolo((long)a, (long)b);
}

typedef int __attribute__((__vector_size__(8))) v2i32_t;
typedef int __attribute__((__vector_size__(16))) v4i32_t;
// CHECK-LABEL: @test_movetobvhi_v2i32(
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <2 x i32> [[TMP0:%.*]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <2 x i32> [[TMP1:%.*]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = call <2 x i64> @llvm.kvx.movetohi(i64 [[TMP3]], i64 [[TMP4]])
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <2 x i64> [[TMP5]] to <4 x i32>
// CHECK-NEXT:    ret <4 x i32> [[TMP6]]
//
v4i32_t test_movetobvhi_v2i32(v2i32_t a, v2i32_t b) {
  return __builtin_kvx_movetohi((long)a, (long)b);
}

// CHECK-LABEL: @test_movetobvlo_v2i32(
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <2 x i32> [[TMP0:%.*]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <2 x i32> [[TMP1:%.*]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = call <2 x i64> @llvm.kvx.movetolo(i64 [[TMP3]], i64 [[TMP4]])
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <2 x i64> [[TMP5]] to <4 x i32>
// CHECK-NEXT:    ret <4 x i32> [[TMP6]]
//
v4i32_t test_movetobvlo_v2i32(v2i32_t a, v2i32_t b) {
  return __builtin_kvx_movetolo((long)a, (long)b);
}

typedef long __attribute__((__vector_size__(16))) v2i64_t;
// CHECK-LABEL: @test_movetobvhi_i64(
// CHECK-NEXT:    [[TMP3:%.*]] = call <2 x i64> @llvm.kvx.movetohi(i64 [[TMP0:%.*]], i64 [[TMP1:%.*]])
// CHECK-NEXT:    ret <2 x i64> [[TMP3]]
//
v2i64_t test_movetobvhi_i64(long a, long b) {
  return __builtin_kvx_movetohi(a, b);
}

// CHECK-LABEL: @test_movetobvlo_i64(
// CHECK-NEXT:    [[TMP3:%.*]] = call <2 x i64> @llvm.kvx.movetolo(i64 [[TMP0:%.*]], i64 [[TMP1:%.*]])
// CHECK-NEXT:    ret <2 x i64> [[TMP3]]
//
v2i64_t test_movetobvlo_i64(long a, long b) {
  return __builtin_kvx_movetolo(a, b);
}

typedef float __attribute__((__vector_size__(8))) v2f32_t;
typedef float __attribute__((__vector_size__(16))) v4f32_t;
// CHECK-LABEL: @test_movetobvhi_v2f32(
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <2 x float> [[TMP0:%.*]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <2 x float> [[TMP1:%.*]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = call <2 x i64> @llvm.kvx.movetohi(i64 [[TMP3]], i64 [[TMP4]])
// CHECK-NEXT:    ret <2 x i64> [[TMP5]]
//
v2i64_t test_movetobvhi_v2f32(v2f32_t a, v2f32_t b) {
  return __builtin_kvx_movetohi((long)a, (long)b);
}

// CHECK-LABEL: @test_movetobvlo_v2f32(
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <2 x float> [[TMP0:%.*]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <2 x float> [[TMP1:%.*]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = call <2 x i64> @llvm.kvx.movetolo(i64 [[TMP3]], i64 [[TMP4]])
// CHECK-NEXT:    ret <2 x i64> [[TMP5]]
//
v2i64_t test_movetobvlo_v2f32(v2f32_t a, v2f32_t b) {
  return __builtin_kvx_movetolo((long)a, (long)b);
}

typedef double __attribute__((__vector_size__(16))) v2f64_t;
typedef union {
  long i;
  double f;
} double_t;

// CHECK-LABEL: @test_movetobvhi_f64(
// CHECK-NEXT:    [[TMP3:%.*]] = call <2 x i64> @llvm.kvx.movetohi(i64 [[TMP0:%.*]], i64 [[TMP1:%.*]])
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <2 x double>
// CHECK-NEXT:    ret <2 x double> [[TMP4]]
//
v2f64_t test_movetobvhi_f64(double_t a, double_t b) {
  return __builtin_kvx_movetohi(a.i, b.i);
}

// CHECK-LABEL: @test_movetobvlo_f64(
// CHECK-NEXT:    [[TMP3:%.*]] = call <2 x i64> @llvm.kvx.movetolo(i64 [[TMP0:%.*]], i64 [[TMP1:%.*]])
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <2 x double>
// CHECK-NEXT:    ret <2 x double> [[TMP4]]
//
v2f64_t test_movetobvlo_f64(double_t a, double_t b) {
  return __builtin_kvx_movetolo(a.i, b.i);
}
