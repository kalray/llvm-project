// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -triple kvx-kalray-cos -emit-llvm %s -O3 -o - | FileCheck %s

// Generated from base/store.C

typedef int __attribute__((__vector_size__(2 * sizeof(int)))) v2i32;
typedef int __attribute__((__vector_size__(4 * sizeof(int)))) v4i32;
typedef int __attribute__((__vector_size__(8 * sizeof(int)))) v8i32;
typedef short __attribute__((__vector_size__(4 * sizeof(short)))) v4i16;
typedef short __attribute__((__vector_size__(8 * sizeof(short)))) v8i16;
typedef short __attribute__((__vector_size__(16 * sizeof(short)))) v16i16;

// CHECK-LABEL: @storebc(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    store i8 [[A:%.*]], ptr [[PTR:%.*]], align 1
// CHECK-NEXT:    ret void
//
void storebc(unsigned char a, void *ptr) { __builtin_kvx_storeb(a, ptr); }

// CHECK-LABEL: @storebc_r(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[LOAD:%.*]], align 4, !tbaa [[TBAA2:![0-9]+]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[TMP0]])
// CHECK-NEXT:    [[CONV:%.*]] = trunc i64 [[TMP1]] to i32
// CHECK-NEXT:    [[CONV1:%.*]] = zext i8 [[A:%.*]] to i64
// CHECK-NEXT:    tail call void @llvm.kvx.store.i64.i32(i64 [[CONV1]], ptr [[PTR:%.*]], i32 8, i32 [[CONV]])
// CHECK-NEXT:    ret i32 [[CONV]]
//
int storebc_r(unsigned char a, void *ptr, int *load) {
  int ready = __builtin_kvx_ready(*load);
  __builtin_kvx_storeb(a, ptr, ready);
  return ready;
}

// CHECK-LABEL: @storebl(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = trunc i64 [[A:%.*]] to i8
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[PTR:%.*]], align 1
// CHECK-NEXT:    ret void
//
void storebl(unsigned long a, void *ptr) { __builtin_kvx_storeb(a, ptr); }

// CHECK-LABEL: @storebl_r(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[LOAD:%.*]], align 4, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[TMP0]])
// CHECK-NEXT:    [[CONV:%.*]] = trunc i64 [[TMP1]] to i32
// CHECK-NEXT:    tail call void @llvm.kvx.store.i64.i32(i64 [[A:%.*]], ptr [[PTR:%.*]], i32 8, i32 [[CONV]])
// CHECK-NEXT:    ret i32 [[CONV]]
//
int storebl_r(unsigned long a, void *ptr, int *load) {
  int ready = __builtin_kvx_ready(*load);
  __builtin_kvx_storeb(a, ptr, ready);
  return ready;
}

// CHECK-LABEL: @storehs(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    store i16 [[A:%.*]], ptr [[PTR:%.*]], align 1
// CHECK-NEXT:    ret void
//
void storehs(unsigned short a, void *ptr) { __builtin_kvx_storeh(a, ptr); }

// CHECK-LABEL: @storehs_r(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[LOAD:%.*]], align 4, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[TMP0]])
// CHECK-NEXT:    [[CONV:%.*]] = trunc i64 [[TMP1]] to i32
// CHECK-NEXT:    [[CONV1:%.*]] = zext i16 [[A:%.*]] to i64
// CHECK-NEXT:    tail call void @llvm.kvx.store.i64.i32(i64 [[CONV1]], ptr [[PTR:%.*]], i32 16, i32 [[CONV]])
// CHECK-NEXT:    ret i32 [[CONV]]
//
int storehs_r(unsigned short a, void *ptr, int *load) {
  int ready = __builtin_kvx_ready(*load);
  __builtin_kvx_storeh(a, ptr, ready);
  return ready;
}

// CHECK-LABEL: @storehl(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = trunc i64 [[A:%.*]] to i16
// CHECK-NEXT:    store i16 [[TMP0]], ptr [[PTR:%.*]], align 1
// CHECK-NEXT:    ret void
//
void storehl(unsigned long a, void *ptr) { __builtin_kvx_storeh(a, ptr); }

// CHECK-LABEL: @storehl_r(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[LOAD:%.*]], align 4, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[TMP0]])
// CHECK-NEXT:    [[CONV:%.*]] = trunc i64 [[TMP1]] to i32
// CHECK-NEXT:    tail call void @llvm.kvx.store.i64.i32(i64 [[A:%.*]], ptr [[PTR:%.*]], i32 16, i32 [[CONV]])
// CHECK-NEXT:    ret i32 [[CONV]]
//
int storehl_r(unsigned long a, void *ptr, int *load) {
  int ready = __builtin_kvx_ready(*load);
  __builtin_kvx_storeh(a, ptr, ready);
  return ready;
}

// CHECK-LABEL: @storewi(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[PTR:%.*]], align 1
// CHECK-NEXT:    ret void
//
void storewi(unsigned int a, void *ptr) { __builtin_kvx_storew(a, ptr); }

// CHECK-LABEL: @storewi_r(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[LOAD:%.*]], align 4, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[TMP0]])
// CHECK-NEXT:    [[CONV:%.*]] = trunc i64 [[TMP1]] to i32
// CHECK-NEXT:    [[CONV1:%.*]] = zext i32 [[A:%.*]] to i64
// CHECK-NEXT:    tail call void @llvm.kvx.store.i64.i32(i64 [[CONV1]], ptr [[PTR:%.*]], i32 32, i32 [[CONV]])
// CHECK-NEXT:    ret i32 [[CONV]]
//
int storewi_r(unsigned int a, void *ptr, int *load) {
  int ready = __builtin_kvx_ready(*load);
  __builtin_kvx_storew(a, ptr, ready);
  return ready;
}

// CHECK-LABEL: @storewl(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = trunc i64 [[A:%.*]] to i32
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[PTR:%.*]], align 1
// CHECK-NEXT:    ret void
//
void storewl(unsigned long a, void *ptr) { __builtin_kvx_storew(a, ptr); }

// CHECK-LABEL: @storewl_r(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[LOAD:%.*]], align 4, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[TMP0]])
// CHECK-NEXT:    [[CONV:%.*]] = trunc i64 [[TMP1]] to i32
// CHECK-NEXT:    tail call void @llvm.kvx.store.i64.i32(i64 [[A:%.*]], ptr [[PTR:%.*]], i32 32, i32 [[CONV]])
// CHECK-NEXT:    ret i32 [[CONV]]
//
int storewl_r(unsigned long a, void *ptr, int *load) {
  int ready = __builtin_kvx_ready(*load);
  __builtin_kvx_storew(a, ptr, ready);
  return ready;
}

// CHECK-LABEL: @storedl(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    store i64 [[A:%.*]], ptr [[PTR:%.*]], align 1
// CHECK-NEXT:    ret void
//
void storedl(unsigned long a, void *ptr) { __builtin_kvx_stored(a, ptr); }

// CHECK-LABEL: @storedl_r(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[LOAD:%.*]], align 4, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[TMP0]])
// CHECK-NEXT:    [[CONV:%.*]] = trunc i64 [[TMP1]] to i32
// CHECK-NEXT:    tail call void @llvm.kvx.store.i64.i32(i64 [[A:%.*]], ptr [[PTR:%.*]], i32 64, i32 [[CONV]])
// CHECK-NEXT:    ret i32 [[CONV]]
//
int storedl_r(unsigned long a, void *ptr, int *load) {
  int ready = __builtin_kvx_ready(*load);
  __builtin_kvx_stored(a, ptr, ready);
  return ready;
}

// CHECK-LABEL: @storeq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    store i128 [[A:%.*]], ptr [[PTR:%.*]], align 1
// CHECK-NEXT:    ret void
//
void storeq(unsigned __int128 a, void *ptr) { __builtin_kvx_storeq(a, ptr); }

// CHECK-LABEL: @storeq_r(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[LOAD:%.*]], align 4, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[TMP0]])
// CHECK-NEXT:    [[CONV:%.*]] = trunc i64 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast i128 [[A:%.*]] to <2 x i64>
// CHECK-NEXT:    tail call void @llvm.kvx.store.v2i64.i32(<2 x i64> [[TMP2]], ptr [[PTR:%.*]], i32 128, i32 [[CONV]])
// CHECK-NEXT:    ret i32 [[CONV]]
//
int storeq_r(unsigned __int128 a, void *ptr, int *load) {
  int ready = __builtin_kvx_ready(*load);
  __builtin_kvx_storeq(a, ptr, ready);
  return ready;
}

// CHECK-LABEL: @storehf(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    store half [[A:%.*]], ptr [[PTR:%.*]], align 1
// CHECK-NEXT:    ret void
//
void storehf(_Float16 a, void *ptr) { __builtin_kvx_storehf(a, ptr); }

// CHECK-LABEL: @storehf_r(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[LOAD:%.*]], align 4, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[TMP0]])
// CHECK-NEXT:    [[CONV:%.*]] = trunc i64 [[TMP1]] to i32
// CHECK-NEXT:    tail call void @llvm.kvx.store.f16.i32(half [[A:%.*]], ptr [[PTR:%.*]], i32 16, i32 [[CONV]])
// CHECK-NEXT:    ret i32 [[CONV]]
//
int storehf_r(_Float16 a, void *ptr, int *load) {
  int ready = __builtin_kvx_ready(*load);
  __builtin_kvx_storehf(a, ptr, ready);
  return ready;
}

// CHECK-LABEL: @storewf(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    store float [[A:%.*]], ptr [[PTR:%.*]], align 1
// CHECK-NEXT:    ret void
//
void storewf(float a, void *ptr) { __builtin_kvx_storewf(a, ptr); }

// CHECK-LABEL: @storewf_r(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[LOAD:%.*]], align 4, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[TMP0]])
// CHECK-NEXT:    [[CONV:%.*]] = trunc i64 [[TMP1]] to i32
// CHECK-NEXT:    tail call void @llvm.kvx.store.f32.i32(float [[A:%.*]], ptr [[PTR:%.*]], i32 32, i32 [[CONV]])
// CHECK-NEXT:    ret i32 [[CONV]]
//
int storewf_r(float a, void *ptr, int *load) {
  int ready = __builtin_kvx_ready(*load);
  __builtin_kvx_storewf(a, ptr, ready);
  return ready;
}

// CHECK-LABEL: @storedf(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    store double [[A:%.*]], ptr [[PTR:%.*]], align 1
// CHECK-NEXT:    ret void
//
void storedf(double a, void *ptr) { __builtin_kvx_storedf(a, ptr); }

// CHECK-LABEL: @storedf_r(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[LOAD:%.*]], align 4, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[TMP0]])
// CHECK-NEXT:    [[CONV:%.*]] = trunc i64 [[TMP1]] to i32
// CHECK-NEXT:    tail call void @llvm.kvx.store.f64.i32(double [[A:%.*]], ptr [[PTR:%.*]], i32 64, i32 [[CONV]])
// CHECK-NEXT:    ret i32 [[CONV]]
//
int storedf_r(double a, void *ptr, int *load) {
  int ready = __builtin_kvx_ready(*load);
  __builtin_kvx_storedf(a, ptr, ready);
  return ready;
}

// CHECK-LABEL: @store64(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    store <2 x i32> [[A:%.*]], ptr [[PTR:%.*]], align 1
// CHECK-NEXT:    ret void
//
void store64(v2i32 a, void *ptr) { __builtin_kvx_store64(a, ptr); }

// CHECK-LABEL: @store64_r(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[LOAD:%.*]], align 4, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[TMP0]])
// CHECK-NEXT:    [[CONV:%.*]] = trunc i64 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <2 x i32> [[A:%.*]] to i64
// CHECK-NEXT:    tail call void @llvm.kvx.store.i64.i32(i64 [[TMP2]], ptr [[PTR:%.*]], i32 64, i32 [[CONV]])
// CHECK-NEXT:    ret i32 [[CONV]]
//
int store64_r(v2i32 a, void *ptr, int *load) {
  int ready = __builtin_kvx_ready(*load);
  __builtin_kvx_store64(a, ptr, ready);
  return ready;
}

// CHECK-LABEL: @store64h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    store <4 x i16> [[A:%.*]], ptr [[PTR:%.*]], align 1
// CHECK-NEXT:    ret void
//
void store64h(v4i16 a, void *ptr) { __builtin_kvx_store64(a, ptr); }

// CHECK-LABEL: @store64h_r(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[LOAD:%.*]], align 4, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[TMP0]])
// CHECK-NEXT:    [[CONV:%.*]] = trunc i64 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <4 x i16> [[A:%.*]] to i64
// CHECK-NEXT:    tail call void @llvm.kvx.store.i64.i32(i64 [[TMP2]], ptr [[PTR:%.*]], i32 64, i32 [[CONV]])
// CHECK-NEXT:    ret i32 [[CONV]]
//
int store64h_r(v4i16 a, void *ptr, int *load) {
  int ready = __builtin_kvx_ready(*load);
  __builtin_kvx_store64(a, ptr, ready);
  return ready;
}

// CHECK-LABEL: @store128(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    store <4 x i32> [[A:%.*]], ptr [[PTR:%.*]], align 1
// CHECK-NEXT:    ret void
//
void store128(v4i32 a, void *ptr) { __builtin_kvx_store128(a, ptr); }

// CHECK-LABEL: @store128_r(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[LOAD:%.*]], align 4, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[TMP0]])
// CHECK-NEXT:    [[CONV:%.*]] = trunc i64 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <4 x i32> [[A:%.*]] to <2 x i64>
// CHECK-NEXT:    tail call void @llvm.kvx.store.v2i64.i32(<2 x i64> [[TMP2]], ptr [[PTR:%.*]], i32 128, i32 [[CONV]])
// CHECK-NEXT:    ret i32 [[CONV]]
//
int store128_r(v4i32 a, void *ptr, int *load) {
  int ready = __builtin_kvx_ready(*load);
  __builtin_kvx_store128(a, ptr, ready);
  return ready;
}

// CHECK-LABEL: @store128h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    store <8 x i16> [[A:%.*]], ptr [[PTR:%.*]], align 1
// CHECK-NEXT:    ret void
//
void store128h(v8i16 a, void *ptr) { __builtin_kvx_store128(a, ptr); }

// CHECK-LABEL: @store128h_r(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[LOAD:%.*]], align 4, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[TMP0]])
// CHECK-NEXT:    [[CONV:%.*]] = trunc i64 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <8 x i16> [[A:%.*]] to <2 x i64>
// CHECK-NEXT:    tail call void @llvm.kvx.store.v2i64.i32(<2 x i64> [[TMP2]], ptr [[PTR:%.*]], i32 128, i32 [[CONV]])
// CHECK-NEXT:    ret i32 [[CONV]]
//
int store128h_r(v8i16 a, void *ptr, int *load) {
  int ready = __builtin_kvx_ready(*load);
  __builtin_kvx_store128(a, ptr, ready);
  return ready;
}

// CHECK-LABEL: @store256(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    store <8 x i32> [[A:%.*]], ptr [[PTR:%.*]], align 1
// CHECK-NEXT:    ret void
//
void store256(v8i32 a, void *ptr) { __builtin_kvx_store256(a, ptr); }

// CHECK-LABEL: @store256_r(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[LOAD:%.*]], align 4, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[TMP0]])
// CHECK-NEXT:    [[CONV:%.*]] = trunc i64 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <8 x i32> [[A:%.*]] to <4 x i64>
// CHECK-NEXT:    tail call void @llvm.kvx.store.v4i64.i32(<4 x i64> [[TMP2]], ptr [[PTR:%.*]], i32 256, i32 [[CONV]])
// CHECK-NEXT:    ret i32 [[CONV]]
//
int store256_r(v8i32 a, void *ptr, int *load) {
  int ready = __builtin_kvx_ready(*load);
  __builtin_kvx_store256(a, ptr, ready);
  return ready;
}

// CHECK-LABEL: @store256h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    store <16 x i16> [[A:%.*]], ptr [[PTR:%.*]], align 1
// CHECK-NEXT:    ret void
//
void store256h(v16i16 a, void *ptr) { __builtin_kvx_store256(a, ptr); }

// CHECK-LABEL: @store256h_r(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[LOAD:%.*]], align 4, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[TMP0]])
// CHECK-NEXT:    [[CONV:%.*]] = trunc i64 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <16 x i16> [[A:%.*]] to <4 x i64>
// CHECK-NEXT:    tail call void @llvm.kvx.store.v4i64.i32(<4 x i64> [[TMP2]], ptr [[PTR:%.*]], i32 256, i32 [[CONV]])
// CHECK-NEXT:    ret i32 [[CONV]]
//
int store256h_r(v16i16 a, void *ptr, int *load) {
  int ready = __builtin_kvx_ready(*load);
  __builtin_kvx_store256(a, ptr, ready);
  return ready;
}

// CHECK-LABEL: @store_vol(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    store volatile i32 [[A:%.*]], ptr [[PTR:%.*]], align 1
// CHECK-NEXT:    store volatile i32 [[A]], ptr [[PTR]], align 1
// CHECK-NEXT:    ret void
//
void store_vol(int a, void *ptr) {
  __builtin_kvx_storew(a, ptr, 1);
  __builtin_kvx_storew(a, ptr, 1);
}

// CHECK-LABEL: @store_novol(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[PTR:%.*]], align 1
// CHECK-NEXT:    ret void
//
void store_novol(int a, void *ptr) {
  __builtin_kvx_storew(a, ptr, 0);
  __builtin_kvx_storew(a, ptr, 0);
}

// CHECK-LABEL: @store_r_vol(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[LOAD:%.*]], align 4, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[TMP0]])
// CHECK-NEXT:    [[CONV:%.*]] = trunc i64 [[TMP1]] to i32
// CHECK-NEXT:    [[CONV1:%.*]] = sext i32 [[A:%.*]] to i64
// CHECK-NEXT:    tail call void @llvm.kvx.store.vol.i64.i32(i64 [[CONV1]], ptr [[PTR:%.*]], i32 32, i32 [[CONV]])
// CHECK-NEXT:    tail call void @llvm.kvx.store.vol.i64.i32(i64 [[CONV1]], ptr [[PTR]], i32 32, i32 [[CONV]])
// CHECK-NEXT:    ret i32 [[CONV]]
//
int store_r_vol(int a, void *ptr, int *load) {
  int ready = __builtin_kvx_ready(*load);
  __builtin_kvx_storew(a, ptr, ready, 1);
  __builtin_kvx_storew(a, ptr, ready, 1);
  return ready;
}

// CHECK-LABEL: @store_r_novol(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[LOAD:%.*]], align 4, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[TMP0]])
// CHECK-NEXT:    [[CONV:%.*]] = trunc i64 [[TMP1]] to i32
// CHECK-NEXT:    [[CONV1:%.*]] = sext i32 [[A:%.*]] to i64
// CHECK-NEXT:    tail call void @llvm.kvx.store.i64.i32(i64 [[CONV1]], ptr [[PTR:%.*]], i32 32, i32 [[CONV]])
// CHECK-NEXT:    tail call void @llvm.kvx.store.i64.i32(i64 [[CONV1]], ptr [[PTR]], i32 32, i32 [[CONV]])
// CHECK-NEXT:    ret i32 [[CONV]]
//
int store_r_novol(int a, void *ptr, int *load) {
  int ready = __builtin_kvx_ready(*load);
  __builtin_kvx_storew(a, ptr, ready, 0);
  __builtin_kvx_storew(a, ptr, ready, 0);
  return ready;
}

// CHECK-LABEL: @ready_then_store(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[ADDR0:%.*]], align 4, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[ADDR1:%.*]], align 4, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[ADDR2:%.*]], align 4, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP3:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[TMP0]], i32 [[TMP1]], i32 [[TMP2]])
// CHECK-NEXT:    [[CONV:%.*]] = trunc i64 [[TMP3]] to i32
// CHECK-NEXT:    [[CONV1:%.*]] = sext i32 [[TMP0]] to i64
// CHECK-NEXT:    tail call void @llvm.kvx.store.i64.i32(i64 [[CONV1]], ptr [[TO0:%.*]], i32 32, i32 [[CONV]])
// CHECK-NEXT:    [[CONV2:%.*]] = sext i32 [[TMP1]] to i64
// CHECK-NEXT:    tail call void @llvm.kvx.store.i64.i32(i64 [[CONV2]], ptr [[TO1:%.*]], i32 32, i32 [[CONV]])
// CHECK-NEXT:    [[CONV3:%.*]] = sext i32 [[TMP2]] to i64
// CHECK-NEXT:    tail call void @llvm.kvx.store.i64.i32(i64 [[CONV3]], ptr [[TO2:%.*]], i32 32, i32 [[CONV]])
// CHECK-NEXT:    ret void
//
void ready_then_store(int *addr0, int *addr1, int *addr2,
                      int *to0, int *to1, int *to2) {
  int a = *(addr0);
  int b = *(addr1);
  int c = *(addr2);
  int ready = __builtin_kvx_ready(a, b, c);
  __builtin_kvx_storew(a, to0, ready);
  __builtin_kvx_storew(b, to1, ready);
  __builtin_kvx_storew(c, to2, ready);
}

// CHECK-LABEL: @load_then_store(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[ADDR0:%.*]], align 4, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[ADDR1:%.*]], align 4, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[ADDR2:%.*]], align 4, !tbaa [[TBAA2]]
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[TO0:%.*]], align 4, !tbaa [[TBAA2]]
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[TO1:%.*]], align 4, !tbaa [[TBAA2]]
// CHECK-NEXT:    store i32 [[TMP2]], ptr [[TO2:%.*]], align 4, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
void load_then_store(int *addr0, int *addr1, int *addr2,
                     int *to0, int *to1, int *to2) {
  int a = *(addr0);
  int b = *(addr1);
  int c = *(addr2);
  *to0 = a;
  *to1 = b;
  *to2 = c;
}

// CHECK-LABEL: @store_imm(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV:%.*]] = sext i32 [[SV:%.*]] to i64
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i32, ptr [[ADDR:%.*]], i64 1
// CHECK-NEXT:    tail call void @llvm.kvx.store.i64.i32(i64 [[CONV]], ptr nonnull [[ARRAYIDX]], i32 32, i32 [[READY:%.*]])
// CHECK-NEXT:    ret void
//
void store_imm(int *addr, int sv, int ready) {
  __builtin_kvx_storew(sv, &addr[1], ready);
}

