// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -triple kvx-kalray-cos -S -O2 -emit-llvm -o - %s | FileCheck %s

// CHECK-LABEL: @abdw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.abdw(i32 [[V1:%.*]], i32 [[V2:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int abdw(int v1, int v2) {
  return __builtin_kvx_abdw(v1, v2);
}

// CHECK-LABEL: @abdd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.abdd(i64 [[V1:%.*]], i64 [[V2:%.*]])
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long abdd(long v1, long v2) {
  return __builtin_kvx_abdd(v1, v2);
}

// CHECK-LABEL: @addsw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.addsw(i32 [[V1:%.*]], i32 [[V2:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int addsw(int v1, int v2) {
  return __builtin_kvx_addsw(v1, v2);
}

// CHECK-LABEL: @addsd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.addsd(i64 [[V1:%.*]], i64 [[V2:%.*]])
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long addsd(long v1, long v2) {
  return __builtin_kvx_addsd(v1, v2);
}

// CHECK-LABEL: @sbfsw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.sbfsw(i32 [[V1:%.*]], i32 [[V2:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int sbfsw(int v1, int v2) {
  return __builtin_kvx_sbfsw(v1, v2);
}

// CHECK-LABEL: @sbfsd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.sbfsd(i64 [[V1:%.*]], i64 [[V2:%.*]])
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long sbfsd(long v1, long v2) {
  return __builtin_kvx_sbfsd(v1, v2);
}

// CHECK-LABEL: @adddc(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.addd(i64 [[V1:%.*]], i64 [[V2:%.*]], i32 1)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long adddc(long v1, long v2) {
  return __builtin_kvx_addd(v1, v2, ".c");
}

// CHECK-LABEL: @adddci(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.addd(i64 [[V1:%.*]], i64 [[V2:%.*]], i32 2)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long adddci(long v1, long v2) {
  return __builtin_kvx_addd(v1, v2, ".ci");
}

// CHECK-LABEL: @sbfdc(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.sbfd(i64 [[V1:%.*]], i64 [[V2:%.*]], i32 1)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long sbfdc(long v1, long v2) {
  return __builtin_kvx_sbfd(v1, v2, ".c");
}

// CHECK-LABEL: @sbfdci(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.sbfd(i64 [[V1:%.*]], i64 [[V2:%.*]], i32 2)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long sbfdci(long v1, long v2) {
  return __builtin_kvx_sbfd(v1, v2, ".ci");
}

// CHECK-LABEL: @avgw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.avgw(i32 [[V1:%.*]], i32 [[V2:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int avgw(int v1, int v2) {
  return __builtin_kvx_avgw(v1, v2);
}

// CHECK-LABEL: @avguw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.avguw(i32 [[V1:%.*]], i32 [[V2:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int avguw(unsigned int v1, unsigned int v2) {
  return __builtin_kvx_avguw(v1, v2);
}

// CHECK-LABEL: @avgrw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.avgrw(i32 [[V1:%.*]], i32 [[V2:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int avgrw(int v1, int v2) {
  return __builtin_kvx_avgrw(v1, v2);
}

// CHECK-LABEL: @avgruw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.avgruw(i32 [[V1:%.*]], i32 [[V2:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int avgruw(int v1, int v2) {
  return __builtin_kvx_avgruw(v1, v2);
}

// CHECK-LABEL: @cbsd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.cbsd(i64 [[L:%.*]])
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long cbsd(long l) {
  return __builtin_kvx_cbsd(l);
}

// CHECK-LABEL: @cbsw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.cbsw(i32 [[I:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int cbsw(int i) {
  return __builtin_kvx_cbsw(i);
}

// CHECK-LABEL: @clzd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.clzd(i64 [[L:%.*]])
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long clzd(long l) {
  return __builtin_kvx_clzd(l);
}

// CHECK-LABEL: @clzw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.clzw(i32 [[I:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int clzw(int i) {
  return __builtin_kvx_clzw(i);
}

// CHECK-LABEL: @ctzd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.ctzd(i64 [[L:%.*]])
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long ctzd(long l) {
  return __builtin_kvx_ctzd(l);
}

// CHECK-LABEL: @ctzw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.ctzw(i32 [[I:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int ctzw(int i) {
  return __builtin_kvx_ctzw(i);
}

// CHECK-LABEL: @fabsw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.fabsw(float [[V:%.*]])
// CHECK-NEXT:    ret float [[TMP0]]
//
float fabsw(float v) {
  return __builtin_kvx_fabsw(v);
}

// CHECK-LABEL: @fabsd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.fabsd(double [[V:%.*]])
// CHECK-NEXT:    ret double [[TMP0]]
//
double fabsd(double v) {
  return __builtin_kvx_fabsd(v);
}

// CHECK-LABEL: @fnegw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.fnegw(float [[V:%.*]])
// CHECK-NEXT:    ret float [[TMP0]]
//
float fnegw(float v) {
  return __builtin_kvx_fnegw(v);
}

// CHECK-LABEL: @fnegd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.fnegd(double [[V:%.*]])
// CHECK-NEXT:    ret double [[TMP0]]
//
double fnegd(double v) {
  return __builtin_kvx_fnegd(v);
}

// CHECK-LABEL: @fmaxw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.fmaxw(float [[V1:%.*]], float [[V2:%.*]])
// CHECK-NEXT:    ret float [[TMP0]]
//
float fmaxw(float v1, float v2) {
  return __builtin_kvx_fmaxw(v1, v2);
}

// CHECK-LABEL: @fmaxd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.fmaxd(double [[V1:%.*]], double [[V2:%.*]])
// CHECK-NEXT:    ret double [[TMP0]]
//
double fmaxd(double v1, double v2) {
  return __builtin_kvx_fmaxd(v1, v2);
}

// CHECK-LABEL: @fminw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.fminw(float [[V1:%.*]], float [[V2:%.*]])
// CHECK-NEXT:    ret float [[TMP0]]
//
float fminw(float v1, float v2) {
  return __builtin_kvx_fminw(v1, v2);
}

// CHECK-LABEL: @fmind(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.fmind(double [[V1:%.*]], double [[V2:%.*]])
// CHECK-NEXT:    ret double [[TMP0]]
//
double fmind(double v1, double v2) {
  return __builtin_kvx_fmind(v1, v2);
}

// CHECK-LABEL: @frecw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.frecw(float [[A:%.*]], i32 3)
// CHECK-NEXT:    ret float [[TMP0]]
//
float frecw(float a) {
  return __builtin_kvx_frecw(a, ".rz");
}

// CHECK-LABEL: @frsrw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.frsrw(float [[A:%.*]], i32 3)
// CHECK-NEXT:    ret float [[TMP0]]
//
float frsrw(float a) {
  return __builtin_kvx_frsrw(a, ".rz");
}

// CHECK-LABEL: @faddw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.faddw(float [[V1:%.*]], float [[V2:%.*]], i32 3)
// CHECK-NEXT:    ret float [[TMP0]]
//
float faddw(float v1, float v2) {
  return __builtin_kvx_faddw(v1, v2, ".rz");
}

// CHECK-LABEL: @faddd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV:%.*]] = fpext float [[V1:%.*]] to double
// CHECK-NEXT:    [[CONV1:%.*]] = fpext float [[V2:%.*]] to double
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.faddd(double [[CONV]], double [[CONV1]], i32 3)
// CHECK-NEXT:    ret double [[TMP0]]
//
double faddd(float v1, float v2) {
  return __builtin_kvx_faddd(v1, v2, ".rz");
}

// CHECK-LABEL: @fsbfw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.fsbfw(float [[V1:%.*]], float [[V2:%.*]], i32 3)
// CHECK-NEXT:    ret float [[TMP0]]
//
float fsbfw(float v1, float v2) {
  return __builtin_kvx_fsbfw(v1, v2, ".rz");
}

// CHECK-LABEL: @fsbfd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.fsbfd(double [[V1:%.*]], double [[V2:%.*]], i32 3)
// CHECK-NEXT:    ret double [[TMP0]]
//
double fsbfd(double v1, double v2) {
  return __builtin_kvx_fsbfd(v1, v2, ".rz");
}

// CHECK-LABEL: @fmulw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.fmulw(float [[V1:%.*]], float [[V2:%.*]], i32 3)
// CHECK-NEXT:    ret float [[TMP0]]
//
float fmulw(float v1, float v2) {
  return __builtin_kvx_fmulw(v1, v2, ".rz");
}

// CHECK-LABEL: @fmuld(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.fmuld(double [[V1:%.*]], double [[V2:%.*]], i32 3)
// CHECK-NEXT:    ret double [[TMP0]]
//
double fmuld(double v1, double v2) {
  return __builtin_kvx_fmuld(v1, v2, ".rz");
}

// CHECK-LABEL: @fmulwd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.fmulwd(float [[V1:%.*]], float [[V2:%.*]], i32 3)
// CHECK-NEXT:    ret double [[TMP0]]
//
double fmulwd(float v1, float v2) {
  return __builtin_kvx_fmulwd(v1, v2, ".rz");
}

// CHECK-LABEL: @ffmaw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.ffmaw(float [[A:%.*]], float [[B:%.*]], float [[C:%.*]], i32 3)
// CHECK-NEXT:    ret float [[TMP0]]
//
float ffmaw(float a, float b, float c) {
  return __builtin_kvx_ffmaw(a, b, c, ".rz");
}

// CHECK-LABEL: @ffmad(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.ffmad(double [[A:%.*]], double [[B:%.*]], double [[C:%.*]], i32 3)
// CHECK-NEXT:    ret double [[TMP0]]
//
double ffmad(double a, double b, double c) {
  return __builtin_kvx_ffmad(a, b, c, ".rz");
}

// CHECK-LABEL: @ffmawd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.ffmawd(float [[A:%.*]], float [[B:%.*]], double [[C:%.*]], i32 3)
// CHECK-NEXT:    ret double [[TMP0]]
//
double ffmawd(float a, float b, double c) {
  return __builtin_kvx_ffmawd(a, b, c, ".rz");
}

// CHECK-LABEL: @ffmsw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.ffmsw(float [[A:%.*]], float [[B:%.*]], float [[C:%.*]], i32 3)
// CHECK-NEXT:    ret float [[TMP0]]
//
float ffmsw(float a, float b, float c) {
  return __builtin_kvx_ffmsw(a, b, c, ".rz");
}

// CHECK-LABEL: @ffmsw_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.ffmsw(float [[A:%.*]], float [[B:%.*]], float [[C:%.*]], i32 7)
// CHECK-NEXT:    ret float [[TMP0]]
//
float ffmsw_(float a, float b, float c) {
  return __builtin_kvx_ffmsw(a, b, c, "");
}

// CHECK-LABEL: @ffmsd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.ffmsd(double [[A:%.*]], double [[B:%.*]], double [[C:%.*]], i32 3)
// CHECK-NEXT:    ret double [[TMP0]]
//
double ffmsd(double a, double b, double c) {
  return __builtin_kvx_ffmsd(a, b, c, ".rz");
}

// CHECK-LABEL: @ffmswd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.ffmswd(float [[A:%.*]], float [[B:%.*]], double [[C:%.*]], i32 3)
// CHECK-NEXT:    ret double [[TMP0]]
//
double ffmswd(float a, float b, double c) {
  return __builtin_kvx_ffmswd(a, b, c, ".rz");
}

// CHECK-LABEL: @floatw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.floatw(i32 [[X:%.*]], i64 3, i32 0)
// CHECK-NEXT:    ret float [[TMP0]]
//
float floatw(int x) {
  return __builtin_kvx_floatw(x, 3, ".rn");
}

// CHECK-LABEL: @floatd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.floatd(i64 [[X:%.*]], i64 3, i32 0)
// CHECK-NEXT:    ret double [[TMP0]]
//
double floatd(long x) {
  return __builtin_kvx_floatd(x, 3, ".rn");
}

// CHECK-LABEL: @floatuw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.floatuw(i32 [[X:%.*]], i64 3, i32 3)
// CHECK-NEXT:    ret float [[TMP0]]
//
float floatuw(unsigned int x) {
  return __builtin_kvx_floatuw(x, 3, ".rz");
}

// CHECK-LABEL: @floatud(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.floatud(i64 [[X:%.*]], i64 3, i32 3)
// CHECK-NEXT:    ret double [[TMP0]]
//
double floatud(unsigned long x) {
  return __builtin_kvx_floatud(x, 3, ".rz");
}

// CHECK-LABEL: @fixedw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.fixedw(float [[X:%.*]], i64 3, i32 0)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int fixedw(float x) {
  return __builtin_kvx_fixedw(x, 3, ".rn");
}

// CHECK-LABEL: @fixedd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.fixedd(double [[X:%.*]], i64 3, i32 0)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long fixedd(double x) {
  return __builtin_kvx_fixedd(x, 3, ".rn");
}

// CHECK-LABEL: @fixeduw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.fixeduw(float [[X:%.*]], i64 3, i32 3)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int fixeduw(float x) {
  return __builtin_kvx_fixeduw(x, 3, ".rz");
}

// CHECK-LABEL: @fixedud(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.fixedud(double [[X:%.*]], i64 3, i32 3)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
unsigned long fixedud(double x) {
  return __builtin_kvx_fixedud(x, 3, ".rz");
}

// CHECK-LABEL: @fcdivw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.fcdivw(float [[V1:%.*]], float [[V2:%.*]])
// CHECK-NEXT:    ret float [[TMP0]]
//
float fcdivw(float v1, float v2) {
  return __builtin_kvx_fcdivw(v1, v2);
}

// CHECK-LABEL: @fcdivd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.fcdivd(double [[V1:%.*]], double [[V2:%.*]])
// CHECK-NEXT:    ret double [[TMP0]]
//
double fcdivd(double v1, double v2) {
  return __builtin_kvx_fcdivd(v1, v2);
}

// CHECK-LABEL: @fsdivw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.fsdivw(float [[V1:%.*]], float [[V2:%.*]])
// CHECK-NEXT:    ret float [[TMP0]]
//
float fsdivw(float v1, float v2) {
  return __builtin_kvx_fsdivw(v1, v2);
}

// CHECK-LABEL: @fsdivd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.fsdivd(double [[V1:%.*]], double [[V2:%.*]])
// CHECK-NEXT:    ret double [[TMP0]]
//
double fsdivd(double v1, double v2) {
  return __builtin_kvx_fsdivd(v1, v2);
}

// CHECK-LABEL: @fsrecw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.fsrecw(float [[V:%.*]])
// CHECK-NEXT:    ret float [[TMP0]]
//
float fsrecw(float v) {
  return __builtin_kvx_fsrecw(v);
}

// CHECK-LABEL: @fsrecd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.fsrecd(double [[V:%.*]])
// CHECK-NEXT:    ret double [[TMP0]]
//
double fsrecd(double v) {
  return __builtin_kvx_fsrecd(v);
}

// CHECK-LABEL: @sbmm8(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.sbmm8(i64 [[A:%.*]], i64 [[B:%.*]])
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long sbmm8(long a, long b) {
  return __builtin_kvx_sbmm8(a, b);
}

// CHECK-LABEL: @sbmmt8(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.sbmmt8(i64 [[A:%.*]], i64 [[B:%.*]])
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long sbmmt8(long a, long b) {
  return __builtin_kvx_sbmmt8(a, b);
}

// CHECK-LABEL: @satd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV:%.*]] = zext i8 [[B:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.satd(i64 [[V:%.*]], i32 [[CONV]])
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long satd(long v, unsigned char b) {
  return __builtin_kvx_satd(v, b);
}

// CHECK-LABEL: @stsuw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.stsuw(i32 [[X:%.*]], i32 [[Y:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int stsuw(unsigned int x, unsigned int y) {
  return __builtin_kvx_stsuw(x, y);
}

// CHECK-LABEL: @stsud(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.stsud(i64 [[X:%.*]], i64 [[Y:%.*]])
// CHECK-NEXT:    ret i64 [[TMP0]]
//
unsigned long stsud(unsigned long x, unsigned long y) {
  return __builtin_kvx_stsud(x, y);
}

// CHECK-LABEL: @fwidenlhw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.fwidenlhw(i32 [[V:%.*]])
// CHECK-NEXT:    ret float [[TMP0]]
//
float fwidenlhw(unsigned int v) {
  return __builtin_kvx_fwidenlhw(v);
}

// CHECK-LABEL: @fwidenmhw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.fwidenmhw(i32 [[V:%.*]])
// CHECK-NEXT:    ret float [[TMP0]]
//
float fwidenmhw(unsigned int v) {
  return __builtin_kvx_fwidenmhw(v);
}

// CHECK-LABEL: @fnarrowwh(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.fnarrowwh(float [[V:%.*]])
// CHECK-NEXT:    [[CONV:%.*]] = trunc i32 [[TMP0]] to i16
// CHECK-NEXT:    ret i16 [[CONV]]
//
unsigned short fnarrowwh(float v) {
  return __builtin_kvx_fnarrowwh(v);
}
