// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -triple kvx-kalray-cos  -S -emit-llvm -o - -O0 %s | FileCheck --check-prefix=O0 %s
// Ensure we emit final stores for these operations
// RUN: %clang_cc1 -triple kvx-kalray-cos  -S -emit-llvm -o - -O1 %s | FileCheck --check-prefix=O2 %s
// Ensure we emit final stores for these operations
// O0-LABEL: @convdhv(
// O0-NEXT:  entry:
// O0-NEXT:    [[V_ADDR:%.*]] = alloca ptr, align 8
// O0-NEXT:    [[M_ADDR:%.*]] = alloca ptr, align 8
// O0-NEXT:    [[LOCAL:%.*]] = alloca <256 x i1>, align 32
// O0-NEXT:    store ptr [[V:%.*]], ptr [[V_ADDR]], align 8
// O0-NEXT:    store ptr [[M:%.*]], ptr [[M_ADDR]], align 8
// O0-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[V_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP0]], i64 0
// O0-NEXT:    [[TMP1:%.*]] = load <256 x i1>, ptr [[ARRAYIDX]], align 32
// O0-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[M_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[TMP2]], i64 0
// O0-NEXT:    [[TMP3:%.*]] = load <1024 x i1>, ptr [[ARRAYIDX1]], align 32
// O0-NEXT:    [[TMP4:%.*]] = call <256 x i1> @llvm.kvx.xconvdhv1(<256 x i1> [[TMP1]], <1024 x i1> [[TMP3]], i32 0, i32 0)
// O0-NEXT:    store <256 x i1> [[TMP4]], ptr [[LOCAL]], align 32
// O0-NEXT:    [[TMP5:%.*]] = load <256 x i1>, ptr [[LOCAL]], align 32
// O0-NEXT:    [[TMP6:%.*]] = load ptr, ptr [[M_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[TMP6]], i64 0
// O0-NEXT:    [[TMP7:%.*]] = load <1024 x i1>, ptr [[ARRAYIDX2]], align 32
// O0-NEXT:    [[TMP8:%.*]] = call <256 x i1> @llvm.kvx.xconvdhv0(<256 x i1> [[TMP5]], <1024 x i1> [[TMP7]], i32 0, i32 1)
// O0-NEXT:    store <256 x i1> [[TMP8]], ptr [[LOCAL]], align 32
// O0-NEXT:    [[TMP9:%.*]] = load <256 x i1>, ptr [[LOCAL]], align 32
// O0-NEXT:    [[TMP10:%.*]] = load ptr, ptr [[V_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP10]], i64 1
// O0-NEXT:    store <256 x i1> [[TMP9]], ptr [[ARRAYIDX3]], align 32
// O0-NEXT:    [[TMP11:%.*]] = load ptr, ptr [[M_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[TMP11]], i64 1
// O0-NEXT:    [[TMP12:%.*]] = load <1024 x i1>, ptr [[ARRAYIDX4]], align 32
// O0-NEXT:    [[TMP13:%.*]] = call <256 x i1> @llvm.kvx.xconvdhv(<1024 x i1> [[TMP12]], i32 3, i32 0)
// O0-NEXT:    [[TMP14:%.*]] = load ptr, ptr [[V_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX5:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP14]], i64 2
// O0-NEXT:    store <256 x i1> [[TMP13]], ptr [[ARRAYIDX5]], align 32
// O0-NEXT:    ret void
//
// O2-LABEL: @convdhv(
// O2-NEXT:  entry:
// O2-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[V:%.*]], align 32, !tbaa [[TBAA2:![0-9]+]]
// O2-NEXT:    [[TMP1:%.*]] = load <1024 x i1>, ptr [[M:%.*]], align 32, !tbaa [[TBAA6:![0-9]+]]
// O2-NEXT:    [[TMP2:%.*]] = tail call <256 x i1> @llvm.kvx.xconvdhv1(<256 x i1> [[TMP0]], <1024 x i1> [[TMP1]], i32 0, i32 0)
// O2-NEXT:    [[TMP3:%.*]] = tail call <256 x i1> @llvm.kvx.xconvdhv0(<256 x i1> [[TMP2]], <1024 x i1> [[TMP1]], i32 0, i32 1)
// O2-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <256 x i1>, ptr [[V]], i64 1
// O2-NEXT:    store <256 x i1> [[TMP3]], ptr [[ARRAYIDX3]], align 32, !tbaa [[TBAA2]]
// O2-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[M]], i64 1
// O2-NEXT:    [[TMP4:%.*]] = load <1024 x i1>, ptr [[ARRAYIDX4]], align 32, !tbaa [[TBAA6]]
// O2-NEXT:    [[TMP5:%.*]] = tail call <256 x i1> @llvm.kvx.xconvdhv(<1024 x i1> [[TMP4]], i32 3, i32 0)
// O2-NEXT:    [[ARRAYIDX5:%.*]] = getelementptr inbounds <256 x i1>, ptr [[V]], i64 2
// O2-NEXT:    store <256 x i1> [[TMP5]], ptr [[ARRAYIDX5]], align 32, !tbaa [[TBAA2]]
// O2-NEXT:    ret void
//
void convdhv(__kvx_x256 *v, __kvx_x1024 *m) {
  __kvx_x256 local;
  local = __builtin_kvx_xconvdhv1(v[0], m[0], ".rn.sat");
  local = __builtin_kvx_xconvdhv0(local, m[0], ".rn.satu");
  v[1] = local;
  v[2] = __builtin_kvx_xconvdhv(m[1], ".rz.sat");
}

// O0-LABEL: @convwbv(
// O0-NEXT:  entry:
// O0-NEXT:    [[V_ADDR:%.*]] = alloca ptr, align 8
// O0-NEXT:    [[M_ADDR:%.*]] = alloca ptr, align 8
// O0-NEXT:    [[LOCAL:%.*]] = alloca <256 x i1>, align 32
// O0-NEXT:    store ptr [[V:%.*]], ptr [[V_ADDR]], align 8
// O0-NEXT:    store ptr [[M:%.*]], ptr [[M_ADDR]], align 8
// O0-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[V_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP0]], i64 0
// O0-NEXT:    [[TMP1:%.*]] = load <256 x i1>, ptr [[ARRAYIDX]], align 32
// O0-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[M_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[TMP2]], i64 0
// O0-NEXT:    [[TMP3:%.*]] = load <1024 x i1>, ptr [[ARRAYIDX1]], align 32
// O0-NEXT:    [[TMP4:%.*]] = call <256 x i1> @llvm.kvx.xconvwbv1(<256 x i1> [[TMP1]], <1024 x i1> [[TMP3]], i32 0, i32 0)
// O0-NEXT:    store <256 x i1> [[TMP4]], ptr [[LOCAL]], align 32
// O0-NEXT:    [[TMP5:%.*]] = load <256 x i1>, ptr [[LOCAL]], align 32
// O0-NEXT:    [[TMP6:%.*]] = load ptr, ptr [[M_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[TMP6]], i64 0
// O0-NEXT:    [[TMP7:%.*]] = load <1024 x i1>, ptr [[ARRAYIDX2]], align 32
// O0-NEXT:    [[TMP8:%.*]] = call <256 x i1> @llvm.kvx.xconvwbv0(<256 x i1> [[TMP5]], <1024 x i1> [[TMP7]], i32 0, i32 1)
// O0-NEXT:    store <256 x i1> [[TMP8]], ptr [[LOCAL]], align 32
// O0-NEXT:    [[TMP9:%.*]] = load <256 x i1>, ptr [[LOCAL]], align 32
// O0-NEXT:    [[TMP10:%.*]] = load ptr, ptr [[M_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[TMP10]], i64 0
// O0-NEXT:    [[TMP11:%.*]] = load <1024 x i1>, ptr [[ARRAYIDX3]], align 32
// O0-NEXT:    [[TMP12:%.*]] = call <256 x i1> @llvm.kvx.xconvwbv2(<256 x i1> [[TMP9]], <1024 x i1> [[TMP11]], i32 2, i32 0)
// O0-NEXT:    store <256 x i1> [[TMP12]], ptr [[LOCAL]], align 32
// O0-NEXT:    [[TMP13:%.*]] = load <256 x i1>, ptr [[LOCAL]], align 32
// O0-NEXT:    [[TMP14:%.*]] = load ptr, ptr [[M_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[TMP14]], i64 0
// O0-NEXT:    [[TMP15:%.*]] = load <1024 x i1>, ptr [[ARRAYIDX4]], align 32
// O0-NEXT:    [[TMP16:%.*]] = call <256 x i1> @llvm.kvx.xconvwbv3(<256 x i1> [[TMP13]], <1024 x i1> [[TMP15]], i32 4, i32 1)
// O0-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[V_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX5:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP17]], i64 1
// O0-NEXT:    store <256 x i1> [[TMP16]], ptr [[ARRAYIDX5]], align 32
// O0-NEXT:    [[TMP18:%.*]] = load <256 x i1>, ptr [[LOCAL]], align 32
// O0-NEXT:    [[TMP19:%.*]] = load ptr, ptr [[M_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[TMP19]], i64 0
// O0-NEXT:    [[TMP20:%.*]] = load <1024 x i1>, ptr [[ARRAYIDX6]], align 32
// O0-NEXT:    [[TMP21:%.*]] = call <256 x i1> @llvm.kvx.xconvwbv3(<256 x i1> [[TMP18]], <1024 x i1> [[TMP20]], i32 0, i32 0)
// O0-NEXT:    [[TMP22:%.*]] = load ptr, ptr [[V_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX7:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP22]], i64 0
// O0-NEXT:    store <256 x i1> [[TMP21]], ptr [[ARRAYIDX7]], align 32
// O0-NEXT:    [[TMP23:%.*]] = load ptr, ptr [[M_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX8:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[TMP23]], i64 1
// O0-NEXT:    [[TMP24:%.*]] = load <1024 x i1>, ptr [[ARRAYIDX8]], align 32
// O0-NEXT:    [[TMP25:%.*]] = call <256 x i1> @llvm.kvx.xconvwbv(<1024 x i1> [[TMP24]], i32 3, i32 1)
// O0-NEXT:    [[TMP26:%.*]] = load ptr, ptr [[V_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX9:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP26]], i64 2
// O0-NEXT:    store <256 x i1> [[TMP25]], ptr [[ARRAYIDX9]], align 32
// O0-NEXT:    ret void
//
// O2-LABEL: @convwbv(
// O2-NEXT:  entry:
// O2-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[V:%.*]], align 32, !tbaa [[TBAA2]]
// O2-NEXT:    [[TMP1:%.*]] = load <1024 x i1>, ptr [[M:%.*]], align 32, !tbaa [[TBAA6]]
// O2-NEXT:    [[TMP2:%.*]] = tail call <256 x i1> @llvm.kvx.xconvwbv1(<256 x i1> [[TMP0]], <1024 x i1> [[TMP1]], i32 0, i32 0)
// O2-NEXT:    [[TMP3:%.*]] = tail call <256 x i1> @llvm.kvx.xconvwbv0(<256 x i1> [[TMP2]], <1024 x i1> [[TMP1]], i32 0, i32 1)
// O2-NEXT:    [[TMP4:%.*]] = tail call <256 x i1> @llvm.kvx.xconvwbv2(<256 x i1> [[TMP3]], <1024 x i1> [[TMP1]], i32 2, i32 0)
// O2-NEXT:    [[TMP5:%.*]] = tail call <256 x i1> @llvm.kvx.xconvwbv3(<256 x i1> [[TMP4]], <1024 x i1> [[TMP1]], i32 4, i32 1)
// O2-NEXT:    [[ARRAYIDX5:%.*]] = getelementptr inbounds <256 x i1>, ptr [[V]], i64 1
// O2-NEXT:    store <256 x i1> [[TMP5]], ptr [[ARRAYIDX5]], align 32, !tbaa [[TBAA2]]
// O2-NEXT:    [[TMP6:%.*]] = tail call <256 x i1> @llvm.kvx.xconvwbv3(<256 x i1> [[TMP4]], <1024 x i1> [[TMP1]], i32 0, i32 0)
// O2-NEXT:    store <256 x i1> [[TMP6]], ptr [[V]], align 32, !tbaa [[TBAA2]]
// O2-NEXT:    [[ARRAYIDX8:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[M]], i64 1
// O2-NEXT:    [[TMP7:%.*]] = load <1024 x i1>, ptr [[ARRAYIDX8]], align 32, !tbaa [[TBAA6]]
// O2-NEXT:    [[TMP8:%.*]] = tail call <256 x i1> @llvm.kvx.xconvwbv(<1024 x i1> [[TMP7]], i32 3, i32 1)
// O2-NEXT:    [[ARRAYIDX9:%.*]] = getelementptr inbounds <256 x i1>, ptr [[V]], i64 2
// O2-NEXT:    store <256 x i1> [[TMP8]], ptr [[ARRAYIDX9]], align 32, !tbaa [[TBAA2]]
// O2-NEXT:    ret void
//
void convwbv(__kvx_x256 *v, __kvx_x1024 *m) {
  __kvx_x256 local;
  local = __builtin_kvx_xconvwbv1(v[0], m[0], ".rn.sat");
  local = __builtin_kvx_xconvwbv0(local, m[0], ".rn.satu");
  local = __builtin_kvx_xconvwbv2(local, m[0], ".rd.sat");
  v[1] = __builtin_kvx_xconvwbv3(local, m[0], ".rhu.satu");
  v[0] = __builtin_kvx_xconvwbv3(local, m[0], ".rn.sat");
  v[2] = __builtin_kvx_xconvwbv(m[1], ".rz.satu");
}

// O0-LABEL: @fmma444hw(
// O0-NEXT:  entry:
// O0-NEXT:    [[V_ADDR:%.*]] = alloca ptr, align 8
// O0-NEXT:    [[W_ADDR:%.*]] = alloca ptr, align 8
// O0-NEXT:    [[LOCAL:%.*]] = alloca <256 x i1>, align 32
// O0-NEXT:    store ptr [[V:%.*]], ptr [[V_ADDR]], align 8
// O0-NEXT:    store ptr [[W:%.*]], ptr [[W_ADDR]], align 8
// O0-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[V_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP0]], i64 0
// O0-NEXT:    [[TMP1:%.*]] = load <256 x i1>, ptr [[ARRAYIDX]], align 32
// O0-NEXT:    store <256 x i1> [[TMP1]], ptr [[LOCAL]], align 32
// O0-NEXT:    [[TMP2:%.*]] = load <256 x i1>, ptr [[LOCAL]], align 32
// O0-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[W_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <512 x i1>, ptr [[TMP3]], i64 0
// O0-NEXT:    [[TMP4:%.*]] = load <512 x i1>, ptr [[ARRAYIDX1]], align 32
// O0-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[V_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP5]], i64 1
// O0-NEXT:    [[TMP6:%.*]] = load <256 x i1>, ptr [[ARRAYIDX2]], align 32
// O0-NEXT:    [[TMP7:%.*]] = load ptr, ptr [[V_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP7]], i64 2
// O0-NEXT:    [[TMP8:%.*]] = load <256 x i1>, ptr [[ARRAYIDX3]], align 32
// O0-NEXT:    [[TMP9:%.*]] = call <256 x i1> @llvm.kvx.xfmma242hw0(<256 x i1> [[TMP2]], <512 x i1> [[TMP4]], <256 x i1> [[TMP6]], <256 x i1> [[TMP8]])
// O0-NEXT:    store <256 x i1> [[TMP9]], ptr [[LOCAL]], align 32
// O0-NEXT:    [[TMP10:%.*]] = load <256 x i1>, ptr [[LOCAL]], align 32
// O0-NEXT:    [[TMP11:%.*]] = load ptr, ptr [[W_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds <512 x i1>, ptr [[TMP11]], i64 0
// O0-NEXT:    [[TMP12:%.*]] = load <512 x i1>, ptr [[ARRAYIDX4]], align 32
// O0-NEXT:    [[TMP13:%.*]] = load ptr, ptr [[V_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX5:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP13]], i64 1
// O0-NEXT:    [[TMP14:%.*]] = load <256 x i1>, ptr [[ARRAYIDX5]], align 32
// O0-NEXT:    [[TMP15:%.*]] = load ptr, ptr [[V_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP15]], i64 2
// O0-NEXT:    [[TMP16:%.*]] = load <256 x i1>, ptr [[ARRAYIDX6]], align 32
// O0-NEXT:    [[TMP17:%.*]] = call <256 x i1> @llvm.kvx.xfmma242hw1(<256 x i1> [[TMP10]], <512 x i1> [[TMP12]], <256 x i1> [[TMP14]], <256 x i1> [[TMP16]])
// O0-NEXT:    store <256 x i1> [[TMP17]], ptr [[LOCAL]], align 32
// O0-NEXT:    [[TMP18:%.*]] = load <256 x i1>, ptr [[LOCAL]], align 32
// O0-NEXT:    [[TMP19:%.*]] = load ptr, ptr [[W_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX7:%.*]] = getelementptr inbounds <512 x i1>, ptr [[TMP19]], i64 0
// O0-NEXT:    [[TMP20:%.*]] = load <512 x i1>, ptr [[ARRAYIDX7]], align 32
// O0-NEXT:    [[TMP21:%.*]] = load ptr, ptr [[V_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX8:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP21]], i64 1
// O0-NEXT:    [[TMP22:%.*]] = load <256 x i1>, ptr [[ARRAYIDX8]], align 32
// O0-NEXT:    [[TMP23:%.*]] = load ptr, ptr [[V_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX9:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP23]], i64 2
// O0-NEXT:    [[TMP24:%.*]] = load <256 x i1>, ptr [[ARRAYIDX9]], align 32
// O0-NEXT:    [[TMP25:%.*]] = call <256 x i1> @llvm.kvx.xfmma242hw2(<256 x i1> [[TMP18]], <512 x i1> [[TMP20]], <256 x i1> [[TMP22]], <256 x i1> [[TMP24]])
// O0-NEXT:    [[TMP26:%.*]] = load ptr, ptr [[V_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX10:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP26]], i64 1
// O0-NEXT:    store <256 x i1> [[TMP25]], ptr [[ARRAYIDX10]], align 32
// O0-NEXT:    [[TMP27:%.*]] = load <256 x i1>, ptr [[LOCAL]], align 32
// O0-NEXT:    [[TMP28:%.*]] = load ptr, ptr [[W_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX11:%.*]] = getelementptr inbounds <512 x i1>, ptr [[TMP28]], i64 0
// O0-NEXT:    [[TMP29:%.*]] = load <512 x i1>, ptr [[ARRAYIDX11]], align 32
// O0-NEXT:    [[TMP30:%.*]] = load ptr, ptr [[V_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX12:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP30]], i64 1
// O0-NEXT:    [[TMP31:%.*]] = load <256 x i1>, ptr [[ARRAYIDX12]], align 32
// O0-NEXT:    [[TMP32:%.*]] = load ptr, ptr [[V_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX13:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP32]], i64 2
// O0-NEXT:    [[TMP33:%.*]] = load <256 x i1>, ptr [[ARRAYIDX13]], align 32
// O0-NEXT:    [[TMP34:%.*]] = call <256 x i1> @llvm.kvx.xfmma242hw3(<256 x i1> [[TMP27]], <512 x i1> [[TMP29]], <256 x i1> [[TMP31]], <256 x i1> [[TMP33]])
// O0-NEXT:    [[TMP35:%.*]] = load ptr, ptr [[V_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX14:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP35]], i64 2
// O0-NEXT:    store <256 x i1> [[TMP34]], ptr [[ARRAYIDX14]], align 32
// O0-NEXT:    [[TMP36:%.*]] = load <256 x i1>, ptr [[LOCAL]], align 32
// O0-NEXT:    [[TMP37:%.*]] = load ptr, ptr [[V_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX15:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP37]], i64 0
// O0-NEXT:    store <256 x i1> [[TMP36]], ptr [[ARRAYIDX15]], align 32
// O0-NEXT:    [[TMP38:%.*]] = load ptr, ptr [[V_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX16:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP38]], i64 2
// O0-NEXT:    [[TMP39:%.*]] = load <256 x i1>, ptr [[ARRAYIDX16]], align 32
// O0-NEXT:    [[TMP40:%.*]] = load ptr, ptr [[V_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX17:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP40]], i64 1
// O0-NEXT:    [[TMP41:%.*]] = load <256 x i1>, ptr [[ARRAYIDX17]], align 32
// O0-NEXT:    [[TMP42:%.*]] = load ptr, ptr [[W_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX18:%.*]] = getelementptr inbounds <512 x i1>, ptr [[TMP42]], i64 1
// O0-NEXT:    [[TMP43:%.*]] = load <512 x i1>, ptr [[ARRAYIDX18]], align 32
// O0-NEXT:    [[TMP44:%.*]] = call <512 x i1> @llvm.kvx.xfmma444hw(<256 x i1> [[TMP39]], <256 x i1> [[TMP41]], <512 x i1> [[TMP43]])
// O0-NEXT:    [[TMP45:%.*]] = load ptr, ptr [[W_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX19:%.*]] = getelementptr inbounds <512 x i1>, ptr [[TMP45]], i64 3
// O0-NEXT:    store <512 x i1> [[TMP44]], ptr [[ARRAYIDX19]], align 32
// O0-NEXT:    ret void
//
// O2-LABEL: @fmma444hw(
// O2-NEXT:  entry:
// O2-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[V:%.*]], align 32, !tbaa [[TBAA2]]
// O2-NEXT:    [[TMP1:%.*]] = load <512 x i1>, ptr [[W:%.*]], align 32, !tbaa [[TBAA8:![0-9]+]]
// O2-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds <256 x i1>, ptr [[V]], i64 1
// O2-NEXT:    [[TMP2:%.*]] = load <256 x i1>, ptr [[ARRAYIDX2]], align 32, !tbaa [[TBAA2]]
// O2-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <256 x i1>, ptr [[V]], i64 2
// O2-NEXT:    [[TMP3:%.*]] = load <256 x i1>, ptr [[ARRAYIDX3]], align 32, !tbaa [[TBAA2]]
// O2-NEXT:    [[TMP4:%.*]] = tail call <256 x i1> @llvm.kvx.xfmma242hw0(<256 x i1> [[TMP0]], <512 x i1> [[TMP1]], <256 x i1> [[TMP2]], <256 x i1> [[TMP3]])
// O2-NEXT:    [[TMP5:%.*]] = tail call <256 x i1> @llvm.kvx.xfmma242hw1(<256 x i1> [[TMP4]], <512 x i1> [[TMP1]], <256 x i1> [[TMP2]], <256 x i1> [[TMP3]])
// O2-NEXT:    [[TMP6:%.*]] = tail call <256 x i1> @llvm.kvx.xfmma242hw2(<256 x i1> [[TMP5]], <512 x i1> [[TMP1]], <256 x i1> [[TMP2]], <256 x i1> [[TMP3]])
// O2-NEXT:    store <256 x i1> [[TMP6]], ptr [[ARRAYIDX2]], align 32, !tbaa [[TBAA2]]
// O2-NEXT:    [[TMP7:%.*]] = tail call <256 x i1> @llvm.kvx.xfmma242hw3(<256 x i1> [[TMP5]], <512 x i1> [[TMP1]], <256 x i1> [[TMP6]], <256 x i1> [[TMP3]])
// O2-NEXT:    store <256 x i1> [[TMP7]], ptr [[ARRAYIDX3]], align 32, !tbaa [[TBAA2]]
// O2-NEXT:    store <256 x i1> [[TMP5]], ptr [[V]], align 32, !tbaa [[TBAA2]]
// O2-NEXT:    [[ARRAYIDX18:%.*]] = getelementptr inbounds <512 x i1>, ptr [[W]], i64 1
// O2-NEXT:    [[TMP8:%.*]] = load <512 x i1>, ptr [[ARRAYIDX18]], align 32, !tbaa [[TBAA8]]
// O2-NEXT:    [[TMP9:%.*]] = tail call <512 x i1> @llvm.kvx.xfmma444hw(<256 x i1> [[TMP7]], <256 x i1> [[TMP6]], <512 x i1> [[TMP8]])
// O2-NEXT:    [[ARRAYIDX19:%.*]] = getelementptr inbounds <512 x i1>, ptr [[W]], i64 3
// O2-NEXT:    store <512 x i1> [[TMP9]], ptr [[ARRAYIDX19]], align 32, !tbaa [[TBAA8]]
// O2-NEXT:    ret void
//
void fmma444hw(__kvx_x256 *v, __kvx_x512 *w) {
  __kvx_x256 local = v[0];
  local = __builtin_kvx_xfmma242hw0(local, w[0], v[1], v[2]);
  local = __builtin_kvx_xfmma242hw1(local, w[0], v[1], v[2]);
  v[1] = __builtin_kvx_xfmma242hw2(local, w[0], v[1], v[2]);
  v[2] = __builtin_kvx_xfmma242hw3(local, w[0], v[1], v[2]);
  v[0] = local;
  w[3] = __builtin_kvx_xfmma444hw(v[2], v[1], w[1]);
}

// O0-LABEL: @test(
// O0-NEXT:  entry:
// O0-NEXT:    [[V_ADDR:%.*]] = alloca ptr, align 8
// O0-NEXT:    [[J:%.*]] = alloca [2 x i64], align 8
// O0-NEXT:    store ptr [[V:%.*]], ptr [[V_ADDR]], align 8
// O0-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 8 [[J]], ptr align 8 @__const.test.j, i64 16, i1 false)
// O0-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[V_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP0]], i64 0
// O0-NEXT:    [[TMP1:%.*]] = load <256 x i1>, ptr [[ARRAYIDX]], align 32
// O0-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds [2 x i64], ptr [[J]], i64 0, i64 1
// O0-NEXT:    [[TMP2:%.*]] = load i64, ptr [[ARRAYIDX1]], align 8
// O0-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [2 x i64], ptr [[J]], i64 0, i64 0
// O0-NEXT:    [[TMP3:%.*]] = load i64, ptr [[ARRAYIDX2]], align 8
// O0-NEXT:    [[TMP4:%.*]] = call <256 x i1> @llvm.kvx.xmovetq(<256 x i1> [[TMP1]], i64 [[TMP2]], i64 [[TMP3]], i32 1)
// O0-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[V_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP5]], i64 0
// O0-NEXT:    store <256 x i1> [[TMP4]], ptr [[ARRAYIDX3]], align 32
// O0-NEXT:    [[TMP6:%.*]] = load ptr, ptr [[V_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP6]], i64 0
// O0-NEXT:    [[TMP7:%.*]] = load <256 x i1>, ptr [[ARRAYIDX4]], align 32
// O0-NEXT:    [[ARRAYIDX5:%.*]] = getelementptr inbounds [2 x i64], ptr [[J]], i64 0, i64 1
// O0-NEXT:    [[TMP8:%.*]] = load i64, ptr [[ARRAYIDX5]], align 8
// O0-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [2 x i64], ptr [[J]], i64 0, i64 0
// O0-NEXT:    [[TMP9:%.*]] = load i64, ptr [[ARRAYIDX6]], align 8
// O0-NEXT:    [[TMP10:%.*]] = call <256 x i1> @llvm.kvx.xmovetq(<256 x i1> [[TMP7]], i64 [[TMP8]], i64 [[TMP9]], i32 0)
// O0-NEXT:    [[TMP11:%.*]] = load ptr, ptr [[V_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX7:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP11]], i64 0
// O0-NEXT:    store <256 x i1> [[TMP10]], ptr [[ARRAYIDX7]], align 32
// O0-NEXT:    ret void
//
// O2-LABEL: @test(
// O2-NEXT:  entry:
// O2-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[V:%.*]], align 32, !tbaa [[TBAA2]]
// O2-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xmovetq(<256 x i1> [[TMP0]], i64 1, i64 0, i32 1)
// O2-NEXT:    [[TMP2:%.*]] = tail call <256 x i1> @llvm.kvx.xmovetq(<256 x i1> [[TMP1]], i64 1, i64 0, i32 0)
// O2-NEXT:    store <256 x i1> [[TMP2]], ptr [[V]], align 32, !tbaa [[TBAA2]]
// O2-NEXT:    ret void
//
void test(__kvx_x256 *v) {
  long j[2] = {0, 1};
  v[0] = __builtin_kvx_xmovetq(v[0], j[1], j[0], ".h1");
  v[0] = __builtin_kvx_xmovetq(v[0], j[1], j[0], ".h0");
}

// O0-LABEL: @insertwm(
// O0-NEXT:  entry:
// O0-NEXT:    [[A0_ADDR:%.*]] = alloca ptr, align 8
// O0-NEXT:    [[A1_ADDR:%.*]] = alloca ptr, align 8
// O0-NEXT:    store ptr [[A0:%.*]], ptr [[A0_ADDR]], align 8
// O0-NEXT:    store ptr [[A1:%.*]], ptr [[A1_ADDR]], align 8
// O0-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A0_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[TMP0]], i64 0
// O0-NEXT:    [[TMP1:%.*]] = load <1024 x i1>, ptr [[ARRAYIDX]], align 32
// O0-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[A1_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <512 x i1>, ptr [[TMP2]], i64 0
// O0-NEXT:    [[TMP3:%.*]] = load <512 x i1>, ptr [[ARRAYIDX1]], align 32
// O0-NEXT:    [[TMP4:%.*]] = call <1024 x i1> @llvm.kvx.xinsertwm(<1024 x i1> [[TMP1]], <512 x i1> [[TMP3]], i32 0)
// O0-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[A0_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[TMP5]], i64 0
// O0-NEXT:    store <1024 x i1> [[TMP4]], ptr [[ARRAYIDX2]], align 32
// O0-NEXT:    [[TMP6:%.*]] = load ptr, ptr [[A0_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[TMP6]], i64 0
// O0-NEXT:    [[TMP7:%.*]] = load <1024 x i1>, ptr [[ARRAYIDX3]], align 32
// O0-NEXT:    [[TMP8:%.*]] = load ptr, ptr [[A1_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds <512 x i1>, ptr [[TMP8]], i64 0
// O0-NEXT:    [[TMP9:%.*]] = load <512 x i1>, ptr [[ARRAYIDX4]], align 32
// O0-NEXT:    [[TMP10:%.*]] = call <1024 x i1> @llvm.kvx.xinsertwm(<1024 x i1> [[TMP7]], <512 x i1> [[TMP9]], i32 1)
// O0-NEXT:    [[TMP11:%.*]] = load ptr, ptr [[A0_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX5:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[TMP11]], i64 0
// O0-NEXT:    store <1024 x i1> [[TMP10]], ptr [[ARRAYIDX5]], align 32
// O0-NEXT:    ret void
//
// O2-LABEL: @insertwm(
// O2-NEXT:  entry:
// O2-NEXT:    [[TMP0:%.*]] = load <1024 x i1>, ptr [[A0:%.*]], align 32, !tbaa [[TBAA6]]
// O2-NEXT:    [[TMP1:%.*]] = load <512 x i1>, ptr [[A1:%.*]], align 32, !tbaa [[TBAA8]]
// O2-NEXT:    [[TMP2:%.*]] = tail call <1024 x i1> @llvm.kvx.xinsertwm(<1024 x i1> [[TMP0]], <512 x i1> [[TMP1]], i32 0)
// O2-NEXT:    [[TMP3:%.*]] = tail call <1024 x i1> @llvm.kvx.xinsertwm(<1024 x i1> [[TMP2]], <512 x i1> [[TMP1]], i32 1)
// O2-NEXT:    store <1024 x i1> [[TMP3]], ptr [[A0]], align 32, !tbaa [[TBAA6]]
// O2-NEXT:    ret void
//
void insertwm(__kvx_x1024 *a0, __kvx_x512 *a1) {
  a0[0] = __builtin_kvx_xinsertwm(a0[0], a1[0], 0);
  a0[0] = __builtin_kvx_xinsertwm(a0[0], a1[0], 1);
}

// O0-LABEL: @insertvm(
// O0-NEXT:  entry:
// O0-NEXT:    [[A0_ADDR:%.*]] = alloca ptr, align 8
// O0-NEXT:    [[A1_ADDR:%.*]] = alloca ptr, align 8
// O0-NEXT:    store ptr [[A0:%.*]], ptr [[A0_ADDR]], align 8
// O0-NEXT:    store ptr [[A1:%.*]], ptr [[A1_ADDR]], align 8
// O0-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A0_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[TMP0]], i64 0
// O0-NEXT:    [[TMP1:%.*]] = load <1024 x i1>, ptr [[ARRAYIDX]], align 32
// O0-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[A1_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP2]], i64 0
// O0-NEXT:    [[TMP3:%.*]] = load <256 x i1>, ptr [[ARRAYIDX1]], align 32
// O0-NEXT:    [[TMP4:%.*]] = call <1024 x i1> @llvm.kvx.xinsertvm(<1024 x i1> [[TMP1]], <256 x i1> [[TMP3]], i32 0)
// O0-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[A0_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[TMP5]], i64 0
// O0-NEXT:    store <1024 x i1> [[TMP4]], ptr [[ARRAYIDX2]], align 32
// O0-NEXT:    [[TMP6:%.*]] = load ptr, ptr [[A0_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[TMP6]], i64 0
// O0-NEXT:    [[TMP7:%.*]] = load <1024 x i1>, ptr [[ARRAYIDX3]], align 32
// O0-NEXT:    [[TMP8:%.*]] = load ptr, ptr [[A1_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP8]], i64 0
// O0-NEXT:    [[TMP9:%.*]] = load <256 x i1>, ptr [[ARRAYIDX4]], align 32
// O0-NEXT:    [[TMP10:%.*]] = call <1024 x i1> @llvm.kvx.xinsertvm(<1024 x i1> [[TMP7]], <256 x i1> [[TMP9]], i32 1)
// O0-NEXT:    [[TMP11:%.*]] = load ptr, ptr [[A0_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX5:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[TMP11]], i64 0
// O0-NEXT:    store <1024 x i1> [[TMP10]], ptr [[ARRAYIDX5]], align 32
// O0-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A0_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[TMP12]], i64 0
// O0-NEXT:    [[TMP13:%.*]] = load <1024 x i1>, ptr [[ARRAYIDX6]], align 32
// O0-NEXT:    [[TMP14:%.*]] = load ptr, ptr [[A1_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX7:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP14]], i64 0
// O0-NEXT:    [[TMP15:%.*]] = load <256 x i1>, ptr [[ARRAYIDX7]], align 32
// O0-NEXT:    [[TMP16:%.*]] = call <1024 x i1> @llvm.kvx.xinsertvm(<1024 x i1> [[TMP13]], <256 x i1> [[TMP15]], i32 2)
// O0-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A0_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX8:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[TMP17]], i64 0
// O0-NEXT:    store <1024 x i1> [[TMP16]], ptr [[ARRAYIDX8]], align 32
// O0-NEXT:    [[TMP18:%.*]] = load ptr, ptr [[A0_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX9:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[TMP18]], i64 0
// O0-NEXT:    [[TMP19:%.*]] = load <1024 x i1>, ptr [[ARRAYIDX9]], align 32
// O0-NEXT:    [[TMP20:%.*]] = load ptr, ptr [[A1_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX10:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP20]], i64 0
// O0-NEXT:    [[TMP21:%.*]] = load <256 x i1>, ptr [[ARRAYIDX10]], align 32
// O0-NEXT:    [[TMP22:%.*]] = call <1024 x i1> @llvm.kvx.xinsertvm(<1024 x i1> [[TMP19]], <256 x i1> [[TMP21]], i32 3)
// O0-NEXT:    [[TMP23:%.*]] = load ptr, ptr [[A0_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX11:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[TMP23]], i64 0
// O0-NEXT:    store <1024 x i1> [[TMP22]], ptr [[ARRAYIDX11]], align 32
// O0-NEXT:    ret void
//
// O2-LABEL: @insertvm(
// O2-NEXT:  entry:
// O2-NEXT:    [[TMP0:%.*]] = load <1024 x i1>, ptr [[A0:%.*]], align 32, !tbaa [[TBAA6]]
// O2-NEXT:    [[TMP1:%.*]] = load <256 x i1>, ptr [[A1:%.*]], align 32, !tbaa [[TBAA2]]
// O2-NEXT:    [[TMP2:%.*]] = tail call <1024 x i1> @llvm.kvx.xinsertvm(<1024 x i1> [[TMP0]], <256 x i1> [[TMP1]], i32 0)
// O2-NEXT:    [[TMP3:%.*]] = tail call <1024 x i1> @llvm.kvx.xinsertvm(<1024 x i1> [[TMP2]], <256 x i1> [[TMP1]], i32 1)
// O2-NEXT:    [[TMP4:%.*]] = tail call <1024 x i1> @llvm.kvx.xinsertvm(<1024 x i1> [[TMP3]], <256 x i1> [[TMP1]], i32 2)
// O2-NEXT:    [[TMP5:%.*]] = tail call <1024 x i1> @llvm.kvx.xinsertvm(<1024 x i1> [[TMP4]], <256 x i1> [[TMP1]], i32 3)
// O2-NEXT:    store <1024 x i1> [[TMP5]], ptr [[A0]], align 32, !tbaa [[TBAA6]]
// O2-NEXT:    ret void
//
void insertvm(__kvx_x1024 *a0, __kvx_x256 *a1) {
  a0[0] = __builtin_kvx_xinsertvm(a0[0], a1[0], 0);
  a0[0] = __builtin_kvx_xinsertvm(a0[0], a1[0], 1);
  a0[0] = __builtin_kvx_xinsertvm(a0[0], a1[0], 2);
  a0[0] = __builtin_kvx_xinsertvm(a0[0], a1[0], 3);
}

// O0-LABEL: @insertvw(
// O0-NEXT:  entry:
// O0-NEXT:    [[A0_ADDR:%.*]] = alloca ptr, align 8
// O0-NEXT:    [[A1_ADDR:%.*]] = alloca ptr, align 8
// O0-NEXT:    store ptr [[A0:%.*]], ptr [[A0_ADDR]], align 8
// O0-NEXT:    store ptr [[A1:%.*]], ptr [[A1_ADDR]], align 8
// O0-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A0_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <512 x i1>, ptr [[TMP0]], i64 0
// O0-NEXT:    [[TMP1:%.*]] = load <512 x i1>, ptr [[ARRAYIDX]], align 32
// O0-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[A1_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP2]], i64 0
// O0-NEXT:    [[TMP3:%.*]] = load <256 x i1>, ptr [[ARRAYIDX1]], align 32
// O0-NEXT:    [[TMP4:%.*]] = call <512 x i1> @llvm.kvx.xinsertvw(<512 x i1> [[TMP1]], <256 x i1> [[TMP3]], i32 0)
// O0-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[A0_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds <512 x i1>, ptr [[TMP5]], i64 0
// O0-NEXT:    store <512 x i1> [[TMP4]], ptr [[ARRAYIDX2]], align 32
// O0-NEXT:    [[TMP6:%.*]] = load ptr, ptr [[A0_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <512 x i1>, ptr [[TMP6]], i64 0
// O0-NEXT:    [[TMP7:%.*]] = load <512 x i1>, ptr [[ARRAYIDX3]], align 32
// O0-NEXT:    [[TMP8:%.*]] = load ptr, ptr [[A1_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP8]], i64 0
// O0-NEXT:    [[TMP9:%.*]] = load <256 x i1>, ptr [[ARRAYIDX4]], align 32
// O0-NEXT:    [[TMP10:%.*]] = call <512 x i1> @llvm.kvx.xinsertvw(<512 x i1> [[TMP7]], <256 x i1> [[TMP9]], i32 1)
// O0-NEXT:    [[TMP11:%.*]] = load ptr, ptr [[A0_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX5:%.*]] = getelementptr inbounds <512 x i1>, ptr [[TMP11]], i64 0
// O0-NEXT:    store <512 x i1> [[TMP10]], ptr [[ARRAYIDX5]], align 32
// O0-NEXT:    ret void
//
// O2-LABEL: @insertvw(
// O2-NEXT:  entry:
// O2-NEXT:    [[TMP0:%.*]] = load <512 x i1>, ptr [[A0:%.*]], align 32, !tbaa [[TBAA8]]
// O2-NEXT:    [[TMP1:%.*]] = load <256 x i1>, ptr [[A1:%.*]], align 32, !tbaa [[TBAA2]]
// O2-NEXT:    [[TMP2:%.*]] = tail call <512 x i1> @llvm.kvx.xinsertvw(<512 x i1> [[TMP0]], <256 x i1> [[TMP1]], i32 0)
// O2-NEXT:    [[TMP3:%.*]] = tail call <512 x i1> @llvm.kvx.xinsertvw(<512 x i1> [[TMP2]], <256 x i1> [[TMP1]], i32 1)
// O2-NEXT:    store <512 x i1> [[TMP3]], ptr [[A0]], align 32, !tbaa [[TBAA8]]
// O2-NEXT:    ret void
//
void insertvw(__kvx_x512 *a0, __kvx_x256 *a1) {
  a0[0] = __builtin_kvx_xinsertvw(a0[0], a1[0], 0);
  a0[0] = __builtin_kvx_xinsertvw(a0[0], a1[0], 1);
}

// O0-LABEL: @movefmw(
// O0-NEXT:  entry:
// O0-NEXT:    [[O_ADDR:%.*]] = alloca ptr, align 8
// O0-NEXT:    [[A0_ADDR:%.*]] = alloca ptr, align 8
// O0-NEXT:    store ptr [[O:%.*]], ptr [[O_ADDR]], align 8
// O0-NEXT:    store ptr [[A0:%.*]], ptr [[A0_ADDR]], align 8
// O0-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A0_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[TMP0]], i64 0
// O0-NEXT:    [[TMP1:%.*]] = load <1024 x i1>, ptr [[ARRAYIDX]], align 32
// O0-NEXT:    [[TMP2:%.*]] = call <512 x i1> @llvm.kvx.xmovefmw(<1024 x i1> [[TMP1]], i32 0)
// O0-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[O_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <512 x i1>, ptr [[TMP3]], i64 0
// O0-NEXT:    store <512 x i1> [[TMP2]], ptr [[ARRAYIDX1]], align 32
// O0-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[A0_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[TMP4]], i64 0
// O0-NEXT:    [[TMP5:%.*]] = load <1024 x i1>, ptr [[ARRAYIDX2]], align 32
// O0-NEXT:    [[TMP6:%.*]] = call <512 x i1> @llvm.kvx.xmovefmw(<1024 x i1> [[TMP5]], i32 1)
// O0-NEXT:    [[TMP7:%.*]] = load ptr, ptr [[O_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <512 x i1>, ptr [[TMP7]], i64 1
// O0-NEXT:    store <512 x i1> [[TMP6]], ptr [[ARRAYIDX3]], align 32
// O0-NEXT:    ret void
//
// O2-LABEL: @movefmw(
// O2-NEXT:  entry:
// O2-NEXT:    [[TMP0:%.*]] = load <1024 x i1>, ptr [[A0:%.*]], align 32, !tbaa [[TBAA6]]
// O2-NEXT:    [[TMP1:%.*]] = tail call <512 x i1> @llvm.kvx.xmovefmw(<1024 x i1> [[TMP0]], i32 0)
// O2-NEXT:    store <512 x i1> [[TMP1]], ptr [[O:%.*]], align 32, !tbaa [[TBAA8]]
// O2-NEXT:    [[TMP2:%.*]] = tail call <512 x i1> @llvm.kvx.xmovefmw(<1024 x i1> [[TMP0]], i32 1)
// O2-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <512 x i1>, ptr [[O]], i64 1
// O2-NEXT:    store <512 x i1> [[TMP2]], ptr [[ARRAYIDX3]], align 32, !tbaa [[TBAA8]]
// O2-NEXT:    ret void
//
void movefmw(__kvx_x512 *o, __kvx_x1024 *a0) {
  o[0] = __builtin_kvx_xmovefmw(a0[0], 0);
  o[1] = __builtin_kvx_xmovefmw(a0[0], 1);
}

// O0-LABEL: @movefmv(
// O0-NEXT:  entry:
// O0-NEXT:    [[O_ADDR:%.*]] = alloca ptr, align 8
// O0-NEXT:    [[A0_ADDR:%.*]] = alloca ptr, align 8
// O0-NEXT:    store ptr [[O:%.*]], ptr [[O_ADDR]], align 8
// O0-NEXT:    store ptr [[A0:%.*]], ptr [[A0_ADDR]], align 8
// O0-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A0_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[TMP0]], i64 0
// O0-NEXT:    [[TMP1:%.*]] = load <1024 x i1>, ptr [[ARRAYIDX]], align 32
// O0-NEXT:    [[TMP2:%.*]] = call <256 x i1> @llvm.kvx.xmovefmv(<1024 x i1> [[TMP1]], i32 0)
// O0-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[O_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP3]], i64 0
// O0-NEXT:    store <256 x i1> [[TMP2]], ptr [[ARRAYIDX1]], align 32
// O0-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[A0_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[TMP4]], i64 0
// O0-NEXT:    [[TMP5:%.*]] = load <1024 x i1>, ptr [[ARRAYIDX2]], align 32
// O0-NEXT:    [[TMP6:%.*]] = call <256 x i1> @llvm.kvx.xmovefmv(<1024 x i1> [[TMP5]], i32 1)
// O0-NEXT:    [[TMP7:%.*]] = load ptr, ptr [[O_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP7]], i64 1
// O0-NEXT:    store <256 x i1> [[TMP6]], ptr [[ARRAYIDX3]], align 32
// O0-NEXT:    [[TMP8:%.*]] = load ptr, ptr [[A0_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[TMP8]], i64 0
// O0-NEXT:    [[TMP9:%.*]] = load <1024 x i1>, ptr [[ARRAYIDX4]], align 32
// O0-NEXT:    [[TMP10:%.*]] = call <256 x i1> @llvm.kvx.xmovefmv(<1024 x i1> [[TMP9]], i32 2)
// O0-NEXT:    [[TMP11:%.*]] = load ptr, ptr [[O_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX5:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP11]], i64 2
// O0-NEXT:    store <256 x i1> [[TMP10]], ptr [[ARRAYIDX5]], align 32
// O0-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A0_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[TMP12]], i64 0
// O0-NEXT:    [[TMP13:%.*]] = load <1024 x i1>, ptr [[ARRAYIDX6]], align 32
// O0-NEXT:    [[TMP14:%.*]] = call <256 x i1> @llvm.kvx.xmovefmv(<1024 x i1> [[TMP13]], i32 3)
// O0-NEXT:    [[TMP15:%.*]] = load ptr, ptr [[O_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX7:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP15]], i64 3
// O0-NEXT:    store <256 x i1> [[TMP14]], ptr [[ARRAYIDX7]], align 32
// O0-NEXT:    ret void
//
// O2-LABEL: @movefmv(
// O2-NEXT:  entry:
// O2-NEXT:    [[TMP0:%.*]] = load <1024 x i1>, ptr [[A0:%.*]], align 32, !tbaa [[TBAA6]]
// O2-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xmovefmv(<1024 x i1> [[TMP0]], i32 0)
// O2-NEXT:    store <256 x i1> [[TMP1]], ptr [[O:%.*]], align 32, !tbaa [[TBAA2]]
// O2-NEXT:    [[TMP2:%.*]] = tail call <256 x i1> @llvm.kvx.xmovefmv(<1024 x i1> [[TMP0]], i32 1)
// O2-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <256 x i1>, ptr [[O]], i64 1
// O2-NEXT:    store <256 x i1> [[TMP2]], ptr [[ARRAYIDX3]], align 32, !tbaa [[TBAA2]]
// O2-NEXT:    [[TMP3:%.*]] = tail call <256 x i1> @llvm.kvx.xmovefmv(<1024 x i1> [[TMP0]], i32 2)
// O2-NEXT:    [[ARRAYIDX5:%.*]] = getelementptr inbounds <256 x i1>, ptr [[O]], i64 2
// O2-NEXT:    store <256 x i1> [[TMP3]], ptr [[ARRAYIDX5]], align 32, !tbaa [[TBAA2]]
// O2-NEXT:    [[TMP4:%.*]] = tail call <256 x i1> @llvm.kvx.xmovefmv(<1024 x i1> [[TMP0]], i32 3)
// O2-NEXT:    [[ARRAYIDX7:%.*]] = getelementptr inbounds <256 x i1>, ptr [[O]], i64 3
// O2-NEXT:    store <256 x i1> [[TMP4]], ptr [[ARRAYIDX7]], align 32, !tbaa [[TBAA2]]
// O2-NEXT:    ret void
//
void movefmv(__kvx_x256 *o, __kvx_x1024 *a0) {
  o[0] = __builtin_kvx_xmovefmv(a0[0], 0);
  o[1] = __builtin_kvx_xmovefmv(a0[0], 1);
  o[2] = __builtin_kvx_xmovefmv(a0[0], 2);
  o[3] = __builtin_kvx_xmovefmv(a0[0], 3);
}

// O0-LABEL: @movefwv(
// O0-NEXT:  entry:
// O0-NEXT:    [[O_ADDR:%.*]] = alloca ptr, align 8
// O0-NEXT:    [[A0_ADDR:%.*]] = alloca ptr, align 8
// O0-NEXT:    store ptr [[O:%.*]], ptr [[O_ADDR]], align 8
// O0-NEXT:    store ptr [[A0:%.*]], ptr [[A0_ADDR]], align 8
// O0-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A0_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <512 x i1>, ptr [[TMP0]], i64 0
// O0-NEXT:    [[TMP1:%.*]] = load <512 x i1>, ptr [[ARRAYIDX]], align 32
// O0-NEXT:    [[TMP2:%.*]] = call <256 x i1> @llvm.kvx.xmovefwv(<512 x i1> [[TMP1]], i32 0)
// O0-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[O_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP3]], i64 0
// O0-NEXT:    store <256 x i1> [[TMP2]], ptr [[ARRAYIDX1]], align 32
// O0-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[A0_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds <512 x i1>, ptr [[TMP4]], i64 0
// O0-NEXT:    [[TMP5:%.*]] = load <512 x i1>, ptr [[ARRAYIDX2]], align 32
// O0-NEXT:    [[TMP6:%.*]] = call <256 x i1> @llvm.kvx.xmovefwv(<512 x i1> [[TMP5]], i32 1)
// O0-NEXT:    [[TMP7:%.*]] = load ptr, ptr [[O_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP7]], i64 1
// O0-NEXT:    store <256 x i1> [[TMP6]], ptr [[ARRAYIDX3]], align 32
// O0-NEXT:    ret void
//
// O2-LABEL: @movefwv(
// O2-NEXT:  entry:
// O2-NEXT:    [[TMP0:%.*]] = load <512 x i1>, ptr [[A0:%.*]], align 32, !tbaa [[TBAA8]]
// O2-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xmovefwv(<512 x i1> [[TMP0]], i32 0)
// O2-NEXT:    store <256 x i1> [[TMP1]], ptr [[O:%.*]], align 32, !tbaa [[TBAA2]]
// O2-NEXT:    [[TMP2:%.*]] = tail call <256 x i1> @llvm.kvx.xmovefwv(<512 x i1> [[TMP0]], i32 1)
// O2-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <256 x i1>, ptr [[O]], i64 1
// O2-NEXT:    store <256 x i1> [[TMP2]], ptr [[ARRAYIDX3]], align 32, !tbaa [[TBAA2]]
// O2-NEXT:    ret void
//
void movefwv(__kvx_x256 *o, __kvx_x512 *a0) {
  o[0] = __builtin_kvx_xmovefwv(a0[0], 0);
  o[1] = __builtin_kvx_xmovefwv(a0[0], 1);
}

// O0-LABEL: @buildfvm(
// O0-NEXT:  entry:
// O0-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// O0-NEXT:    [[M_ADDR:%.*]] = alloca ptr, align 8
// O0-NEXT:    store ptr [[A:%.*]], ptr [[A_ADDR]], align 8
// O0-NEXT:    store ptr [[M:%.*]], ptr [[M_ADDR]], align 8
// O0-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP0]], i64 0
// O0-NEXT:    [[TMP1:%.*]] = load <256 x i1>, ptr [[ARRAYIDX]], align 32
// O0-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP2]], i64 2
// O0-NEXT:    [[TMP3:%.*]] = load <256 x i1>, ptr [[ARRAYIDX1]], align 32
// O0-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP4]], i64 0
// O0-NEXT:    [[TMP5:%.*]] = load <256 x i1>, ptr [[ARRAYIDX2]], align 32
// O0-NEXT:    [[TMP6:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP6]], i64 1
// O0-NEXT:    [[TMP7:%.*]] = load <256 x i1>, ptr [[ARRAYIDX3]], align 32
// O0-NEXT:    [[TMP8:%.*]] = call <1024 x i1> @llvm.kvx.xbuild1024(<256 x i1> [[TMP1]], <256 x i1> [[TMP3]], <256 x i1> [[TMP5]], <256 x i1> [[TMP7]])
// O0-NEXT:    [[TMP9:%.*]] = load ptr, ptr [[M_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[TMP9]], i64 0
// O0-NEXT:    store <1024 x i1> [[TMP8]], ptr [[ARRAYIDX4]], align 32
// O0-NEXT:    ret void
//
// O2-LABEL: @buildfvm(
// O2-NEXT:  entry:
// O2-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[A:%.*]], align 32, !tbaa [[TBAA2]]
// O2-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <256 x i1>, ptr [[A]], i64 2
// O2-NEXT:    [[TMP1:%.*]] = load <256 x i1>, ptr [[ARRAYIDX1]], align 32, !tbaa [[TBAA2]]
// O2-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <256 x i1>, ptr [[A]], i64 1
// O2-NEXT:    [[TMP2:%.*]] = load <256 x i1>, ptr [[ARRAYIDX3]], align 32, !tbaa [[TBAA2]]
// O2-NEXT:    [[TMP3:%.*]] = tail call <1024 x i1> @llvm.kvx.xbuild1024(<256 x i1> [[TMP0]], <256 x i1> [[TMP1]], <256 x i1> [[TMP0]], <256 x i1> [[TMP2]])
// O2-NEXT:    store <1024 x i1> [[TMP3]], ptr [[M:%.*]], align 32, !tbaa [[TBAA6]]
// O2-NEXT:    ret void
//
void buildfvm(__kvx_x256 *a, __kvx_x1024 *M) {
  M[0] = __builtin_kvx_xbuild1024(a[0], a[2], a[0], a[1]);
}

// O0-LABEL: @buildfwm(
// O0-NEXT:  entry:
// O0-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// O0-NEXT:    [[M_ADDR:%.*]] = alloca ptr, align 8
// O0-NEXT:    store ptr [[A:%.*]], ptr [[A_ADDR]], align 8
// O0-NEXT:    store ptr [[M:%.*]], ptr [[M_ADDR]], align 8
// O0-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <512 x i1>, ptr [[TMP0]], i64 2
// O0-NEXT:    [[TMP1:%.*]] = load <512 x i1>, ptr [[ARRAYIDX]], align 32
// O0-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <512 x i1>, ptr [[TMP2]], i64 2
// O0-NEXT:    [[TMP3:%.*]] = load <512 x i1>, ptr [[ARRAYIDX1]], align 32
// O0-NEXT:    [[TMP4:%.*]] = call <1024 x i1> @llvm.kvx.cat.v1024i1(<512 x i1> [[TMP1]], <512 x i1> [[TMP3]])
// O0-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[M_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[TMP5]], i64 1
// O0-NEXT:    store <1024 x i1> [[TMP4]], ptr [[ARRAYIDX2]], align 32
// O0-NEXT:    [[TMP6:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <512 x i1>, ptr [[TMP6]], i64 2
// O0-NEXT:    [[TMP7:%.*]] = load <512 x i1>, ptr [[ARRAYIDX3]], align 32
// O0-NEXT:    [[TMP8:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds <512 x i1>, ptr [[TMP8]], i64 1
// O0-NEXT:    [[TMP9:%.*]] = load <512 x i1>, ptr [[ARRAYIDX4]], align 32
// O0-NEXT:    [[TMP10:%.*]] = call <1024 x i1> @llvm.kvx.cat.v1024i1(<512 x i1> [[TMP7]], <512 x i1> [[TMP9]])
// O0-NEXT:    [[TMP11:%.*]] = load ptr, ptr [[M_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX5:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[TMP11]], i64 0
// O0-NEXT:    store <1024 x i1> [[TMP10]], ptr [[ARRAYIDX5]], align 32
// O0-NEXT:    ret void
//
// O2-LABEL: @buildfwm(
// O2-NEXT:  entry:
// O2-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <512 x i1>, ptr [[A:%.*]], i64 2
// O2-NEXT:    [[TMP0:%.*]] = load <512 x i1>, ptr [[ARRAYIDX]], align 32, !tbaa [[TBAA8]]
// O2-NEXT:    [[TMP1:%.*]] = tail call <1024 x i1> @llvm.kvx.cat.v1024i1(<512 x i1> [[TMP0]], <512 x i1> [[TMP0]])
// O2-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[M:%.*]], i64 1
// O2-NEXT:    store <1024 x i1> [[TMP1]], ptr [[ARRAYIDX2]], align 32, !tbaa [[TBAA6]]
// O2-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds <512 x i1>, ptr [[A]], i64 1
// O2-NEXT:    [[TMP2:%.*]] = load <512 x i1>, ptr [[ARRAYIDX4]], align 32, !tbaa [[TBAA8]]
// O2-NEXT:    [[TMP3:%.*]] = tail call <1024 x i1> @llvm.kvx.cat.v1024i1(<512 x i1> [[TMP0]], <512 x i1> [[TMP2]])
// O2-NEXT:    store <1024 x i1> [[TMP3]], ptr [[M]], align 32, !tbaa [[TBAA6]]
// O2-NEXT:    ret void
//
void buildfwm(__kvx_x512 *a, __kvx_x1024 *M) {
  M[1] = __builtin_kvx_xcat1024(a[2], a[2]);
  M[0] = __builtin_kvx_xcat1024(a[2], a[1]);
}

// O0-LABEL: @buildfvw(
// O0-NEXT:  entry:
// O0-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// O0-NEXT:    [[W_ADDR:%.*]] = alloca ptr, align 8
// O0-NEXT:    store ptr [[A:%.*]], ptr [[A_ADDR]], align 8
// O0-NEXT:    store ptr [[W:%.*]], ptr [[W_ADDR]], align 8
// O0-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP0]], i64 0
// O0-NEXT:    [[TMP1:%.*]] = load <256 x i1>, ptr [[ARRAYIDX]], align 32
// O0-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <256 x i1>, ptr [[TMP2]], i64 2
// O0-NEXT:    [[TMP3:%.*]] = load <256 x i1>, ptr [[ARRAYIDX1]], align 32
// O0-NEXT:    [[TMP4:%.*]] = call <512 x i1> @llvm.kvx.cat.v512i1(<256 x i1> [[TMP1]], <256 x i1> [[TMP3]])
// O0-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[W_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds <512 x i1>, ptr [[TMP5]], i64 0
// O0-NEXT:    store <512 x i1> [[TMP4]], ptr [[ARRAYIDX2]], align 32
// O0-NEXT:    ret void
//
// O2-LABEL: @buildfvw(
// O2-NEXT:  entry:
// O2-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[A:%.*]], align 32, !tbaa [[TBAA2]]
// O2-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <256 x i1>, ptr [[A]], i64 2
// O2-NEXT:    [[TMP1:%.*]] = load <256 x i1>, ptr [[ARRAYIDX1]], align 32, !tbaa [[TBAA2]]
// O2-NEXT:    [[TMP2:%.*]] = tail call <512 x i1> @llvm.kvx.cat.v512i1(<256 x i1> [[TMP0]], <256 x i1> [[TMP1]])
// O2-NEXT:    store <512 x i1> [[TMP2]], ptr [[W:%.*]], align 32, !tbaa [[TBAA8]]
// O2-NEXT:    ret void
//
void buildfvw(__kvx_x256 *a, __kvx_x512 *W) {
  W[0] = __builtin_kvx_xcat512(a[0], a[2]);
}

// O0-LABEL: @xcat2048(
// O0-NEXT:  entry:
// O0-NEXT:    [[W_ADDR:%.*]] = alloca ptr, align 8
// O0-NEXT:    store ptr [[W:%.*]], ptr [[W_ADDR]], align 8
// O0-NEXT:    [[TMP0:%.*]] = call <2048 x i1> @llvm.kvx.cat.v2048i1(<1024 x i1> zeroinitializer, <1024 x i1> zeroinitializer)
// O0-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[W_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <2048 x i1>, ptr [[TMP1]], i64 0
// O0-NEXT:    store <2048 x i1> [[TMP0]], ptr [[ARRAYIDX]], align 32
// O0-NEXT:    ret void
//
// O2-LABEL: @xcat2048(
// O2-NEXT:  entry:
// O2-NEXT:    [[TMP0:%.*]] = tail call <2048 x i1> @llvm.kvx.cat.v2048i1(<1024 x i1> zeroinitializer, <1024 x i1> zeroinitializer)
// O2-NEXT:    store <2048 x i1> [[TMP0]], ptr [[W:%.*]], align 32, !tbaa [[TBAA10:![0-9]+]]
// O2-NEXT:    ret void
//
void xcat2048(__kvx_x2048 *W) {
  W[0] = __builtin_kvx_xcat2048(
              __builtin_kvx_xzero1024(),
              __builtin_kvx_xzero1024());
}

// O0-LABEL: @xcat4096(
// O0-NEXT:  entry:
// O0-NEXT:    [[W_ADDR:%.*]] = alloca ptr, align 8
// O0-NEXT:    store ptr [[W:%.*]], ptr [[W_ADDR]], align 8
// O0-NEXT:    [[TMP0:%.*]] = call <4096 x i1> @llvm.kvx.cat.v4096i1(<2048 x i1> zeroinitializer, <2048 x i1> zeroinitializer)
// O0-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[W_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <4096 x i1>, ptr [[TMP1]], i64 0
// O0-NEXT:    store <4096 x i1> [[TMP0]], ptr [[ARRAYIDX]], align 32
// O0-NEXT:    ret void
//
// O2-LABEL: @xcat4096(
// O2-NEXT:  entry:
// O2-NEXT:    [[TMP0:%.*]] = tail call <4096 x i1> @llvm.kvx.cat.v4096i1(<2048 x i1> zeroinitializer, <2048 x i1> zeroinitializer)
// O2-NEXT:    store <4096 x i1> [[TMP0]], ptr [[W:%.*]], align 32, !tbaa [[TBAA12:![0-9]+]]
// O2-NEXT:    ret void
//
void xcat4096(__kvx_x4096 *W) {
  W[0] = __builtin_kvx_xcat4096(
              __builtin_kvx_xzero2048(),
              __builtin_kvx_xzero2048());
}
