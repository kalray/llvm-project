// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -triple kvx-kalray-cos -target-cpu kv3-2 -S -emit-llvm %s -o - -O1 | FileCheck %s --check-prefixes=CHECK

#include "vector-types.h"

// CHECK-LABEL: @xmovefo256(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[A:%.*]], align 32, !tbaa [[TBAA2:![0-9]+]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <4 x i64> @llvm.kvx.xmovefo256(<256 x i1> [[TMP0]])
// CHECK-NEXT:    ret <4 x i64> [[TMP1]]
//
v4i64 xmovefo256(__kvx_x256 *a){
  return __builtin_kvx_xmovefo256(*a);
}

// CHECK-LABEL: @xmovefq256_h0(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[A:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <2 x i64> @llvm.kvx.xmovefq256(<256 x i1> [[TMP0]], i32 0)
// CHECK-NEXT:    ret <2 x i64> [[TMP1]]
//
v2i64 xmovefq256_h0(__kvx_x256 *a){
  return __builtin_kvx_xmovefq256(*a, ".h0");
}

// CHECK-LABEL: @xmovefq256_h1(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[A:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <2 x i64> @llvm.kvx.xmovefq256(<256 x i1> [[TMP0]], i32 1)
// CHECK-NEXT:    ret <2 x i64> [[TMP1]]
//
v2i64 xmovefq256_h1(__kvx_x256 *a){
  return __builtin_kvx_xmovefq256(*a, ".h1");
}

// CHECK-LABEL: @xmovefd256_q0(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[A:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 @llvm.kvx.xmovefd256(<256 x i1> [[TMP0]], i32 0)
// CHECK-NEXT:    ret i64 [[TMP1]]
//
unsigned long xmovefd256_q0(__kvx_x256 *a){
  return __builtin_kvx_xmovefd256(*a, ".q0");
}

// CHECK-LABEL: @xmovefd256_q1(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[A:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 @llvm.kvx.xmovefd256(<256 x i1> [[TMP0]], i32 1)
// CHECK-NEXT:    ret i64 [[TMP1]]
//
unsigned long xmovefd256_q1(__kvx_x256 *a){
  return __builtin_kvx_xmovefd256(*a, ".q1");
}

// CHECK-LABEL: @xmovefd256_q2(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[A:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 @llvm.kvx.xmovefd256(<256 x i1> [[TMP0]], i32 2)
// CHECK-NEXT:    ret i64 [[TMP1]]
//
unsigned long xmovefd256_q2(__kvx_x256 *a){
  return __builtin_kvx_xmovefd256(*a, ".q2");
}

// CHECK-LABEL: @xmovefd256_q3(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[A:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 @llvm.kvx.xmovefd256(<256 x i1> [[TMP0]], i32 3)
// CHECK-NEXT:    ret i64 [[TMP1]]
//
unsigned long xmovefd256_q3(__kvx_x256 *a){
  return __builtin_kvx_xmovefd256(*a, ".q3");
}

// CHECK-LABEL: @xmoveto256(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <256 x i1> @llvm.kvx.xmoveto256(<4 x i64> [[A:%.*]])
// CHECK-NEXT:    store <256 x i1> [[TMP0]], ptr [[C:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xmoveto256(__kvx_x256 *c, v4i64 a){
  *c = __builtin_kvx_xmoveto256(a);
}

// CHECK-LABEL: @xmovetq256_h0(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[C:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xmovetq256(<256 x i1> [[TMP0]], <2 x i64> [[A:%.*]], i32 0)
// CHECK-NEXT:    store <256 x i1> [[TMP1]], ptr [[C]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xmovetq256_h0(__kvx_x256 *c, v2i64 a){
  *c = __builtin_kvx_xmovetq256(*c, a, ".h0");
}

// CHECK-LABEL: @xmovetq256_h1(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[C:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xmovetq256(<256 x i1> [[TMP0]], <2 x i64> [[A:%.*]], i32 1)
// CHECK-NEXT:    store <256 x i1> [[TMP1]], ptr [[C]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xmovetq256_h1(__kvx_x256 *c, v2i64 a){
  *c = __builtin_kvx_xmovetq256(*c, a, ".h1");
}

// CHECK-LABEL: @xmovetd256_q0(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[C:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xmovetd256(<256 x i1> [[TMP0]], i64 [[A:%.*]], i32 0)
// CHECK-NEXT:    store <256 x i1> [[TMP1]], ptr [[C]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xmovetd256_q0(__kvx_x256 *c, unsigned long a){
  *c = __builtin_kvx_xmovetd256(*c, a, ".q0");
}

// CHECK-LABEL: @xmovetd256_q1(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[C:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xmovetd256(<256 x i1> [[TMP0]], i64 [[A:%.*]], i32 1)
// CHECK-NEXT:    store <256 x i1> [[TMP1]], ptr [[C]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xmovetd256_q1(__kvx_x256 *c, unsigned long a){
  *c = __builtin_kvx_xmovetd256(*c, a, ".q1");
}

// CHECK-LABEL: @xmovetd256_q2(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[C:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xmovetd256(<256 x i1> [[TMP0]], i64 [[A:%.*]], i32 2)
// CHECK-NEXT:    store <256 x i1> [[TMP1]], ptr [[C]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xmovetd256_q2(__kvx_x256 *c, unsigned long a){
  *c = __builtin_kvx_xmovetd256(*c, a, ".q2");
}

// CHECK-LABEL: @xmovetd256_q3(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[C:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xmovetd256(<256 x i1> [[TMP0]], i64 [[A:%.*]], i32 3)
// CHECK-NEXT:    store <256 x i1> [[TMP1]], ptr [[C]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xmovetd256_q3(__kvx_x256 *c, unsigned long a){
  *c = __builtin_kvx_xmovetd256(*c, a, ".q3");
}
