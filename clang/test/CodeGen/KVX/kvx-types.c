// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -triple kvx-kalray-cos -emit-llvm %s -O3 -o - | FileCheck %s

/* Tests the KVX types such as __kvx_v8qi, defined as macros to the appropriate
 * vector types in LLVM */

// CHECK-LABEL: @qi(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[VECEXT:%.*]] = extractelement <2 x i8> [[V2:%.*]], i64 1
// CHECK-NEXT:    [[VECEXT1:%.*]] = extractelement <4 x i8> [[V4:%.*]], i64 3
// CHECK-NEXT:    [[ADD:%.*]] = add i8 [[VECEXT1]], [[VECEXT]]
// CHECK-NEXT:    [[VECEXT3:%.*]] = extractelement <8 x i8> [[V8:%.*]], i64 7
// CHECK-NEXT:    [[ADD5:%.*]] = add i8 [[ADD]], [[VECEXT3]]
// CHECK-NEXT:    [[VECEXT6:%.*]] = extractelement <16 x i8> [[V16:%.*]], i64 15
// CHECK-NEXT:    [[ADD8:%.*]] = add i8 [[ADD5]], [[VECEXT6]]
// CHECK-NEXT:    [[VECEXT9:%.*]] = extractelement <32 x i8> [[V32:%.*]], i64 31
// CHECK-NEXT:    [[ADD11:%.*]] = add i8 [[ADD8]], [[VECEXT9]]
// CHECK-NEXT:    [[VECEXT12:%.*]] = extractelement <64 x i8> [[V64:%.*]], i64 63
// CHECK-NEXT:    [[ADD14:%.*]] = add i8 [[ADD11]], [[VECEXT12]]
// CHECK-NEXT:    ret i8 [[ADD14]]
//
char qi(__kvx_v2qi v2, __kvx_v4qi v4, __kvx_v8qi v8, __kvx_v16qi v16, __kvx_v32qi v32, __kvx_v64qi v64) {
  return v2[1] + v4[3] + v8[7] + v16[15] + v32[31] + v64[63];
}

// CHECK-LABEL: @hi(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[VECEXT:%.*]] = extractelement <2 x i16> [[V2:%.*]], i64 1
// CHECK-NEXT:    [[VECEXT1:%.*]] = extractelement <4 x i16> [[V4:%.*]], i64 3
// CHECK-NEXT:    [[ADD:%.*]] = add i16 [[VECEXT1]], [[VECEXT]]
// CHECK-NEXT:    [[VECEXT3:%.*]] = extractelement <8 x i16> [[V8:%.*]], i64 7
// CHECK-NEXT:    [[ADD5:%.*]] = add i16 [[ADD]], [[VECEXT3]]
// CHECK-NEXT:    [[VECEXT6:%.*]] = extractelement <16 x i16> [[V16:%.*]], i64 15
// CHECK-NEXT:    [[ADD8:%.*]] = add i16 [[ADD5]], [[VECEXT6]]
// CHECK-NEXT:    [[VECEXT9:%.*]] = extractelement <32 x i16> [[V32:%.*]], i64 31
// CHECK-NEXT:    [[ADD11:%.*]] = add i16 [[ADD8]], [[VECEXT9]]
// CHECK-NEXT:    ret i16 [[ADD11]]
//
short hi(__kvx_v2hi v2, __kvx_v4hi v4, __kvx_v8hi v8, __kvx_v16hi v16, __kvx_v32hi v32) {
  return v2[1] + v4[3] + v8[7] + v16[15] + v32[31];
}

// CHECK-LABEL: @si(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[VECEXT:%.*]] = extractelement <2 x i32> [[V2:%.*]], i64 1
// CHECK-NEXT:    [[VECEXT1:%.*]] = extractelement <4 x i32> [[V4:%.*]], i64 3
// CHECK-NEXT:    [[ADD:%.*]] = add nsw i32 [[VECEXT1]], [[VECEXT]]
// CHECK-NEXT:    [[VECEXT2:%.*]] = extractelement <8 x i32> [[V8:%.*]], i64 7
// CHECK-NEXT:    [[ADD3:%.*]] = add nsw i32 [[ADD]], [[VECEXT2]]
// CHECK-NEXT:    [[VECEXT4:%.*]] = extractelement <16 x i32> [[V16:%.*]], i64 15
// CHECK-NEXT:    [[ADD5:%.*]] = add nsw i32 [[ADD3]], [[VECEXT4]]
// CHECK-NEXT:    ret i32 [[ADD5]]
//
int si(__kvx_v2si v2, __kvx_v4si v4, __kvx_v8si v8, __kvx_v16si v16) {
  return v2[1] + v4[3] + v8[7] + v16[15];
}

// CHECK-LABEL: @di(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[VECEXT:%.*]] = extractelement <2 x i64> [[V2:%.*]], i64 1
// CHECK-NEXT:    [[VECEXT1:%.*]] = extractelement <4 x i64> [[V4:%.*]], i64 3
// CHECK-NEXT:    [[ADD:%.*]] = add nsw i64 [[VECEXT1]], [[VECEXT]]
// CHECK-NEXT:    [[VECEXT2:%.*]] = extractelement <8 x i64> [[V8:%.*]], i64 7
// CHECK-NEXT:    [[ADD3:%.*]] = add nsw i64 [[ADD]], [[VECEXT2]]
// CHECK-NEXT:    ret i64 [[ADD3]]
//
long di(__kvx_v2di v2, __kvx_v4di v4, __kvx_v8di v8) {
  return v2[1] + v4[3] + v8[7];
}

// CHECK-LABEL: @hf(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[VECEXT:%.*]] = extractelement <4 x half> [[V4:%.*]], i64 3
// CHECK-NEXT:    [[VECEXT1:%.*]] = extractelement <8 x half> [[V8:%.*]], i64 7
// CHECK-NEXT:    [[ADD:%.*]] = fadd half [[VECEXT]], [[VECEXT1]]
// CHECK-NEXT:    [[VECEXT2:%.*]] = extractelement <16 x half> [[V16:%.*]], i64 15
// CHECK-NEXT:    [[ADD3:%.*]] = fadd half [[ADD]], [[VECEXT2]]
// CHECK-NEXT:    [[VECEXT4:%.*]] = extractelement <32 x half> [[V32:%.*]], i64 31
// CHECK-NEXT:    [[ADD5:%.*]] = fadd half [[ADD3]], [[VECEXT4]]
// CHECK-NEXT:    ret half [[ADD5]]
//
_Float16 hf(__kvx_v4hf v4, __kvx_v8hf v8, __kvx_v16hf v16, __kvx_v32hf v32) {
  return v4[3] + v8[7] + v16[15] + v32[31];
}

// CHECK-LABEL: @sf(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[VECEXT:%.*]] = extractelement <2 x float> [[V2:%.*]], i64 1
// CHECK-NEXT:    [[VECEXT1:%.*]] = extractelement <4 x float> [[V4:%.*]], i64 3
// CHECK-NEXT:    [[ADD:%.*]] = fadd float [[VECEXT]], [[VECEXT1]]
// CHECK-NEXT:    [[VECEXT2:%.*]] = extractelement <8 x float> [[V8:%.*]], i64 7
// CHECK-NEXT:    [[ADD3:%.*]] = fadd float [[ADD]], [[VECEXT2]]
// CHECK-NEXT:    [[VECEXT4:%.*]] = extractelement <16 x float> [[V16:%.*]], i64 15
// CHECK-NEXT:    [[ADD5:%.*]] = fadd float [[ADD3]], [[VECEXT4]]
// CHECK-NEXT:    ret float [[ADD5]]
//
float sf(__kvx_v2sf v2, __kvx_v4sf v4, __kvx_v8sf v8, __kvx_v16sf v16) {
  return v2[1] + v4[3] + v8[7] + v16[15];
}

// CHECK-LABEL: @df(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[VECEXT:%.*]] = extractelement <2 x double> [[V2:%.*]], i64 1
// CHECK-NEXT:    [[VECEXT1:%.*]] = extractelement <4 x double> [[V4:%.*]], i64 3
// CHECK-NEXT:    [[ADD:%.*]] = fadd double [[VECEXT]], [[VECEXT1]]
// CHECK-NEXT:    [[VECEXT2:%.*]] = extractelement <8 x double> [[V8:%.*]], i64 7
// CHECK-NEXT:    [[ADD3:%.*]] = fadd double [[ADD]], [[VECEXT2]]
// CHECK-NEXT:    ret double [[ADD3]]
//
double df(__kvx_v2df v2, __kvx_v4df v4, __kvx_v8df v8) {
  return v2[1] + v4[3] + v8[7];
}

// CHECK-LABEL: @test_vector(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ADD_PTR:%.*]] = getelementptr inbounds i8, ptr [[PTR1:%.*]], i64 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[ADD_PTR]], align 32, !tbaa [[TBAA2:![0-9]+]]
// CHECK-NEXT:    [[ADD_PTR1:%.*]] = getelementptr inbounds i8, ptr [[PTR2:%.*]], i64 32
// CHECK-NEXT:    store <256 x i1> [[TMP0]], ptr [[ADD_PTR1]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
void test_vector(void *ptr1, void *ptr2) {
  __kvx_x256 *inp = (__kvx_x256 *)ptr1;
  __kvx_x256 *outp = (__kvx_x256 *)ptr2;
  *(outp + 1) = *(inp + 2);
}

// CHECK-LABEL: @qu(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[VECEXT:%.*]] = extractelement <2 x i8> [[V2:%.*]], i64 1
// CHECK-NEXT:    [[VECEXT1:%.*]] = extractelement <4 x i8> [[V4:%.*]], i64 3
// CHECK-NEXT:    [[VECEXT3:%.*]] = extractelement <8 x i8> [[V8:%.*]], i64 7
// CHECK-NEXT:    [[VECEXT6:%.*]] = extractelement <16 x i8> [[V16:%.*]], i64 15
// CHECK-NEXT:    [[VECEXT9:%.*]] = extractelement <32 x i8> [[V32:%.*]], i64 31
// CHECK-NEXT:    [[VECEXT12:%.*]] = extractelement <64 x i8> [[V64:%.*]], i64 63
// CHECK-NEXT:    [[ADD:%.*]] = add i8 [[VECEXT9]], [[VECEXT12]]
// CHECK-NEXT:    [[ADD5:%.*]] = add i8 [[ADD]], [[VECEXT6]]
// CHECK-NEXT:    [[ADD8:%.*]] = add i8 [[ADD5]], [[VECEXT3]]
// CHECK-NEXT:    [[ADD11:%.*]] = add i8 [[ADD8]], [[VECEXT1]]
// CHECK-NEXT:    [[ADD14:%.*]] = add i8 [[ADD11]], [[VECEXT]]
// CHECK-NEXT:    ret i8 [[ADD14]]
//
unsigned char qu(__kvx_v64qu v64, __kvx_v32qu v32, __kvx_v16qu v16, __kvx_v8qu v8, __kvx_v4qu v4, __kvx_v2qu v2) {
  return v2[1] + v4[3] + v8[7] + v16[15] + v32[31] + v64[63];
}

// CHECK-LABEL: @hu(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[VECEXT:%.*]] = extractelement <2 x i16> [[V2:%.*]], i64 1
// CHECK-NEXT:    [[VECEXT1:%.*]] = extractelement <4 x i16> [[V4:%.*]], i64 3
// CHECK-NEXT:    [[VECEXT3:%.*]] = extractelement <8 x i16> [[V8:%.*]], i64 7
// CHECK-NEXT:    [[VECEXT6:%.*]] = extractelement <16 x i16> [[V16:%.*]], i64 15
// CHECK-NEXT:    [[VECEXT9:%.*]] = extractelement <32 x i16> [[V32:%.*]], i64 31
// CHECK-NEXT:    [[ADD:%.*]] = add i16 [[VECEXT6]], [[VECEXT9]]
// CHECK-NEXT:    [[ADD5:%.*]] = add i16 [[ADD]], [[VECEXT3]]
// CHECK-NEXT:    [[ADD8:%.*]] = add i16 [[ADD5]], [[VECEXT1]]
// CHECK-NEXT:    [[ADD11:%.*]] = add i16 [[ADD8]], [[VECEXT]]
// CHECK-NEXT:    ret i16 [[ADD11]]
//
unsigned short hu(__kvx_v32hu v32, __kvx_v16hu v16, __kvx_v8hu v8, __kvx_v4hu v4, __kvx_v2hu v2) {
  return v2[1] + v4[3] + v8[7] + v16[15] + v32[31];
}

// CHECK-LABEL: @su(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[VECEXT:%.*]] = extractelement <2 x i32> [[V2:%.*]], i64 1
// CHECK-NEXT:    [[VECEXT1:%.*]] = extractelement <4 x i32> [[V4:%.*]], i64 3
// CHECK-NEXT:    [[ADD:%.*]] = add i32 [[VECEXT1]], [[VECEXT]]
// CHECK-NEXT:    [[VECEXT2:%.*]] = extractelement <8 x i32> [[V8:%.*]], i64 7
// CHECK-NEXT:    [[ADD3:%.*]] = add i32 [[ADD]], [[VECEXT2]]
// CHECK-NEXT:    [[VECEXT4:%.*]] = extractelement <16 x i32> [[V16:%.*]], i64 15
// CHECK-NEXT:    [[ADD5:%.*]] = add i32 [[ADD3]], [[VECEXT4]]
// CHECK-NEXT:    ret i32 [[ADD5]]
//
unsigned int su(__kvx_v2su v2, __kvx_v4su v4, __kvx_v8su v8, __kvx_v16su v16) {
  return v2[1] + v4[3] + v8[7] + v16[15];
}

// CHECK-LABEL: @du(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[VECEXT:%.*]] = extractelement <2 x i64> [[V2:%.*]], i64 1
// CHECK-NEXT:    [[VECEXT1:%.*]] = extractelement <4 x i64> [[V4:%.*]], i64 3
// CHECK-NEXT:    [[ADD:%.*]] = add i64 [[VECEXT1]], [[VECEXT]]
// CHECK-NEXT:    [[VECEXT2:%.*]] = extractelement <8 x i64> [[V8:%.*]], i64 7
// CHECK-NEXT:    [[ADD3:%.*]] = add i64 [[ADD]], [[VECEXT2]]
// CHECK-NEXT:    ret i64 [[ADD3]]
//
unsigned long du(__kvx_v2du v2, __kvx_v4du v4, __kvx_v8du v8) {
  return v2[1] + v4[3] + v8[7];
}

// CHECK-LABEL: @zext(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV:%.*]] = zext <2 x i32> [[A:%.*]] to <2 x i64>
// CHECK-NEXT:    ret <2 x i64> [[CONV]]
//
__kvx_v2du zext(__kvx_v2su a) {
  return __builtin_convertvector (a, __kvx_v2du);
}

// CHECK-LABEL: @sext(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV:%.*]] = sext <2 x i32> [[A:%.*]] to <2 x i64>
// CHECK-NEXT:    ret <2 x i64> [[CONV]]
//
__kvx_v2di sext(__kvx_v2si a) {
  return __builtin_convertvector (a, __kvx_v2di);
}
