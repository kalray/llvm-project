// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -triple kvx-kalray-cos -S -O2 -emit-llvm -o - %s | FileCheck %s

// CHECK-LABEL: @abdw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.abdw(i32 [[V1:%.*]], i32 [[V2:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int abdw(int v1, int v2) {
  return __builtin_kvx_abdw(v1, v2);
}

// CHECK-LABEL: @abdd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.abdd(i64 [[V1:%.*]], i64 [[V2:%.*]])
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long abdd(long v1, long v2) {
  return __builtin_kvx_abdd(v1, v2);
}

/**
 * TODO Reintroduce addsw - addsd once string modifiers are there
 */

// int addsw(int v1, int v2) {
//   return __builtin_kvx_addsw(v1, v2);
// }

// long addsd(long v1, long v2) {
//   return __builtin_kvx_addsd(v1, v2);
// }

/**
 * TODO Reintroduce sbfsw - sbfsd once string modifiers are there
 */

// int sbfsw(int v1, int v2) {
//   return __builtin_kvx_sbfsw(v1, v2);
// }

// long sbfsd(long v1, long v2) {
//   return __builtin_kvx_sbfsd(v1, v2);
// }

// CHECK-LABEL: @addcd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.addcd(i64 [[V1:%.*]], i64 [[V2:%.*]], i32 0)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long addcd(long v1, long v2) {
  return __builtin_kvx_addcd(v1, v2, "");
}

// CHECK-LABEL: @addcdi(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.addcd(i64 [[V1:%.*]], i64 [[V2:%.*]], i32 1)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long addcdi(long v1, long v2) {
  return __builtin_kvx_addcd(v1, v2, ".i");
}

// CHECK-LABEL: @sbfcd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.sbfcd(i64 [[V2:%.*]], i64 [[V1:%.*]], i32 0)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long sbfcd(long v1, long v2) {
  return __builtin_kvx_sbfcd(v1, v2, "");
}

// CHECK-LABEL: @sbfcdi(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.sbfcd(i64 [[V2:%.*]], i64 [[V1:%.*]], i32 1)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long sbfcdi(long v1, long v2) {
  return __builtin_kvx_sbfcd(v1, v2, ".i");
}

// CHECK-LABEL: @avgw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.avgw(i32 [[V1:%.*]], i32 [[V2:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int avgw(int v1, int v2) {
  return __builtin_kvx_avgw(v1, v2, "");
}

// CHECK-LABEL: @avguw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.avguw(i32 [[V1:%.*]], i32 [[V2:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int avguw(unsigned int v1, unsigned int v2) {
  return __builtin_kvx_avgw(v1, v2, ".u");
}

// CHECK-LABEL: @avgrw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.avgrw(i32 [[V1:%.*]], i32 [[V2:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int avgrw(int v1, int v2) {
  return __builtin_kvx_avgw(v1, v2, ".r");
}

// CHECK-LABEL: @avgruw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.avgruw(i32 [[V1:%.*]], i32 [[V2:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int avgruw(int v1, int v2) {
  return __builtin_kvx_avgw(v1, v2, ".ru");
}

// CHECK-LABEL: @fnegw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.fnegw(float [[V:%.*]])
// CHECK-NEXT:    ret float [[TMP0]]
//
float fnegw(float v) {
  return __builtin_kvx_fnegw(v);
}

// CHECK-LABEL: @fnegd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.fnegd(double [[V:%.*]])
// CHECK-NEXT:    ret double [[TMP0]]
//
double fnegd(double v) {
  return __builtin_kvx_fnegd(v);
}

// CHECK-LABEL: @fmaxh(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = insertelement <4 x half> undef, half [[V1:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = insertelement <4 x half> undef, half [[V2:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <4 x half> @llvm.kvx.fmaxhq(<4 x half> [[TMP0]], <4 x half> [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <4 x half> [[TMP2]], i64 0
// CHECK-NEXT:    ret half [[TMP3]]
//
_Float16 fmaxh(_Float16 v1, _Float16 v2) {
  return __builtin_kvx_fmaxh(v1, v2);
}

// CHECK-LABEL: @fmaxw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.fmaxw(float [[V1:%.*]], float [[V2:%.*]])
// CHECK-NEXT:    ret float [[TMP0]]
//
float fmaxw(float v1, float v2) {
  return __builtin_kvx_fmaxw(v1, v2);
}

// CHECK-LABEL: @fmaxd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.fmaxd(double [[V1:%.*]], double [[V2:%.*]])
// CHECK-NEXT:    ret double [[TMP0]]
//
double fmaxd(double v1, double v2) {
  return __builtin_kvx_fmaxd(v1, v2);
}

// CHECK-LABEL: @fminh(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = insertelement <4 x half> undef, half [[V1:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = insertelement <4 x half> undef, half [[V2:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <4 x half> @llvm.kvx.fminhq(<4 x half> [[TMP0]], <4 x half> [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <4 x half> [[TMP2]], i64 0
// CHECK-NEXT:    ret half [[TMP3]]
//
_Float16 fminh(_Float16 v1, _Float16 v2) {
  return __builtin_kvx_fminh(v1, v2);
}

// CHECK-LABEL: @fminw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.fminw(float [[V1:%.*]], float [[V2:%.*]])
// CHECK-NEXT:    ret float [[TMP0]]
//
float fminw(float v1, float v2) {
  return __builtin_kvx_fminw(v1, v2);
}

// CHECK-LABEL: @fmind(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.fmind(double [[V1:%.*]], double [[V2:%.*]])
// CHECK-NEXT:    ret double [[TMP0]]
//
double fmind(double v1, double v2) {
  return __builtin_kvx_fmind(v1, v2);
}

// CHECK-LABEL: @frecw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.frecw(float [[A:%.*]], i32 3, i32 0)
// CHECK-NEXT:    ret float [[TMP0]]
//
float frecw(float a) {
  return __builtin_kvx_frecw(a, ".rz");
}

// CHECK-LABEL: @frsrw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.frsrw(float [[A:%.*]], i32 3, i32 0)
// CHECK-NEXT:    ret float [[TMP0]]
//
float frsrw(float a) {
  return __builtin_kvx_frsrw(a, ".rz");
}

// CHECK-LABEL: @faddw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.faddw(float [[V1:%.*]], float [[V2:%.*]], i32 3, i32 0)
// CHECK-NEXT:    ret float [[TMP0]]
//
float faddw(float v1, float v2) {
  return __builtin_kvx_faddw(v1, v2, ".rz");
}

// CHECK-LABEL: @faddd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV:%.*]] = fpext float [[V1:%.*]] to double
// CHECK-NEXT:    [[CONV1:%.*]] = fpext float [[V2:%.*]] to double
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.faddd(double [[CONV]], double [[CONV1]], i32 3, i32 0)
// CHECK-NEXT:    ret double [[TMP0]]
//
double faddd(float v1, float v2) {
  return __builtin_kvx_faddd(v1, v2, ".rz");
}

// CHECK-LABEL: @fsbfw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.fsbfw(float [[V1:%.*]], float [[V2:%.*]], i32 3, i32 0)
// CHECK-NEXT:    ret float [[TMP0]]
//
float fsbfw(float v1, float v2) {
  return __builtin_kvx_fsbfw(v1, v2, ".rz");
}

// CHECK-LABEL: @fsbfd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.fsbfd(double [[V1:%.*]], double [[V2:%.*]], i32 3, i32 0)
// CHECK-NEXT:    ret double [[TMP0]]
//
double fsbfd(double v1, double v2) {
  return __builtin_kvx_fsbfd(v1, v2, ".rz");
}

// CHECK-LABEL: @fmulh(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = insertelement <4 x half> undef, half [[A:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = insertelement <4 x half> undef, half [[B:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <4 x half> @llvm.kvx.fmulhq(<4 x half> [[TMP0]], <4 x half> [[TMP1]], i32 1, i32 1)
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <4 x half> [[TMP2]], i64 0
// CHECK-NEXT:    ret half [[TMP3]]
//
_Float16 fmulh(_Float16 a, _Float16 b){
  return __builtin_kvx_fmulh(a, b, ".ru.s");
}

// CHECK-LABEL: @fmulw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.fmulw(float [[V1:%.*]], float [[V2:%.*]], i32 3, i32 0)
// CHECK-NEXT:    ret float [[TMP0]]
//
float fmulw(float v1, float v2) {
  return __builtin_kvx_fmulw(v1, v2, ".rz");
}

// CHECK-LABEL: @fmuld(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.fmuld(double [[V1:%.*]], double [[V2:%.*]], i32 3, i32 0)
// CHECK-NEXT:    ret double [[TMP0]]
//
double fmuld(double v1, double v2) {
  return __builtin_kvx_fmuld(v1, v2, ".rz");
}

// CHECK-LABEL: @ffmah(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = insertelement <4 x half> undef, half [[A:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = insertelement <4 x half> undef, half [[B:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = insertelement <4 x half> undef, half [[C:%.*]], i64 0
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <4 x half> @llvm.kvx.ffmahq(<4 x half> [[TMP0]], <4 x half> [[TMP1]], <4 x half> [[TMP2]], i32 1, i32 1)
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <4 x half> [[TMP3]], i64 0
// CHECK-NEXT:    ret half [[TMP4]]
//
_Float16 ffmah(_Float16 a, _Float16 b, _Float16 c){
  return __builtin_kvx_ffmah(a, b, c, ".ru.s");
}

// CHECK-LABEL: @ffmaw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.ffmaw(float [[A:%.*]], float [[B:%.*]], float [[C:%.*]], i32 3, i32 0)
// CHECK-NEXT:    ret float [[TMP0]]
//
float ffmaw(float a, float b, float c) {
  return __builtin_kvx_ffmaw(a, b, c, ".rz");
}

// CHECK-LABEL: @ffmad(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.ffmad(double [[A:%.*]], double [[B:%.*]], double [[C:%.*]], i32 3, i32 0)
// CHECK-NEXT:    ret double [[TMP0]]
//
double ffmad(double a, double b, double c) {
  return __builtin_kvx_ffmad(a, b, c, ".rz");
}

// CHECK-LABEL: @ffmawd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.ffmawd(float [[A:%.*]], float [[B:%.*]], double [[C:%.*]], i32 3, i32 0)
// CHECK-NEXT:    ret double [[TMP0]]
//
double ffmawd(float a, float b, double c) {
  return __builtin_kvx_ffmawd(a, b, c, ".rz");
}

// CHECK-LABEL: @ffmsw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.ffmsw(float [[A:%.*]], float [[B:%.*]], float [[C:%.*]], i32 3, i32 0)
// CHECK-NEXT:    ret float [[TMP0]]
//
float ffmsw(float a, float b, float c) {
  return __builtin_kvx_ffmsw(a, b, c, ".rz");
}

// CHECK-LABEL: @ffmsw_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.ffmsw(float [[A:%.*]], float [[B:%.*]], float [[C:%.*]], i32 7, i32 0)
// CHECK-NEXT:    ret float [[TMP0]]
//
float ffmsw_(float a, float b, float c) {
  return __builtin_kvx_ffmsw(a, b, c, "");
}

// CHECK-LABEL: @ffmsd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.ffmsd(double [[A:%.*]], double [[B:%.*]], double [[C:%.*]], i32 3, i32 0)
// CHECK-NEXT:    ret double [[TMP0]]
//
double ffmsd(double a, double b, double c) {
  return __builtin_kvx_ffmsd(a, b, c, ".rz");
}

// CHECK-LABEL: @ffmsxwd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.ffmswd(float [[A:%.*]], float [[B:%.*]], double [[C:%.*]], i32 3, i32 0)
// CHECK-NEXT:    ret double [[TMP0]]
//
double ffmsxwd(float a, float b, double c) {
  return __builtin_kvx_ffmsxwd(a, b, c, ".rz");
}

// CHECK-LABEL: @floatw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.floatw(i32 [[X:%.*]], i64 3, i32 0, i32 0)
// CHECK-NEXT:    ret float [[TMP0]]
//
float floatw(int x) {
  return __builtin_kvx_floatw(x, 3, ".rn");
}

// CHECK-LABEL: @floatd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.floatd(i64 [[X:%.*]], i64 3, i32 0, i32 0)
// CHECK-NEXT:    ret double [[TMP0]]
//
double floatd(long x) {
  return __builtin_kvx_floatd(x, 3, ".rn");
}

// CHECK-LABEL: @floatw_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.floatw(i32 [[X:%.*]], i64 3, i32 0, i32 1)
// CHECK-NEXT:    ret float [[TMP0]]
//
float floatw_s(int x) {
  return __builtin_kvx_floatw(x, 3, ".rn.s");
}

// CHECK-LABEL: @floatd_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.floatd(i64 [[X:%.*]], i64 3, i32 0, i32 1)
// CHECK-NEXT:    ret double [[TMP0]]
//
double floatd_s(long x) {
  return __builtin_kvx_floatd(x, 3, ".rn.s");
}

// CHECK-LABEL: @floatd_limit(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.floatd(i64 [[X:%.*]], i64 63, i32 0, i32 0)
// CHECK-NEXT:    ret double [[TMP0]]
//
double floatd_limit(long x) {
  return __builtin_kvx_floatd(x, 63, ".rn");
}

// CHECK-LABEL: @floatuw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.floatuw(i32 [[X:%.*]], i64 3, i32 3, i32 0)
// CHECK-NEXT:    ret float [[TMP0]]
//
float floatuw(unsigned int x) {
  return __builtin_kvx_floatuw(x, 3, ".rz");
}

// CHECK-LABEL: @floatud(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.floatud(i64 [[X:%.*]], i64 3, i32 3, i32 0)
// CHECK-NEXT:    ret double [[TMP0]]
//
double floatud(unsigned long x) {
  return __builtin_kvx_floatud(x, 3, ".rz");
}

// CHECK-LABEL: @fixedw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.fixedw(float [[X:%.*]], i64 3, i32 0, i32 0)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int fixedw(float x) {
  return __builtin_kvx_fixedw(x, 3, ".rn");
}

// CHECK-LABEL: @fixedd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.fixedd(double [[X:%.*]], i64 3, i32 0, i32 0)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long fixedd(double x) {
  return __builtin_kvx_fixedd(x, 3, ".rn");
}

// CHECK-LABEL: @fixedw_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.fixedw(float [[X:%.*]], i64 3, i32 0, i32 1)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int fixedw_s(float x) {
  return __builtin_kvx_fixedw(x, 3, ".rn.s");
}

// CHECK-LABEL: @fixedd_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.fixedd(double [[X:%.*]], i64 3, i32 0, i32 1)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long fixedd_s(double x) {
  return __builtin_kvx_fixedd(x, 3, ".rn.s");
}

// CHECK-LABEL: @fixedw_s_only(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.fixedw(float [[X:%.*]], i64 3, i32 7, i32 1)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int fixedw_s_only(float x) {
  return __builtin_kvx_fixedw(x, 3, ".s");
}

// CHECK-LABEL: @fixedd_s_only(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.fixedd(double [[X:%.*]], i64 3, i32 7, i32 1)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long fixedd_s_only(double x) {
  return __builtin_kvx_fixedd(x, 3, ".s");
}

// CHECK-LABEL: @fixeduw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.fixeduw(float [[X:%.*]], i64 3, i32 3, i32 0)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int fixeduw(float x) {
  return __builtin_kvx_fixeduw(x, 3, ".rz");
}

// CHECK-LABEL: @fixedud(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.fixedud(double [[X:%.*]], i64 3, i32 3, i32 0)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
unsigned long fixedud(double x) {
  return __builtin_kvx_fixedud(x, 3, ".rz");
}

// CHECK-LABEL: @fcdivw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.fcdivw(float [[V1:%.*]], float [[V2:%.*]], i32 0)
// CHECK-NEXT:    ret float [[TMP0]]
//
float fcdivw(float v1, float v2) {
  return __builtin_kvx_fcdivw(v1, v2, "");
}

// CHECK-LABEL: @fcdivd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.fcdivd(double [[V1:%.*]], double [[V2:%.*]], i32 0)
// CHECK-NEXT:    ret double [[TMP0]]
//
double fcdivd(double v1, double v2) {
  return __builtin_kvx_fcdivd(v1, v2, "");
}

// CHECK-LABEL: @fsdivw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.fsdivw(float [[V1:%.*]], float [[V2:%.*]], i32 0)
// CHECK-NEXT:    ret float [[TMP0]]
//
float fsdivw(float v1, float v2) {
  return __builtin_kvx_fsdivw(v1, v2, "");
}

// CHECK-LABEL: @fsdivd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.fsdivd(double [[V1:%.*]], double [[V2:%.*]], i32 0)
// CHECK-NEXT:    ret double [[TMP0]]
//
double fsdivd(double v1, double v2) {
  return __builtin_kvx_fsdivd(v1, v2, "");
}

// CHECK-LABEL: @fsrecw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.fsrecw(float [[V:%.*]], i32 0)
// CHECK-NEXT:    ret float [[TMP0]]
//
float fsrecw(float v) {
  return __builtin_kvx_fsrecw(v, "");
}

// CHECK-LABEL: @fsrecd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.fsrecd(double [[V:%.*]], i32 0)
// CHECK-NEXT:    ret double [[TMP0]]
//
double fsrecd(double v) {
  return __builtin_kvx_fsrecd(v, "");
}

// CHECK-LABEL: @sbmm8(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.sbmm8(i64 [[A:%.*]], i64 [[B:%.*]])
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long sbmm8(long a, long b) {
  return __builtin_kvx_sbmm8(a, b);
}

// CHECK-LABEL: @sbmmt8(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.sbmmt8(i64 [[A:%.*]], i64 [[B:%.*]])
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long sbmmt8(long a, long b) {
  return __builtin_kvx_sbmmt8(a, b);
}

// CHECK-LABEL: @stsuw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.stsuw(i32 [[X:%.*]], i32 [[Y:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int stsuw(unsigned int x, unsigned int y) {
  return __builtin_kvx_stsuw(x, y);
}

// CHECK-LABEL: @stsud(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.stsud(i64 [[X:%.*]], i64 [[Y:%.*]])
// CHECK-NEXT:    ret i64 [[TMP0]]
//
unsigned long stsud(unsigned long x, unsigned long y) {
  return __builtin_kvx_stsud(x, y);
}

// CHECK-LABEL: @fnarrowwh(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call half @llvm.kvx.fnarrowwh(float [[V:%.*]], i32 1, i32 1)
// CHECK-NEXT:    ret half [[TMP0]]
//
_Float16 fnarrowwh(float v) {
  return __builtin_kvx_fnarrowwh(v, ".ru.s");
}

// CHECK-LABEL: @fnarrowdw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.fnarrowdw(double [[V:%.*]], i32 1, i32 1)
// CHECK-NEXT:    ret float [[TMP0]]
//
float fnarrowdw(double v) {
  return __builtin_kvx_fnarrowdw(v, ".ru.s");
}
