// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -triple kvx-kalray-cos -S -O2 -emit-llvm -o - %s | FileCheck %s

// CHECK-LABEL: @addcd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.addcd(i64 [[V1:%.*]], i64 [[V2:%.*]], i32 0)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long addcd(long v1, long v2) {
  return __builtin_kvx_addcd(v1, v2, "");
}

// CHECK-LABEL: @addcdi(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.addcd(i64 [[V1:%.*]], i64 [[V2:%.*]], i32 1)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long addcdi(long v1, long v2) {
  return __builtin_kvx_addcd(v1, v2, ".i");
}

// CHECK-LABEL: @sbfcd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.sbfcd(i64 [[V2:%.*]], i64 [[V1:%.*]], i32 0)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long sbfcd(long v1, long v2) {
  return __builtin_kvx_sbfcd(v1, v2, "");
}

// CHECK-LABEL: @sbfcdi(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.sbfcd(i64 [[V2:%.*]], i64 [[V1:%.*]], i32 1)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long sbfcdi(long v1, long v2) {
  return __builtin_kvx_sbfcd(v1, v2, ".i");
}

// CHECK-LABEL: @fmulh(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = insertelement <4 x half> undef, half [[A:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = insertelement <4 x half> undef, half [[B:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <4 x half> @llvm.kvx.fmulhq(<4 x half> [[TMP0]], <4 x half> [[TMP1]], i32 1, i32 1)
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <4 x half> [[TMP2]], i64 0
// CHECK-NEXT:    ret half [[TMP3]]
//
_Float16 fmulh(_Float16 a, _Float16 b){
  return __builtin_kvx_fmulh(a, b, ".ru.s");
}

// CHECK-LABEL: @fmulw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.fmulw(float [[V1:%.*]], float [[V2:%.*]], i32 3, i32 0)
// CHECK-NEXT:    ret float [[TMP0]]
//
float fmulw(float v1, float v2) {
  return __builtin_kvx_fmulw(v1, v2, ".rz");
}

// CHECK-LABEL: @fmuld(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.fmuld(double [[V1:%.*]], double [[V2:%.*]], i32 3, i32 0)
// CHECK-NEXT:    ret double [[TMP0]]
//
double fmuld(double v1, double v2) {
  return __builtin_kvx_fmuld(v1, v2, ".rz");
}

// CHECK-LABEL: @ffmah(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = insertelement <4 x half> undef, half [[A:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = insertelement <4 x half> undef, half [[B:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = insertelement <4 x half> undef, half [[C:%.*]], i64 0
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <4 x half> @llvm.kvx.ffmahq(<4 x half> [[TMP0]], <4 x half> [[TMP1]], <4 x half> [[TMP2]], i32 1, i32 1)
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <4 x half> [[TMP3]], i64 0
// CHECK-NEXT:    ret half [[TMP4]]
//
_Float16 ffmah(_Float16 a, _Float16 b, _Float16 c){
  return __builtin_kvx_ffmah(a, b, c, ".ru.s");
}

// CHECK-LABEL: @ffmaw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.ffmaw(float [[A:%.*]], float [[B:%.*]], float [[C:%.*]], i32 3, i32 0)
// CHECK-NEXT:    ret float [[TMP0]]
//
float ffmaw(float a, float b, float c) {
  return __builtin_kvx_ffmaw(a, b, c, ".rz");
}

// CHECK-LABEL: @ffmad(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.ffmad(double [[A:%.*]], double [[B:%.*]], double [[C:%.*]], i32 3, i32 0)
// CHECK-NEXT:    ret double [[TMP0]]
//
double ffmad(double a, double b, double c) {
  return __builtin_kvx_ffmad(a, b, c, ".rz");
}

// CHECK-LABEL: @ffmawd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.ffmawd(float [[A:%.*]], float [[B:%.*]], double [[C:%.*]], i32 3, i32 0)
// CHECK-NEXT:    ret double [[TMP0]]
//
double ffmawd(float a, float b, double c) {
  return __builtin_kvx_ffmaxwd(a, b, c, ".rz");
}

// CHECK-LABEL: @ffmsw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.ffmsw(float [[A:%.*]], float [[B:%.*]], float [[C:%.*]], i32 3, i32 0)
// CHECK-NEXT:    ret float [[TMP0]]
//
float ffmsw(float a, float b, float c) {
  return __builtin_kvx_ffmsw(a, b, c, ".rz");
}

// CHECK-LABEL: @ffmsw_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.ffmsw(float [[A:%.*]], float [[B:%.*]], float [[C:%.*]], i32 7, i32 0)
// CHECK-NEXT:    ret float [[TMP0]]
//
float ffmsw_(float a, float b, float c) {
  return __builtin_kvx_ffmsw(a, b, c, "");
}

// CHECK-LABEL: @ffmsd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.ffmsd(double [[A:%.*]], double [[B:%.*]], double [[C:%.*]], i32 3, i32 0)
// CHECK-NEXT:    ret double [[TMP0]]
//
double ffmsd(double a, double b, double c) {
  return __builtin_kvx_ffmsd(a, b, c, ".rz");
}

// CHECK-LABEL: @ffmsxwd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.ffmswd(float [[A:%.*]], float [[B:%.*]], double [[C:%.*]], i32 3, i32 0)
// CHECK-NEXT:    ret double [[TMP0]]
//
double ffmsxwd(float a, float b, double c) {
  return __builtin_kvx_ffmsxwd(a, b, c, ".rz");
}

// CHECK-LABEL: @floatw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.floatw(i32 [[X:%.*]], i64 3, i32 0, i32 0)
// CHECK-NEXT:    ret float [[TMP0]]
//
float floatw(int x) {
  return __builtin_kvx_floatw(x, 3, ".rn");
}

// CHECK-LABEL: @floatd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.floatd(i64 [[X:%.*]], i64 3, i32 0, i32 0)
// CHECK-NEXT:    ret double [[TMP0]]
//
double floatd(long x) {
  return __builtin_kvx_floatd(x, 3, ".rn");
}

// CHECK-LABEL: @floatw_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.floatw(i32 [[X:%.*]], i64 3, i32 0, i32 1)
// CHECK-NEXT:    ret float [[TMP0]]
//
float floatw_s(int x) {
  return __builtin_kvx_floatw(x, 3, ".rn.s");
}

// CHECK-LABEL: @floatd_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.floatd(i64 [[X:%.*]], i64 3, i32 0, i32 1)
// CHECK-NEXT:    ret double [[TMP0]]
//
double floatd_s(long x) {
  return __builtin_kvx_floatd(x, 3, ".rn.s");
}

// CHECK-LABEL: @floatd_limit(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.floatd(i64 [[X:%.*]], i64 63, i32 0, i32 0)
// CHECK-NEXT:    ret double [[TMP0]]
//
double floatd_limit(long x) {
  return __builtin_kvx_floatd(x, 63, ".rn");
}

// CHECK-LABEL: @floatuw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.floatuw(i32 [[X:%.*]], i64 3, i32 3, i32 0)
// CHECK-NEXT:    ret float [[TMP0]]
//
float floatuw(unsigned int x) {
  return __builtin_kvx_floatuw(x, 3, ".rz");
}

// CHECK-LABEL: @floatud(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call double @llvm.kvx.floatud(i64 [[X:%.*]], i64 3, i32 3, i32 0)
// CHECK-NEXT:    ret double [[TMP0]]
//
double floatud(unsigned long x) {
  return __builtin_kvx_floatud(x, 3, ".rz");
}

// CHECK-LABEL: @fixedw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.fixedw(float [[X:%.*]], i64 3, i32 0, i32 0)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int fixedw(float x) {
  return __builtin_kvx_fixedw(x, 3, ".rn");
}

// CHECK-LABEL: @fixedd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.fixedd(double [[X:%.*]], i64 3, i32 0, i32 0)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long fixedd(double x) {
  return __builtin_kvx_fixedd(x, 3, ".rn");
}

// CHECK-LABEL: @fixedw_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.fixedw(float [[X:%.*]], i64 3, i32 0, i32 1)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int fixedw_s(float x) {
  return __builtin_kvx_fixedw(x, 3, ".rn.s");
}

// CHECK-LABEL: @fixedd_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.fixedd(double [[X:%.*]], i64 3, i32 0, i32 1)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long fixedd_s(double x) {
  return __builtin_kvx_fixedd(x, 3, ".rn.s");
}

// CHECK-LABEL: @fixedw_s_only(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.fixedw(float [[X:%.*]], i64 3, i32 7, i32 1)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int fixedw_s_only(float x) {
  return __builtin_kvx_fixedw(x, 3, ".s");
}

// CHECK-LABEL: @fixedd_s_only(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.fixedd(double [[X:%.*]], i64 3, i32 7, i32 1)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long fixedd_s_only(double x) {
  return __builtin_kvx_fixedd(x, 3, ".s");
}

// CHECK-LABEL: @fixeduw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.fixeduw(float [[X:%.*]], i64 3, i32 3, i32 0)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int fixeduw(float x) {
  return __builtin_kvx_fixeduw(x, 3, ".rz");
}

// CHECK-LABEL: @fixedud(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.fixedud(double [[X:%.*]], i64 3, i32 3, i32 0)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
unsigned long fixedud(double x) {
  return __builtin_kvx_fixedud(x, 3, ".rz");
}

// CHECK-LABEL: @sbmm8(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.sbmm8(i64 [[A:%.*]], i64 [[B:%.*]])
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long sbmm8(long a, long b) {
  return __builtin_kvx_sbmm8(a, b);
}

// CHECK-LABEL: @sbmmt8(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.sbmmt8(i64 [[A:%.*]], i64 [[B:%.*]])
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long sbmmt8(long a, long b) {
  return __builtin_kvx_sbmmt8(a, b);
}

// CHECK-LABEL: @fnarrowwh(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call half @llvm.kvx.fnarrowwh(float [[V:%.*]], i32 1, i32 1)
// CHECK-NEXT:    ret half [[TMP0]]
//
_Float16 fnarrowwh(float v) {
  return __builtin_kvx_fnarrowwh(v, ".ru.s");
}

// CHECK-LABEL: @fnarrowdw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.kvx.fnarrowdw(double [[V:%.*]], i32 1, i32 1)
// CHECK-NEXT:    ret float [[TMP0]]
//
float fnarrowdw(double v) {
  return __builtin_kvx_fnarrowdw(v, ".ru.s");
}
