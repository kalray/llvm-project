// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -triple kvx-kalray-cos -S -O2 -emit-llvm -o - %s | FileCheck %s

// CHECK-LABEL: @sbmm8d(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.sbmm8(i64 [[A:%.*]], i64 [[B:%.*]])
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long sbmm8d(long a, long b) {
  return __builtin_kvx_sbmm8d(a, b);
}

// CHECK-LABEL: @sbmm8ri10(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.sbmm8(i64 [[A:%.*]], i64 -512)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long sbmm8ri10(long a) {
  return __builtin_kvx_sbmm8d(a, -512);
}

// CHECK-LABEL: @sbmm8ri37(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.sbmm8(i64 [[A:%.*]], i64 -513)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long sbmm8ri37(long a) {
  return __builtin_kvx_sbmm8d(a, -513);
}

// CHECK-LABEL: @sbmm8ri64(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.sbmm8(i64 [[A:%.*]], i64 206158430207)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long sbmm8ri64(long a) {
  return __builtin_kvx_sbmm8d(a, 0x2FFFFFFFFF);
}

// CHECK-LABEL: @sbmmt8d(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.sbmmt8(i64 [[A:%.*]], i64 [[B:%.*]])
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long sbmmt8d(long a, long b) {
  return __builtin_kvx_sbmmt8d(a, b);
}

// CHECK-LABEL: @sbmmt8ri10(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.sbmmt8(i64 [[A:%.*]], i64 512)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long sbmmt8ri10(long a) {
  return __builtin_kvx_sbmmt8d(a, 512);
}

// CHECK-LABEL: @sbmm8dp(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <2 x i64> [[A:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = extractelement <2 x i64> [[B:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 @llvm.kvx.sbmm8(i64 [[TMP0]], i64 [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <2 x i64> [[A]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <2 x i64> [[B]], i64 1
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i64 @llvm.kvx.sbmm8(i64 [[TMP3]], i64 [[TMP4]])
// CHECK-NEXT:    [[TMP6:%.*]] = insertelement <2 x i64> undef, i64 [[TMP2]], i64 0
// CHECK-NEXT:    [[TMP7:%.*]] = insertelement <2 x i64> [[TMP6]], i64 [[TMP5]], i64 1
// CHECK-NEXT:    ret <2 x i64> [[TMP7]]
//
__kvx_v2du sbmm8dp(__kvx_v2du a, __kvx_v2du b) {
  return __builtin_kvx_sbmm8dp(a, b);
}

// CHECK-LABEL: @sbmmt8dp(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <2 x i64> [[A:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = extractelement <2 x i64> [[B:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 @llvm.kvx.sbmmt8(i64 [[TMP0]], i64 [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <2 x i64> [[A]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <2 x i64> [[B]], i64 1
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i64 @llvm.kvx.sbmmt8(i64 [[TMP3]], i64 [[TMP4]])
// CHECK-NEXT:    [[TMP6:%.*]] = insertelement <2 x i64> undef, i64 [[TMP2]], i64 0
// CHECK-NEXT:    [[TMP7:%.*]] = insertelement <2 x i64> [[TMP6]], i64 [[TMP5]], i64 1
// CHECK-NEXT:    ret <2 x i64> [[TMP7]]
//
__kvx_v2du sbmmt8dp(__kvx_v2du a, __kvx_v2du b) {
  return __builtin_kvx_sbmmt8dp(a, b);
}

// CHECK-LABEL: @sbmm8dq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <4 x i64> [[A:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = extractelement <4 x i64> [[B:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 @llvm.kvx.sbmm8(i64 [[TMP0]], i64 [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <4 x i64> [[A]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <4 x i64> [[B]], i64 1
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i64 @llvm.kvx.sbmm8(i64 [[TMP3]], i64 [[TMP4]])
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <4 x i64> [[A]], i64 2
// CHECK-NEXT:    [[TMP7:%.*]] = extractelement <4 x i64> [[B]], i64 2
// CHECK-NEXT:    [[TMP8:%.*]] = tail call i64 @llvm.kvx.sbmm8(i64 [[TMP6]], i64 [[TMP7]])
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <4 x i64> [[A]], i64 3
// CHECK-NEXT:    [[TMP10:%.*]] = extractelement <4 x i64> [[B]], i64 3
// CHECK-NEXT:    [[TMP11:%.*]] = tail call i64 @llvm.kvx.sbmm8(i64 [[TMP9]], i64 [[TMP10]])
// CHECK-NEXT:    [[TMP12:%.*]] = insertelement <4 x i64> undef, i64 [[TMP2]], i64 0
// CHECK-NEXT:    [[TMP13:%.*]] = insertelement <4 x i64> [[TMP12]], i64 [[TMP5]], i64 1
// CHECK-NEXT:    [[TMP14:%.*]] = insertelement <4 x i64> [[TMP13]], i64 [[TMP8]], i64 2
// CHECK-NEXT:    [[TMP15:%.*]] = insertelement <4 x i64> [[TMP14]], i64 [[TMP11]], i64 3
// CHECK-NEXT:    ret <4 x i64> [[TMP15]]
//
__kvx_v4du sbmm8dq(__kvx_v4du a, __kvx_v4du b) {
  return __builtin_kvx_sbmm8dq(a, b);
}

// CHECK-LABEL: @sbmmt8dq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <4 x i64> [[A:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = extractelement <4 x i64> [[B:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 @llvm.kvx.sbmmt8(i64 [[TMP0]], i64 [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <4 x i64> [[A]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <4 x i64> [[B]], i64 1
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i64 @llvm.kvx.sbmmt8(i64 [[TMP3]], i64 [[TMP4]])
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <4 x i64> [[A]], i64 2
// CHECK-NEXT:    [[TMP7:%.*]] = extractelement <4 x i64> [[B]], i64 2
// CHECK-NEXT:    [[TMP8:%.*]] = tail call i64 @llvm.kvx.sbmmt8(i64 [[TMP6]], i64 [[TMP7]])
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <4 x i64> [[A]], i64 3
// CHECK-NEXT:    [[TMP10:%.*]] = extractelement <4 x i64> [[B]], i64 3
// CHECK-NEXT:    [[TMP11:%.*]] = tail call i64 @llvm.kvx.sbmmt8(i64 [[TMP9]], i64 [[TMP10]])
// CHECK-NEXT:    [[TMP12:%.*]] = insertelement <4 x i64> undef, i64 [[TMP2]], i64 0
// CHECK-NEXT:    [[TMP13:%.*]] = insertelement <4 x i64> [[TMP12]], i64 [[TMP5]], i64 1
// CHECK-NEXT:    [[TMP14:%.*]] = insertelement <4 x i64> [[TMP13]], i64 [[TMP8]], i64 2
// CHECK-NEXT:    [[TMP15:%.*]] = insertelement <4 x i64> [[TMP14]], i64 [[TMP11]], i64 3
// CHECK-NEXT:    ret <4 x i64> [[TMP15]]
//
__kvx_v4du sbmmt8dq(__kvx_v4du a, __kvx_v4du b) {
  return __builtin_kvx_sbmmt8dq(a, b);
}

// CHECK-LABEL: @sbmm8do(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = load <8 x i64>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2:![0-9]+]]
// CHECK-NEXT:    [[B:%.*]] = load <8 x i64>, ptr [[TMP1:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP2:%.*]] = extractelement <8 x i64> [[A]], i64 0
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <8 x i64> [[B]], i64 0
// CHECK-NEXT:    [[TMP4:%.*]] = tail call i64 @llvm.kvx.sbmm8(i64 [[TMP2]], i64 [[TMP3]])
// CHECK-NEXT:    [[TMP5:%.*]] = extractelement <8 x i64> [[A]], i64 1
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <8 x i64> [[B]], i64 1
// CHECK-NEXT:    [[TMP7:%.*]] = tail call i64 @llvm.kvx.sbmm8(i64 [[TMP5]], i64 [[TMP6]])
// CHECK-NEXT:    [[TMP8:%.*]] = extractelement <8 x i64> [[A]], i64 2
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <8 x i64> [[B]], i64 2
// CHECK-NEXT:    [[TMP10:%.*]] = tail call i64 @llvm.kvx.sbmm8(i64 [[TMP8]], i64 [[TMP9]])
// CHECK-NEXT:    [[TMP11:%.*]] = extractelement <8 x i64> [[A]], i64 3
// CHECK-NEXT:    [[TMP12:%.*]] = extractelement <8 x i64> [[B]], i64 3
// CHECK-NEXT:    [[TMP13:%.*]] = tail call i64 @llvm.kvx.sbmm8(i64 [[TMP11]], i64 [[TMP12]])
// CHECK-NEXT:    [[TMP14:%.*]] = extractelement <8 x i64> [[A]], i64 4
// CHECK-NEXT:    [[TMP15:%.*]] = extractelement <8 x i64> [[B]], i64 4
// CHECK-NEXT:    [[TMP16:%.*]] = tail call i64 @llvm.kvx.sbmm8(i64 [[TMP14]], i64 [[TMP15]])
// CHECK-NEXT:    [[TMP17:%.*]] = extractelement <8 x i64> [[A]], i64 5
// CHECK-NEXT:    [[TMP18:%.*]] = extractelement <8 x i64> [[B]], i64 5
// CHECK-NEXT:    [[TMP19:%.*]] = tail call i64 @llvm.kvx.sbmm8(i64 [[TMP17]], i64 [[TMP18]])
// CHECK-NEXT:    [[TMP20:%.*]] = extractelement <8 x i64> [[A]], i64 6
// CHECK-NEXT:    [[TMP21:%.*]] = extractelement <8 x i64> [[B]], i64 6
// CHECK-NEXT:    [[TMP22:%.*]] = tail call i64 @llvm.kvx.sbmm8(i64 [[TMP20]], i64 [[TMP21]])
// CHECK-NEXT:    [[TMP23:%.*]] = extractelement <8 x i64> [[A]], i64 7
// CHECK-NEXT:    [[TMP24:%.*]] = extractelement <8 x i64> [[B]], i64 7
// CHECK-NEXT:    [[TMP25:%.*]] = tail call i64 @llvm.kvx.sbmm8(i64 [[TMP23]], i64 [[TMP24]])
// CHECK-NEXT:    [[TMP26:%.*]] = insertelement <8 x i64> undef, i64 [[TMP4]], i64 0
// CHECK-NEXT:    [[TMP27:%.*]] = insertelement <8 x i64> [[TMP26]], i64 [[TMP7]], i64 1
// CHECK-NEXT:    [[TMP28:%.*]] = insertelement <8 x i64> [[TMP27]], i64 [[TMP10]], i64 2
// CHECK-NEXT:    [[TMP29:%.*]] = insertelement <8 x i64> [[TMP28]], i64 [[TMP13]], i64 3
// CHECK-NEXT:    [[TMP30:%.*]] = insertelement <8 x i64> [[TMP29]], i64 [[TMP16]], i64 4
// CHECK-NEXT:    [[TMP31:%.*]] = insertelement <8 x i64> [[TMP30]], i64 [[TMP19]], i64 5
// CHECK-NEXT:    [[TMP32:%.*]] = insertelement <8 x i64> [[TMP31]], i64 [[TMP22]], i64 6
// CHECK-NEXT:    [[TMP33:%.*]] = insertelement <8 x i64> [[TMP32]], i64 [[TMP25]], i64 7
// CHECK-NEXT:    store <8 x i64> [[TMP33]], ptr [[AGG_RESULT:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
__kvx_v8du sbmm8do(__kvx_v8du a, __kvx_v8du b) {
  return __builtin_kvx_sbmm8do(a, b);
}

// CHECK-LABEL: @sbmmt8do(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = load <8 x i64>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[B:%.*]] = load <8 x i64>, ptr [[TMP1:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP2:%.*]] = extractelement <8 x i64> [[A]], i64 0
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <8 x i64> [[B]], i64 0
// CHECK-NEXT:    [[TMP4:%.*]] = tail call i64 @llvm.kvx.sbmmt8(i64 [[TMP2]], i64 [[TMP3]])
// CHECK-NEXT:    [[TMP5:%.*]] = extractelement <8 x i64> [[A]], i64 1
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <8 x i64> [[B]], i64 1
// CHECK-NEXT:    [[TMP7:%.*]] = tail call i64 @llvm.kvx.sbmmt8(i64 [[TMP5]], i64 [[TMP6]])
// CHECK-NEXT:    [[TMP8:%.*]] = extractelement <8 x i64> [[A]], i64 2
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <8 x i64> [[B]], i64 2
// CHECK-NEXT:    [[TMP10:%.*]] = tail call i64 @llvm.kvx.sbmmt8(i64 [[TMP8]], i64 [[TMP9]])
// CHECK-NEXT:    [[TMP11:%.*]] = extractelement <8 x i64> [[A]], i64 3
// CHECK-NEXT:    [[TMP12:%.*]] = extractelement <8 x i64> [[B]], i64 3
// CHECK-NEXT:    [[TMP13:%.*]] = tail call i64 @llvm.kvx.sbmmt8(i64 [[TMP11]], i64 [[TMP12]])
// CHECK-NEXT:    [[TMP14:%.*]] = extractelement <8 x i64> [[A]], i64 4
// CHECK-NEXT:    [[TMP15:%.*]] = extractelement <8 x i64> [[B]], i64 4
// CHECK-NEXT:    [[TMP16:%.*]] = tail call i64 @llvm.kvx.sbmmt8(i64 [[TMP14]], i64 [[TMP15]])
// CHECK-NEXT:    [[TMP17:%.*]] = extractelement <8 x i64> [[A]], i64 5
// CHECK-NEXT:    [[TMP18:%.*]] = extractelement <8 x i64> [[B]], i64 5
// CHECK-NEXT:    [[TMP19:%.*]] = tail call i64 @llvm.kvx.sbmmt8(i64 [[TMP17]], i64 [[TMP18]])
// CHECK-NEXT:    [[TMP20:%.*]] = extractelement <8 x i64> [[A]], i64 6
// CHECK-NEXT:    [[TMP21:%.*]] = extractelement <8 x i64> [[B]], i64 6
// CHECK-NEXT:    [[TMP22:%.*]] = tail call i64 @llvm.kvx.sbmmt8(i64 [[TMP20]], i64 [[TMP21]])
// CHECK-NEXT:    [[TMP23:%.*]] = extractelement <8 x i64> [[A]], i64 7
// CHECK-NEXT:    [[TMP24:%.*]] = extractelement <8 x i64> [[B]], i64 7
// CHECK-NEXT:    [[TMP25:%.*]] = tail call i64 @llvm.kvx.sbmmt8(i64 [[TMP23]], i64 [[TMP24]])
// CHECK-NEXT:    [[TMP26:%.*]] = insertelement <8 x i64> undef, i64 [[TMP4]], i64 0
// CHECK-NEXT:    [[TMP27:%.*]] = insertelement <8 x i64> [[TMP26]], i64 [[TMP7]], i64 1
// CHECK-NEXT:    [[TMP28:%.*]] = insertelement <8 x i64> [[TMP27]], i64 [[TMP10]], i64 2
// CHECK-NEXT:    [[TMP29:%.*]] = insertelement <8 x i64> [[TMP28]], i64 [[TMP13]], i64 3
// CHECK-NEXT:    [[TMP30:%.*]] = insertelement <8 x i64> [[TMP29]], i64 [[TMP16]], i64 4
// CHECK-NEXT:    [[TMP31:%.*]] = insertelement <8 x i64> [[TMP30]], i64 [[TMP19]], i64 5
// CHECK-NEXT:    [[TMP32:%.*]] = insertelement <8 x i64> [[TMP31]], i64 [[TMP22]], i64 6
// CHECK-NEXT:    [[TMP33:%.*]] = insertelement <8 x i64> [[TMP32]], i64 [[TMP25]], i64 7
// CHECK-NEXT:    store <8 x i64> [[TMP33]], ptr [[AGG_RESULT:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
__kvx_v8du sbmmt8do(__kvx_v8du a, __kvx_v8du b) {
  return __builtin_kvx_sbmmt8do(a, b);
}
