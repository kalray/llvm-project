// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -triple kvx-kalray-cos -target-cpu kv3-1 -S -O2 -emit-llvm -o - %s | FileCheck %s
// RUN: %clang_cc1 -triple kvx-kalray-cos -target-cpu kv3-2 -S -O2 -emit-llvm -o - %s | FileCheck %s

#include "vector-types.h"

// CHECK-LABEL: @maxbo(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <8 x i8> @llvm.smax.v8i8(<8 x i8> [[L:%.*]], <8 x i8> [[R:%.*]])
// CHECK-NEXT:    ret <8 x i8> [[TMP0]]
//
v8i8 maxbo (v8i8 l, v8i8 r) {
  return __builtin_kvx_maxbo(l, r);
}

// CHECK-LABEL: @maxbp(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i8> @llvm.smax.v2i8(<2 x i8> [[L:%.*]], <2 x i8> [[R:%.*]])
// CHECK-NEXT:    ret <2 x i8> [[TMP0]]
//
v2i8 maxbp (v2i8 l, v2i8 r) {
  return __builtin_kvx_maxbp(l, r);
}

// CHECK-LABEL: @maxbq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <4 x i8> @llvm.smax.v4i8(<4 x i8> [[L:%.*]], <4 x i8> [[R:%.*]])
// CHECK-NEXT:    ret <4 x i8> [[TMP0]]
//
v4i8 maxbq (v4i8 l, v4i8 r) {
  return __builtin_kvx_maxbq(l, r);
}

// CHECK-LABEL: @maxbv(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <32 x i8> @llvm.smax.v32i8(<32 x i8> [[L:%.*]], <32 x i8> [[R:%.*]])
// CHECK-NEXT:    ret <32 x i8> [[TMP0]]
//
v32i8 maxbv (v32i8 l, v32i8 r) {
  return __builtin_kvx_maxbv(l, r);
}

// CHECK-LABEL: @maxbx(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <16 x i8> @llvm.smax.v16i8(<16 x i8> [[L:%.*]], <16 x i8> [[R:%.*]])
// CHECK-NEXT:    ret <16 x i8> [[TMP0]]
//
v16i8 maxbx (v16i8 l, v16i8 r) {
  return __builtin_kvx_maxbx(l, r);
}

// CHECK-LABEL: @maxd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.smax.i64(i64 [[L:%.*]], i64 [[R:%.*]])
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long maxd (long l, long r) {
  return __builtin_kvx_maxd(l, r);
}

// CHECK-LABEL: @maxdp(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <2 x i64> [[L:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = extractelement <2 x i64> [[R:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 @llvm.smax.i64(i64 [[TMP0]], i64 [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <2 x i64> [[L]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <2 x i64> [[R]], i64 1
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i64 @llvm.smax.i64(i64 [[TMP3]], i64 [[TMP4]])
// CHECK-NEXT:    [[TMP6:%.*]] = insertelement <2 x i64> poison, i64 [[TMP2]], i64 0
// CHECK-NEXT:    [[TMP7:%.*]] = insertelement <2 x i64> [[TMP6]], i64 [[TMP5]], i64 1
// CHECK-NEXT:    ret <2 x i64> [[TMP7]]
//
v2i64 maxdp (v2i64 l, v2i64 r) {
  return __builtin_kvx_maxdp(l, r);
}

// CHECK-LABEL: @maxdq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <4 x i64> [[L:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = extractelement <4 x i64> [[R:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 @llvm.smax.i64(i64 [[TMP0]], i64 [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <4 x i64> [[L]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <4 x i64> [[R]], i64 1
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i64 @llvm.smax.i64(i64 [[TMP3]], i64 [[TMP4]])
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <4 x i64> [[L]], i64 2
// CHECK-NEXT:    [[TMP7:%.*]] = extractelement <4 x i64> [[R]], i64 2
// CHECK-NEXT:    [[TMP8:%.*]] = tail call i64 @llvm.smax.i64(i64 [[TMP6]], i64 [[TMP7]])
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <4 x i64> [[L]], i64 3
// CHECK-NEXT:    [[TMP10:%.*]] = extractelement <4 x i64> [[R]], i64 3
// CHECK-NEXT:    [[TMP11:%.*]] = tail call i64 @llvm.smax.i64(i64 [[TMP9]], i64 [[TMP10]])
// CHECK-NEXT:    [[TMP12:%.*]] = insertelement <4 x i64> poison, i64 [[TMP2]], i64 0
// CHECK-NEXT:    [[TMP13:%.*]] = insertelement <4 x i64> [[TMP12]], i64 [[TMP5]], i64 1
// CHECK-NEXT:    [[TMP14:%.*]] = insertelement <4 x i64> [[TMP13]], i64 [[TMP8]], i64 2
// CHECK-NEXT:    [[TMP15:%.*]] = insertelement <4 x i64> [[TMP14]], i64 [[TMP11]], i64 3
// CHECK-NEXT:    ret <4 x i64> [[TMP15]]
//
v4i64 maxdq (v4i64 l, v4i64 r) {
  return __builtin_kvx_maxdq(l, r);
}

// CHECK-LABEL: @maxho(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <8 x i16> @llvm.smax.v8i16(<8 x i16> [[L:%.*]], <8 x i16> [[R:%.*]])
// CHECK-NEXT:    ret <8 x i16> [[TMP0]]
//
v8i16 maxho (v8i16 l, v8i16 r) {
  return __builtin_kvx_maxho(l, r);
}

// CHECK-LABEL: @maxhp(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i16> @llvm.smax.v2i16(<2 x i16> [[L:%.*]], <2 x i16> [[R:%.*]])
// CHECK-NEXT:    ret <2 x i16> [[TMP0]]
//
v2i16 maxhp (v2i16 l, v2i16 r) {
  return __builtin_kvx_maxhp(l, r);
}

// CHECK-LABEL: @maxhq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <4 x i16> @llvm.smax.v4i16(<4 x i16> [[L:%.*]], <4 x i16> [[R:%.*]])
// CHECK-NEXT:    ret <4 x i16> [[TMP0]]
//
v4i16 maxhq (v4i16 l, v4i16 r) {
  return __builtin_kvx_maxhq(l, r);
}

// CHECK-LABEL: @maxhx(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <16 x i16> @llvm.smax.v16i16(<16 x i16> [[L:%.*]], <16 x i16> [[R:%.*]])
// CHECK-NEXT:    ret <16 x i16> [[TMP0]]
//
v16i16 maxhx (v16i16 l, v16i16 r) {
  return __builtin_kvx_maxhx(l, r);
}

// CHECK-LABEL: @maxubo(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <8 x i8> @llvm.umax.v8i8(<8 x i8> [[L:%.*]], <8 x i8> [[R:%.*]])
// CHECK-NEXT:    ret <8 x i8> [[TMP0]]
//
v8u8 maxubo (v8u8 l, v8u8 r) {
  return __builtin_kvx_maxubo(l, r);
}

// CHECK-LABEL: @maxubp(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i8> @llvm.umax.v2i8(<2 x i8> [[L:%.*]], <2 x i8> [[R:%.*]])
// CHECK-NEXT:    ret <2 x i8> [[TMP0]]
//
v2u8 maxubp (v2u8 l, v2u8 r) {
  return __builtin_kvx_maxubp(l, r);
}

// CHECK-LABEL: @maxubq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <4 x i8> @llvm.umax.v4i8(<4 x i8> [[L:%.*]], <4 x i8> [[R:%.*]])
// CHECK-NEXT:    ret <4 x i8> [[TMP0]]
//
v4u8 maxubq (v4u8 l, v4u8 r) {
  return __builtin_kvx_maxubq(l, r);
}

// CHECK-LABEL: @maxubv(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <32 x i8> @llvm.umax.v32i8(<32 x i8> [[L:%.*]], <32 x i8> [[R:%.*]])
// CHECK-NEXT:    ret <32 x i8> [[TMP0]]
//
v32u8 maxubv (v32u8 l, v32u8 r) {
  return __builtin_kvx_maxubv(l, r);
}

// CHECK-LABEL: @maxubx(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <16 x i8> @llvm.umax.v16i8(<16 x i8> [[L:%.*]], <16 x i8> [[R:%.*]])
// CHECK-NEXT:    ret <16 x i8> [[TMP0]]
//
v16u8 maxubx (v16u8 l, v16u8 r) {
  return __builtin_kvx_maxubx(l, r);
}

// CHECK-LABEL: @maxud(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.umax.i64(i64 [[L:%.*]], i64 [[R:%.*]])
// CHECK-NEXT:    ret i64 [[TMP0]]
//
unsigned long maxud (unsigned long l, unsigned long r) {
  return __builtin_kvx_maxud(l, r);
}

// CHECK-LABEL: @maxudp(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <2 x i64> [[L:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = extractelement <2 x i64> [[R:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 @llvm.umax.i64(i64 [[TMP0]], i64 [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <2 x i64> [[L]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <2 x i64> [[R]], i64 1
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i64 @llvm.umax.i64(i64 [[TMP3]], i64 [[TMP4]])
// CHECK-NEXT:    [[TMP6:%.*]] = insertelement <2 x i64> poison, i64 [[TMP2]], i64 0
// CHECK-NEXT:    [[TMP7:%.*]] = insertelement <2 x i64> [[TMP6]], i64 [[TMP5]], i64 1
// CHECK-NEXT:    ret <2 x i64> [[TMP7]]
//
v2u64 maxudp (v2u64 l, v2u64 r) {
  return __builtin_kvx_maxudp(l, r);
}

// CHECK-LABEL: @maxudq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <4 x i64> [[L:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = extractelement <4 x i64> [[R:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 @llvm.umax.i64(i64 [[TMP0]], i64 [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <4 x i64> [[L]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <4 x i64> [[R]], i64 1
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i64 @llvm.umax.i64(i64 [[TMP3]], i64 [[TMP4]])
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <4 x i64> [[L]], i64 2
// CHECK-NEXT:    [[TMP7:%.*]] = extractelement <4 x i64> [[R]], i64 2
// CHECK-NEXT:    [[TMP8:%.*]] = tail call i64 @llvm.umax.i64(i64 [[TMP6]], i64 [[TMP7]])
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <4 x i64> [[L]], i64 3
// CHECK-NEXT:    [[TMP10:%.*]] = extractelement <4 x i64> [[R]], i64 3
// CHECK-NEXT:    [[TMP11:%.*]] = tail call i64 @llvm.umax.i64(i64 [[TMP9]], i64 [[TMP10]])
// CHECK-NEXT:    [[TMP12:%.*]] = insertelement <4 x i64> poison, i64 [[TMP2]], i64 0
// CHECK-NEXT:    [[TMP13:%.*]] = insertelement <4 x i64> [[TMP12]], i64 [[TMP5]], i64 1
// CHECK-NEXT:    [[TMP14:%.*]] = insertelement <4 x i64> [[TMP13]], i64 [[TMP8]], i64 2
// CHECK-NEXT:    [[TMP15:%.*]] = insertelement <4 x i64> [[TMP14]], i64 [[TMP11]], i64 3
// CHECK-NEXT:    ret <4 x i64> [[TMP15]]
//
v4u64 maxudq (v4u64 l, v4u64 r) {
  return __builtin_kvx_maxudq(l, r);
}

// CHECK-LABEL: @maxuho(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <8 x i16> @llvm.umax.v8i16(<8 x i16> [[L:%.*]], <8 x i16> [[R:%.*]])
// CHECK-NEXT:    ret <8 x i16> [[TMP0]]
//
v8u16 maxuho (v8u16 l, v8u16 r) {
  return __builtin_kvx_maxuho(l, r);
}

// CHECK-LABEL: @maxuhp(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i16> @llvm.umax.v2i16(<2 x i16> [[L:%.*]], <2 x i16> [[R:%.*]])
// CHECK-NEXT:    ret <2 x i16> [[TMP0]]
//
v2u16 maxuhp (v2u16 l, v2u16 r) {
  return __builtin_kvx_maxuhp(l, r);
}

// CHECK-LABEL: @maxuhq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <4 x i16> @llvm.umax.v4i16(<4 x i16> [[L:%.*]], <4 x i16> [[R:%.*]])
// CHECK-NEXT:    ret <4 x i16> [[TMP0]]
//
v4u16 maxuhq (v4u16 l, v4u16 r) {
  return __builtin_kvx_maxuhq(l, r);
}

// CHECK-LABEL: @maxuhx(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <16 x i16> @llvm.umax.v16i16(<16 x i16> [[L:%.*]], <16 x i16> [[R:%.*]])
// CHECK-NEXT:    ret <16 x i16> [[TMP0]]
//
v16u16 maxuhx (v16u16 l, v16u16 r) {
  return __builtin_kvx_maxuhx(l, r);
}

// CHECK-LABEL: @maxuw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.umax.i32(i32 [[L:%.*]], i32 [[R:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned maxuw (unsigned l, unsigned r) {
  return __builtin_kvx_maxuw(l, r);
}

// CHECK-LABEL: @maxuwo(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <8 x i32> @llvm.umax.v8i32(<8 x i32> [[L:%.*]], <8 x i32> [[R:%.*]])
// CHECK-NEXT:    ret <8 x i32> [[TMP0]]
//
v8u32 maxuwo (v8u32 l, v8u32 r) {
  return __builtin_kvx_maxuwo(l, r);
}

// CHECK-LABEL: @maxuwp(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.umax.v2i32(<2 x i32> [[L:%.*]], <2 x i32> [[R:%.*]])
// CHECK-NEXT:    ret <2 x i32> [[TMP0]]
//
v2u32 maxuwp (v2u32 l, v2u32 r) {
  return __builtin_kvx_maxuwp(l, r);
}

// CHECK-LABEL: @maxuwq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <4 x i32> @llvm.umax.v4i32(<4 x i32> [[L:%.*]], <4 x i32> [[R:%.*]])
// CHECK-NEXT:    ret <4 x i32> [[TMP0]]
//
v4u32 maxuwq (v4u32 l, v4u32 r) {
  return __builtin_kvx_maxuwq(l, r);
}

// CHECK-LABEL: @maxw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.smax.i32(i32 [[L:%.*]], i32 [[R:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int maxw (int l, int r) {
  return __builtin_kvx_maxw(l, r);
}

// CHECK-LABEL: @maxwo(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <8 x i32> @llvm.smax.v8i32(<8 x i32> [[L:%.*]], <8 x i32> [[R:%.*]])
// CHECK-NEXT:    ret <8 x i32> [[TMP0]]
//
v8i32 maxwo (v8i32 l, v8i32 r) {
  return __builtin_kvx_maxwo(l, r);
}

// CHECK-LABEL: @maxwp(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.smax.v2i32(<2 x i32> [[L:%.*]], <2 x i32> [[R:%.*]])
// CHECK-NEXT:    ret <2 x i32> [[TMP0]]
//
v2i32 maxwp (v2i32 l, v2i32 r) {
  return __builtin_kvx_maxwp(l, r);
}

// CHECK-LABEL: @maxwq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <4 x i32> @llvm.smax.v4i32(<4 x i32> [[L:%.*]], <4 x i32> [[R:%.*]])
// CHECK-NEXT:    ret <4 x i32> [[TMP0]]
//
v4i32 maxwq (v4i32 l, v4i32 r) {
  return __builtin_kvx_maxwq(l, r);
}

// CHECK-LABEL: @minbo(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <8 x i8> @llvm.smin.v8i8(<8 x i8> [[L:%.*]], <8 x i8> [[R:%.*]])
// CHECK-NEXT:    ret <8 x i8> [[TMP0]]
//
v8i8 minbo (v8i8 l, v8i8 r) {
  return __builtin_kvx_minbo(l, r);
}

// CHECK-LABEL: @minbp(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i8> @llvm.smin.v2i8(<2 x i8> [[L:%.*]], <2 x i8> [[R:%.*]])
// CHECK-NEXT:    ret <2 x i8> [[TMP0]]
//
v2i8 minbp (v2i8 l, v2i8 r) {
  return __builtin_kvx_minbp(l, r);
}

// CHECK-LABEL: @minbq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <4 x i8> @llvm.smin.v4i8(<4 x i8> [[L:%.*]], <4 x i8> [[R:%.*]])
// CHECK-NEXT:    ret <4 x i8> [[TMP0]]
//
v4i8 minbq (v4i8 l, v4i8 r) {
  return __builtin_kvx_minbq(l, r);
}

// CHECK-LABEL: @minbv(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <32 x i8> @llvm.smin.v32i8(<32 x i8> [[L:%.*]], <32 x i8> [[R:%.*]])
// CHECK-NEXT:    ret <32 x i8> [[TMP0]]
//
v32i8 minbv (v32i8 l, v32i8 r) {
  return __builtin_kvx_minbv(l, r);
}

// CHECK-LABEL: @minbx(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <16 x i8> @llvm.smin.v16i8(<16 x i8> [[L:%.*]], <16 x i8> [[R:%.*]])
// CHECK-NEXT:    ret <16 x i8> [[TMP0]]
//
v16i8 minbx (v16i8 l, v16i8 r) {
  return __builtin_kvx_minbx(l, r);
}

// CHECK-LABEL: @mind(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.smin.i64(i64 [[L:%.*]], i64 [[R:%.*]])
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long mind (long l, long r) {
  return __builtin_kvx_mind(l, r);
}

// CHECK-LABEL: @mindp(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <2 x i64> [[L:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = extractelement <2 x i64> [[R:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 @llvm.smin.i64(i64 [[TMP0]], i64 [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <2 x i64> [[L]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <2 x i64> [[R]], i64 1
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i64 @llvm.smin.i64(i64 [[TMP3]], i64 [[TMP4]])
// CHECK-NEXT:    [[TMP6:%.*]] = insertelement <2 x i64> poison, i64 [[TMP2]], i64 0
// CHECK-NEXT:    [[TMP7:%.*]] = insertelement <2 x i64> [[TMP6]], i64 [[TMP5]], i64 1
// CHECK-NEXT:    ret <2 x i64> [[TMP7]]
//
v2i64 mindp (v2i64 l, v2i64 r) {
  return __builtin_kvx_mindp(l, r);
}

// CHECK-LABEL: @mindq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <4 x i64> [[L:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = extractelement <4 x i64> [[R:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 @llvm.smin.i64(i64 [[TMP0]], i64 [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <4 x i64> [[L]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <4 x i64> [[R]], i64 1
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i64 @llvm.smin.i64(i64 [[TMP3]], i64 [[TMP4]])
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <4 x i64> [[L]], i64 2
// CHECK-NEXT:    [[TMP7:%.*]] = extractelement <4 x i64> [[R]], i64 2
// CHECK-NEXT:    [[TMP8:%.*]] = tail call i64 @llvm.smin.i64(i64 [[TMP6]], i64 [[TMP7]])
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <4 x i64> [[L]], i64 3
// CHECK-NEXT:    [[TMP10:%.*]] = extractelement <4 x i64> [[R]], i64 3
// CHECK-NEXT:    [[TMP11:%.*]] = tail call i64 @llvm.smin.i64(i64 [[TMP9]], i64 [[TMP10]])
// CHECK-NEXT:    [[TMP12:%.*]] = insertelement <4 x i64> poison, i64 [[TMP2]], i64 0
// CHECK-NEXT:    [[TMP13:%.*]] = insertelement <4 x i64> [[TMP12]], i64 [[TMP5]], i64 1
// CHECK-NEXT:    [[TMP14:%.*]] = insertelement <4 x i64> [[TMP13]], i64 [[TMP8]], i64 2
// CHECK-NEXT:    [[TMP15:%.*]] = insertelement <4 x i64> [[TMP14]], i64 [[TMP11]], i64 3
// CHECK-NEXT:    ret <4 x i64> [[TMP15]]
//
v4i64 mindq (v4i64 l, v4i64 r) {
  return __builtin_kvx_mindq(l, r);
}

// CHECK-LABEL: @minho(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <8 x i16> @llvm.smin.v8i16(<8 x i16> [[L:%.*]], <8 x i16> [[R:%.*]])
// CHECK-NEXT:    ret <8 x i16> [[TMP0]]
//
v8i16 minho (v8i16 l, v8i16 r) {
  return __builtin_kvx_minho(l, r);
}

// CHECK-LABEL: @minhp(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i16> @llvm.smin.v2i16(<2 x i16> [[L:%.*]], <2 x i16> [[R:%.*]])
// CHECK-NEXT:    ret <2 x i16> [[TMP0]]
//
v2i16 minhp (v2i16 l, v2i16 r) {
  return __builtin_kvx_minhp(l, r);
}

// CHECK-LABEL: @minhq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <4 x i16> @llvm.smin.v4i16(<4 x i16> [[L:%.*]], <4 x i16> [[R:%.*]])
// CHECK-NEXT:    ret <4 x i16> [[TMP0]]
//
v4i16 minhq (v4i16 l, v4i16 r) {
  return __builtin_kvx_minhq(l, r);
}

// CHECK-LABEL: @minhx(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <16 x i16> @llvm.smin.v16i16(<16 x i16> [[L:%.*]], <16 x i16> [[R:%.*]])
// CHECK-NEXT:    ret <16 x i16> [[TMP0]]
//
v16i16 minhx (v16i16 l, v16i16 r) {
  return __builtin_kvx_minhx(l, r);
}

// CHECK-LABEL: @minubo(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <8 x i8> @llvm.umin.v8i8(<8 x i8> [[L:%.*]], <8 x i8> [[R:%.*]])
// CHECK-NEXT:    ret <8 x i8> [[TMP0]]
//
v8u8 minubo (v8u8 l, v8u8 r) {
  return __builtin_kvx_minubo(l, r);
}

// CHECK-LABEL: @minubp(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i8> @llvm.umin.v2i8(<2 x i8> [[L:%.*]], <2 x i8> [[R:%.*]])
// CHECK-NEXT:    ret <2 x i8> [[TMP0]]
//
v2u8 minubp (v2u8 l, v2u8 r) {
  return __builtin_kvx_minubp(l, r);
}

// CHECK-LABEL: @minubq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <4 x i8> @llvm.umin.v4i8(<4 x i8> [[L:%.*]], <4 x i8> [[R:%.*]])
// CHECK-NEXT:    ret <4 x i8> [[TMP0]]
//
v4u8 minubq (v4u8 l, v4u8 r) {
  return __builtin_kvx_minubq(l, r);
}

// CHECK-LABEL: @minubv(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <32 x i8> @llvm.umin.v32i8(<32 x i8> [[L:%.*]], <32 x i8> [[R:%.*]])
// CHECK-NEXT:    ret <32 x i8> [[TMP0]]
//
v32i8 minubv (v32i8 l, v32i8 r) {
  return __builtin_kvx_minubv(l, r);
}

// CHECK-LABEL: @minubx(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <16 x i8> @llvm.umin.v16i8(<16 x i8> [[L:%.*]], <16 x i8> [[R:%.*]])
// CHECK-NEXT:    ret <16 x i8> [[TMP0]]
//
v16u8 minubx (v16u8 l, v16u8 r) {
  return __builtin_kvx_minubx(l, r);
}

// CHECK-LABEL: @minud(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.umin.i64(i64 [[L:%.*]], i64 [[R:%.*]])
// CHECK-NEXT:    ret i64 [[TMP0]]
//
unsigned long minud (unsigned long l, unsigned long r) {
  return __builtin_kvx_minud(l, r);
}

// CHECK-LABEL: @minudp(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <2 x i64> [[L:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = extractelement <2 x i64> [[R:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 @llvm.umin.i64(i64 [[TMP0]], i64 [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <2 x i64> [[L]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <2 x i64> [[R]], i64 1
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i64 @llvm.umin.i64(i64 [[TMP3]], i64 [[TMP4]])
// CHECK-NEXT:    [[TMP6:%.*]] = insertelement <2 x i64> poison, i64 [[TMP2]], i64 0
// CHECK-NEXT:    [[TMP7:%.*]] = insertelement <2 x i64> [[TMP6]], i64 [[TMP5]], i64 1
// CHECK-NEXT:    ret <2 x i64> [[TMP7]]
//
v2u64 minudp (v2u64 l, v2u64 r) {
  return __builtin_kvx_minudp(l, r);
}

// CHECK-LABEL: @minudq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <4 x i64> [[L:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = extractelement <4 x i64> [[R:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 @llvm.umin.i64(i64 [[TMP0]], i64 [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <4 x i64> [[L]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <4 x i64> [[R]], i64 1
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i64 @llvm.umin.i64(i64 [[TMP3]], i64 [[TMP4]])
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <4 x i64> [[L]], i64 2
// CHECK-NEXT:    [[TMP7:%.*]] = extractelement <4 x i64> [[R]], i64 2
// CHECK-NEXT:    [[TMP8:%.*]] = tail call i64 @llvm.umin.i64(i64 [[TMP6]], i64 [[TMP7]])
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <4 x i64> [[L]], i64 3
// CHECK-NEXT:    [[TMP10:%.*]] = extractelement <4 x i64> [[R]], i64 3
// CHECK-NEXT:    [[TMP11:%.*]] = tail call i64 @llvm.umin.i64(i64 [[TMP9]], i64 [[TMP10]])
// CHECK-NEXT:    [[TMP12:%.*]] = insertelement <4 x i64> poison, i64 [[TMP2]], i64 0
// CHECK-NEXT:    [[TMP13:%.*]] = insertelement <4 x i64> [[TMP12]], i64 [[TMP5]], i64 1
// CHECK-NEXT:    [[TMP14:%.*]] = insertelement <4 x i64> [[TMP13]], i64 [[TMP8]], i64 2
// CHECK-NEXT:    [[TMP15:%.*]] = insertelement <4 x i64> [[TMP14]], i64 [[TMP11]], i64 3
// CHECK-NEXT:    ret <4 x i64> [[TMP15]]
//
v4u64 minudq (v4u64 l, v4u64 r) {
  return __builtin_kvx_minudq(l, r);
}

// CHECK-LABEL: @minuho(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <8 x i16> @llvm.umin.v8i16(<8 x i16> [[L:%.*]], <8 x i16> [[R:%.*]])
// CHECK-NEXT:    ret <8 x i16> [[TMP0]]
//
v8u16 minuho (v8u16 l, v8u16 r) {
  return __builtin_kvx_minuho(l, r);
}

// CHECK-LABEL: @minuhp(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i16> @llvm.umin.v2i16(<2 x i16> [[L:%.*]], <2 x i16> [[R:%.*]])
// CHECK-NEXT:    ret <2 x i16> [[TMP0]]
//
v2u16 minuhp (v2u16 l, v2u16 r) {
  return __builtin_kvx_minuhp(l, r);
}

// CHECK-LABEL: @minuhq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <4 x i16> @llvm.umin.v4i16(<4 x i16> [[L:%.*]], <4 x i16> [[R:%.*]])
// CHECK-NEXT:    ret <4 x i16> [[TMP0]]
//
v4u16 minuhq (v4u16 l, v4u16 r) {
  return __builtin_kvx_minuhq(l, r);
}

// CHECK-LABEL: @minuhx(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <16 x i16> @llvm.umin.v16i16(<16 x i16> [[L:%.*]], <16 x i16> [[R:%.*]])
// CHECK-NEXT:    ret <16 x i16> [[TMP0]]
//
v16u16 minuhx (v16u16 l, v16u16 r) {
  return __builtin_kvx_minuhx(l, r);
}

// CHECK-LABEL: @minuw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.umin.i32(i32 [[L:%.*]], i32 [[R:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned minuw (unsigned l, unsigned r) {
  return __builtin_kvx_minuw(l, r);
}

// CHECK-LABEL: @minuwo(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <8 x i32> @llvm.umin.v8i32(<8 x i32> [[L:%.*]], <8 x i32> [[R:%.*]])
// CHECK-NEXT:    ret <8 x i32> [[TMP0]]
//
v8u32 minuwo (v8u32 l, v8u32 r) {
  return __builtin_kvx_minuwo(l, r);
}

// CHECK-LABEL: @minuwp(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.umin.v2i32(<2 x i32> [[L:%.*]], <2 x i32> [[R:%.*]])
// CHECK-NEXT:    ret <2 x i32> [[TMP0]]
//
v2u32 minuwp (v2u32 l, v2u32 r) {
  return __builtin_kvx_minuwp(l, r);
}

// CHECK-LABEL: @minuwq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <4 x i32> @llvm.umin.v4i32(<4 x i32> [[L:%.*]], <4 x i32> [[R:%.*]])
// CHECK-NEXT:    ret <4 x i32> [[TMP0]]
//
v4u32 minuwq (v4u32 l, v4u32 r) {
  return __builtin_kvx_minuwq(l, r);
}

// CHECK-LABEL: @minw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.smin.i32(i32 [[L:%.*]], i32 [[R:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int minw (int l, int r) {
  return __builtin_kvx_minw(l, r);
}

// CHECK-LABEL: @minwo(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <8 x i32> @llvm.smin.v8i32(<8 x i32> [[L:%.*]], <8 x i32> [[R:%.*]])
// CHECK-NEXT:    ret <8 x i32> [[TMP0]]
//
v8i32 minwo (v8i32 l, v8i32 r) {
  return __builtin_kvx_minwo(l, r);
}

// CHECK-LABEL: @minwp(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.smin.v2i32(<2 x i32> [[L:%.*]], <2 x i32> [[R:%.*]])
// CHECK-NEXT:    ret <2 x i32> [[TMP0]]
//
v2i32 minwp (v2i32 l, v2i32 r) {
  return __builtin_kvx_minwp(l, r);
}

// CHECK-LABEL: @minwq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <4 x i32> @llvm.smin.v4i32(<4 x i32> [[L:%.*]], <4 x i32> [[R:%.*]])
// CHECK-NEXT:    ret <4 x i32> [[TMP0]]
//
v4i32 minwq (v4i32 l, v4i32 r) {
  return __builtin_kvx_minwq(l, r);
}
