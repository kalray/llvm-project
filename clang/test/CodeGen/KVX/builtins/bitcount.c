// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -triple kvx-kalray-cos -emit-llvm %s -O2 -o - | FileCheck %s

#include "vector-types.h"

// CHECK-LABEL: @cbsw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.ctpop.i32(i32 [[I:%.*]]), [[RNG2:!range !.*]]
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int cbsw(int i) {
  return __builtin_kvx_bitcntw(i, "");
}

// CHECK-LABEL: @clzw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.ctlz.i32(i32 [[I:%.*]], i1 false), [[RNG2]]
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int clzw(int i) {
  return __builtin_kvx_bitcntw(i, ".lz");
}

// CHECK-LABEL: @clsw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.clsw(i32 [[I:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int clsw(int i) {
  return __builtin_kvx_bitcntw(i, ".ls");
}

// CHECK-LABEL: @ctzw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.cttz.i32(i32 [[I:%.*]], i1 false), [[RNG2]]
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int ctzw(int i) {
  return __builtin_kvx_bitcntw(i, ".tz");
}

// CHECK-LABEL: @cbswp(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.ctpop.v2i32(<2 x i32> [[V:%.*]])
// CHECK-NEXT:    ret <2 x i32> [[TMP0]]
//
v2i32 cbswp(v2i32 v) {
  return __builtin_kvx_bitcntwp(v, "");
}

// CHECK-LABEL: @clzwp(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.ctlz.v2i32(<2 x i32> [[V:%.*]], i1 false)
// CHECK-NEXT:    ret <2 x i32> [[TMP0]]
//
v2i32 clzwp(v2i32 v) {
  return __builtin_kvx_bitcntwp(v, ".lz");
}

// CHECK-LABEL: @clswp(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.kvx.clswp(<2 x i32> [[V:%.*]])
// CHECK-NEXT:    ret <2 x i32> [[TMP0]]
//
v2i32 clswp(v2i32 v) {
  return __builtin_kvx_bitcntwp(v, ".ls");
}

// CHECK-LABEL: @ctzwp(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.cttz.v2i32(<2 x i32> [[V:%.*]], i1 false)
// CHECK-NEXT:    ret <2 x i32> [[TMP0]]
//
v2i32 ctzwp(v2i32 v) {
  return __builtin_kvx_bitcntwp(v, ".tz");
}

// CHECK-LABEL: @cbswq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <4 x i32> @llvm.ctpop.v4i32(<4 x i32> [[V:%.*]])
// CHECK-NEXT:    ret <4 x i32> [[TMP0]]
//
v4i32 cbswq(v4i32 v) { return __builtin_kvx_bitcntwq(v, ""); }


// CHECK-LABEL: @clzwq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <4 x i32> @llvm.ctlz.v4i32(<4 x i32> [[V:%.*]], i1 false)
// CHECK-NEXT:    ret <4 x i32> [[TMP0]]
//
v4i32 clzwq(v4i32 v) { return __builtin_kvx_bitcntwq(v, ".lz"); }

// CHECK-LABEL: @clswq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = shufflevector <4 x i32> [[V:%.*]], <4 x i32> undef, <2 x i32> <i32 0, i32 1>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <2 x i32> @llvm.kvx.clswp(<2 x i32> [[TMP0]])
// CHECK-NEXT:    [[TMP2:%.*]] = shufflevector <4 x i32> [[V]], <4 x i32> undef, <2 x i32> <i32 2, i32 3>
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <2 x i32> @llvm.kvx.clswp(<2 x i32> [[TMP2]])
// CHECK-NEXT:    [[TMP4:%.*]] = shufflevector <2 x i32> [[TMP1]], <2 x i32> [[TMP3]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// CHECK-NEXT:    ret <4 x i32> [[TMP4]]
//
v4i32 clswq(v4i32 v) { return __builtin_kvx_bitcntwq(v, ".ls"); }

// CHECK-LABEL: @ctzwq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <4 x i32> @llvm.cttz.v4i32(<4 x i32> [[V:%.*]], i1 false)
// CHECK-NEXT:    ret <4 x i32> [[TMP0]]
//
v4i32 ctzwq(v4i32 v) { return __builtin_kvx_bitcntwq(v, ".tz"); }

// CHECK-LABEL: @cbswo(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <8 x i32> @llvm.ctpop.v8i32(<8 x i32> [[V:%.*]])
// CHECK-NEXT:    ret <8 x i32> [[TMP0]]
//
v8i32 cbswo(v8i32 v) { return __builtin_kvx_bitcntwo(v, ""); }

// CHECK-LABEL: @clzwo(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <8 x i32> @llvm.ctlz.v8i32(<8 x i32> [[V:%.*]], i1 false)
// CHECK-NEXT:    ret <8 x i32> [[TMP0]]
//
v8i32 clzwo(v8i32 v) { return __builtin_kvx_bitcntwo(v, ".lz"); }

// CHECK-LABEL: @clswo(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = shufflevector <8 x i32> [[V:%.*]], <8 x i32> undef, <2 x i32> <i32 0, i32 1>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <2 x i32> @llvm.kvx.clswp(<2 x i32> [[TMP0]])
// CHECK-NEXT:    [[TMP2:%.*]] = shufflevector <8 x i32> [[V]], <8 x i32> undef, <2 x i32> <i32 2, i32 3>
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <2 x i32> @llvm.kvx.clswp(<2 x i32> [[TMP2]])
// CHECK-NEXT:    [[TMP4:%.*]] = shufflevector <2 x i32> [[TMP1]], <2 x i32> [[TMP3]], <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
// CHECK-NEXT:    [[TMP5:%.*]] = shufflevector <8 x i32> [[V]], <8 x i32> undef, <2 x i32> <i32 4, i32 5>
// CHECK-NEXT:    [[TMP6:%.*]] = tail call <2 x i32> @llvm.kvx.clswp(<2 x i32> [[TMP5]])
// CHECK-NEXT:    [[TMP7:%.*]] = shufflevector <2 x i32> [[TMP6]], <2 x i32> undef, <8 x i32> <i32 0, i32 1, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
// CHECK-NEXT:    [[TMP8:%.*]] = shufflevector <8 x i32> [[TMP4]], <8 x i32> [[TMP7]], <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 undef, i32 undef>
// CHECK-NEXT:    [[TMP9:%.*]] = shufflevector <8 x i32> [[V]], <8 x i32> undef, <2 x i32> <i32 6, i32 7>
// CHECK-NEXT:    [[TMP10:%.*]] = tail call <2 x i32> @llvm.kvx.clswp(<2 x i32> [[TMP9]])
// CHECK-NEXT:    [[TMP11:%.*]] = shufflevector <2 x i32> [[TMP10]], <2 x i32> undef, <8 x i32> <i32 0, i32 1, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
// CHECK-NEXT:    [[TMP12:%.*]] = shufflevector <8 x i32> [[TMP8]], <8 x i32> [[TMP11]], <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 8, i32 9>
// CHECK-NEXT:    ret <8 x i32> [[TMP12]]
//
v8i32 clswo(v8i32 v) { return __builtin_kvx_bitcntwo(v, ".ls"); }

// CHECK-LABEL: @ctzwo(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <8 x i32> @llvm.cttz.v8i32(<8 x i32> [[V:%.*]], i1 false)
// CHECK-NEXT:    ret <8 x i32> [[TMP0]]
//
v8i32 ctzwo(v8i32 v) { return __builtin_kvx_bitcntwo(v, ".tz"); }

// CHECK-LABEL: @cbsd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.ctpop.i64(i64 [[L:%.*]]), [[RNG3:!range !.*]]
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long cbsd(long l) {
  return __builtin_kvx_bitcntd(l, "");
}

// CHECK-LABEL: @clzd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.ctlz.i64(i64 [[L:%.*]], i1 false), [[RNG3]]
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long clzd(long l) {
  return __builtin_kvx_bitcntd(l, ".lz");
}

// CHECK-LABEL: @clsd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.clsd(i64 [[L:%.*]])
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long clsd(long l) {
  return __builtin_kvx_bitcntd(l, ".ls");
}

// CHECK-LABEL: @ctzd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.cttz.i64(i64 [[L:%.*]], i1 false), [[RNG3]]
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long ctzd(long l) {
  return __builtin_kvx_bitcntd(l, ".tz");
}

// CHECK-LABEL: @cbsdp(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i64> @llvm.ctpop.v2i64(<2 x i64> [[V:%.*]])
// CHECK-NEXT:    ret <2 x i64> [[TMP0]]
//
v2i64 cbsdp(v2i64 v) { return __builtin_kvx_bitcntdp(v, ""); }

// CHECK-LABEL: @clzdp(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i64> @llvm.ctlz.v2i64(<2 x i64> [[V:%.*]], i1 false)
// CHECK-NEXT:    ret <2 x i64> [[TMP0]]
//
v2i64 clzdp(v2i64 v) { return __builtin_kvx_bitcntdp(v, ".lz"); }

// CHECK-LABEL: @clsdp(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <2 x i64> [[V:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 @llvm.kvx.clsd(i64 [[TMP0]])
// CHECK-NEXT:    [[TMP2:%.*]] = insertelement <2 x i64> undef, i64 [[TMP1]], i64 0
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <2 x i64> [[V]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = tail call i64 @llvm.kvx.clsd(i64 [[TMP3]])
// CHECK-NEXT:    [[TMP5:%.*]] = insertelement <2 x i64> [[TMP2]], i64 [[TMP4]], i64 1
// CHECK-NEXT:    ret <2 x i64> [[TMP5]]
//
v2i64 clsdp(v2i64 v) { return __builtin_kvx_bitcntdp(v, ".ls"); }

// CHECK-LABEL: @ctzdp(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i64> @llvm.cttz.v2i64(<2 x i64> [[V:%.*]], i1 false)
// CHECK-NEXT:    ret <2 x i64> [[TMP0]]
//
v2i64 ctzdp(v2i64 v) { return __builtin_kvx_bitcntdp(v, ".tz"); }

// CHECK-LABEL: @cbsdq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <4 x i64> @llvm.ctpop.v4i64(<4 x i64> [[V:%.*]])
// CHECK-NEXT:    ret <4 x i64> [[TMP0]]
//
v4i64 cbsdq(v4i64 v) { return __builtin_kvx_bitcntdq(v, ""); }

// CHECK-LABEL: @clzdq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <4 x i64> @llvm.ctlz.v4i64(<4 x i64> [[V:%.*]], i1 false)
// CHECK-NEXT:    ret <4 x i64> [[TMP0]]
//
v4i64 clzdq(v4i64 v) { return __builtin_kvx_bitcntdq(v, ".lz"); }

// CHECK-LABEL: @clsdq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <4 x i64> [[V:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 @llvm.kvx.clsd(i64 [[TMP0]])
// CHECK-NEXT:    [[TMP2:%.*]] = insertelement <4 x i64> undef, i64 [[TMP1]], i64 0
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <4 x i64> [[V]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = tail call i64 @llvm.kvx.clsd(i64 [[TMP3]])
// CHECK-NEXT:    [[TMP5:%.*]] = insertelement <4 x i64> [[TMP2]], i64 [[TMP4]], i64 1
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <4 x i64> [[V]], i64 2
// CHECK-NEXT:    [[TMP7:%.*]] = tail call i64 @llvm.kvx.clsd(i64 [[TMP6]])
// CHECK-NEXT:    [[TMP8:%.*]] = insertelement <4 x i64> [[TMP5]], i64 [[TMP7]], i64 2
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <4 x i64> [[V]], i64 3
// CHECK-NEXT:    [[TMP10:%.*]] = tail call i64 @llvm.kvx.clsd(i64 [[TMP9]])
// CHECK-NEXT:    [[TMP11:%.*]] = insertelement <4 x i64> [[TMP8]], i64 [[TMP10]], i64 3
// CHECK-NEXT:    ret <4 x i64> [[TMP11]]
//
v4i64 clsdq(v4i64 v) { return __builtin_kvx_bitcntdq(v, ".ls"); }

// CHECK-LABEL: @ctzdq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <4 x i64> @llvm.cttz.v4i64(<4 x i64> [[V:%.*]], i1 false)
// CHECK-NEXT:    ret <4 x i64> [[TMP0]]
//
v4i64 ctzdq(v4i64 v) { return __builtin_kvx_bitcntdq(v, ".tz"); }


















