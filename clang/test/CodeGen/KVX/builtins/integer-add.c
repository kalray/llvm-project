// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -triple kvx-kalray-cos -S -O2 -emit-llvm -o - %s | FileCheck %s

#include "vector-types.h"

// CHECK-LABEL: @addbo(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = add <8 x i8> [[B:%.*]], [[A:%.*]]
// CHECK-NEXT:    ret <8 x i8> [[TMP0]]
//
v8i8 addbo(v8i8 a, v8i8 b) {
  return __builtin_kvx_addbo(a, b, "");
}

// CHECK-LABEL: @addbo_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <8 x i8> @llvm.sadd.sat.v8i8(<8 x i8> [[A:%.*]], <8 x i8> [[B:%.*]])
// CHECK-NEXT:    ret <8 x i8> [[TMP0]]
//
v8i8 addbo_s(v8i8 a, v8i8 b) {
  return __builtin_kvx_addbo(a, b, ".s");
}

// CHECK-LABEL: @addbo_us(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <8 x i8> @llvm.uadd.sat.v8i8(<8 x i8> [[A:%.*]], <8 x i8> [[B:%.*]])
// CHECK-NEXT:    ret <8 x i8> [[TMP0]]
//
v8i8 addbo_us(v8i8 a, v8i8 b) {
  return __builtin_kvx_addbo(a, b, ".us");
}

// CHECK-LABEL: @addbx(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = add <16 x i8> [[B:%.*]], [[A:%.*]]
// CHECK-NEXT:    ret <16 x i8> [[TMP0]]
//
v16i8 addbx(v16i8 a, v16i8 b) {
  return __builtin_kvx_addbx(a, b, "");
}

// CHECK-LABEL: @addbx_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <16 x i8> @llvm.sadd.sat.v16i8(<16 x i8> [[A:%.*]], <16 x i8> [[B:%.*]])
// CHECK-NEXT:    ret <16 x i8> [[TMP0]]
//
v16i8 addbx_s(v16i8 a, v16i8 b) {
  return __builtin_kvx_addbx(a, b, ".s");
}

// CHECK-LABEL: @addbx_us(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <16 x i8> @llvm.uadd.sat.v16i8(<16 x i8> [[A:%.*]], <16 x i8> [[B:%.*]])
// CHECK-NEXT:    ret <16 x i8> [[TMP0]]
//
v16i8 addbx_us(v16i8 a, v16i8 b) {
  return __builtin_kvx_addbx(a, b, ".us");
}

// CHECK-LABEL: @addbv(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = add <32 x i8> [[B:%.*]], [[A:%.*]]
// CHECK-NEXT:    ret <32 x i8> [[TMP0]]
//
v32i8 addbv(v32i8 a, v32i8 b) {
  return __builtin_kvx_addbv(a, b, "");
}

// CHECK-LABEL: @addbv_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <32 x i8> @llvm.sadd.sat.v32i8(<32 x i8> [[A:%.*]], <32 x i8> [[B:%.*]])
// CHECK-NEXT:    ret <32 x i8> [[TMP0]]
//
v32i8 addbv_s(v32i8 a, v32i8 b) {
  return __builtin_kvx_addbv(a, b, ".s");
}

// CHECK-LABEL: @addbv_us(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <32 x i8> @llvm.uadd.sat.v32i8(<32 x i8> [[A:%.*]], <32 x i8> [[B:%.*]])
// CHECK-NEXT:    ret <32 x i8> [[TMP0]]
//
v32i8 addbv_us(v32i8 a, v32i8 b) {
  return __builtin_kvx_addbv(a, b, ".us");
}

// CHECK-LABEL: @addhq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = add <4 x i16> [[B:%.*]], [[A:%.*]]
// CHECK-NEXT:    ret <4 x i16> [[TMP0]]
//
v4i16 addhq(v4i16 a, v4i16 b) {
  return __builtin_kvx_addhq(a, b, "");
}

// CHECK-LABEL: @addhq_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <4 x i16> @llvm.sadd.sat.v4i16(<4 x i16> [[A:%.*]], <4 x i16> [[B:%.*]])
// CHECK-NEXT:    ret <4 x i16> [[TMP0]]
//
v4i16 addhq_s(v4i16 a, v4i16 b) {
  return __builtin_kvx_addhq(a, b, ".s");
}

// CHECK-LABEL: @addhq_us(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <4 x i16> @llvm.uadd.sat.v4i16(<4 x i16> [[A:%.*]], <4 x i16> [[B:%.*]])
// CHECK-NEXT:    ret <4 x i16> [[TMP0]]
//
v4i16 addhq_us(v4i16 a, v4i16 b) {
  return __builtin_kvx_addhq(a, b, ".us");
}

// CHECK-LABEL: @addho(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = add <8 x i16> [[B:%.*]], [[A:%.*]]
// CHECK-NEXT:    ret <8 x i16> [[TMP0]]
//
v8i16 addho(v8i16 a, v8i16 b) {
  return __builtin_kvx_addho(a, b, "");
}

// CHECK-LABEL: @addho_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> [[A:%.*]], <8 x i16> [[B:%.*]])
// CHECK-NEXT:    ret <8 x i16> [[TMP0]]
//
v8i16 addho_s(v8i16 a, v8i16 b) {
  return __builtin_kvx_addho(a, b, ".s");
}

// CHECK-LABEL: @addho_us(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> [[A:%.*]], <8 x i16> [[B:%.*]])
// CHECK-NEXT:    ret <8 x i16> [[TMP0]]
//
v8i16 addho_us(v8i16 a, v8i16 b) {
  return __builtin_kvx_addho(a, b, ".us");
}

// CHECK-LABEL: @addhx(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = add <16 x i16> [[B:%.*]], [[A:%.*]]
// CHECK-NEXT:    ret <16 x i16> [[TMP0]]
//
v16i16 addhx(v16i16 a, v16i16 b) {
  return __builtin_kvx_addhx(a, b, "");
}

// CHECK-LABEL: @addhx_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> [[A:%.*]], <16 x i16> [[B:%.*]])
// CHECK-NEXT:    ret <16 x i16> [[TMP0]]
//
v16i16 addhx_s(v16i16 a, v16i16 b) {
  return __builtin_kvx_addhx(a, b, ".s");
}

// CHECK-LABEL: @addhx_us(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> [[A:%.*]], <16 x i16> [[B:%.*]])
// CHECK-NEXT:    ret <16 x i16> [[TMP0]]
//
v16i16 addhx_us(v16i16 a, v16i16 b) {
  return __builtin_kvx_addhx(a, b, ".us");
}

// CHECK-LABEL: @addw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = add i32 [[B:%.*]], [[A:%.*]]
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int addw(int a, int b) {
  return __builtin_kvx_addw(a, b, "");
}

// CHECK-LABEL: @addw_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.sadd.sat.i32(i32 [[A:%.*]], i32 [[B:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int addw_s(int a, int b) {
  return __builtin_kvx_addw(a, b, ".s");
}

// CHECK-LABEL: @addw_us(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.uadd.sat.i32(i32 [[A:%.*]], i32 [[B:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int addw_us(int a, int b) {
  return __builtin_kvx_addw(a, b, ".us");
}

// CHECK-LABEL: @addwp(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = add <2 x i32> [[B:%.*]], [[A:%.*]]
// CHECK-NEXT:    ret <2 x i32> [[TMP0]]
//
v2i32 addwp(v2i32 a, v2i32 b) {
  return __builtin_kvx_addwp(a, b, "");
}

// CHECK-LABEL: @addwp_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.sadd.sat.v2i32(<2 x i32> [[A:%.*]], <2 x i32> [[B:%.*]])
// CHECK-NEXT:    ret <2 x i32> [[TMP0]]
//
v2i32 addwp_s(v2i32 a, v2i32 b) {
  return __builtin_kvx_addwp(a, b, ".s");
}

// CHECK-LABEL: @addwp_us(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.uadd.sat.v2i32(<2 x i32> [[A:%.*]], <2 x i32> [[B:%.*]])
// CHECK-NEXT:    ret <2 x i32> [[TMP0]]
//
v2i32 addwp_us(v2i32 a, v2i32 b) {
  return __builtin_kvx_addwp(a, b, ".us");
}

// CHECK-LABEL: @addwq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = add <4 x i32> [[B:%.*]], [[A:%.*]]
// CHECK-NEXT:    ret <4 x i32> [[TMP0]]
//
v4i32 addwq(v4i32 a, v4i32 b) {
  return __builtin_kvx_addwq(a, b, "");
}

// CHECK-LABEL: @addwq_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <4 x i32> @llvm.sadd.sat.v4i32(<4 x i32> [[A:%.*]], <4 x i32> [[B:%.*]])
// CHECK-NEXT:    ret <4 x i32> [[TMP0]]
//
v4i32 addwq_s(v4i32 a, v4i32 b) {
  return __builtin_kvx_addwq(a, b, ".s");
}

// CHECK-LABEL: @addwq_us(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <4 x i32> @llvm.uadd.sat.v4i32(<4 x i32> [[A:%.*]], <4 x i32> [[B:%.*]])
// CHECK-NEXT:    ret <4 x i32> [[TMP0]]
//
v4i32 addwq_us(v4i32 a, v4i32 b) {
  return __builtin_kvx_addwq(a, b, ".us");
}

// CHECK-LABEL: @addwo(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = add <8 x i32> [[B:%.*]], [[A:%.*]]
// CHECK-NEXT:    ret <8 x i32> [[TMP0]]
//
v8i32 addwo(v8i32 a, v8i32 b) {
  return __builtin_kvx_addwo(a, b, "");
}

// CHECK-LABEL: @addwo_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <8 x i32> @llvm.sadd.sat.v8i32(<8 x i32> [[A:%.*]], <8 x i32> [[B:%.*]])
// CHECK-NEXT:    ret <8 x i32> [[TMP0]]
//
v8i32 addwo_s(v8i32 a, v8i32 b) {
  return __builtin_kvx_addwo(a, b, ".s");
}

// CHECK-LABEL: @addwo_us(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <8 x i32> @llvm.uadd.sat.v8i32(<8 x i32> [[A:%.*]], <8 x i32> [[B:%.*]])
// CHECK-NEXT:    ret <8 x i32> [[TMP0]]
//
v8i32 addwo_us(v8i32 a, v8i32 b) {
  return __builtin_kvx_addwo(a, b, ".us");
}

// CHECK-LABEL: @addd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[B:%.*]], [[A:%.*]]
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long int addd(long int a, long int b) {
  return __builtin_kvx_addd(a, b, "");
}

// CHECK-LABEL: @addd_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.sadd.sat.i64(i64 [[A:%.*]], i64 [[B:%.*]])
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long int addd_s(long int a, long int b) {
  return __builtin_kvx_addd(a, b, ".s");
}

// CHECK-LABEL: @saddd_us(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.uadd.sat.i64(i64 [[A:%.*]], i64 [[B:%.*]])
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long int saddd_us(long int a, long int b) {
  return __builtin_kvx_addd(a, b, ".us");
}

// CHECK-LABEL: @adddp(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = add <2 x i64> [[B:%.*]], [[A:%.*]]
// CHECK-NEXT:    ret <2 x i64> [[TMP0]]
//
v2i64 adddp(v2i64 a, v2i64 b) {
  return __builtin_kvx_adddp(a, b, "");
}

// CHECK-LABEL: @adddp_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i64> @llvm.sadd.sat.v2i64(<2 x i64> [[A:%.*]], <2 x i64> [[B:%.*]])
// CHECK-NEXT:    ret <2 x i64> [[TMP0]]
//
v2i64 adddp_s(v2i64 a, v2i64 b) {
  return __builtin_kvx_adddp(a, b, ".s");
}

// CHECK-LABEL: @adddp_us(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i64> @llvm.uadd.sat.v2i64(<2 x i64> [[A:%.*]], <2 x i64> [[B:%.*]])
// CHECK-NEXT:    ret <2 x i64> [[TMP0]]
//
v2i64 adddp_us(v2i64 a, v2i64 b) {
  return __builtin_kvx_adddp(a, b, ".us");
}

// CHECK-LABEL: @adddq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = add <4 x i64> [[B:%.*]], [[A:%.*]]
// CHECK-NEXT:    ret <4 x i64> [[TMP0]]
//
v4i64 adddq(v4i64 a, v4i64 b) {
  return __builtin_kvx_adddq(a, b, "");
}

// CHECK-LABEL: @adddq_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <4 x i64> @llvm.sadd.sat.v4i64(<4 x i64> [[A:%.*]], <4 x i64> [[B:%.*]])
// CHECK-NEXT:    ret <4 x i64> [[TMP0]]
//
v4i64 adddq_s(v4i64 a, v4i64 b) {
  return __builtin_kvx_adddq(a, b, ".s");
}

// CHECK-LABEL: @adddq_us(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <4 x i64> @llvm.uadd.sat.v4i64(<4 x i64> [[A:%.*]], <4 x i64> [[B:%.*]])
// CHECK-NEXT:    ret <4 x i64> [[TMP0]]
//
v4i64 adddq_us(v4i64 a, v4i64 b) {
  return __builtin_kvx_adddq(a, b, ".us");
}
