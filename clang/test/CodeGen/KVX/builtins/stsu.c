// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -O2 -triple kvx-kalray-cos -S -emit-llvm -target-cpu kv3-2 -o - -x c %s | FileCheck %s
// RUN: %clang_cc1 -O2 -triple kvx-kalray-cos -S -emit-llvm -target-cpu kv3-1 -o - -x c %s | FileCheck %s

typedef unsigned short short2 __attribute__ ((vector_size (sizeof(short) * 2)));
typedef unsigned short short4 __attribute__ ((vector_size (sizeof(short) * 4)));
typedef unsigned int int2 __attribute__ ((vector_size (sizeof(int) * 2)));
typedef unsigned long ulong;
typedef ulong ulong2 __attribute__ ((vector_size (sizeof(ulong) * 2)));
typedef ulong ulong4 __attribute__ ((vector_size (sizeof(ulong) * 4)));
typedef ulong ulong8 __attribute__ ((vector_size (sizeof(ulong) * 8)));
typedef ulong ulong16 __attribute__ ((vector_size (sizeof(ulong) * 16)));
typedef unsigned uint;

// CHECK-LABEL: @stsudp(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <2 x i64> [[L:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = extractelement <2 x i64> [[R:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 @llvm.kvx.stsu.i64(i64 [[TMP0]], i64 [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <2 x i64> [[L]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <2 x i64> [[R]], i64 1
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i64 @llvm.kvx.stsu.i64(i64 [[TMP3]], i64 [[TMP4]])
// CHECK-NEXT:    [[TMP6:%.*]] = insertelement <2 x i64> undef, i64 [[TMP2]], i64 0
// CHECK-NEXT:    [[TMP7:%.*]] = insertelement <2 x i64> [[TMP6]], i64 [[TMP5]], i64 1
// CHECK-NEXT:    ret <2 x i64> [[TMP7]]
//
ulong2 stsudp(ulong2 l, ulong2 r) {
  return __builtin_kvx_stsudp(l, r);
}

// CHECK-LABEL: @stsudq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <4 x i64> [[L:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = extractelement <4 x i64> [[R:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 @llvm.kvx.stsu.i64(i64 [[TMP0]], i64 [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <4 x i64> [[L]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <4 x i64> [[R]], i64 1
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i64 @llvm.kvx.stsu.i64(i64 [[TMP3]], i64 [[TMP4]])
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <4 x i64> [[L]], i64 2
// CHECK-NEXT:    [[TMP7:%.*]] = extractelement <4 x i64> [[R]], i64 2
// CHECK-NEXT:    [[TMP8:%.*]] = tail call i64 @llvm.kvx.stsu.i64(i64 [[TMP6]], i64 [[TMP7]])
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <4 x i64> [[L]], i64 3
// CHECK-NEXT:    [[TMP10:%.*]] = extractelement <4 x i64> [[R]], i64 3
// CHECK-NEXT:    [[TMP11:%.*]] = tail call i64 @llvm.kvx.stsu.i64(i64 [[TMP9]], i64 [[TMP10]])
// CHECK-NEXT:    [[TMP12:%.*]] = insertelement <4 x i64> undef, i64 [[TMP2]], i64 0
// CHECK-NEXT:    [[TMP13:%.*]] = insertelement <4 x i64> [[TMP12]], i64 [[TMP5]], i64 1
// CHECK-NEXT:    [[TMP14:%.*]] = insertelement <4 x i64> [[TMP13]], i64 [[TMP8]], i64 2
// CHECK-NEXT:    [[TMP15:%.*]] = insertelement <4 x i64> [[TMP14]], i64 [[TMP11]], i64 3
// CHECK-NEXT:    ret <4 x i64> [[TMP15]]
//
ulong4 stsudq(ulong4 l, ulong4 r) {
  return __builtin_kvx_stsudq(l, r);
}

// CHECK-LABEL: @stsudo(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[L:%.*]] = load <8 x i64>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2:![0-9]+]]
// CHECK-NEXT:    [[R:%.*]] = load <8 x i64>, ptr [[TMP1:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP2:%.*]] = extractelement <8 x i64> [[L]], i64 0
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <8 x i64> [[R]], i64 0
// CHECK-NEXT:    [[TMP4:%.*]] = tail call i64 @llvm.kvx.stsu.i64(i64 [[TMP2]], i64 [[TMP3]])
// CHECK-NEXT:    [[TMP5:%.*]] = extractelement <8 x i64> [[L]], i64 1
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <8 x i64> [[R]], i64 1
// CHECK-NEXT:    [[TMP7:%.*]] = tail call i64 @llvm.kvx.stsu.i64(i64 [[TMP5]], i64 [[TMP6]])
// CHECK-NEXT:    [[TMP8:%.*]] = extractelement <8 x i64> [[L]], i64 2
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <8 x i64> [[R]], i64 2
// CHECK-NEXT:    [[TMP10:%.*]] = tail call i64 @llvm.kvx.stsu.i64(i64 [[TMP8]], i64 [[TMP9]])
// CHECK-NEXT:    [[TMP11:%.*]] = extractelement <8 x i64> [[L]], i64 3
// CHECK-NEXT:    [[TMP12:%.*]] = extractelement <8 x i64> [[R]], i64 3
// CHECK-NEXT:    [[TMP13:%.*]] = tail call i64 @llvm.kvx.stsu.i64(i64 [[TMP11]], i64 [[TMP12]])
// CHECK-NEXT:    [[TMP14:%.*]] = extractelement <8 x i64> [[L]], i64 4
// CHECK-NEXT:    [[TMP15:%.*]] = extractelement <8 x i64> [[R]], i64 4
// CHECK-NEXT:    [[TMP16:%.*]] = tail call i64 @llvm.kvx.stsu.i64(i64 [[TMP14]], i64 [[TMP15]])
// CHECK-NEXT:    [[TMP17:%.*]] = extractelement <8 x i64> [[L]], i64 5
// CHECK-NEXT:    [[TMP18:%.*]] = extractelement <8 x i64> [[R]], i64 5
// CHECK-NEXT:    [[TMP19:%.*]] = tail call i64 @llvm.kvx.stsu.i64(i64 [[TMP17]], i64 [[TMP18]])
// CHECK-NEXT:    [[TMP20:%.*]] = extractelement <8 x i64> [[L]], i64 6
// CHECK-NEXT:    [[TMP21:%.*]] = extractelement <8 x i64> [[R]], i64 6
// CHECK-NEXT:    [[TMP22:%.*]] = tail call i64 @llvm.kvx.stsu.i64(i64 [[TMP20]], i64 [[TMP21]])
// CHECK-NEXT:    [[TMP23:%.*]] = extractelement <8 x i64> [[L]], i64 7
// CHECK-NEXT:    [[TMP24:%.*]] = extractelement <8 x i64> [[R]], i64 7
// CHECK-NEXT:    [[TMP25:%.*]] = tail call i64 @llvm.kvx.stsu.i64(i64 [[TMP23]], i64 [[TMP24]])
// CHECK-NEXT:    [[TMP26:%.*]] = insertelement <8 x i64> undef, i64 [[TMP4]], i64 0
// CHECK-NEXT:    [[TMP27:%.*]] = insertelement <8 x i64> [[TMP26]], i64 [[TMP7]], i64 1
// CHECK-NEXT:    [[TMP28:%.*]] = insertelement <8 x i64> [[TMP27]], i64 [[TMP10]], i64 2
// CHECK-NEXT:    [[TMP29:%.*]] = insertelement <8 x i64> [[TMP28]], i64 [[TMP13]], i64 3
// CHECK-NEXT:    [[TMP30:%.*]] = insertelement <8 x i64> [[TMP29]], i64 [[TMP16]], i64 4
// CHECK-NEXT:    [[TMP31:%.*]] = insertelement <8 x i64> [[TMP30]], i64 [[TMP19]], i64 5
// CHECK-NEXT:    [[TMP32:%.*]] = insertelement <8 x i64> [[TMP31]], i64 [[TMP22]], i64 6
// CHECK-NEXT:    [[TMP33:%.*]] = insertelement <8 x i64> [[TMP32]], i64 [[TMP25]], i64 7
// CHECK-NEXT:    store <8 x i64> [[TMP33]], ptr [[AGG_RESULT:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
ulong8 stsudo(ulong8 l, ulong8 r) {
  return __builtin_kvx_stsudo(l, r);
}

// CHECK-LABEL: @stsudx(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[L:%.*]] = load <16 x i64>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[R:%.*]] = load <16 x i64>, ptr [[TMP1:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP2:%.*]] = extractelement <16 x i64> [[L]], i64 0
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <16 x i64> [[R]], i64 0
// CHECK-NEXT:    [[TMP4:%.*]] = tail call i64 @llvm.kvx.stsu.i64(i64 [[TMP2]], i64 [[TMP3]])
// CHECK-NEXT:    [[TMP5:%.*]] = extractelement <16 x i64> [[L]], i64 1
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <16 x i64> [[R]], i64 1
// CHECK-NEXT:    [[TMP7:%.*]] = tail call i64 @llvm.kvx.stsu.i64(i64 [[TMP5]], i64 [[TMP6]])
// CHECK-NEXT:    [[TMP8:%.*]] = extractelement <16 x i64> [[L]], i64 2
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <16 x i64> [[R]], i64 2
// CHECK-NEXT:    [[TMP10:%.*]] = tail call i64 @llvm.kvx.stsu.i64(i64 [[TMP8]], i64 [[TMP9]])
// CHECK-NEXT:    [[TMP11:%.*]] = extractelement <16 x i64> [[L]], i64 3
// CHECK-NEXT:    [[TMP12:%.*]] = extractelement <16 x i64> [[R]], i64 3
// CHECK-NEXT:    [[TMP13:%.*]] = tail call i64 @llvm.kvx.stsu.i64(i64 [[TMP11]], i64 [[TMP12]])
// CHECK-NEXT:    [[TMP14:%.*]] = extractelement <16 x i64> [[L]], i64 4
// CHECK-NEXT:    [[TMP15:%.*]] = extractelement <16 x i64> [[R]], i64 4
// CHECK-NEXT:    [[TMP16:%.*]] = tail call i64 @llvm.kvx.stsu.i64(i64 [[TMP14]], i64 [[TMP15]])
// CHECK-NEXT:    [[TMP17:%.*]] = extractelement <16 x i64> [[L]], i64 5
// CHECK-NEXT:    [[TMP18:%.*]] = extractelement <16 x i64> [[R]], i64 5
// CHECK-NEXT:    [[TMP19:%.*]] = tail call i64 @llvm.kvx.stsu.i64(i64 [[TMP17]], i64 [[TMP18]])
// CHECK-NEXT:    [[TMP20:%.*]] = extractelement <16 x i64> [[L]], i64 6
// CHECK-NEXT:    [[TMP21:%.*]] = extractelement <16 x i64> [[R]], i64 6
// CHECK-NEXT:    [[TMP22:%.*]] = tail call i64 @llvm.kvx.stsu.i64(i64 [[TMP20]], i64 [[TMP21]])
// CHECK-NEXT:    [[TMP23:%.*]] = extractelement <16 x i64> [[L]], i64 7
// CHECK-NEXT:    [[TMP24:%.*]] = extractelement <16 x i64> [[R]], i64 7
// CHECK-NEXT:    [[TMP25:%.*]] = tail call i64 @llvm.kvx.stsu.i64(i64 [[TMP23]], i64 [[TMP24]])
// CHECK-NEXT:    [[TMP26:%.*]] = extractelement <16 x i64> [[L]], i64 8
// CHECK-NEXT:    [[TMP27:%.*]] = extractelement <16 x i64> [[R]], i64 8
// CHECK-NEXT:    [[TMP28:%.*]] = tail call i64 @llvm.kvx.stsu.i64(i64 [[TMP26]], i64 [[TMP27]])
// CHECK-NEXT:    [[TMP29:%.*]] = extractelement <16 x i64> [[L]], i64 9
// CHECK-NEXT:    [[TMP30:%.*]] = extractelement <16 x i64> [[R]], i64 9
// CHECK-NEXT:    [[TMP31:%.*]] = tail call i64 @llvm.kvx.stsu.i64(i64 [[TMP29]], i64 [[TMP30]])
// CHECK-NEXT:    [[TMP32:%.*]] = extractelement <16 x i64> [[L]], i64 10
// CHECK-NEXT:    [[TMP33:%.*]] = extractelement <16 x i64> [[R]], i64 10
// CHECK-NEXT:    [[TMP34:%.*]] = tail call i64 @llvm.kvx.stsu.i64(i64 [[TMP32]], i64 [[TMP33]])
// CHECK-NEXT:    [[TMP35:%.*]] = extractelement <16 x i64> [[L]], i64 11
// CHECK-NEXT:    [[TMP36:%.*]] = extractelement <16 x i64> [[R]], i64 11
// CHECK-NEXT:    [[TMP37:%.*]] = tail call i64 @llvm.kvx.stsu.i64(i64 [[TMP35]], i64 [[TMP36]])
// CHECK-NEXT:    [[TMP38:%.*]] = extractelement <16 x i64> [[L]], i64 12
// CHECK-NEXT:    [[TMP39:%.*]] = extractelement <16 x i64> [[R]], i64 12
// CHECK-NEXT:    [[TMP40:%.*]] = tail call i64 @llvm.kvx.stsu.i64(i64 [[TMP38]], i64 [[TMP39]])
// CHECK-NEXT:    [[TMP41:%.*]] = extractelement <16 x i64> [[L]], i64 13
// CHECK-NEXT:    [[TMP42:%.*]] = extractelement <16 x i64> [[R]], i64 13
// CHECK-NEXT:    [[TMP43:%.*]] = tail call i64 @llvm.kvx.stsu.i64(i64 [[TMP41]], i64 [[TMP42]])
// CHECK-NEXT:    [[TMP44:%.*]] = extractelement <16 x i64> [[L]], i64 14
// CHECK-NEXT:    [[TMP45:%.*]] = extractelement <16 x i64> [[R]], i64 14
// CHECK-NEXT:    [[TMP46:%.*]] = tail call i64 @llvm.kvx.stsu.i64(i64 [[TMP44]], i64 [[TMP45]])
// CHECK-NEXT:    [[TMP47:%.*]] = extractelement <16 x i64> [[L]], i64 15
// CHECK-NEXT:    [[TMP48:%.*]] = extractelement <16 x i64> [[R]], i64 15
// CHECK-NEXT:    [[TMP49:%.*]] = tail call i64 @llvm.kvx.stsu.i64(i64 [[TMP47]], i64 [[TMP48]])
// CHECK-NEXT:    [[TMP50:%.*]] = insertelement <16 x i64> undef, i64 [[TMP4]], i64 0
// CHECK-NEXT:    [[TMP51:%.*]] = insertelement <16 x i64> [[TMP50]], i64 [[TMP7]], i64 1
// CHECK-NEXT:    [[TMP52:%.*]] = insertelement <16 x i64> [[TMP51]], i64 [[TMP10]], i64 2
// CHECK-NEXT:    [[TMP53:%.*]] = insertelement <16 x i64> [[TMP52]], i64 [[TMP13]], i64 3
// CHECK-NEXT:    [[TMP54:%.*]] = insertelement <16 x i64> [[TMP53]], i64 [[TMP16]], i64 4
// CHECK-NEXT:    [[TMP55:%.*]] = insertelement <16 x i64> [[TMP54]], i64 [[TMP19]], i64 5
// CHECK-NEXT:    [[TMP56:%.*]] = insertelement <16 x i64> [[TMP55]], i64 [[TMP22]], i64 6
// CHECK-NEXT:    [[TMP57:%.*]] = insertelement <16 x i64> [[TMP56]], i64 [[TMP25]], i64 7
// CHECK-NEXT:    [[TMP58:%.*]] = insertelement <16 x i64> [[TMP57]], i64 [[TMP28]], i64 8
// CHECK-NEXT:    [[TMP59:%.*]] = insertelement <16 x i64> [[TMP58]], i64 [[TMP31]], i64 9
// CHECK-NEXT:    [[TMP60:%.*]] = insertelement <16 x i64> [[TMP59]], i64 [[TMP34]], i64 10
// CHECK-NEXT:    [[TMP61:%.*]] = insertelement <16 x i64> [[TMP60]], i64 [[TMP37]], i64 11
// CHECK-NEXT:    [[TMP62:%.*]] = insertelement <16 x i64> [[TMP61]], i64 [[TMP40]], i64 12
// CHECK-NEXT:    [[TMP63:%.*]] = insertelement <16 x i64> [[TMP62]], i64 [[TMP43]], i64 13
// CHECK-NEXT:    [[TMP64:%.*]] = insertelement <16 x i64> [[TMP63]], i64 [[TMP46]], i64 14
// CHECK-NEXT:    [[TMP65:%.*]] = insertelement <16 x i64> [[TMP64]], i64 [[TMP49]], i64 15
// CHECK-NEXT:    store <16 x i64> [[TMP65]], ptr [[AGG_RESULT:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
ulong16 stsudx(ulong16 l, ulong16 r) {
  return __builtin_kvx_stsudx(l, r);
}

// CHECK-LABEL: @stsuw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.stsu.i32(i32 [[V:%.*]], i32 [[A:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
uint stsuw(uint v, uint a){
  return __builtin_kvx_stsuw(v, a);
}
