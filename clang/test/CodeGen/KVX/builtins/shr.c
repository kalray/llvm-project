// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -target-cpu kv3-1 -triple kvx-kalray-cos -S -O2 -emit-llvm -o - %s | FileCheck %s
// RUN: %clang_cc1 -target-cpu kv3-2 -triple kvx-kalray-cos -S -O2 -emit-llvm -o - %s | FileCheck %s

#include "vector-types.h"

// CHECK-LABEL: @shrd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[V:%.*]], i32 [[SHT:%.*]], i32 0)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long shrd (long v, unsigned sht) { return __builtin_kvx_shrd(v, sht, ""); }

// CHECK-LABEL: @shrd_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[V:%.*]], i32 [[SHT:%.*]], i32 1)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long shrd_s (long v, unsigned sht) { return __builtin_kvx_shrd(v, sht, ".a"); }

// CHECK-LABEL: @shrd_us(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[V:%.*]], i32 [[SHT:%.*]], i32 2)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long shrd_us (long v, unsigned sht) { return __builtin_kvx_shrd(v, sht, ".as"); }

// CHECK-LABEL: @shrd_r(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[V:%.*]], i32 [[SHT:%.*]], i32 3)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long shrd_r (long v, unsigned sht) { return __builtin_kvx_shrd(v, sht, ".r"); }

// CHECK-LABEL: @shrdp(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <2 x i64> [[V:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = extractelement <2 x i32> [[SHT:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP0]], i32 [[TMP1]], i32 0)
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <2 x i64> [[V]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <2 x i32> [[SHT]], i64 1
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP3]], i32 [[TMP4]], i32 0)
// CHECK-NEXT:    [[TMP6:%.*]] = insertelement <2 x i64> undef, i64 [[TMP2]], i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = insertelement <2 x i64> [[TMP6]], i64 [[TMP5]], i32 1
// CHECK-NEXT:    ret <2 x i64> [[TMP7]]
//
v2i64 shrdp (v2i64 v, v2u32 sht) { return __builtin_kvx_shrdp(v, sht, ""); }

// CHECK-LABEL: @shrdp_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <2 x i64> [[V:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = extractelement <2 x i32> [[SHT:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP0]], i32 [[TMP1]], i32 1)
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <2 x i64> [[V]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <2 x i32> [[SHT]], i64 1
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP3]], i32 [[TMP4]], i32 1)
// CHECK-NEXT:    [[TMP6:%.*]] = insertelement <2 x i64> undef, i64 [[TMP2]], i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = insertelement <2 x i64> [[TMP6]], i64 [[TMP5]], i32 1
// CHECK-NEXT:    ret <2 x i64> [[TMP7]]
//
v2i64 shrdp_s (v2i64 v, v2u32 sht) { return __builtin_kvx_shrdp(v, sht, ".a"); }

// CHECK-LABEL: @shrdp_us(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <2 x i64> [[V:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = extractelement <2 x i32> [[SHT:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP0]], i32 [[TMP1]], i32 2)
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <2 x i64> [[V]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <2 x i32> [[SHT]], i64 1
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP3]], i32 [[TMP4]], i32 2)
// CHECK-NEXT:    [[TMP6:%.*]] = insertelement <2 x i64> undef, i64 [[TMP2]], i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = insertelement <2 x i64> [[TMP6]], i64 [[TMP5]], i32 1
// CHECK-NEXT:    ret <2 x i64> [[TMP7]]
//
v2i64 shrdp_us (v2i64 v, v2u32 sht) { return __builtin_kvx_shrdp(v, sht, ".as"); }

// CHECK-LABEL: @shrdp_r(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <2 x i64> [[V:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = extractelement <2 x i32> [[SHT:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP0]], i32 [[TMP1]], i32 3)
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <2 x i64> [[V]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <2 x i32> [[SHT]], i64 1
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP3]], i32 [[TMP4]], i32 3)
// CHECK-NEXT:    [[TMP6:%.*]] = insertelement <2 x i64> undef, i64 [[TMP2]], i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = insertelement <2 x i64> [[TMP6]], i64 [[TMP5]], i32 1
// CHECK-NEXT:    ret <2 x i64> [[TMP7]]
//
v2i64 shrdp_r (v2i64 v, v2u32 sht) { return __builtin_kvx_shrdp(v, sht, ".r"); }

// CHECK-LABEL: @shrdps(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <2 x i64> [[V:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP0]], i32 [[SHT:%.*]], i32 0)
// CHECK-NEXT:    [[TMP2:%.*]] = extractelement <2 x i64> [[V]], i64 1
// CHECK-NEXT:    [[TMP3:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP2]], i32 [[SHT]], i32 0)
// CHECK-NEXT:    [[TMP4:%.*]] = insertelement <2 x i64> undef, i64 [[TMP1]], i32 0
// CHECK-NEXT:    [[TMP5:%.*]] = insertelement <2 x i64> [[TMP4]], i64 [[TMP3]], i32 1
// CHECK-NEXT:    ret <2 x i64> [[TMP5]]
//
v2i64 shrdps (v2i64 v, unsigned sht) { return __builtin_kvx_shrdps(v, sht, ""); }

// CHECK-LABEL: @shrdps_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <2 x i64> [[V:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP0]], i32 [[SHT:%.*]], i32 1)
// CHECK-NEXT:    [[TMP2:%.*]] = extractelement <2 x i64> [[V]], i64 1
// CHECK-NEXT:    [[TMP3:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP2]], i32 [[SHT]], i32 1)
// CHECK-NEXT:    [[TMP4:%.*]] = insertelement <2 x i64> undef, i64 [[TMP1]], i32 0
// CHECK-NEXT:    [[TMP5:%.*]] = insertelement <2 x i64> [[TMP4]], i64 [[TMP3]], i32 1
// CHECK-NEXT:    ret <2 x i64> [[TMP5]]
//
v2i64 shrdps_s (v2i64 v, unsigned sht) { return __builtin_kvx_shrdps(v, sht, ".a"); }

// CHECK-LABEL: @shrdps_us(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <2 x i64> [[V:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP0]], i32 [[SHT:%.*]], i32 2)
// CHECK-NEXT:    [[TMP2:%.*]] = extractelement <2 x i64> [[V]], i64 1
// CHECK-NEXT:    [[TMP3:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP2]], i32 [[SHT]], i32 2)
// CHECK-NEXT:    [[TMP4:%.*]] = insertelement <2 x i64> undef, i64 [[TMP1]], i32 0
// CHECK-NEXT:    [[TMP5:%.*]] = insertelement <2 x i64> [[TMP4]], i64 [[TMP3]], i32 1
// CHECK-NEXT:    ret <2 x i64> [[TMP5]]
//
v2i64 shrdps_us (v2i64 v, unsigned sht) { return __builtin_kvx_shrdps(v, sht, ".as"); }

// CHECK-LABEL: @shrdps_r(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <2 x i64> [[V:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP0]], i32 [[SHT:%.*]], i32 3)
// CHECK-NEXT:    [[TMP2:%.*]] = extractelement <2 x i64> [[V]], i64 1
// CHECK-NEXT:    [[TMP3:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP2]], i32 [[SHT]], i32 3)
// CHECK-NEXT:    [[TMP4:%.*]] = insertelement <2 x i64> undef, i64 [[TMP1]], i32 0
// CHECK-NEXT:    [[TMP5:%.*]] = insertelement <2 x i64> [[TMP4]], i64 [[TMP3]], i32 1
// CHECK-NEXT:    ret <2 x i64> [[TMP5]]
//
v2i64 shrdps_r (v2i64 v, unsigned sht) { return __builtin_kvx_shrdps(v, sht, ".r"); }

// CHECK-LABEL: @shrdq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <4 x i64> [[V:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = extractelement <4 x i32> [[SHT:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP0]], i32 [[TMP1]], i32 0)
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <4 x i64> [[V]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <4 x i32> [[SHT]], i64 1
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP3]], i32 [[TMP4]], i32 0)
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <4 x i64> [[V]], i64 2
// CHECK-NEXT:    [[TMP7:%.*]] = extractelement <4 x i32> [[SHT]], i64 2
// CHECK-NEXT:    [[TMP8:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP6]], i32 [[TMP7]], i32 0)
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <4 x i64> [[V]], i64 3
// CHECK-NEXT:    [[TMP10:%.*]] = extractelement <4 x i32> [[SHT]], i64 3
// CHECK-NEXT:    [[TMP11:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP9]], i32 [[TMP10]], i32 0)
// CHECK-NEXT:    [[TMP12:%.*]] = insertelement <4 x i64> undef, i64 [[TMP2]], i32 0
// CHECK-NEXT:    [[TMP13:%.*]] = insertelement <4 x i64> [[TMP12]], i64 [[TMP5]], i32 1
// CHECK-NEXT:    [[TMP14:%.*]] = insertelement <4 x i64> [[TMP13]], i64 [[TMP8]], i32 2
// CHECK-NEXT:    [[TMP15:%.*]] = insertelement <4 x i64> [[TMP14]], i64 [[TMP11]], i32 3
// CHECK-NEXT:    ret <4 x i64> [[TMP15]]
//
v4i64 shrdq (v4i64 v, v4u32 sht) { return __builtin_kvx_shrdq(v, sht, ""); }

// CHECK-LABEL: @shrdq_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <4 x i64> [[V:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = extractelement <4 x i32> [[SHT:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP0]], i32 [[TMP1]], i32 1)
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <4 x i64> [[V]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <4 x i32> [[SHT]], i64 1
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP3]], i32 [[TMP4]], i32 1)
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <4 x i64> [[V]], i64 2
// CHECK-NEXT:    [[TMP7:%.*]] = extractelement <4 x i32> [[SHT]], i64 2
// CHECK-NEXT:    [[TMP8:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP6]], i32 [[TMP7]], i32 1)
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <4 x i64> [[V]], i64 3
// CHECK-NEXT:    [[TMP10:%.*]] = extractelement <4 x i32> [[SHT]], i64 3
// CHECK-NEXT:    [[TMP11:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP9]], i32 [[TMP10]], i32 1)
// CHECK-NEXT:    [[TMP12:%.*]] = insertelement <4 x i64> undef, i64 [[TMP2]], i32 0
// CHECK-NEXT:    [[TMP13:%.*]] = insertelement <4 x i64> [[TMP12]], i64 [[TMP5]], i32 1
// CHECK-NEXT:    [[TMP14:%.*]] = insertelement <4 x i64> [[TMP13]], i64 [[TMP8]], i32 2
// CHECK-NEXT:    [[TMP15:%.*]] = insertelement <4 x i64> [[TMP14]], i64 [[TMP11]], i32 3
// CHECK-NEXT:    ret <4 x i64> [[TMP15]]
//
v4i64 shrdq_s (v4i64 v, v4u32 sht) { return __builtin_kvx_shrdq(v, sht, ".a"); }

// CHECK-LABEL: @shrdq_us(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <4 x i64> [[V:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = extractelement <4 x i32> [[SHT:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP0]], i32 [[TMP1]], i32 2)
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <4 x i64> [[V]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <4 x i32> [[SHT]], i64 1
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP3]], i32 [[TMP4]], i32 2)
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <4 x i64> [[V]], i64 2
// CHECK-NEXT:    [[TMP7:%.*]] = extractelement <4 x i32> [[SHT]], i64 2
// CHECK-NEXT:    [[TMP8:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP6]], i32 [[TMP7]], i32 2)
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <4 x i64> [[V]], i64 3
// CHECK-NEXT:    [[TMP10:%.*]] = extractelement <4 x i32> [[SHT]], i64 3
// CHECK-NEXT:    [[TMP11:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP9]], i32 [[TMP10]], i32 2)
// CHECK-NEXT:    [[TMP12:%.*]] = insertelement <4 x i64> undef, i64 [[TMP2]], i32 0
// CHECK-NEXT:    [[TMP13:%.*]] = insertelement <4 x i64> [[TMP12]], i64 [[TMP5]], i32 1
// CHECK-NEXT:    [[TMP14:%.*]] = insertelement <4 x i64> [[TMP13]], i64 [[TMP8]], i32 2
// CHECK-NEXT:    [[TMP15:%.*]] = insertelement <4 x i64> [[TMP14]], i64 [[TMP11]], i32 3
// CHECK-NEXT:    ret <4 x i64> [[TMP15]]
//
v4i64 shrdq_us (v4i64 v, v4u32 sht) { return __builtin_kvx_shrdq(v, sht, ".as"); }

// CHECK-LABEL: @shrdq_r(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <4 x i64> [[V:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = extractelement <4 x i32> [[SHT:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP0]], i32 [[TMP1]], i32 3)
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <4 x i64> [[V]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <4 x i32> [[SHT]], i64 1
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP3]], i32 [[TMP4]], i32 3)
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <4 x i64> [[V]], i64 2
// CHECK-NEXT:    [[TMP7:%.*]] = extractelement <4 x i32> [[SHT]], i64 2
// CHECK-NEXT:    [[TMP8:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP6]], i32 [[TMP7]], i32 3)
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <4 x i64> [[V]], i64 3
// CHECK-NEXT:    [[TMP10:%.*]] = extractelement <4 x i32> [[SHT]], i64 3
// CHECK-NEXT:    [[TMP11:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP9]], i32 [[TMP10]], i32 3)
// CHECK-NEXT:    [[TMP12:%.*]] = insertelement <4 x i64> undef, i64 [[TMP2]], i32 0
// CHECK-NEXT:    [[TMP13:%.*]] = insertelement <4 x i64> [[TMP12]], i64 [[TMP5]], i32 1
// CHECK-NEXT:    [[TMP14:%.*]] = insertelement <4 x i64> [[TMP13]], i64 [[TMP8]], i32 2
// CHECK-NEXT:    [[TMP15:%.*]] = insertelement <4 x i64> [[TMP14]], i64 [[TMP11]], i32 3
// CHECK-NEXT:    ret <4 x i64> [[TMP15]]
//
v4i64 shrdq_r (v4i64 v, v4u32 sht) { return __builtin_kvx_shrdq(v, sht, ".r"); }

// CHECK-LABEL: @shrdqs(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <4 x i64> [[V:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP0]], i32 [[SHT:%.*]], i32 0)
// CHECK-NEXT:    [[TMP2:%.*]] = extractelement <4 x i64> [[V]], i64 1
// CHECK-NEXT:    [[TMP3:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP2]], i32 [[SHT]], i32 0)
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <4 x i64> [[V]], i64 2
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP4]], i32 [[SHT]], i32 0)
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <4 x i64> [[V]], i64 3
// CHECK-NEXT:    [[TMP7:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP6]], i32 [[SHT]], i32 0)
// CHECK-NEXT:    [[TMP8:%.*]] = insertelement <4 x i64> undef, i64 [[TMP1]], i32 0
// CHECK-NEXT:    [[TMP9:%.*]] = insertelement <4 x i64> [[TMP8]], i64 [[TMP3]], i32 1
// CHECK-NEXT:    [[TMP10:%.*]] = insertelement <4 x i64> [[TMP9]], i64 [[TMP5]], i32 2
// CHECK-NEXT:    [[TMP11:%.*]] = insertelement <4 x i64> [[TMP10]], i64 [[TMP7]], i32 3
// CHECK-NEXT:    ret <4 x i64> [[TMP11]]
//
v4i64 shrdqs (v4i64 v, unsigned sht) { return __builtin_kvx_shrdqs(v, sht, ""); }

// CHECK-LABEL: @shrdqs_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <4 x i64> [[V:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP0]], i32 [[SHT:%.*]], i32 1)
// CHECK-NEXT:    [[TMP2:%.*]] = extractelement <4 x i64> [[V]], i64 1
// CHECK-NEXT:    [[TMP3:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP2]], i32 [[SHT]], i32 1)
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <4 x i64> [[V]], i64 2
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP4]], i32 [[SHT]], i32 1)
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <4 x i64> [[V]], i64 3
// CHECK-NEXT:    [[TMP7:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP6]], i32 [[SHT]], i32 1)
// CHECK-NEXT:    [[TMP8:%.*]] = insertelement <4 x i64> undef, i64 [[TMP1]], i32 0
// CHECK-NEXT:    [[TMP9:%.*]] = insertelement <4 x i64> [[TMP8]], i64 [[TMP3]], i32 1
// CHECK-NEXT:    [[TMP10:%.*]] = insertelement <4 x i64> [[TMP9]], i64 [[TMP5]], i32 2
// CHECK-NEXT:    [[TMP11:%.*]] = insertelement <4 x i64> [[TMP10]], i64 [[TMP7]], i32 3
// CHECK-NEXT:    ret <4 x i64> [[TMP11]]
//
v4i64 shrdqs_s (v4i64 v, unsigned sht) { return __builtin_kvx_shrdqs(v, sht, ".a"); }

// CHECK-LABEL: @shrdqs_us(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <4 x i64> [[V:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP0]], i32 [[SHT:%.*]], i32 2)
// CHECK-NEXT:    [[TMP2:%.*]] = extractelement <4 x i64> [[V]], i64 1
// CHECK-NEXT:    [[TMP3:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP2]], i32 [[SHT]], i32 2)
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <4 x i64> [[V]], i64 2
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP4]], i32 [[SHT]], i32 2)
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <4 x i64> [[V]], i64 3
// CHECK-NEXT:    [[TMP7:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP6]], i32 [[SHT]], i32 2)
// CHECK-NEXT:    [[TMP8:%.*]] = insertelement <4 x i64> undef, i64 [[TMP1]], i32 0
// CHECK-NEXT:    [[TMP9:%.*]] = insertelement <4 x i64> [[TMP8]], i64 [[TMP3]], i32 1
// CHECK-NEXT:    [[TMP10:%.*]] = insertelement <4 x i64> [[TMP9]], i64 [[TMP5]], i32 2
// CHECK-NEXT:    [[TMP11:%.*]] = insertelement <4 x i64> [[TMP10]], i64 [[TMP7]], i32 3
// CHECK-NEXT:    ret <4 x i64> [[TMP11]]
//
v4i64 shrdqs_us (v4i64 v, unsigned sht) { return __builtin_kvx_shrdqs(v, sht, ".as"); }

// CHECK-LABEL: @shrdqs_r(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <4 x i64> [[V:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP0]], i32 [[SHT:%.*]], i32 3)
// CHECK-NEXT:    [[TMP2:%.*]] = extractelement <4 x i64> [[V]], i64 1
// CHECK-NEXT:    [[TMP3:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP2]], i32 [[SHT]], i32 3)
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <4 x i64> [[V]], i64 2
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP4]], i32 [[SHT]], i32 3)
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <4 x i64> [[V]], i64 3
// CHECK-NEXT:    [[TMP7:%.*]] = tail call i64 @llvm.kvx.shr.i64(i64 [[TMP6]], i32 [[SHT]], i32 3)
// CHECK-NEXT:    [[TMP8:%.*]] = insertelement <4 x i64> undef, i64 [[TMP1]], i32 0
// CHECK-NEXT:    [[TMP9:%.*]] = insertelement <4 x i64> [[TMP8]], i64 [[TMP3]], i32 1
// CHECK-NEXT:    [[TMP10:%.*]] = insertelement <4 x i64> [[TMP9]], i64 [[TMP5]], i32 2
// CHECK-NEXT:    [[TMP11:%.*]] = insertelement <4 x i64> [[TMP10]], i64 [[TMP7]], i32 3
// CHECK-NEXT:    ret <4 x i64> [[TMP11]]
//
v4i64 shrdqs_r (v4i64 v, unsigned sht) { return __builtin_kvx_shrdqs(v, sht, ".r"); }

// CHECK-LABEL: @shrhos(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = shufflevector <8 x i16> [[V:%.*]], <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> [[TMP0]], i32 [[SHT:%.*]], i32 0)
// CHECK-NEXT:    [[TMP2:%.*]] = shufflevector <8 x i16> [[V]], <8 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> [[TMP2]], i32 [[SHT]], i32 0)
// CHECK-NEXT:    [[TMP4:%.*]] = shufflevector <4 x i16> [[TMP1]], <4 x i16> [[TMP3]], <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
// CHECK-NEXT:    ret <8 x i16> [[TMP4]]
//
v8i16 shrhos (v8i16 v, unsigned sht) { return __builtin_kvx_shrhos(v, sht, ""); }

// CHECK-LABEL: @shrhos_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = shufflevector <8 x i16> [[V:%.*]], <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> [[TMP0]], i32 [[SHT:%.*]], i32 1)
// CHECK-NEXT:    [[TMP2:%.*]] = shufflevector <8 x i16> [[V]], <8 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> [[TMP2]], i32 [[SHT]], i32 1)
// CHECK-NEXT:    [[TMP4:%.*]] = shufflevector <4 x i16> [[TMP1]], <4 x i16> [[TMP3]], <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
// CHECK-NEXT:    ret <8 x i16> [[TMP4]]
//
v8i16 shrhos_s (v8i16 v, unsigned sht) { return __builtin_kvx_shrhos(v, sht, ".a"); }

// CHECK-LABEL: @shrhos_us(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = shufflevector <8 x i16> [[V:%.*]], <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> [[TMP0]], i32 [[SHT:%.*]], i32 2)
// CHECK-NEXT:    [[TMP2:%.*]] = shufflevector <8 x i16> [[V]], <8 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> [[TMP2]], i32 [[SHT]], i32 2)
// CHECK-NEXT:    [[TMP4:%.*]] = shufflevector <4 x i16> [[TMP1]], <4 x i16> [[TMP3]], <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
// CHECK-NEXT:    ret <8 x i16> [[TMP4]]
//
v8i16 shrhos_us (v8i16 v, unsigned sht) { return __builtin_kvx_shrhos(v, sht, ".as"); }

// CHECK-LABEL: @shrhos_r(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = shufflevector <8 x i16> [[V:%.*]], <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> [[TMP0]], i32 [[SHT:%.*]], i32 3)
// CHECK-NEXT:    [[TMP2:%.*]] = shufflevector <8 x i16> [[V]], <8 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> [[TMP2]], i32 [[SHT]], i32 3)
// CHECK-NEXT:    [[TMP4:%.*]] = shufflevector <4 x i16> [[TMP1]], <4 x i16> [[TMP3]], <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
// CHECK-NEXT:    ret <8 x i16> [[TMP4]]
//
v8i16 shrhos_r (v8i16 v, unsigned sht) { return __builtin_kvx_shrhos(v, sht, ".r"); }

// CHECK-LABEL: @shrhps(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i16> @llvm.kvx.shr.v2i16(<2 x i16> [[V:%.*]], i32 [[SHT:%.*]], i32 0)
// CHECK-NEXT:    ret <2 x i16> [[TMP0]]
//
v2i16 shrhps (v2i16 v, unsigned sht) { return __builtin_kvx_shrhps(v, sht, ""); }

// CHECK-LABEL: @shrhps_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i16> @llvm.kvx.shr.v2i16(<2 x i16> [[V:%.*]], i32 [[SHT:%.*]], i32 1)
// CHECK-NEXT:    ret <2 x i16> [[TMP0]]
//
v2i16 shrhps_s (v2i16 v, unsigned sht) { return __builtin_kvx_shrhps(v, sht, ".a"); }

// CHECK-LABEL: @shrhps_us(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i16> @llvm.kvx.shr.v2i16(<2 x i16> [[V:%.*]], i32 [[SHT:%.*]], i32 2)
// CHECK-NEXT:    ret <2 x i16> [[TMP0]]
//
v2i16 shrhps_us (v2i16 v, unsigned sht) { return __builtin_kvx_shrhps(v, sht, ".as"); }

// CHECK-LABEL: @shrhps_r(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i16> @llvm.kvx.shr.v2i16(<2 x i16> [[V:%.*]], i32 [[SHT:%.*]], i32 3)
// CHECK-NEXT:    ret <2 x i16> [[TMP0]]
//
v2i16 shrhps_r (v2i16 v, unsigned sht) { return __builtin_kvx_shrhps(v, sht, ".r"); }

// CHECK-LABEL: @shrhqs(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> [[V:%.*]], i32 [[SHT:%.*]], i32 0)
// CHECK-NEXT:    ret <4 x i16> [[TMP0]]
//
v4i16 shrhqs (v4i16 v, unsigned sht) { return __builtin_kvx_shrhqs(v, sht, ""); }

// CHECK-LABEL: @shrhqs_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> [[V:%.*]], i32 [[SHT:%.*]], i32 1)
// CHECK-NEXT:    ret <4 x i16> [[TMP0]]
//
v4i16 shrhqs_s (v4i16 v, unsigned sht) { return __builtin_kvx_shrhqs(v, sht, ".a"); }

// CHECK-LABEL: @shrhqs_us(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> [[V:%.*]], i32 [[SHT:%.*]], i32 2)
// CHECK-NEXT:    ret <4 x i16> [[TMP0]]
//
v4i16 shrhqs_us (v4i16 v, unsigned sht) { return __builtin_kvx_shrhqs(v, sht, ".as"); }

// CHECK-LABEL: @shrhqs_r(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> [[V:%.*]], i32 [[SHT:%.*]], i32 3)
// CHECK-NEXT:    ret <4 x i16> [[TMP0]]
//
v4i16 shrhqs_r (v4i16 v, unsigned sht) { return __builtin_kvx_shrhqs(v, sht, ".r"); }

// CHECK-LABEL: @shrhxs(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = shufflevector <16 x i16> [[V:%.*]], <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> [[TMP0]], i32 [[SHT:%.*]], i32 0)
// CHECK-NEXT:    [[TMP2:%.*]] = shufflevector <16 x i16> [[V]], <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> [[TMP2]], i32 [[SHT]], i32 0)
// CHECK-NEXT:    [[TMP4:%.*]] = shufflevector <16 x i16> [[V]], <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
// CHECK-NEXT:    [[TMP5:%.*]] = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> [[TMP4]], i32 [[SHT]], i32 0)
// CHECK-NEXT:    [[TMP6:%.*]] = shufflevector <16 x i16> [[V]], <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
// CHECK-NEXT:    [[TMP7:%.*]] = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> [[TMP6]], i32 [[SHT]], i32 0)
// CHECK-NEXT:    [[TMP8:%.*]] = shufflevector <4 x i16> [[TMP1]], <4 x i16> [[TMP3]], <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
// CHECK-NEXT:    [[TMP9:%.*]] = shufflevector <4 x i16> [[TMP5]], <4 x i16> [[TMP7]], <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
// CHECK-NEXT:    [[TMP10:%.*]] = shufflevector <8 x i16> [[TMP8]], <8 x i16> [[TMP9]], <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
// CHECK-NEXT:    ret <16 x i16> [[TMP10]]
//
v16i16 shrhxs (v16i16 v, unsigned sht) { return __builtin_kvx_shrhxs(v, sht, ""); }

// CHECK-LABEL: @shrhxs_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = shufflevector <16 x i16> [[V:%.*]], <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> [[TMP0]], i32 [[SHT:%.*]], i32 1)
// CHECK-NEXT:    [[TMP2:%.*]] = shufflevector <16 x i16> [[V]], <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> [[TMP2]], i32 [[SHT]], i32 1)
// CHECK-NEXT:    [[TMP4:%.*]] = shufflevector <16 x i16> [[V]], <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
// CHECK-NEXT:    [[TMP5:%.*]] = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> [[TMP4]], i32 [[SHT]], i32 1)
// CHECK-NEXT:    [[TMP6:%.*]] = shufflevector <16 x i16> [[V]], <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
// CHECK-NEXT:    [[TMP7:%.*]] = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> [[TMP6]], i32 [[SHT]], i32 1)
// CHECK-NEXT:    [[TMP8:%.*]] = shufflevector <4 x i16> [[TMP1]], <4 x i16> [[TMP3]], <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
// CHECK-NEXT:    [[TMP9:%.*]] = shufflevector <4 x i16> [[TMP5]], <4 x i16> [[TMP7]], <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
// CHECK-NEXT:    [[TMP10:%.*]] = shufflevector <8 x i16> [[TMP8]], <8 x i16> [[TMP9]], <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
// CHECK-NEXT:    ret <16 x i16> [[TMP10]]
//
v16i16 shrhxs_s (v16i16 v, unsigned sht) { return __builtin_kvx_shrhxs(v, sht, ".a"); }

// CHECK-LABEL: @shrhxs_us(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = shufflevector <16 x i16> [[V:%.*]], <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> [[TMP0]], i32 [[SHT:%.*]], i32 2)
// CHECK-NEXT:    [[TMP2:%.*]] = shufflevector <16 x i16> [[V]], <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> [[TMP2]], i32 [[SHT]], i32 2)
// CHECK-NEXT:    [[TMP4:%.*]] = shufflevector <16 x i16> [[V]], <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
// CHECK-NEXT:    [[TMP5:%.*]] = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> [[TMP4]], i32 [[SHT]], i32 2)
// CHECK-NEXT:    [[TMP6:%.*]] = shufflevector <16 x i16> [[V]], <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
// CHECK-NEXT:    [[TMP7:%.*]] = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> [[TMP6]], i32 [[SHT]], i32 2)
// CHECK-NEXT:    [[TMP8:%.*]] = shufflevector <4 x i16> [[TMP1]], <4 x i16> [[TMP3]], <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
// CHECK-NEXT:    [[TMP9:%.*]] = shufflevector <4 x i16> [[TMP5]], <4 x i16> [[TMP7]], <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
// CHECK-NEXT:    [[TMP10:%.*]] = shufflevector <8 x i16> [[TMP8]], <8 x i16> [[TMP9]], <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
// CHECK-NEXT:    ret <16 x i16> [[TMP10]]
//
v16i16 shrhxs_us (v16i16 v, unsigned sht) { return __builtin_kvx_shrhxs(v, sht, ".as"); }

// CHECK-LABEL: @shrhxs_r(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = shufflevector <16 x i16> [[V:%.*]], <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> [[TMP0]], i32 [[SHT:%.*]], i32 3)
// CHECK-NEXT:    [[TMP2:%.*]] = shufflevector <16 x i16> [[V]], <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> [[TMP2]], i32 [[SHT]], i32 3)
// CHECK-NEXT:    [[TMP4:%.*]] = shufflevector <16 x i16> [[V]], <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
// CHECK-NEXT:    [[TMP5:%.*]] = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> [[TMP4]], i32 [[SHT]], i32 3)
// CHECK-NEXT:    [[TMP6:%.*]] = shufflevector <16 x i16> [[V]], <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
// CHECK-NEXT:    [[TMP7:%.*]] = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> [[TMP6]], i32 [[SHT]], i32 3)
// CHECK-NEXT:    [[TMP8:%.*]] = shufflevector <4 x i16> [[TMP1]], <4 x i16> [[TMP3]], <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
// CHECK-NEXT:    [[TMP9:%.*]] = shufflevector <4 x i16> [[TMP5]], <4 x i16> [[TMP7]], <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
// CHECK-NEXT:    [[TMP10:%.*]] = shufflevector <8 x i16> [[TMP8]], <8 x i16> [[TMP9]], <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
// CHECK-NEXT:    ret <16 x i16> [[TMP10]]
//
v16i16 shrhxs_r (v16i16 v, unsigned sht) { return __builtin_kvx_shrhxs(v, sht, ".r"); }

// CHECK-LABEL: @shrw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[V:%.*]], i32 [[SHT:%.*]], i32 0)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int shrw (int v, unsigned sht) { return __builtin_kvx_shrw(v, sht, ""); }

// CHECK-LABEL: @shrw_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[V:%.*]], i32 [[SHT:%.*]], i32 1)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int shrw_s (int v, unsigned sht) { return __builtin_kvx_shrw(v, sht, ".a"); }

// CHECK-LABEL: @shrw_us(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[V:%.*]], i32 [[SHT:%.*]], i32 2)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int shrw_us (int v, unsigned sht) { return __builtin_kvx_shrw(v, sht, ".as"); }

// CHECK-LABEL: @shrw_r(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[V:%.*]], i32 [[SHT:%.*]], i32 3)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int shrw_r (int v, unsigned sht) { return __builtin_kvx_shrw(v, sht, ".r"); }

// CHECK-LABEL: @shrwo(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <8 x i32> [[V:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = extractelement <8 x i32> [[SHT:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP0]], i32 [[TMP1]], i32 0)
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <8 x i32> [[V]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <8 x i32> [[SHT]], i64 1
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP3]], i32 [[TMP4]], i32 0)
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <8 x i32> [[V]], i64 2
// CHECK-NEXT:    [[TMP7:%.*]] = extractelement <8 x i32> [[SHT]], i64 2
// CHECK-NEXT:    [[TMP8:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP6]], i32 [[TMP7]], i32 0)
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <8 x i32> [[V]], i64 3
// CHECK-NEXT:    [[TMP10:%.*]] = extractelement <8 x i32> [[SHT]], i64 3
// CHECK-NEXT:    [[TMP11:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP9]], i32 [[TMP10]], i32 0)
// CHECK-NEXT:    [[TMP12:%.*]] = extractelement <8 x i32> [[V]], i64 4
// CHECK-NEXT:    [[TMP13:%.*]] = extractelement <8 x i32> [[SHT]], i64 4
// CHECK-NEXT:    [[TMP14:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP12]], i32 [[TMP13]], i32 0)
// CHECK-NEXT:    [[TMP15:%.*]] = extractelement <8 x i32> [[V]], i64 5
// CHECK-NEXT:    [[TMP16:%.*]] = extractelement <8 x i32> [[SHT]], i64 5
// CHECK-NEXT:    [[TMP17:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP15]], i32 [[TMP16]], i32 0)
// CHECK-NEXT:    [[TMP18:%.*]] = extractelement <8 x i32> [[V]], i64 6
// CHECK-NEXT:    [[TMP19:%.*]] = extractelement <8 x i32> [[SHT]], i64 6
// CHECK-NEXT:    [[TMP20:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP18]], i32 [[TMP19]], i32 0)
// CHECK-NEXT:    [[TMP21:%.*]] = extractelement <8 x i32> [[V]], i64 7
// CHECK-NEXT:    [[TMP22:%.*]] = extractelement <8 x i32> [[SHT]], i64 7
// CHECK-NEXT:    [[TMP23:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP21]], i32 [[TMP22]], i32 0)
// CHECK-NEXT:    [[TMP24:%.*]] = insertelement <8 x i32> undef, i32 [[TMP2]], i32 0
// CHECK-NEXT:    [[TMP25:%.*]] = insertelement <8 x i32> [[TMP24]], i32 [[TMP5]], i32 1
// CHECK-NEXT:    [[TMP26:%.*]] = insertelement <8 x i32> [[TMP25]], i32 [[TMP8]], i32 2
// CHECK-NEXT:    [[TMP27:%.*]] = insertelement <8 x i32> [[TMP26]], i32 [[TMP11]], i32 3
// CHECK-NEXT:    [[TMP28:%.*]] = insertelement <8 x i32> [[TMP27]], i32 [[TMP14]], i32 4
// CHECK-NEXT:    [[TMP29:%.*]] = insertelement <8 x i32> [[TMP28]], i32 [[TMP17]], i32 5
// CHECK-NEXT:    [[TMP30:%.*]] = insertelement <8 x i32> [[TMP29]], i32 [[TMP20]], i32 6
// CHECK-NEXT:    [[TMP31:%.*]] = insertelement <8 x i32> [[TMP30]], i32 [[TMP23]], i32 7
// CHECK-NEXT:    ret <8 x i32> [[TMP31]]
//
v8i32 shrwo (v8i32 v, v8u32 sht) { return __builtin_kvx_shrwo(v, sht, ""); }

// CHECK-LABEL: @shrwo_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <8 x i32> [[V:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = extractelement <8 x i32> [[SHT:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP0]], i32 [[TMP1]], i32 1)
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <8 x i32> [[V]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <8 x i32> [[SHT]], i64 1
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP3]], i32 [[TMP4]], i32 1)
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <8 x i32> [[V]], i64 2
// CHECK-NEXT:    [[TMP7:%.*]] = extractelement <8 x i32> [[SHT]], i64 2
// CHECK-NEXT:    [[TMP8:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP6]], i32 [[TMP7]], i32 1)
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <8 x i32> [[V]], i64 3
// CHECK-NEXT:    [[TMP10:%.*]] = extractelement <8 x i32> [[SHT]], i64 3
// CHECK-NEXT:    [[TMP11:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP9]], i32 [[TMP10]], i32 1)
// CHECK-NEXT:    [[TMP12:%.*]] = extractelement <8 x i32> [[V]], i64 4
// CHECK-NEXT:    [[TMP13:%.*]] = extractelement <8 x i32> [[SHT]], i64 4
// CHECK-NEXT:    [[TMP14:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP12]], i32 [[TMP13]], i32 1)
// CHECK-NEXT:    [[TMP15:%.*]] = extractelement <8 x i32> [[V]], i64 5
// CHECK-NEXT:    [[TMP16:%.*]] = extractelement <8 x i32> [[SHT]], i64 5
// CHECK-NEXT:    [[TMP17:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP15]], i32 [[TMP16]], i32 1)
// CHECK-NEXT:    [[TMP18:%.*]] = extractelement <8 x i32> [[V]], i64 6
// CHECK-NEXT:    [[TMP19:%.*]] = extractelement <8 x i32> [[SHT]], i64 6
// CHECK-NEXT:    [[TMP20:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP18]], i32 [[TMP19]], i32 1)
// CHECK-NEXT:    [[TMP21:%.*]] = extractelement <8 x i32> [[V]], i64 7
// CHECK-NEXT:    [[TMP22:%.*]] = extractelement <8 x i32> [[SHT]], i64 7
// CHECK-NEXT:    [[TMP23:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP21]], i32 [[TMP22]], i32 1)
// CHECK-NEXT:    [[TMP24:%.*]] = insertelement <8 x i32> undef, i32 [[TMP2]], i32 0
// CHECK-NEXT:    [[TMP25:%.*]] = insertelement <8 x i32> [[TMP24]], i32 [[TMP5]], i32 1
// CHECK-NEXT:    [[TMP26:%.*]] = insertelement <8 x i32> [[TMP25]], i32 [[TMP8]], i32 2
// CHECK-NEXT:    [[TMP27:%.*]] = insertelement <8 x i32> [[TMP26]], i32 [[TMP11]], i32 3
// CHECK-NEXT:    [[TMP28:%.*]] = insertelement <8 x i32> [[TMP27]], i32 [[TMP14]], i32 4
// CHECK-NEXT:    [[TMP29:%.*]] = insertelement <8 x i32> [[TMP28]], i32 [[TMP17]], i32 5
// CHECK-NEXT:    [[TMP30:%.*]] = insertelement <8 x i32> [[TMP29]], i32 [[TMP20]], i32 6
// CHECK-NEXT:    [[TMP31:%.*]] = insertelement <8 x i32> [[TMP30]], i32 [[TMP23]], i32 7
// CHECK-NEXT:    ret <8 x i32> [[TMP31]]
//
v8i32 shrwo_s (v8i32 v, v8u32 sht) { return __builtin_kvx_shrwo(v, sht, ".a"); }

// CHECK-LABEL: @shrwo_us(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <8 x i32> [[V:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = extractelement <8 x i32> [[SHT:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP0]], i32 [[TMP1]], i32 2)
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <8 x i32> [[V]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <8 x i32> [[SHT]], i64 1
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP3]], i32 [[TMP4]], i32 2)
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <8 x i32> [[V]], i64 2
// CHECK-NEXT:    [[TMP7:%.*]] = extractelement <8 x i32> [[SHT]], i64 2
// CHECK-NEXT:    [[TMP8:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP6]], i32 [[TMP7]], i32 2)
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <8 x i32> [[V]], i64 3
// CHECK-NEXT:    [[TMP10:%.*]] = extractelement <8 x i32> [[SHT]], i64 3
// CHECK-NEXT:    [[TMP11:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP9]], i32 [[TMP10]], i32 2)
// CHECK-NEXT:    [[TMP12:%.*]] = extractelement <8 x i32> [[V]], i64 4
// CHECK-NEXT:    [[TMP13:%.*]] = extractelement <8 x i32> [[SHT]], i64 4
// CHECK-NEXT:    [[TMP14:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP12]], i32 [[TMP13]], i32 2)
// CHECK-NEXT:    [[TMP15:%.*]] = extractelement <8 x i32> [[V]], i64 5
// CHECK-NEXT:    [[TMP16:%.*]] = extractelement <8 x i32> [[SHT]], i64 5
// CHECK-NEXT:    [[TMP17:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP15]], i32 [[TMP16]], i32 2)
// CHECK-NEXT:    [[TMP18:%.*]] = extractelement <8 x i32> [[V]], i64 6
// CHECK-NEXT:    [[TMP19:%.*]] = extractelement <8 x i32> [[SHT]], i64 6
// CHECK-NEXT:    [[TMP20:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP18]], i32 [[TMP19]], i32 2)
// CHECK-NEXT:    [[TMP21:%.*]] = extractelement <8 x i32> [[V]], i64 7
// CHECK-NEXT:    [[TMP22:%.*]] = extractelement <8 x i32> [[SHT]], i64 7
// CHECK-NEXT:    [[TMP23:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP21]], i32 [[TMP22]], i32 2)
// CHECK-NEXT:    [[TMP24:%.*]] = insertelement <8 x i32> undef, i32 [[TMP2]], i32 0
// CHECK-NEXT:    [[TMP25:%.*]] = insertelement <8 x i32> [[TMP24]], i32 [[TMP5]], i32 1
// CHECK-NEXT:    [[TMP26:%.*]] = insertelement <8 x i32> [[TMP25]], i32 [[TMP8]], i32 2
// CHECK-NEXT:    [[TMP27:%.*]] = insertelement <8 x i32> [[TMP26]], i32 [[TMP11]], i32 3
// CHECK-NEXT:    [[TMP28:%.*]] = insertelement <8 x i32> [[TMP27]], i32 [[TMP14]], i32 4
// CHECK-NEXT:    [[TMP29:%.*]] = insertelement <8 x i32> [[TMP28]], i32 [[TMP17]], i32 5
// CHECK-NEXT:    [[TMP30:%.*]] = insertelement <8 x i32> [[TMP29]], i32 [[TMP20]], i32 6
// CHECK-NEXT:    [[TMP31:%.*]] = insertelement <8 x i32> [[TMP30]], i32 [[TMP23]], i32 7
// CHECK-NEXT:    ret <8 x i32> [[TMP31]]
//
v8i32 shrwo_us (v8i32 v, v8u32 sht) { return __builtin_kvx_shrwo(v, sht, ".as"); }

// CHECK-LABEL: @shrwo_r(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <8 x i32> [[V:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = extractelement <8 x i32> [[SHT:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP0]], i32 [[TMP1]], i32 3)
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <8 x i32> [[V]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <8 x i32> [[SHT]], i64 1
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP3]], i32 [[TMP4]], i32 3)
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <8 x i32> [[V]], i64 2
// CHECK-NEXT:    [[TMP7:%.*]] = extractelement <8 x i32> [[SHT]], i64 2
// CHECK-NEXT:    [[TMP8:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP6]], i32 [[TMP7]], i32 3)
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <8 x i32> [[V]], i64 3
// CHECK-NEXT:    [[TMP10:%.*]] = extractelement <8 x i32> [[SHT]], i64 3
// CHECK-NEXT:    [[TMP11:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP9]], i32 [[TMP10]], i32 3)
// CHECK-NEXT:    [[TMP12:%.*]] = extractelement <8 x i32> [[V]], i64 4
// CHECK-NEXT:    [[TMP13:%.*]] = extractelement <8 x i32> [[SHT]], i64 4
// CHECK-NEXT:    [[TMP14:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP12]], i32 [[TMP13]], i32 3)
// CHECK-NEXT:    [[TMP15:%.*]] = extractelement <8 x i32> [[V]], i64 5
// CHECK-NEXT:    [[TMP16:%.*]] = extractelement <8 x i32> [[SHT]], i64 5
// CHECK-NEXT:    [[TMP17:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP15]], i32 [[TMP16]], i32 3)
// CHECK-NEXT:    [[TMP18:%.*]] = extractelement <8 x i32> [[V]], i64 6
// CHECK-NEXT:    [[TMP19:%.*]] = extractelement <8 x i32> [[SHT]], i64 6
// CHECK-NEXT:    [[TMP20:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP18]], i32 [[TMP19]], i32 3)
// CHECK-NEXT:    [[TMP21:%.*]] = extractelement <8 x i32> [[V]], i64 7
// CHECK-NEXT:    [[TMP22:%.*]] = extractelement <8 x i32> [[SHT]], i64 7
// CHECK-NEXT:    [[TMP23:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP21]], i32 [[TMP22]], i32 3)
// CHECK-NEXT:    [[TMP24:%.*]] = insertelement <8 x i32> undef, i32 [[TMP2]], i32 0
// CHECK-NEXT:    [[TMP25:%.*]] = insertelement <8 x i32> [[TMP24]], i32 [[TMP5]], i32 1
// CHECK-NEXT:    [[TMP26:%.*]] = insertelement <8 x i32> [[TMP25]], i32 [[TMP8]], i32 2
// CHECK-NEXT:    [[TMP27:%.*]] = insertelement <8 x i32> [[TMP26]], i32 [[TMP11]], i32 3
// CHECK-NEXT:    [[TMP28:%.*]] = insertelement <8 x i32> [[TMP27]], i32 [[TMP14]], i32 4
// CHECK-NEXT:    [[TMP29:%.*]] = insertelement <8 x i32> [[TMP28]], i32 [[TMP17]], i32 5
// CHECK-NEXT:    [[TMP30:%.*]] = insertelement <8 x i32> [[TMP29]], i32 [[TMP20]], i32 6
// CHECK-NEXT:    [[TMP31:%.*]] = insertelement <8 x i32> [[TMP30]], i32 [[TMP23]], i32 7
// CHECK-NEXT:    ret <8 x i32> [[TMP31]]
//
v8i32 shrwo_r (v8i32 v, v8u32 sht) { return __builtin_kvx_shrwo(v, sht, ".r"); }

// CHECK-LABEL: @shrwos(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = shufflevector <8 x i32> [[V:%.*]], <8 x i32> undef, <2 x i32> <i32 0, i32 1>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> [[TMP0]], i32 [[SHT:%.*]], i32 0)
// CHECK-NEXT:    [[TMP2:%.*]] = shufflevector <8 x i32> [[V]], <8 x i32> undef, <2 x i32> <i32 2, i32 3>
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> [[TMP2]], i32 [[SHT]], i32 0)
// CHECK-NEXT:    [[TMP4:%.*]] = shufflevector <8 x i32> [[V]], <8 x i32> undef, <2 x i32> <i32 4, i32 5>
// CHECK-NEXT:    [[TMP5:%.*]] = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> [[TMP4]], i32 [[SHT]], i32 0)
// CHECK-NEXT:    [[TMP6:%.*]] = shufflevector <8 x i32> [[V]], <8 x i32> undef, <2 x i32> <i32 6, i32 7>
// CHECK-NEXT:    [[TMP7:%.*]] = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> [[TMP6]], i32 [[SHT]], i32 0)
// CHECK-NEXT:    [[TMP8:%.*]] = shufflevector <2 x i32> [[TMP1]], <2 x i32> [[TMP3]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// CHECK-NEXT:    [[TMP9:%.*]] = shufflevector <2 x i32> [[TMP5]], <2 x i32> [[TMP7]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// CHECK-NEXT:    [[TMP10:%.*]] = shufflevector <4 x i32> [[TMP8]], <4 x i32> [[TMP9]], <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
// CHECK-NEXT:    ret <8 x i32> [[TMP10]]
//
v8i32 shrwos (v8i32 v, unsigned sht) { return __builtin_kvx_shrwos(v, sht, ""); }

// CHECK-LABEL: @shrwos_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = shufflevector <8 x i32> [[V:%.*]], <8 x i32> undef, <2 x i32> <i32 0, i32 1>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> [[TMP0]], i32 [[SHT:%.*]], i32 1)
// CHECK-NEXT:    [[TMP2:%.*]] = shufflevector <8 x i32> [[V]], <8 x i32> undef, <2 x i32> <i32 2, i32 3>
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> [[TMP2]], i32 [[SHT]], i32 1)
// CHECK-NEXT:    [[TMP4:%.*]] = shufflevector <8 x i32> [[V]], <8 x i32> undef, <2 x i32> <i32 4, i32 5>
// CHECK-NEXT:    [[TMP5:%.*]] = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> [[TMP4]], i32 [[SHT]], i32 1)
// CHECK-NEXT:    [[TMP6:%.*]] = shufflevector <8 x i32> [[V]], <8 x i32> undef, <2 x i32> <i32 6, i32 7>
// CHECK-NEXT:    [[TMP7:%.*]] = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> [[TMP6]], i32 [[SHT]], i32 1)
// CHECK-NEXT:    [[TMP8:%.*]] = shufflevector <2 x i32> [[TMP1]], <2 x i32> [[TMP3]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// CHECK-NEXT:    [[TMP9:%.*]] = shufflevector <2 x i32> [[TMP5]], <2 x i32> [[TMP7]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// CHECK-NEXT:    [[TMP10:%.*]] = shufflevector <4 x i32> [[TMP8]], <4 x i32> [[TMP9]], <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
// CHECK-NEXT:    ret <8 x i32> [[TMP10]]
//
v8i32 shrwos_s (v8i32 v, unsigned sht) { return __builtin_kvx_shrwos(v, sht, ".a"); }

// CHECK-LABEL: @shrwos_us(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = shufflevector <8 x i32> [[V:%.*]], <8 x i32> undef, <2 x i32> <i32 0, i32 1>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> [[TMP0]], i32 [[SHT:%.*]], i32 2)
// CHECK-NEXT:    [[TMP2:%.*]] = shufflevector <8 x i32> [[V]], <8 x i32> undef, <2 x i32> <i32 2, i32 3>
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> [[TMP2]], i32 [[SHT]], i32 2)
// CHECK-NEXT:    [[TMP4:%.*]] = shufflevector <8 x i32> [[V]], <8 x i32> undef, <2 x i32> <i32 4, i32 5>
// CHECK-NEXT:    [[TMP5:%.*]] = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> [[TMP4]], i32 [[SHT]], i32 2)
// CHECK-NEXT:    [[TMP6:%.*]] = shufflevector <8 x i32> [[V]], <8 x i32> undef, <2 x i32> <i32 6, i32 7>
// CHECK-NEXT:    [[TMP7:%.*]] = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> [[TMP6]], i32 [[SHT]], i32 2)
// CHECK-NEXT:    [[TMP8:%.*]] = shufflevector <2 x i32> [[TMP1]], <2 x i32> [[TMP3]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// CHECK-NEXT:    [[TMP9:%.*]] = shufflevector <2 x i32> [[TMP5]], <2 x i32> [[TMP7]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// CHECK-NEXT:    [[TMP10:%.*]] = shufflevector <4 x i32> [[TMP8]], <4 x i32> [[TMP9]], <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
// CHECK-NEXT:    ret <8 x i32> [[TMP10]]
//
v8i32 shrwos_us (v8i32 v, unsigned sht) { return __builtin_kvx_shrwos(v, sht, ".as"); }

// CHECK-LABEL: @shrwos_r(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = shufflevector <8 x i32> [[V:%.*]], <8 x i32> undef, <2 x i32> <i32 0, i32 1>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> [[TMP0]], i32 [[SHT:%.*]], i32 3)
// CHECK-NEXT:    [[TMP2:%.*]] = shufflevector <8 x i32> [[V]], <8 x i32> undef, <2 x i32> <i32 2, i32 3>
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> [[TMP2]], i32 [[SHT]], i32 3)
// CHECK-NEXT:    [[TMP4:%.*]] = shufflevector <8 x i32> [[V]], <8 x i32> undef, <2 x i32> <i32 4, i32 5>
// CHECK-NEXT:    [[TMP5:%.*]] = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> [[TMP4]], i32 [[SHT]], i32 3)
// CHECK-NEXT:    [[TMP6:%.*]] = shufflevector <8 x i32> [[V]], <8 x i32> undef, <2 x i32> <i32 6, i32 7>
// CHECK-NEXT:    [[TMP7:%.*]] = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> [[TMP6]], i32 [[SHT]], i32 3)
// CHECK-NEXT:    [[TMP8:%.*]] = shufflevector <2 x i32> [[TMP1]], <2 x i32> [[TMP3]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// CHECK-NEXT:    [[TMP9:%.*]] = shufflevector <2 x i32> [[TMP5]], <2 x i32> [[TMP7]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// CHECK-NEXT:    [[TMP10:%.*]] = shufflevector <4 x i32> [[TMP8]], <4 x i32> [[TMP9]], <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
// CHECK-NEXT:    ret <8 x i32> [[TMP10]]
//
v8i32 shrwos_r (v8i32 v, unsigned sht) { return __builtin_kvx_shrwos(v, sht, ".r"); }

// CHECK-LABEL: @shrwp(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <2 x i32> [[V:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = extractelement <2 x i32> [[SHT:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP0]], i32 [[TMP1]], i32 0)
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <2 x i32> [[V]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <2 x i32> [[SHT]], i64 1
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP3]], i32 [[TMP4]], i32 0)
// CHECK-NEXT:    [[TMP6:%.*]] = insertelement <2 x i32> undef, i32 [[TMP2]], i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = insertelement <2 x i32> [[TMP6]], i32 [[TMP5]], i32 1
// CHECK-NEXT:    ret <2 x i32> [[TMP7]]
//
v2i32 shrwp (v2i32 v, v2u32 sht) { return __builtin_kvx_shrwp(v, sht, ""); }

// CHECK-LABEL: @shrwp_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <2 x i32> [[V:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = extractelement <2 x i32> [[SHT:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP0]], i32 [[TMP1]], i32 1)
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <2 x i32> [[V]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <2 x i32> [[SHT]], i64 1
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP3]], i32 [[TMP4]], i32 1)
// CHECK-NEXT:    [[TMP6:%.*]] = insertelement <2 x i32> undef, i32 [[TMP2]], i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = insertelement <2 x i32> [[TMP6]], i32 [[TMP5]], i32 1
// CHECK-NEXT:    ret <2 x i32> [[TMP7]]
//
v2i32 shrwp_s (v2i32 v, v2u32 sht) { return __builtin_kvx_shrwp(v, sht, ".a"); }

// CHECK-LABEL: @shrwp_us(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <2 x i32> [[V:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = extractelement <2 x i32> [[SHT:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP0]], i32 [[TMP1]], i32 2)
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <2 x i32> [[V]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <2 x i32> [[SHT]], i64 1
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP3]], i32 [[TMP4]], i32 2)
// CHECK-NEXT:    [[TMP6:%.*]] = insertelement <2 x i32> undef, i32 [[TMP2]], i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = insertelement <2 x i32> [[TMP6]], i32 [[TMP5]], i32 1
// CHECK-NEXT:    ret <2 x i32> [[TMP7]]
//
v2i32 shrwp_us (v2i32 v, v2u32 sht) { return __builtin_kvx_shrwp(v, sht, ".as"); }

// CHECK-LABEL: @shrwp_r(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <2 x i32> [[V:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = extractelement <2 x i32> [[SHT:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP0]], i32 [[TMP1]], i32 3)
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <2 x i32> [[V]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <2 x i32> [[SHT]], i64 1
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP3]], i32 [[TMP4]], i32 3)
// CHECK-NEXT:    [[TMP6:%.*]] = insertelement <2 x i32> undef, i32 [[TMP2]], i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = insertelement <2 x i32> [[TMP6]], i32 [[TMP5]], i32 1
// CHECK-NEXT:    ret <2 x i32> [[TMP7]]
//
v2i32 shrwp_r (v2i32 v, v2u32 sht) { return __builtin_kvx_shrwp(v, sht, ".r"); }

// CHECK-LABEL: @shrwps(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> [[V:%.*]], i32 [[SHT:%.*]], i32 0)
// CHECK-NEXT:    ret <2 x i32> [[TMP0]]
//
v2i32 shrwps (v2i32 v, unsigned sht) { return __builtin_kvx_shrwps(v, sht, ""); }

// CHECK-LABEL: @shrwps_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> [[V:%.*]], i32 [[SHT:%.*]], i32 1)
// CHECK-NEXT:    ret <2 x i32> [[TMP0]]
//
v2i32 shrwps_s (v2i32 v, unsigned sht) { return __builtin_kvx_shrwps(v, sht, ".a"); }

// CHECK-LABEL: @shrwps_us(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> [[V:%.*]], i32 [[SHT:%.*]], i32 2)
// CHECK-NEXT:    ret <2 x i32> [[TMP0]]
//
v2i32 shrwps_us (v2i32 v, unsigned sht) { return __builtin_kvx_shrwps(v, sht, ".as"); }

// CHECK-LABEL: @shrwps_r(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> [[V:%.*]], i32 [[SHT:%.*]], i32 3)
// CHECK-NEXT:    ret <2 x i32> [[TMP0]]
//
v2i32 shrwps_r (v2i32 v, unsigned sht) { return __builtin_kvx_shrwps(v, sht, ".r"); }

// CHECK-LABEL: @shrwq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <4 x i32> [[V:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = extractelement <4 x i32> [[SHT:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP0]], i32 [[TMP1]], i32 0)
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <4 x i32> [[V]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <4 x i32> [[SHT]], i64 1
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP3]], i32 [[TMP4]], i32 0)
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <4 x i32> [[V]], i64 2
// CHECK-NEXT:    [[TMP7:%.*]] = extractelement <4 x i32> [[SHT]], i64 2
// CHECK-NEXT:    [[TMP8:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP6]], i32 [[TMP7]], i32 0)
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <4 x i32> [[V]], i64 3
// CHECK-NEXT:    [[TMP10:%.*]] = extractelement <4 x i32> [[SHT]], i64 3
// CHECK-NEXT:    [[TMP11:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP9]], i32 [[TMP10]], i32 0)
// CHECK-NEXT:    [[TMP12:%.*]] = insertelement <4 x i32> undef, i32 [[TMP2]], i32 0
// CHECK-NEXT:    [[TMP13:%.*]] = insertelement <4 x i32> [[TMP12]], i32 [[TMP5]], i32 1
// CHECK-NEXT:    [[TMP14:%.*]] = insertelement <4 x i32> [[TMP13]], i32 [[TMP8]], i32 2
// CHECK-NEXT:    [[TMP15:%.*]] = insertelement <4 x i32> [[TMP14]], i32 [[TMP11]], i32 3
// CHECK-NEXT:    ret <4 x i32> [[TMP15]]
//
v4i32 shrwq (v4i32 v, v4u32 sht) { return __builtin_kvx_shrwq(v, sht, ""); }

// CHECK-LABEL: @shrwq_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <4 x i32> [[V:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = extractelement <4 x i32> [[SHT:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP0]], i32 [[TMP1]], i32 1)
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <4 x i32> [[V]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <4 x i32> [[SHT]], i64 1
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP3]], i32 [[TMP4]], i32 1)
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <4 x i32> [[V]], i64 2
// CHECK-NEXT:    [[TMP7:%.*]] = extractelement <4 x i32> [[SHT]], i64 2
// CHECK-NEXT:    [[TMP8:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP6]], i32 [[TMP7]], i32 1)
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <4 x i32> [[V]], i64 3
// CHECK-NEXT:    [[TMP10:%.*]] = extractelement <4 x i32> [[SHT]], i64 3
// CHECK-NEXT:    [[TMP11:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP9]], i32 [[TMP10]], i32 1)
// CHECK-NEXT:    [[TMP12:%.*]] = insertelement <4 x i32> undef, i32 [[TMP2]], i32 0
// CHECK-NEXT:    [[TMP13:%.*]] = insertelement <4 x i32> [[TMP12]], i32 [[TMP5]], i32 1
// CHECK-NEXT:    [[TMP14:%.*]] = insertelement <4 x i32> [[TMP13]], i32 [[TMP8]], i32 2
// CHECK-NEXT:    [[TMP15:%.*]] = insertelement <4 x i32> [[TMP14]], i32 [[TMP11]], i32 3
// CHECK-NEXT:    ret <4 x i32> [[TMP15]]
//
v4i32 shrwq_s (v4i32 v, v4u32 sht) { return __builtin_kvx_shrwq(v, sht, ".a"); }

// CHECK-LABEL: @shrwq_us(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <4 x i32> [[V:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = extractelement <4 x i32> [[SHT:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP0]], i32 [[TMP1]], i32 2)
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <4 x i32> [[V]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <4 x i32> [[SHT]], i64 1
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP3]], i32 [[TMP4]], i32 2)
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <4 x i32> [[V]], i64 2
// CHECK-NEXT:    [[TMP7:%.*]] = extractelement <4 x i32> [[SHT]], i64 2
// CHECK-NEXT:    [[TMP8:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP6]], i32 [[TMP7]], i32 2)
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <4 x i32> [[V]], i64 3
// CHECK-NEXT:    [[TMP10:%.*]] = extractelement <4 x i32> [[SHT]], i64 3
// CHECK-NEXT:    [[TMP11:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP9]], i32 [[TMP10]], i32 2)
// CHECK-NEXT:    [[TMP12:%.*]] = insertelement <4 x i32> undef, i32 [[TMP2]], i32 0
// CHECK-NEXT:    [[TMP13:%.*]] = insertelement <4 x i32> [[TMP12]], i32 [[TMP5]], i32 1
// CHECK-NEXT:    [[TMP14:%.*]] = insertelement <4 x i32> [[TMP13]], i32 [[TMP8]], i32 2
// CHECK-NEXT:    [[TMP15:%.*]] = insertelement <4 x i32> [[TMP14]], i32 [[TMP11]], i32 3
// CHECK-NEXT:    ret <4 x i32> [[TMP15]]
//
v4i32 shrwq_us (v4i32 v, v4u32 sht) { return __builtin_kvx_shrwq(v, sht, ".as"); }

// CHECK-LABEL: @shrwq_r(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <4 x i32> [[V:%.*]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = extractelement <4 x i32> [[SHT:%.*]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP0]], i32 [[TMP1]], i32 3)
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <4 x i32> [[V]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <4 x i32> [[SHT]], i64 1
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP3]], i32 [[TMP4]], i32 3)
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <4 x i32> [[V]], i64 2
// CHECK-NEXT:    [[TMP7:%.*]] = extractelement <4 x i32> [[SHT]], i64 2
// CHECK-NEXT:    [[TMP8:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP6]], i32 [[TMP7]], i32 3)
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <4 x i32> [[V]], i64 3
// CHECK-NEXT:    [[TMP10:%.*]] = extractelement <4 x i32> [[SHT]], i64 3
// CHECK-NEXT:    [[TMP11:%.*]] = tail call i32 @llvm.kvx.shr.i32(i32 [[TMP9]], i32 [[TMP10]], i32 3)
// CHECK-NEXT:    [[TMP12:%.*]] = insertelement <4 x i32> undef, i32 [[TMP2]], i32 0
// CHECK-NEXT:    [[TMP13:%.*]] = insertelement <4 x i32> [[TMP12]], i32 [[TMP5]], i32 1
// CHECK-NEXT:    [[TMP14:%.*]] = insertelement <4 x i32> [[TMP13]], i32 [[TMP8]], i32 2
// CHECK-NEXT:    [[TMP15:%.*]] = insertelement <4 x i32> [[TMP14]], i32 [[TMP11]], i32 3
// CHECK-NEXT:    ret <4 x i32> [[TMP15]]
//
v4i32 shrwq_r (v4i32 v, v4u32 sht) { return __builtin_kvx_shrwq(v, sht, ".r"); }

// CHECK-LABEL: @shrwqs(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = shufflevector <4 x i32> [[V:%.*]], <4 x i32> undef, <2 x i32> <i32 0, i32 1>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> [[TMP0]], i32 [[SHT:%.*]], i32 0)
// CHECK-NEXT:    [[TMP2:%.*]] = shufflevector <4 x i32> [[V]], <4 x i32> undef, <2 x i32> <i32 2, i32 3>
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> [[TMP2]], i32 [[SHT]], i32 0)
// CHECK-NEXT:    [[TMP4:%.*]] = shufflevector <2 x i32> [[TMP1]], <2 x i32> [[TMP3]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// CHECK-NEXT:    ret <4 x i32> [[TMP4]]
//
v4i32 shrwqs (v4i32 v, unsigned sht) { return __builtin_kvx_shrwqs(v, sht, ""); }

// CHECK-LABEL: @shrwqs_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = shufflevector <4 x i32> [[V:%.*]], <4 x i32> undef, <2 x i32> <i32 0, i32 1>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> [[TMP0]], i32 [[SHT:%.*]], i32 1)
// CHECK-NEXT:    [[TMP2:%.*]] = shufflevector <4 x i32> [[V]], <4 x i32> undef, <2 x i32> <i32 2, i32 3>
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> [[TMP2]], i32 [[SHT]], i32 1)
// CHECK-NEXT:    [[TMP4:%.*]] = shufflevector <2 x i32> [[TMP1]], <2 x i32> [[TMP3]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// CHECK-NEXT:    ret <4 x i32> [[TMP4]]
//
v4i32 shrwqs_s (v4i32 v, unsigned sht) { return __builtin_kvx_shrwqs(v, sht, ".a"); }

// CHECK-LABEL: @shrwqs_us(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = shufflevector <4 x i32> [[V:%.*]], <4 x i32> undef, <2 x i32> <i32 0, i32 1>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> [[TMP0]], i32 [[SHT:%.*]], i32 2)
// CHECK-NEXT:    [[TMP2:%.*]] = shufflevector <4 x i32> [[V]], <4 x i32> undef, <2 x i32> <i32 2, i32 3>
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> [[TMP2]], i32 [[SHT]], i32 2)
// CHECK-NEXT:    [[TMP4:%.*]] = shufflevector <2 x i32> [[TMP1]], <2 x i32> [[TMP3]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// CHECK-NEXT:    ret <4 x i32> [[TMP4]]
//
v4i32 shrwqs_us (v4i32 v, unsigned sht) { return __builtin_kvx_shrwqs(v, sht, ".as"); }

// CHECK-LABEL: @shrwqs_r(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = shufflevector <4 x i32> [[V:%.*]], <4 x i32> undef, <2 x i32> <i32 0, i32 1>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> [[TMP0]], i32 [[SHT:%.*]], i32 3)
// CHECK-NEXT:    [[TMP2:%.*]] = shufflevector <4 x i32> [[V]], <4 x i32> undef, <2 x i32> <i32 2, i32 3>
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> [[TMP2]], i32 [[SHT]], i32 3)
// CHECK-NEXT:    [[TMP4:%.*]] = shufflevector <2 x i32> [[TMP1]], <2 x i32> [[TMP3]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// CHECK-NEXT:    ret <4 x i32> [[TMP4]]
//
v4i32 shrwqs_r (v4i32 v, unsigned sht) { return __builtin_kvx_shrwqs(v, sht, ".r"); }
