// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -O2 -triple kvx-kalray-cos -S -emit-llvm -target-cpu kv3-2 -o - -x c %s | FileCheck %s

// CHECK-LABEL: @xfscalewo_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, <256 x i1>* [[V:%.*]], align 32, [[TBAA2:!tbaa !.*]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xfscalewo(<256 x i1> [[TMP0]], i32 [[S:%.*]], i32 7, i32 0)
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <256 x i1> @llvm.kvx.xfscalewo(<256 x i1> [[TMP1]], i32 [[S]], i32 7, i32 1)
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <256 x i1> @llvm.kvx.xfscalewo(<256 x i1> [[TMP2]], i32 [[S]], i32 0, i32 0)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <256 x i1> @llvm.kvx.xfscalewo(<256 x i1> [[TMP3]], i32 [[S]], i32 0, i32 1)
// CHECK-NEXT:    store <256 x i1> [[TMP4]], <256 x i1>* [[V]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xfscalewo_test(__kvx_x256 *v, int s) {
  __kvx_x256 r = __builtin_kvx_xfscalewo(v[0], s, "");
  r = __builtin_kvx_xfscalewo(r, s, ".s");
  r = __builtin_kvx_xfscalewo(r, s, ".rn");
  v[0] = __builtin_kvx_xfscalewo(r, s, ".rn.s");
}

// CHECK-LABEL: @xclampwo_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xclampwo(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <256 x i1> [[TMP0]])
// CHECK-NEXT:    store <256 x i1> [[TMP1]], <256 x i1>* [[V]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xclampwo_test(__kvx_x256 *v) {
  v[0] = __builtin_kvx_xclampwo(v[0], v[0], v[0]);
}

// CHECK-LABEL: @xffma44hw_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = load <512 x i1>, <512 x i1>* [[ACC:%.*]], align 32, [[TBAA6:!tbaa !.*]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <512 x i1> @llvm.kvx.xffma44hw(<512 x i1> [[TMP1]], <256 x i1> [[TMP0]], <256 x i1> [[TMP0]], i32 7, i32 0)
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <512 x i1> @llvm.kvx.xffma44hw(<512 x i1> [[TMP2]], <256 x i1> [[TMP0]], <256 x i1> [[TMP0]], i32 7, i32 1)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <512 x i1> @llvm.kvx.xffma44hw(<512 x i1> [[TMP3]], <256 x i1> [[TMP0]], <256 x i1> [[TMP0]], i32 0, i32 0)
// CHECK-NEXT:    [[TMP5:%.*]] = tail call <512 x i1> @llvm.kvx.xffma44hw(<512 x i1> [[TMP4]], <256 x i1> [[TMP0]], <256 x i1> [[TMP0]], i32 3, i32 1)
// CHECK-NEXT:    store <512 x i1> [[TMP5]], <512 x i1>* [[ACC]], align 32, [[TBAA6]]
// CHECK-NEXT:    ret void
//
void xffma44hw_test(__kvx_x512 *acc, __kvx_x256 *v) {
  __kvx_x256 l = v[0];
  __kvx_x512 r = __builtin_kvx_xffma44hw(acc[0], l, l, "");
  r = __builtin_kvx_xffma44hw(r, l, l, ".s");
  r = __builtin_kvx_xffma44hw(r, l, l, ".rn");
  acc[0] = __builtin_kvx_xffma44hw(r, l, l, ".rz.s");
}

// CHECK-LABEL: @xfmma484hw_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <512 x i1>, <512 x i1>* [[ACC:%.*]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <512 x i1> @llvm.kvx.xfmma484hw(<512 x i1> [[TMP0]], <512 x i1> [[TMP0]], <512 x i1> [[TMP0]], i32 7, i32 0)
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <512 x i1> @llvm.kvx.xfmma484hw(<512 x i1> [[TMP1]], <512 x i1> [[TMP1]], <512 x i1> [[TMP1]], i32 7, i32 1)
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <512 x i1> @llvm.kvx.xfmma484hw(<512 x i1> [[TMP2]], <512 x i1> [[TMP2]], <512 x i1> [[TMP2]], i32 0, i32 0)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <512 x i1> @llvm.kvx.xfmma484hw(<512 x i1> [[TMP3]], <512 x i1> [[TMP3]], <512 x i1> [[TMP3]], i32 3, i32 1)
// CHECK-NEXT:    store <512 x i1> [[TMP4]], <512 x i1>* [[ACC]], align 32, [[TBAA6]]
// CHECK-NEXT:    ret void
//
void xfmma484hw_test(__kvx_x512 *acc) {
  __kvx_x512 r = __builtin_kvx_xfmma484hw(acc[0], acc[0], acc[0], "");
  r = __builtin_kvx_xfmma484hw(r, r, r, ".s");
  r = __builtin_kvx_xfmma484hw(r, r, r, ".rn");
  acc[0] = __builtin_kvx_xfmma484hw(r, r, r, ".rz.s");
}

// CHECK-LABEL: @xfnarrow44wh_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <512 x i1>, <512 x i1>* [[W:%.*]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xfnarrow44wh(<512 x i1> [[TMP0]], i32 7, i32 0)
// CHECK-NEXT:    store <256 x i1> [[TMP1]], <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP2:%.*]] = load <512 x i1>, <512 x i1>* [[W]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <256 x i1> @llvm.kvx.xfnarrow44wh(<512 x i1> [[TMP2]], i32 7, i32 1)
// CHECK-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <256 x i1>, <256 x i1>* [[V]], i64 1
// CHECK-NEXT:    store <256 x i1> [[TMP3]], <256 x i1>* [[ARRAYIDX3]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP4:%.*]] = load <512 x i1>, <512 x i1>* [[W]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP5:%.*]] = tail call <256 x i1> @llvm.kvx.xfnarrow44wh(<512 x i1> [[TMP4]], i32 3, i32 0)
// CHECK-NEXT:    [[ARRAYIDX5:%.*]] = getelementptr inbounds <256 x i1>, <256 x i1>* [[V]], i64 2
// CHECK-NEXT:    store <256 x i1> [[TMP5]], <256 x i1>* [[ARRAYIDX5]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP6:%.*]] = load <512 x i1>, <512 x i1>* [[W]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP7:%.*]] = tail call <256 x i1> @llvm.kvx.xfnarrow44wh(<512 x i1> [[TMP6]], i32 1, i32 1)
// CHECK-NEXT:    [[ARRAYIDX7:%.*]] = getelementptr inbounds <256 x i1>, <256 x i1>* [[V]], i64 3
// CHECK-NEXT:    store <256 x i1> [[TMP7]], <256 x i1>* [[ARRAYIDX7]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xfnarrow44wh_test(__kvx_x256 *v, __kvx_x512 *w) {
  v[0] = __builtin_kvx_xfnarrow44wh(w[0], "");
  v[1] = __builtin_kvx_xfnarrow44wh(w[0], ".s");
  v[2] = __builtin_kvx_xfnarrow44wh(w[0], ".rz");
  v[3] = __builtin_kvx_xfnarrow44wh(w[0], ".ru.s");
}

// CHECK-LABEL: @xmadd44bw_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <512 x i1>, <512 x i1>* [[W:%.*]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP1:%.*]] = load <256 x i1>, <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <512 x i1> @llvm.kvx.xmadd44bw0(<512 x i1> [[TMP0]], <256 x i1> [[TMP1]], <256 x i1> [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <512 x i1> @llvm.kvx.xmadd44bw1(<512 x i1> [[TMP2]], <256 x i1> [[TMP1]], <256 x i1> [[TMP1]])
// CHECK-NEXT:    store <512 x i1> [[TMP3]], <512 x i1>* [[W]], align 32, [[TBAA6]]
// CHECK-NEXT:    ret void
//
void xmadd44bw_test(__kvx_x256 *v, __kvx_x512 *w) {
  __kvx_x512 r = __builtin_kvx_xmadd44bw0(w[0], v[0], v[0]);
  w[0] = __builtin_kvx_xmadd44bw1(r, v[0], v[0]);
}

// CHECK-LABEL: @xmaddifwo_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xmaddifwo(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <256 x i1> [[TMP0]], i32 7, i32 0)
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <256 x i1> @llvm.kvx.xmaddifwo(<256 x i1> [[TMP1]], <256 x i1> [[TMP1]], <256 x i1> [[TMP1]], i32 7, i32 1)
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <256 x i1> @llvm.kvx.xmaddifwo(<256 x i1> [[TMP2]], <256 x i1> [[TMP2]], <256 x i1> [[TMP2]], i32 0, i32 0)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <256 x i1> @llvm.kvx.xmaddifwo(<256 x i1> [[TMP3]], <256 x i1> [[TMP3]], <256 x i1> [[TMP3]], i32 3, i32 1)
// CHECK-NEXT:    store <256 x i1> [[TMP4]], <256 x i1>* [[V]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xmaddifwo_test(__kvx_x256 *v) {
  __kvx_x256 r = __builtin_kvx_xmaddifwo(v[0], v[0], v[0], "");
  r = __builtin_kvx_xmaddifwo(r, r, r, ".s");
  r = __builtin_kvx_xmaddifwo(r, r, r, ".rn");
  v[0] = __builtin_kvx_xmaddifwo(r, r, r, ".rz.s");
}

// CHECK-LABEL: @xmaddsu44bw_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <512 x i1>, <512 x i1>* [[ACC:%.*]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP1:%.*]] = load <256 x i1>, <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <512 x i1> @llvm.kvx.xmaddsu44bw0(<512 x i1> [[TMP0]], <256 x i1> [[TMP1]], <256 x i1> [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <512 x i1> @llvm.kvx.xmaddsu44bw1(<512 x i1> [[TMP2]], <256 x i1> [[TMP1]], <256 x i1> [[TMP1]])
// CHECK-NEXT:    store <512 x i1> [[TMP3]], <512 x i1>* [[ACC]], align 32, [[TBAA6]]
// CHECK-NEXT:    ret void
//
void xmaddsu44bw_test(__kvx_x512 *acc, __kvx_x256 *v) {
  __kvx_x512 r = __builtin_kvx_xmaddsu44bw0(acc[0], v[0], v[0]);
  acc[0] = __builtin_kvx_xmaddsu44bw1(r, v[0], v[0]);
}

// CHECK-LABEL: @xmaddu44bw_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <512 x i1>, <512 x i1>* [[ACC:%.*]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP1:%.*]] = load <256 x i1>, <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <512 x i1> @llvm.kvx.xmaddu44bw0(<512 x i1> [[TMP0]], <256 x i1> [[TMP1]], <256 x i1> [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <512 x i1> @llvm.kvx.xmaddu44bw1(<512 x i1> [[TMP2]], <256 x i1> [[TMP1]], <256 x i1> [[TMP1]])
// CHECK-NEXT:    store <512 x i1> [[TMP3]], <512 x i1>* [[ACC]], align 32, [[TBAA6]]
// CHECK-NEXT:    ret void
//
void xmaddu44bw_test(__kvx_x512 *acc, __kvx_x256 *v) {
  __kvx_x512 r = __builtin_kvx_xmaddu44bw0(acc[0], v[0], v[0]);
  acc[0] = __builtin_kvx_xmaddu44bw1(r, v[0], v[0]);
}

// CHECK-LABEL: @xmma4164bw_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <512 x i1>, <512 x i1>* [[W:%.*]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <512 x i1> @llvm.kvx.xmma4164bw(<512 x i1> [[TMP0]], <512 x i1> [[TMP0]], <512 x i1> [[TMP0]])
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <512 x i1> @llvm.kvx.xmmasu4164bw(<512 x i1> [[TMP1]], <512 x i1> [[TMP1]], <512 x i1> [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <512 x i1> @llvm.kvx.xmmau4164bw(<512 x i1> [[TMP2]], <512 x i1> [[TMP2]], <512 x i1> [[TMP2]])
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <512 x i1> @llvm.kvx.xmmaus4164bw(<512 x i1> [[TMP3]], <512 x i1> [[TMP3]], <512 x i1> [[TMP3]])
// CHECK-NEXT:    store <512 x i1> [[TMP4]], <512 x i1>* [[W]], align 32, [[TBAA6]]
// CHECK-NEXT:    ret void
//
void xmma4164bw_test(__kvx_x512 *w) {
  __kvx_x512 r =  __builtin_kvx_xmma4164bw(w[0], w[0], w[0]);
  r =  __builtin_kvx_xmmasu4164bw(r, r, r);
  r =  __builtin_kvx_xmmau4164bw(r, r, r);
  w[0] =  __builtin_kvx_xmmaus4164bw(r, r, r);
}

// CHECK-LABEL: @xmma484bw_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = load <512 x i1>, <512 x i1>* [[W:%.*]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <512 x i1> @llvm.kvx.xmma484bw(<512 x i1> [[TMP1]], <256 x i1> [[TMP0]], <256 x i1> [[TMP0]])
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <512 x i1> @llvm.kvx.xmmasu484bw(<512 x i1> [[TMP2]], <256 x i1> [[TMP0]], <256 x i1> [[TMP0]])
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <512 x i1> @llvm.kvx.xmmau484bw(<512 x i1> [[TMP3]], <256 x i1> [[TMP0]], <256 x i1> [[TMP0]])
// CHECK-NEXT:    [[TMP5:%.*]] = tail call <512 x i1> @llvm.kvx.xmmaus484bw(<512 x i1> [[TMP4]], <256 x i1> [[TMP0]], <256 x i1> [[TMP0]])
// CHECK-NEXT:    store <512 x i1> [[TMP5]], <512 x i1>* [[W]], align 32, [[TBAA6]]
// CHECK-NEXT:    ret void
//
void xmma484bw_test(__kvx_x512 *w, __kvx_x256 *v) {
  __kvx_x256 c = v[0];
  __kvx_x512 r =  __builtin_kvx_xmma484bw(w[0], c, c);
  r =  __builtin_kvx_xmmasu484bw(r, c, c);
  r =  __builtin_kvx_xmmau484bw(r, c, c);
  w[0] =  __builtin_kvx_xmmaus484bw(r, c, c);
}

// CHECK-LABEL: @xmsbfifwo_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xmsbfifwo(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <256 x i1> [[TMP0]], i32 7, i32 0)
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <256 x i1> @llvm.kvx.xmsbfifwo(<256 x i1> [[TMP1]], <256 x i1> [[TMP1]], <256 x i1> [[TMP1]], i32 7, i32 1)
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <256 x i1> @llvm.kvx.xmsbfifwo(<256 x i1> [[TMP2]], <256 x i1> [[TMP2]], <256 x i1> [[TMP2]], i32 0, i32 0)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <256 x i1> @llvm.kvx.xmsbfifwo(<256 x i1> [[TMP3]], <256 x i1> [[TMP3]], <256 x i1> [[TMP3]], i32 3, i32 1)
// CHECK-NEXT:    store <256 x i1> [[TMP4]], <256 x i1>* [[V]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xmsbfifwo_test(__kvx_x256 *v) {
  __kvx_x256 r = __builtin_kvx_xmsbfifwo(v[0], v[0], v[0], "");
  r = __builtin_kvx_xmsbfifwo(r, r, r, ".s");
  r = __builtin_kvx_xmsbfifwo(r, r, r, ".rn");
  v[0] = __builtin_kvx_xmsbfifwo(r, r, r, ".rz.s");
}

// CHECK-LABEL: @xsx48bw_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <1024 x i1> @llvm.kvx.xsx48bw(<256 x i1> [[TMP0]])
// CHECK-NEXT:    store <1024 x i1> [[TMP1]], <1024 x i1>* [[M:%.*]], align 32, [[TBAA8:!tbaa !.*]]
// CHECK-NEXT:    ret void
//
void xsx48bw_test(__kvx_x1024 *m, __kvx_x256 *v) {
  m[0] = __builtin_kvx_xsx48bw(v[0]);
}

// CHECK-LABEL: @xzx48bw_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <1024 x i1> @llvm.kvx.xzx48bw(<256 x i1> [[TMP0]])
// CHECK-NEXT:    store <1024 x i1> [[TMP1]], <1024 x i1>* [[M:%.*]], align 32, [[TBAA8]]
// CHECK-NEXT:    ret void
//
void xzx48bw_test(__kvx_x1024 *m, __kvx_x256 *v) {
  m[0] = __builtin_kvx_xzx48bw(v[0]);
}

// CHECK-LABEL: @xtrunc48wb_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <1024 x i1>, <1024 x i1>* [[M:%.*]], align 32, [[TBAA8]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xtrunc48wb(<1024 x i1> [[TMP0]])
// CHECK-NEXT:    store <256 x i1> [[TMP1]], <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xtrunc48wb_test(__kvx_x1024 *m, __kvx_x256 *v) {
  v[0] = __builtin_kvx_xtrunc48wb(m[0]);
}

