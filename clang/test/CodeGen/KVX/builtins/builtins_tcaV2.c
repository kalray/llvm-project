// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -O2 -triple kvx-kalray-cos -S -emit-llvm -target-cpu kv3-2 -o - -x c %s | FileCheck %s

// CHECK-LABEL: @xfscalewo_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, <256 x i1>* [[V:%.*]], align 32, [[TBAA2:!tbaa !.*]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xfscalewo(<256 x i1> [[TMP0]], i32 [[S:%.*]], i32 7, i32 0)
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <256 x i1> @llvm.kvx.xfscalewo(<256 x i1> [[TMP1]], i32 [[S]], i32 7, i32 1)
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <256 x i1> @llvm.kvx.xfscalewo(<256 x i1> [[TMP2]], i32 [[S]], i32 0, i32 0)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <256 x i1> @llvm.kvx.xfscalewo(<256 x i1> [[TMP3]], i32 [[S]], i32 0, i32 1)
// CHECK-NEXT:    store <256 x i1> [[TMP4]], <256 x i1>* [[V]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xfscalewo_test(__kvx_x256 *v, int s) {
  __kvx_x256 r = __builtin_kvx_xfscalewo(v[0], s, "");
  r = __builtin_kvx_xfscalewo(r, s, ".s");
  r = __builtin_kvx_xfscalewo(r, s, ".rn");
  v[0] = __builtin_kvx_xfscalewo(r, s, ".rn.s");
}

// CHECK-LABEL: @xclampwo_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xclampwo(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <256 x i1> [[TMP0]])
// CHECK-NEXT:    store <256 x i1> [[TMP1]], <256 x i1>* [[V]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xclampwo_test(__kvx_x256 *v) {
  v[0] = __builtin_kvx_xclampwo(v[0], v[0], v[0]);
}

// CHECK-LABEL: @xffma44hw_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = load <512 x i1>, <512 x i1>* [[ACC:%.*]], align 32, [[TBAA6:!tbaa !.*]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <512 x i1> @llvm.kvx.xffma44hw(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <512 x i1> [[TMP1]], i32 7, i32 0)
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <512 x i1> @llvm.kvx.xffma44hw(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <512 x i1> [[TMP2]], i32 7, i32 1)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <512 x i1> @llvm.kvx.xffma44hw(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <512 x i1> [[TMP3]], i32 0, i32 0)
// CHECK-NEXT:    [[TMP5:%.*]] = tail call <512 x i1> @llvm.kvx.xffma44hw(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <512 x i1> [[TMP4]], i32 3, i32 1)
// CHECK-NEXT:    store <512 x i1> [[TMP5]], <512 x i1>* [[ACC]], align 32, [[TBAA6]]
// CHECK-NEXT:    ret void
//
void xffma44hw_test(__kvx_x512 *acc, __kvx_x256 *v) {
  __kvx_x256 l = v[0];
  __kvx_x512 r = __builtin_kvx_xffma44hw(l, l, acc[0], "");
  r = __builtin_kvx_xffma44hw(l, l, r, ".s");
  r = __builtin_kvx_xffma44hw(l, l, r, ".rn");
  acc[0] = __builtin_kvx_xffma44hw(l, l, r, ".rz.s");
}

// CHECK-LABEL: @xfmma484hw_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <512 x i1>, <512 x i1>* [[ACC:%.*]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <512 x i1> @llvm.kvx.xfmma484hw(<512 x i1> [[TMP0]], <512 x i1> [[TMP0]], <512 x i1> [[TMP0]], i32 7, i32 0)
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <512 x i1> @llvm.kvx.xfmma484hw(<512 x i1> [[TMP1]], <512 x i1> [[TMP1]], <512 x i1> [[TMP1]], i32 7, i32 1)
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <512 x i1> @llvm.kvx.xfmma484hw(<512 x i1> [[TMP2]], <512 x i1> [[TMP2]], <512 x i1> [[TMP2]], i32 0, i32 0)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <512 x i1> @llvm.kvx.xfmma484hw(<512 x i1> [[TMP3]], <512 x i1> [[TMP3]], <512 x i1> [[TMP3]], i32 3, i32 1)
// CHECK-NEXT:    store <512 x i1> [[TMP4]], <512 x i1>* [[ACC]], align 32, [[TBAA6]]
// CHECK-NEXT:    ret void
//
void xfmma484hw_test(__kvx_x512 *acc) {
  __kvx_x512 r = __builtin_kvx_xfmma484hw(acc[0], acc[0], acc[0], "");
  r = __builtin_kvx_xfmma484hw(r, r, r, ".s");
  r = __builtin_kvx_xfmma484hw(r, r, r, ".rn");
  acc[0] = __builtin_kvx_xfmma484hw(r, r, r, ".rz.s");
}

// CHECK-LABEL: @xfnarrow44wh_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <512 x i1>, <512 x i1>* [[W:%.*]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xfnarrow44wh(<512 x i1> [[TMP0]], i32 7, i32 0)
// CHECK-NEXT:    store <256 x i1> [[TMP1]], <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP2:%.*]] = load <512 x i1>, <512 x i1>* [[W]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <256 x i1> @llvm.kvx.xfnarrow44wh(<512 x i1> [[TMP2]], i32 7, i32 1)
// CHECK-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <256 x i1>, <256 x i1>* [[V]], i64 1
// CHECK-NEXT:    store <256 x i1> [[TMP3]], <256 x i1>* [[ARRAYIDX3]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP4:%.*]] = load <512 x i1>, <512 x i1>* [[W]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP5:%.*]] = tail call <256 x i1> @llvm.kvx.xfnarrow44wh(<512 x i1> [[TMP4]], i32 3, i32 0)
// CHECK-NEXT:    [[ARRAYIDX5:%.*]] = getelementptr inbounds <256 x i1>, <256 x i1>* [[V]], i64 2
// CHECK-NEXT:    store <256 x i1> [[TMP5]], <256 x i1>* [[ARRAYIDX5]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP6:%.*]] = load <512 x i1>, <512 x i1>* [[W]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP7:%.*]] = tail call <256 x i1> @llvm.kvx.xfnarrow44wh(<512 x i1> [[TMP6]], i32 1, i32 1)
// CHECK-NEXT:    [[ARRAYIDX7:%.*]] = getelementptr inbounds <256 x i1>, <256 x i1>* [[V]], i64 3
// CHECK-NEXT:    store <256 x i1> [[TMP7]], <256 x i1>* [[ARRAYIDX7]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xfnarrow44wh_test(__kvx_x256 *v, __kvx_x512 *w) {
  v[0] = __builtin_kvx_xfnarrow44wh(w[0], "");
  v[1] = __builtin_kvx_xfnarrow44wh(w[0], ".s");
  v[2] = __builtin_kvx_xfnarrow44wh(w[0], ".rz");
  v[3] = __builtin_kvx_xfnarrow44wh(w[0], ".ru.s");
}

// CHECK-LABEL: @xmadd44bw_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <512 x i1>, <512 x i1>* [[W:%.*]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP1:%.*]] = load <256 x i1>, <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <512 x i1> @llvm.kvx.xmadd44bw0(<512 x i1> [[TMP0]], <256 x i1> [[TMP1]], <256 x i1> [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <512 x i1> @llvm.kvx.xmadd44bw1(<512 x i1> [[TMP2]], <256 x i1> [[TMP1]], <256 x i1> [[TMP1]])
// CHECK-NEXT:    store <512 x i1> [[TMP3]], <512 x i1>* [[W]], align 32, [[TBAA6]]
// CHECK-NEXT:    ret void
//
void xmadd44bw_test(__kvx_x256 *v, __kvx_x512 *w) {
  __kvx_x512 r = __builtin_kvx_xmadd44bw0(w[0], v[0], v[0]);
  w[0] = __builtin_kvx_xmadd44bw1(r, v[0], v[0]);
}

// CHECK-LABEL: @xmaddifwo_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xmaddifwo(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <256 x i1> [[TMP0]], i32 7, i32 0)
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <256 x i1> @llvm.kvx.xmaddifwo(<256 x i1> [[TMP1]], <256 x i1> [[TMP1]], <256 x i1> [[TMP1]], i32 7, i32 1)
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <256 x i1> @llvm.kvx.xmaddifwo(<256 x i1> [[TMP2]], <256 x i1> [[TMP2]], <256 x i1> [[TMP2]], i32 0, i32 0)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <256 x i1> @llvm.kvx.xmaddifwo(<256 x i1> [[TMP3]], <256 x i1> [[TMP3]], <256 x i1> [[TMP3]], i32 3, i32 1)
// CHECK-NEXT:    store <256 x i1> [[TMP4]], <256 x i1>* [[V]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xmaddifwo_test(__kvx_x256 *v) {
  __kvx_x256 r = __builtin_kvx_xmaddifwo(v[0], v[0], v[0], "");
  r = __builtin_kvx_xmaddifwo(r, r, r, ".s");
  r = __builtin_kvx_xmaddifwo(r, r, r, ".rn");
  v[0] = __builtin_kvx_xmaddifwo(r, r, r, ".rz.s");
}

// CHECK-LABEL: @xmaddsu44bw_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <512 x i1>, <512 x i1>* [[ACC:%.*]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP1:%.*]] = load <256 x i1>, <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <512 x i1> @llvm.kvx.xmaddsu44bw0(<512 x i1> [[TMP0]], <256 x i1> [[TMP1]], <256 x i1> [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <512 x i1> @llvm.kvx.xmaddsu44bw1(<512 x i1> [[TMP2]], <256 x i1> [[TMP1]], <256 x i1> [[TMP1]])
// CHECK-NEXT:    store <512 x i1> [[TMP3]], <512 x i1>* [[ACC]], align 32, [[TBAA6]]
// CHECK-NEXT:    ret void
//
void xmaddsu44bw_test(__kvx_x512 *acc, __kvx_x256 *v) {
  __kvx_x512 r = __builtin_kvx_xmaddsu44bw0(acc[0], v[0], v[0]);
  acc[0] = __builtin_kvx_xmaddsu44bw1(r, v[0], v[0]);
}

// CHECK-LABEL: @xmaddu44bw_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <512 x i1>, <512 x i1>* [[ACC:%.*]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP1:%.*]] = load <256 x i1>, <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <512 x i1> @llvm.kvx.xmaddu44bw0(<512 x i1> [[TMP0]], <256 x i1> [[TMP1]], <256 x i1> [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <512 x i1> @llvm.kvx.xmaddu44bw1(<512 x i1> [[TMP2]], <256 x i1> [[TMP1]], <256 x i1> [[TMP1]])
// CHECK-NEXT:    store <512 x i1> [[TMP3]], <512 x i1>* [[ACC]], align 32, [[TBAA6]]
// CHECK-NEXT:    ret void
//
void xmaddu44bw_test(__kvx_x512 *acc, __kvx_x256 *v) {
  __kvx_x512 r = __builtin_kvx_xmaddu44bw0(acc[0], v[0], v[0]);
  acc[0] = __builtin_kvx_xmaddu44bw1(r, v[0], v[0]);
}

// CHECK-LABEL: @xmma4164bw_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <512 x i1>, <512 x i1>* [[W:%.*]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <512 x i1> @llvm.kvx.xmma4164bw(<512 x i1> [[TMP0]], <512 x i1> [[TMP0]], <512 x i1> [[TMP0]])
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <512 x i1> @llvm.kvx.xmmasu4164bw(<512 x i1> [[TMP1]], <512 x i1> [[TMP1]], <512 x i1> [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <512 x i1> @llvm.kvx.xmmau4164bw(<512 x i1> [[TMP2]], <512 x i1> [[TMP2]], <512 x i1> [[TMP2]])
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <512 x i1> @llvm.kvx.xmmaus4164bw(<512 x i1> [[TMP3]], <512 x i1> [[TMP3]], <512 x i1> [[TMP3]])
// CHECK-NEXT:    store <512 x i1> [[TMP4]], <512 x i1>* [[W]], align 32, [[TBAA6]]
// CHECK-NEXT:    ret void
//
void xmma4164bw_test(__kvx_x512 *w) {
  __kvx_x512 r =  __builtin_kvx_xmma4164bw(w[0], w[0], w[0]);
  r =  __builtin_kvx_xmmasu4164bw(r, r, r);
  r =  __builtin_kvx_xmmau4164bw(r, r, r);
  w[0] =  __builtin_kvx_xmmaus4164bw(r, r, r);
}

// CHECK-LABEL: @xmma484bw_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = load <512 x i1>, <512 x i1>* [[W:%.*]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <512 x i1> @llvm.kvx.xmma484bw(<512 x i1> [[TMP1]], <256 x i1> [[TMP0]], <256 x i1> [[TMP0]])
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <512 x i1> @llvm.kvx.xmmasu484bw(<512 x i1> [[TMP2]], <256 x i1> [[TMP0]], <256 x i1> [[TMP0]])
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <512 x i1> @llvm.kvx.xmmau484bw(<512 x i1> [[TMP3]], <256 x i1> [[TMP0]], <256 x i1> [[TMP0]])
// CHECK-NEXT:    [[TMP5:%.*]] = tail call <512 x i1> @llvm.kvx.xmmaus484bw(<512 x i1> [[TMP4]], <256 x i1> [[TMP0]], <256 x i1> [[TMP0]])
// CHECK-NEXT:    store <512 x i1> [[TMP5]], <512 x i1>* [[W]], align 32, [[TBAA6]]
// CHECK-NEXT:    ret void
//
void xmma484bw_test(__kvx_x512 *w, __kvx_x256 *v) {
  __kvx_x256 c = v[0];
  __kvx_x512 r =  __builtin_kvx_xmma484bw(w[0], c, c);
  r =  __builtin_kvx_xmmasu484bw(r, c, c);
  r =  __builtin_kvx_xmmau484bw(r, c, c);
  w[0] =  __builtin_kvx_xmmaus484bw(r, c, c);
}

// CHECK-LABEL: @xmsbfifwo_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xmsbfifwo(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <256 x i1> [[TMP0]], i32 7, i32 0)
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <256 x i1> @llvm.kvx.xmsbfifwo(<256 x i1> [[TMP1]], <256 x i1> [[TMP1]], <256 x i1> [[TMP1]], i32 7, i32 1)
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <256 x i1> @llvm.kvx.xmsbfifwo(<256 x i1> [[TMP2]], <256 x i1> [[TMP2]], <256 x i1> [[TMP2]], i32 0, i32 0)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <256 x i1> @llvm.kvx.xmsbfifwo(<256 x i1> [[TMP3]], <256 x i1> [[TMP3]], <256 x i1> [[TMP3]], i32 3, i32 1)
// CHECK-NEXT:    store <256 x i1> [[TMP4]], <256 x i1>* [[V]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xmsbfifwo_test(__kvx_x256 *v) {
  __kvx_x256 r = __builtin_kvx_xmsbfifwo(v[0], v[0], v[0], "");
  r = __builtin_kvx_xmsbfifwo(r, r, r, ".s");
  r = __builtin_kvx_xmsbfifwo(r, r, r, ".rn");
  v[0] = __builtin_kvx_xmsbfifwo(r, r, r, ".rz.s");
}

// CHECK-LABEL: @xsx48bw_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <1024 x i1> @llvm.kvx.xsx48bw(<256 x i1> [[TMP0]])
// CHECK-NEXT:    store <1024 x i1> [[TMP1]], <1024 x i1>* [[M:%.*]], align 32, [[TBAA8:!tbaa !.*]]
// CHECK-NEXT:    ret void
//
void xsx48bw_test(__kvx_x1024 *m, __kvx_x256 *v) {
  m[0] = __builtin_kvx_xsx48bw(v[0]);
}

// CHECK-LABEL: @xzx48bw_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <1024 x i1> @llvm.kvx.xzx48bw(<256 x i1> [[TMP0]])
// CHECK-NEXT:    store <1024 x i1> [[TMP1]], <1024 x i1>* [[M:%.*]], align 32, [[TBAA8]]
// CHECK-NEXT:    ret void
//
void xzx48bw_test(__kvx_x1024 *m, __kvx_x256 *v) {
  m[0] = __builtin_kvx_xzx48bw(v[0]);
}

// CHECK-LABEL: @xtrunc48wb_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <1024 x i1>, <1024 x i1>* [[M:%.*]], align 32, [[TBAA8]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xtrunc48wb(<1024 x i1> [[TMP0]])
// CHECK-NEXT:    store <256 x i1> [[TMP1]], <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xtrunc48wb_test(__kvx_x1024 *m, __kvx_x256 *v) {
  v[0] = __builtin_kvx_xtrunc48wb(m[0]);
}

// CHECK-LABEL: @xmt44d_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <1024 x i1>, <1024 x i1>* [[M:%.*]], align 32, [[TBAA8]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <1024 x i1> @llvm.kvx.xmt44d(<1024 x i1> [[TMP0]])
// CHECK-NEXT:    store <1024 x i1> [[TMP1]], <1024 x i1>* [[M]], align 32, [[TBAA8]]
// CHECK-NEXT:    ret void
//
void xmt44d_test(__kvx_x1024 *m) {
  m[0] = __builtin_kvx_xmt44d(m[0]);
}

// CHECK-LABEL: @xload256(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <256 x i1>, <256 x i1>* [[V:%.*]], i64 1
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <256 x i1>* [[ARRAYIDX]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xload256(i8* nonnull [[TMP0]], i32 0)
// CHECK-NEXT:    store <256 x i1> [[TMP1]], <256 x i1>* [[V]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds <256 x i1>, <256 x i1>* [[V]], i64 3
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <256 x i1>* [[ARRAYIDX2]] to i8*
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <256 x i1> @llvm.kvx.xload256(i8* nonnull [[TMP2]], i32 1)
// CHECK-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <256 x i1>, <256 x i1>* [[V]], i64 2
// CHECK-NEXT:    store <256 x i1> [[TMP3]], <256 x i1>* [[ARRAYIDX3]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <256 x i1> @llvm.kvx.xload256(i8* nonnull [[TMP2]], i32 2)
// CHECK-NEXT:    store <256 x i1> [[TMP4]], <256 x i1>* [[ARRAYIDX3]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <256 x i1>* [[ARRAYIDX3]] to i8*
// CHECK-NEXT:    [[TMP6:%.*]] = tail call <256 x i1> @llvm.kvx.xload256(i8* nonnull [[TMP5]], i32 3)
// CHECK-NEXT:    store <256 x i1> [[TMP6]], <256 x i1>* [[V]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xload256(__kvx_x256 *v) {
  v[0] = __builtin_kvx_xload256(&v[1], "");
  v[2] = __builtin_kvx_xload256(&v[3], ".s");
  v[2] = __builtin_kvx_xload256(&v[3], ".u");
  v[0] = __builtin_kvx_xload256(&v[2], ".us");
}

// CHECK-LABEL: @xloadc256(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <256 x i1>, <256 x i1>* [[V:%.*]], i64 1
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <256 x i1>* [[ARRAYIDX]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xloadc256(<256 x i1> undef, i8* nonnull [[TMP0]], i64 [[X:%.*]], i32 0, i32 4)
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <256 x i1>, <256 x i1>* [[V]], i64 3
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <256 x i1>* [[ARRAYIDX1]] to i8*
// CHECK-NEXT:    [[TOBOOL_NOT:%.*]] = icmp eq i64 [[X]], 0
// CHECK-NEXT:    [[CONV:%.*]] = zext i1 [[TOBOOL_NOT]] to i64
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <256 x i1> @llvm.kvx.xloadc256(<256 x i1> [[TMP1]], i8* nonnull [[TMP2]], i64 [[CONV]], i32 1, i32 5)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <256 x i1> @llvm.kvx.xloadc256(<256 x i1> [[TMP3]], i8* nonnull [[TMP2]], i64 0, i32 2, i32 6)
// CHECK-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <256 x i1>, <256 x i1>* [[V]], i64 2
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <256 x i1>* [[ARRAYIDX3]] to i8*
// CHECK-NEXT:    [[TMP6:%.*]] = tail call <256 x i1> @llvm.kvx.xloadc256(<256 x i1> [[TMP4]], i8* nonnull [[TMP5]], i64 1, i32 3, i32 7)
// CHECK-NEXT:    store <256 x i1> [[TMP6]], <256 x i1>* [[V]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xloadc256(__kvx_x256 *v, long x) {
  __kvx_x256 c;
  c = __builtin_kvx_xloadc256(c, &v[1], x, ".mt");
  c = __builtin_kvx_xloadc256(c, &v[3], !x, ".s.mf");
  c = __builtin_kvx_xloadc256(c, &v[3], 0, ".u.mtc");
  v[0] = __builtin_kvx_xloadc256(c, &v[2], 1, ".us.mfc");
}

// CHECK-LABEL: @xloads1024(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <1024 x i1>* [[M:%.*]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <1024 x i1> @llvm.kvx.xloads1024(<1024 x i1> undef, i8* [[TMP0]], i32 0, i32 0)
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <1024 x i1> @llvm.kvx.xloads1024(<1024 x i1> [[TMP1]], i8* [[TMP0]], i32 0, i32 1)
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <1024 x i1> @llvm.kvx.xloads1024(<1024 x i1> [[TMP2]], i8* [[TMP0]], i32 1, i32 2)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <1024 x i1> @llvm.kvx.xloads1024(<1024 x i1> [[TMP3]], i8* [[TMP0]], i32 3, i32 3)
// CHECK-NEXT:    store <1024 x i1> [[TMP4]], <1024 x i1>* [[M]], align 32, [[TBAA8]]
// CHECK-NEXT:    ret void
//
void xloads1024(__kvx_x1024 *m) {
  __kvx_x1024 l;
  l = __builtin_kvx_xloads1024(l, &m[0], ".q0");
  l = __builtin_kvx_xloads1024(l, &m[0], "..q1");
  l = __builtin_kvx_xloads1024(l, &m[0], ".s.q2");
  m[0] = __builtin_kvx_xloads1024(l, &m[0], ".us.q3");
}

// CHECK-LABEL: @xloadsc1024(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <1024 x i1>* [[M:%.*]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <1024 x i1> @llvm.kvx.xloadsc1024(<1024 x i1> undef, i8* [[TMP0]], i64 [[X:%.*]], i32 0, i32 4, i32 0)
// CHECK-NEXT:    [[TOBOOL_NOT:%.*]] = icmp eq i64 [[X]], 0
// CHECK-NEXT:    [[CONV:%.*]] = zext i1 [[TOBOOL_NOT]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <1024 x i1> @llvm.kvx.xloadsc1024(<1024 x i1> [[TMP1]], i8* [[TMP0]], i64 [[CONV]], i32 1, i32 5, i32 1)
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <1024 x i1> @llvm.kvx.xloadsc1024(<1024 x i1> [[TMP2]], i8* [[TMP0]], i64 0, i32 2, i32 6, i32 2)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <1024 x i1> @llvm.kvx.xloadsc1024(<1024 x i1> [[TMP3]], i8* [[TMP0]], i64 1, i32 3, i32 7, i32 3)
// CHECK-NEXT:    store <1024 x i1> [[TMP4]], <1024 x i1>* [[M]], align 32, [[TBAA8]]
// CHECK-NEXT:    ret void
//
void xloadsc1024(__kvx_x1024 *m, long x) {
  __kvx_x1024 l;
  l = __builtin_kvx_xloadsc1024(l, &m[0], x, ".mt.q0");
  l = __builtin_kvx_xloadsc1024(l, &m[0], !x, ".s.mf.q1");
  l = __builtin_kvx_xloadsc1024(l, &m[0], 0, ".u.mtc.q2");
  m[0] = __builtin_kvx_xloadsc1024(l, &m[0], 1, ".us.mfc.q3");
}
