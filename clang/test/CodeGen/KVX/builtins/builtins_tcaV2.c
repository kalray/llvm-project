// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -O2 -triple kvx-kalray-cos -S -emit-llvm -target-cpu kv3-2 -o - -x c %s | FileCheck %s

typedef int int4 __attribute__ ((vector_size (sizeof(int) * 4)));

// CHECK-LABEL: @xfscalewo_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, <256 x i1>* [[V:%.*]], align 32, [[TBAA2:!tbaa !.*]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xfscalewo(<256 x i1> [[TMP0]], i32 [[S:%.*]], i32 7, i32 0)
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <256 x i1> @llvm.kvx.xfscalewo(<256 x i1> [[TMP1]], i32 [[S]], i32 7, i32 1)
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <256 x i1> @llvm.kvx.xfscalewo(<256 x i1> [[TMP2]], i32 [[S]], i32 0, i32 0)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <256 x i1> @llvm.kvx.xfscalewo(<256 x i1> [[TMP3]], i32 [[S]], i32 0, i32 1)
// CHECK-NEXT:    store <256 x i1> [[TMP4]], <256 x i1>* [[V]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xfscalewo_test(__kvx_x256 *v, int s) {
  __kvx_x256 r = __builtin_kvx_xfscalewo(v[0], s, "");
  r = __builtin_kvx_xfscalewo(r, s, ".s");
  r = __builtin_kvx_xfscalewo(r, s, ".rn");
  v[0] = __builtin_kvx_xfscalewo(r, s, ".rn.s");
}

// CHECK-LABEL: @xclampwo_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xclampwo(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <256 x i1> [[TMP0]])
// CHECK-NEXT:    store <256 x i1> [[TMP1]], <256 x i1>* [[V]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xclampwo_test(__kvx_x256 *v) {
  v[0] = __builtin_kvx_xclampwo(v[0], v[0], v[0]);
}

// CHECK-LABEL: @xffma44hw_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = load <512 x i1>, <512 x i1>* [[ACC:%.*]], align 32, [[TBAA6:!tbaa !.*]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <512 x i1> @llvm.kvx.xffma44hw(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <512 x i1> [[TMP1]], i32 7, i32 0)
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <512 x i1> @llvm.kvx.xffma44hw(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <512 x i1> [[TMP2]], i32 7, i32 1)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <512 x i1> @llvm.kvx.xffma44hw(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <512 x i1> [[TMP3]], i32 0, i32 0)
// CHECK-NEXT:    [[TMP5:%.*]] = tail call <512 x i1> @llvm.kvx.xffma44hw(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <512 x i1> [[TMP4]], i32 3, i32 1)
// CHECK-NEXT:    store <512 x i1> [[TMP5]], <512 x i1>* [[ACC]], align 32, [[TBAA6]]
// CHECK-NEXT:    ret void
//
void xffma44hw_test(__kvx_x512 *acc, __kvx_x256 *v) {
  __kvx_x256 l = v[0];
  __kvx_x512 r = __builtin_kvx_xffma44hw(l, l, acc[0], "");
  r = __builtin_kvx_xffma44hw(l, l, r, ".s");
  r = __builtin_kvx_xffma44hw(l, l, r, ".rN");
  acc[0] = __builtin_kvx_xffma44hw(l, l, r, ".Rz.S");
}

// CHECK-LABEL: @xfmma484hw_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <512 x i1>, <512 x i1>* [[ACC:%.*]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <512 x i1> @llvm.kvx.xfmma484hw(<512 x i1> [[TMP0]], <512 x i1> [[TMP0]], <512 x i1> [[TMP0]], i32 7, i32 0)
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <512 x i1> @llvm.kvx.xfmma484hw(<512 x i1> [[TMP1]], <512 x i1> [[TMP1]], <512 x i1> [[TMP1]], i32 7, i32 1)
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <512 x i1> @llvm.kvx.xfmma484hw(<512 x i1> [[TMP2]], <512 x i1> [[TMP2]], <512 x i1> [[TMP2]], i32 0, i32 0)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <512 x i1> @llvm.kvx.xfmma484hw(<512 x i1> [[TMP3]], <512 x i1> [[TMP3]], <512 x i1> [[TMP3]], i32 3, i32 1)
// CHECK-NEXT:    store <512 x i1> [[TMP4]], <512 x i1>* [[ACC]], align 32, [[TBAA6]]
// CHECK-NEXT:    ret void
//
void xfmma484hw_test(__kvx_x512 *acc) {
  __kvx_x512 r = __builtin_kvx_xfmma484hw(acc[0], acc[0], acc[0], "");
  r = __builtin_kvx_xfmma484hw(r, r, r, ".s");
  r = __builtin_kvx_xfmma484hw(r, r, r, ".rn");
  acc[0] = __builtin_kvx_xfmma484hw(r, r, r, ".rz.s");
}

// CHECK-LABEL: @xfnarrow44wh_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <512 x i1>, <512 x i1>* [[W:%.*]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xfnarrow44wh(<512 x i1> [[TMP0]], i32 7, i32 0)
// CHECK-NEXT:    store <256 x i1> [[TMP1]], <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP2:%.*]] = load <512 x i1>, <512 x i1>* [[W]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <256 x i1> @llvm.kvx.xfnarrow44wh(<512 x i1> [[TMP2]], i32 7, i32 1)
// CHECK-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <256 x i1>, <256 x i1>* [[V]], i64 1
// CHECK-NEXT:    store <256 x i1> [[TMP3]], <256 x i1>* [[ARRAYIDX3]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP4:%.*]] = load <512 x i1>, <512 x i1>* [[W]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP5:%.*]] = tail call <256 x i1> @llvm.kvx.xfnarrow44wh(<512 x i1> [[TMP4]], i32 3, i32 0)
// CHECK-NEXT:    [[ARRAYIDX5:%.*]] = getelementptr inbounds <256 x i1>, <256 x i1>* [[V]], i64 2
// CHECK-NEXT:    store <256 x i1> [[TMP5]], <256 x i1>* [[ARRAYIDX5]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP6:%.*]] = load <512 x i1>, <512 x i1>* [[W]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP7:%.*]] = tail call <256 x i1> @llvm.kvx.xfnarrow44wh(<512 x i1> [[TMP6]], i32 1, i32 1)
// CHECK-NEXT:    [[ARRAYIDX7:%.*]] = getelementptr inbounds <256 x i1>, <256 x i1>* [[V]], i64 3
// CHECK-NEXT:    store <256 x i1> [[TMP7]], <256 x i1>* [[ARRAYIDX7]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xfnarrow44wh_test(__kvx_x256 *v, __kvx_x512 *w) {
  v[0] = __builtin_kvx_xfnarrow44wh(w[0], "");
  v[1] = __builtin_kvx_xfnarrow44wh(w[0], ".s");
  v[2] = __builtin_kvx_xfnarrow44wh(w[0], ".rz");
  v[3] = __builtin_kvx_xfnarrow44wh(w[0], ".ru.s");
}

// CHECK-LABEL: @xmadd44bw_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = load <512 x i1>, <512 x i1>* [[W:%.*]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <512 x i1> @llvm.kvx.xmadd44bw0(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <512 x i1> [[TMP1]], i32 0)
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <512 x i1> @llvm.kvx.xmadd44bw1(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <512 x i1> [[TMP2]], i32 0)
// CHECK-NEXT:    store <512 x i1> [[TMP3]], <512 x i1>* [[W]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <512 x i1> @llvm.kvx.xmadd44bw0(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <512 x i1> [[TMP2]], i32 0)
// CHECK-NEXT:    store <512 x i1> [[TMP4]], <512 x i1>* [[W]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP5:%.*]] = tail call <512 x i1> @llvm.kvx.xmadd44bw1(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <512 x i1> [[TMP2]], i32 1)
// CHECK-NEXT:    store <512 x i1> [[TMP5]], <512 x i1>* [[W]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP6:%.*]] = tail call <512 x i1> @llvm.kvx.xmadd44bw0(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <512 x i1> [[TMP2]], i32 2)
// CHECK-NEXT:    store <512 x i1> [[TMP6]], <512 x i1>* [[W]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP7:%.*]] = tail call <512 x i1> @llvm.kvx.xmadd44bw1(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <512 x i1> [[TMP2]], i32 1)
// CHECK-NEXT:    store <512 x i1> [[TMP7]], <512 x i1>* [[W]], align 32, [[TBAA6]]
// CHECK-NEXT:    ret void
//
void xmadd44bw_test(__kvx_x256 *v, __kvx_x512 *w) {
  __kvx_x512 r = __builtin_kvx_xmadd44bw0(v[0], v[0], w[0], "");
  w[0] = __builtin_kvx_xmadd44bw1(v[0], v[0], r, ".");
  w[0] = __builtin_kvx_xmadd44bw0(v[0], v[0], r, ".s");
  w[0] = __builtin_kvx_xmadd44bw1(v[0], v[0], r, ".su");
  w[0] = __builtin_kvx_xmadd44bw0(v[0], v[0], r, ".u");
  w[0] = __builtin_kvx_xmadd44bw1(v[0], v[0], r, ".su");
}

// CHECK-LABEL: @xmaddifwo_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xmaddifwo(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <256 x i1> [[TMP0]], i32 7, i32 0)
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <256 x i1> @llvm.kvx.xmaddifwo(<256 x i1> [[TMP1]], <256 x i1> [[TMP1]], <256 x i1> [[TMP1]], i32 7, i32 1)
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <256 x i1> @llvm.kvx.xmaddifwo(<256 x i1> [[TMP2]], <256 x i1> [[TMP2]], <256 x i1> [[TMP2]], i32 0, i32 0)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <256 x i1> @llvm.kvx.xmaddifwo(<256 x i1> [[TMP3]], <256 x i1> [[TMP3]], <256 x i1> [[TMP3]], i32 3, i32 1)
// CHECK-NEXT:    store <256 x i1> [[TMP4]], <256 x i1>* [[V]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xmaddifwo_test(__kvx_x256 *v) {
  __kvx_x256 r = __builtin_kvx_xmaddifwo(v[0], v[0], v[0], "");
  r = __builtin_kvx_xmaddifwo(r, r, r, ".s");
  r = __builtin_kvx_xmaddifwo(r, r, r, ".rn");
  v[0] = __builtin_kvx_xmaddifwo(r, r, r, ".rz.s");
}

// CHECK-LABEL: @xmma4164bw_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <512 x i1>, <512 x i1>* [[W:%.*]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <512 x i1> @llvm.kvx.xmma4164bw(<512 x i1> [[TMP0]], <512 x i1> [[TMP0]], <512 x i1> [[TMP0]], i32 0)
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <512 x i1> @llvm.kvx.xmma4164bw(<512 x i1> [[TMP1]], <512 x i1> [[TMP1]], <512 x i1> [[TMP1]], i32 0)
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <512 x i1> @llvm.kvx.xmma4164bw(<512 x i1> [[TMP2]], <512 x i1> [[TMP2]], <512 x i1> [[TMP2]], i32 1)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <512 x i1> @llvm.kvx.xmma4164bw(<512 x i1> [[TMP3]], <512 x i1> [[TMP3]], <512 x i1> [[TMP3]], i32 2)
// CHECK-NEXT:    [[TMP5:%.*]] = tail call <512 x i1> @llvm.kvx.xmma4164bw(<512 x i1> [[TMP4]], <512 x i1> [[TMP4]], <512 x i1> [[TMP4]], i32 3)
// CHECK-NEXT:    store <512 x i1> [[TMP5]], <512 x i1>* [[W]], align 32, [[TBAA6]]
// CHECK-NEXT:    ret void
//
void xmma4164bw_test(__kvx_x512 *w) {
  __kvx_x512 r =  __builtin_kvx_xmma4164bw(w[0], w[0], w[0], "");
  r =  __builtin_kvx_xmma4164bw(r, r, r, ".s");
  r =  __builtin_kvx_xmma4164bw(r, r, r, ".su");
  r =  __builtin_kvx_xmma4164bw(r, r, r, ".u");
  w[0] =  __builtin_kvx_xmma4164bw(r, r, r, ".us");
}

// CHECK-LABEL: @xmma484bw_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = load <512 x i1>, <512 x i1>* [[W:%.*]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <512 x i1> @llvm.kvx.xmma484bw(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <512 x i1> [[TMP1]], i32 0)
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <512 x i1> @llvm.kvx.xmma484bw(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <512 x i1> [[TMP2]], i32 1)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <512 x i1> @llvm.kvx.xmma484bw(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <512 x i1> [[TMP3]], i32 2)
// CHECK-NEXT:    [[TMP5:%.*]] = tail call <512 x i1> @llvm.kvx.xmma484bw(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <512 x i1> [[TMP4]], i32 3)
// CHECK-NEXT:    store <512 x i1> [[TMP5]], <512 x i1>* [[W]], align 32, [[TBAA6]]
// CHECK-NEXT:    ret void
//
void xmma484bw_test(__kvx_x512 *w, __kvx_x256 *v) {
  __kvx_x256 c = v[0];
  __kvx_x512 r =  __builtin_kvx_xmma484bw(c, c, w[0], "");
  r =  __builtin_kvx_xmma484bw(c, c, r, ".su");
  r =  __builtin_kvx_xmma484bw(c, c, r, ".u");
  w[0] =  __builtin_kvx_xmma484bw(c, c, r, ".us");
}

// CHECK-LABEL: @xmsbfifwo_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xmsbfifwo(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <256 x i1> [[TMP0]], i32 7, i32 0)
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <256 x i1> @llvm.kvx.xmsbfifwo(<256 x i1> [[TMP1]], <256 x i1> [[TMP1]], <256 x i1> [[TMP1]], i32 7, i32 1)
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <256 x i1> @llvm.kvx.xmsbfifwo(<256 x i1> [[TMP2]], <256 x i1> [[TMP2]], <256 x i1> [[TMP2]], i32 0, i32 0)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <256 x i1> @llvm.kvx.xmsbfifwo(<256 x i1> [[TMP3]], <256 x i1> [[TMP3]], <256 x i1> [[TMP3]], i32 3, i32 1)
// CHECK-NEXT:    store <256 x i1> [[TMP4]], <256 x i1>* [[V]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xmsbfifwo_test(__kvx_x256 *v) {
  __kvx_x256 r = __builtin_kvx_xmsbfifwo(v[0], v[0], v[0], "");
  r = __builtin_kvx_xmsbfifwo(r, r, r, ".s");
  r = __builtin_kvx_xmsbfifwo(r, r, r, ".rn");
  v[0] = __builtin_kvx_xmsbfifwo(r, r, r, ".rz.s");
}

// CHECK-LABEL: @xsx48bw_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <1024 x i1> @llvm.kvx.xsx48bw(<256 x i1> [[TMP0]])
// CHECK-NEXT:    store <1024 x i1> [[TMP1]], <1024 x i1>* [[M:%.*]], align 32, [[TBAA8:!tbaa !.*]]
// CHECK-NEXT:    ret void
//
void xsx48bw_test(__kvx_x1024 *m, __kvx_x256 *v) {
  m[0] = __builtin_kvx_xsx48bw(v[0]);
}

// CHECK-LABEL: @xzx48bw_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <1024 x i1> @llvm.kvx.xzx48bw(<256 x i1> [[TMP0]])
// CHECK-NEXT:    store <1024 x i1> [[TMP1]], <1024 x i1>* [[M:%.*]], align 32, [[TBAA8]]
// CHECK-NEXT:    ret void
//
void xzx48bw_test(__kvx_x1024 *m, __kvx_x256 *v) {
  m[0] = __builtin_kvx_xzx48bw(v[0]);
}

// CHECK-LABEL: @xtrunc48wb_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <1024 x i1>, <1024 x i1>* [[M:%.*]], align 32, [[TBAA8]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xtrunc48wb(<1024 x i1> [[TMP0]])
// CHECK-NEXT:    store <256 x i1> [[TMP1]], <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xtrunc48wb_test(__kvx_x1024 *m, __kvx_x256 *v) {
  v[0] = __builtin_kvx_xtrunc48wb(m[0]);
}

// CHECK-LABEL: @xmt44d_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <1024 x i1>, <1024 x i1>* [[M:%.*]], align 32, [[TBAA8]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <1024 x i1> @llvm.kvx.xmt44d(<1024 x i1> [[TMP0]])
// CHECK-NEXT:    store <1024 x i1> [[TMP1]], <1024 x i1>* [[M]], align 32, [[TBAA8]]
// CHECK-NEXT:    ret void
//
void xmt44d_test(__kvx_x1024 *m) {
  m[0] = __builtin_kvx_xmt44d(m[0]);
}

// CHECK-LABEL: @xload256(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <256 x i1>, <256 x i1>* [[V:%.*]], i64 1
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <256 x i1>* [[ARRAYIDX]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xload256(i8* nonnull [[TMP0]], i32 0)
// CHECK-NEXT:    store <256 x i1> [[TMP1]], <256 x i1>* [[V]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds <256 x i1>, <256 x i1>* [[V]], i64 3
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <256 x i1>* [[ARRAYIDX2]] to i8*
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <256 x i1> @llvm.kvx.xload256(i8* nonnull [[TMP2]], i32 1)
// CHECK-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <256 x i1>, <256 x i1>* [[V]], i64 2
// CHECK-NEXT:    store <256 x i1> [[TMP3]], <256 x i1>* [[ARRAYIDX3]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <256 x i1> @llvm.kvx.xload256(i8* nonnull [[TMP2]], i32 2)
// CHECK-NEXT:    store <256 x i1> [[TMP4]], <256 x i1>* [[ARRAYIDX3]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <256 x i1>* [[ARRAYIDX3]] to i8*
// CHECK-NEXT:    [[TMP6:%.*]] = tail call <256 x i1> @llvm.kvx.xload256(i8* nonnull [[TMP5]], i32 3)
// CHECK-NEXT:    store <256 x i1> [[TMP6]], <256 x i1>* [[V]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xload256(__kvx_x256 *v) {
  v[0] = __builtin_kvx_xload256(&v[1], "");
  v[2] = __builtin_kvx_xload256(&v[3], ".s");
  v[2] = __builtin_kvx_xload256(&v[3], ".u");
  v[0] = __builtin_kvx_xload256(&v[2], ".us");
}

// CHECK-LABEL: @xloadc256(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <256 x i1>, <256 x i1>* [[V:%.*]], i64 1
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <256 x i1>* [[ARRAYIDX]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xloadc256(<256 x i1> undef, i8* nonnull [[TMP0]], i64 [[X:%.*]], i32 0, i32 4)
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <256 x i1>, <256 x i1>* [[V]], i64 3
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <256 x i1>* [[ARRAYIDX1]] to i8*
// CHECK-NEXT:    [[TOBOOL_NOT:%.*]] = icmp eq i64 [[X]], 0
// CHECK-NEXT:    [[CONV:%.*]] = zext i1 [[TOBOOL_NOT]] to i64
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <256 x i1> @llvm.kvx.xloadc256(<256 x i1> [[TMP1]], i8* nonnull [[TMP2]], i64 [[CONV]], i32 1, i32 5)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <256 x i1> @llvm.kvx.xloadc256(<256 x i1> [[TMP3]], i8* nonnull [[TMP2]], i64 0, i32 2, i32 6)
// CHECK-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <256 x i1>, <256 x i1>* [[V]], i64 2
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <256 x i1>* [[ARRAYIDX3]] to i8*
// CHECK-NEXT:    [[TMP6:%.*]] = tail call <256 x i1> @llvm.kvx.xloadc256(<256 x i1> [[TMP4]], i8* nonnull [[TMP5]], i64 1, i32 3, i32 7)
// CHECK-NEXT:    store <256 x i1> [[TMP6]], <256 x i1>* [[V]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xloadc256(__kvx_x256 *v, long x) {
  __kvx_x256 c;
  c = __builtin_kvx_xloadc256(c, &v[1], x, ".mt");
  c = __builtin_kvx_xloadc256(c, &v[3], !x, ".s.mf");
  c = __builtin_kvx_xloadc256(c, &v[3], 0, ".u.mtc");
  v[0] = __builtin_kvx_xloadc256(c, &v[2], 1, ".us.mfc");
}

// CHECK-LABEL: @xloads1024(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <1024 x i1>* [[M:%.*]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <1024 x i1> @llvm.kvx.xloads1024(<1024 x i1> undef, i8* [[TMP0]], i32 0, i32 0)
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <1024 x i1> @llvm.kvx.xloads1024(<1024 x i1> [[TMP1]], i8* [[TMP0]], i32 0, i32 1)
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <1024 x i1> @llvm.kvx.xloads1024(<1024 x i1> [[TMP2]], i8* [[TMP0]], i32 1, i32 2)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <1024 x i1> @llvm.kvx.xloads1024(<1024 x i1> [[TMP3]], i8* [[TMP0]], i32 3, i32 3)
// CHECK-NEXT:    store <1024 x i1> [[TMP4]], <1024 x i1>* [[M]], align 32, [[TBAA8]]
// CHECK-NEXT:    ret void
//
void xloads1024(__kvx_x1024 *m) {
  __kvx_x1024 l;
  l = __builtin_kvx_xloads1024(l, &m[0], ".q0");
  l = __builtin_kvx_xloads1024(l, &m[0], "..q1");
  l = __builtin_kvx_xloads1024(l, &m[0], ".s.q2");
  m[0] = __builtin_kvx_xloads1024(l, &m[0], ".us.q3");
}

// CHECK-LABEL: @xloadsc1024(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <1024 x i1>* [[M:%.*]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <1024 x i1> @llvm.kvx.xloadsc1024(<1024 x i1> undef, i8* [[TMP0]], i64 [[X:%.*]], i32 0, i32 4, i32 0)
// CHECK-NEXT:    [[TOBOOL_NOT:%.*]] = icmp eq i64 [[X]], 0
// CHECK-NEXT:    [[CONV:%.*]] = zext i1 [[TOBOOL_NOT]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <1024 x i1> @llvm.kvx.xloadsc1024(<1024 x i1> [[TMP1]], i8* [[TMP0]], i64 [[CONV]], i32 1, i32 5, i32 1)
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <1024 x i1> @llvm.kvx.xloadsc1024(<1024 x i1> [[TMP2]], i8* [[TMP0]], i64 0, i32 2, i32 6, i32 2)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <1024 x i1> @llvm.kvx.xloadsc1024(<1024 x i1> [[TMP3]], i8* [[TMP0]], i64 1, i32 3, i32 7, i32 3)
// CHECK-NEXT:    store <1024 x i1> [[TMP4]], <1024 x i1>* [[M]], align 32, [[TBAA8]]
// CHECK-NEXT:    ret void
//
void xloadsc1024(__kvx_x1024 *m, long x) {
  __kvx_x1024 l;
  l = __builtin_kvx_xloadsc1024(l, &m[0], x, ".mt.q0");
  l = __builtin_kvx_xloadsc1024(l, &m[0], !x, ".s.mf.q1");
  l = __builtin_kvx_xloadsc1024(l, &m[0], 0, ".u.mtc.q2");
  m[0] = __builtin_kvx_xloadsc1024(l, &m[0], 1, ".us.mfc.q3");
}

// CHECK-LABEL: @xload512(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <512 x i1>* [[M:%.*]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <512 x i1> @llvm.kvx.xload512(i8* [[TMP0]], i32 0)
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <512 x i1>, <512 x i1>* [[M]], i64 1
// CHECK-NEXT:    store <512 x i1> [[TMP1]], <512 x i1>* [[ARRAYIDX1]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds <512 x i1>, <512 x i1>* [[M]], i64 [[X:%.*]]
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <512 x i1>* [[ARRAYIDX2]] to i8*
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <512 x i1> @llvm.kvx.xload512(i8* [[TMP2]], i32 0)
// CHECK-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <512 x i1>, <512 x i1>* [[M]], i64 2
// CHECK-NEXT:    store <512 x i1> [[TMP3]], <512 x i1>* [[ARRAYIDX3]], align 32, [[TBAA6]]
// CHECK-NEXT:    ret void
//
void xload512(__kvx_x512 *m, long x) {
  __kvx_x512 w = __builtin_kvx_xload512(&m[0], "");
  m[1] = w;
  w = __builtin_kvx_xload512(&m[x], "");
  m[2] = w;
}

// CHECK-LABEL: @xload1024(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <1024 x i1>* [[M:%.*]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <1024 x i1> @llvm.kvx.xload1024(i8* [[TMP0]], i32 0)
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <1024 x i1>, <1024 x i1>* [[M]], i64 1
// CHECK-NEXT:    store <1024 x i1> [[TMP1]], <1024 x i1>* [[ARRAYIDX1]], align 32, [[TBAA8]]
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds <1024 x i1>, <1024 x i1>* [[M]], i64 [[X:%.*]]
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <1024 x i1>* [[ARRAYIDX2]] to i8*
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <1024 x i1> @llvm.kvx.xload1024(i8* [[TMP2]], i32 1)
// CHECK-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <1024 x i1>, <1024 x i1>* [[M]], i64 2
// CHECK-NEXT:    store <1024 x i1> [[TMP3]], <1024 x i1>* [[ARRAYIDX3]], align 32, [[TBAA8]]
// CHECK-NEXT:    ret void
//
void xload1024(__kvx_x1024 *m, long x) {
  m[1] = __builtin_kvx_xload1024(&m[0], "");
  m[2] = __builtin_kvx_xload1024(&m[x], ".s");
}

// CHECK-LABEL: @xloadc512(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <512 x i1>, <512 x i1>* [[V:%.*]], i64 1
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <512 x i1>* [[ARRAYIDX]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <512 x i1> @llvm.kvx.xloadc512(<512 x i1> undef, i8* nonnull [[TMP0]], i64 [[X:%.*]], i32 0, i32 0)
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <512 x i1>, <512 x i1>* [[V]], i64 3
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <512 x i1>* [[ARRAYIDX1]] to i8*
// CHECK-NEXT:    [[TOBOOL_NOT:%.*]] = icmp eq i64 [[X]], 0
// CHECK-NEXT:    [[CONV:%.*]] = zext i1 [[TOBOOL_NOT]] to i64
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <512 x i1> @llvm.kvx.xloadc512(<512 x i1> [[TMP1]], i8* nonnull [[TMP2]], i64 [[CONV]], i32 1, i32 3)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <512 x i1> @llvm.kvx.xloadc512(<512 x i1> [[TMP3]], i8* nonnull [[TMP2]], i64 0, i32 2, i32 6)
// CHECK-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <512 x i1>, <512 x i1>* [[V]], i64 2
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <512 x i1>* [[ARRAYIDX3]] to i8*
// CHECK-NEXT:    [[TMP6:%.*]] = tail call <512 x i1> @llvm.kvx.xloadc512(<512 x i1> [[TMP4]], i8* nonnull [[TMP5]], i64 1, i32 3, i32 7)
// CHECK-NEXT:    store <512 x i1> [[TMP6]], <512 x i1>* [[V]], align 32, [[TBAA6]]
// CHECK-NEXT:    ret void
//
void xloadc512(__kvx_x512 *v, long x) {
  __kvx_x512 c;
  c = __builtin_kvx_xloadc512(c, &v[1], x, ".dnez");
  c = __builtin_kvx_xloadc512(c, &v[3], !x, ".s.weqz");
  c = __builtin_kvx_xloadc512(c, &v[3], 0, ".u.mtc");
  v[0] = __builtin_kvx_xloadc512(c, &v[2], 1, ".us.mfc");
}

// CHECK-LABEL: @xloadc1024(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <1024 x i1>, <1024 x i1>* [[V:%.*]], i64 1
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <1024 x i1>* [[ARRAYIDX]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <1024 x i1> @llvm.kvx.xloadc1024(<1024 x i1> undef, i8* nonnull [[TMP0]], <4 x i32> [[X:%.*]], i32 0, i32 0)
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <1024 x i1>, <1024 x i1>* [[V]], i64 3
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <1024 x i1>* [[ARRAYIDX1]] to i8*
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <1024 x i1> @llvm.kvx.xloadc1024(<1024 x i1> [[TMP1]], i8* nonnull [[TMP2]], <4 x i32> [[X]], i32 1, i32 3)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <1024 x i1> @llvm.kvx.xloadc1024(<1024 x i1> [[TMP3]], i8* nonnull [[TMP2]], <4 x i32> [[X]], i32 2, i32 6)
// CHECK-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <1024 x i1>, <1024 x i1>* [[V]], i64 2
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <1024 x i1>* [[ARRAYIDX3]] to i8*
// CHECK-NEXT:    [[TMP6:%.*]] = tail call <1024 x i1> @llvm.kvx.xloadc1024(<1024 x i1> [[TMP4]], i8* nonnull [[TMP5]], <4 x i32> [[X]], i32 3, i32 7)
// CHECK-NEXT:    store <1024 x i1> [[TMP6]], <1024 x i1>* [[V]], align 32, [[TBAA8]]
// CHECK-NEXT:    ret void
//
void xloadc1024(__kvx_x1024 *v, int4 x) {
  __kvx_x1024 c;
  c = __builtin_kvx_xloadc1024(c, &v[1], x, ".dnez");
  c = __builtin_kvx_xloadc1024(c, &v[3], x, ".s.weqz");
  c = __builtin_kvx_xloadc1024(c, &v[3], x, ".u.mtc");
  v[0] = __builtin_kvx_xloadc1024(c, &v[2], x, ".us.mfc");
}

// CHECK-LABEL: @xstore256(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    tail call void @llvm.kvx.xstore256(<256 x i1> [[TMP0]], i8* [[P:%.*]])
// CHECK-NEXT:    ret void
//
void xstore256(__kvx_x256 *v, void *p) {
  __builtin_kvx_xstore256(*v, p);
}

// CHECK-LABEL: @xstorec256(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    tail call void @llvm.kvx.xstorec256(<256 x i1> [[TMP0]], i8* [[P:%.*]], i64 1, i32 4)
// CHECK-NEXT:    ret void
//
void xstorec256( __kvx_x256 *v, void *p, long c) {
  __builtin_kvx_xstorec256(*v, p, 1, ".mt");
}

// CHECK-LABEL: @xloadStore256(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <256 x i1>, <256 x i1> addrspace(257)* [[V:%.*]], i64 1
// CHECK-NEXT:    [[TMP0:%.*]] = load volatile <256 x i1>, <256 x i1> addrspace(257)* [[ARRAYIDX]], align 32, [[TBAA2]]
// CHECK-NEXT:    store volatile <256 x i1> [[TMP0]], <256 x i1> addrspace(257)* [[V]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xloadStore256(volatile __preload __kvx_x256 *v) {
  v[0] = v[1];
}

// CHECK-LABEL: @xloadStore512(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <512 x i1>, <512 x i1> addrspace(258)* [[V:%.*]], i64 1
// CHECK-NEXT:    [[TMP0:%.*]] = load volatile <512 x i1>, <512 x i1> addrspace(258)* [[ARRAYIDX]], align 32, [[TBAA6]]
// CHECK-NEXT:    store volatile <512 x i1> [[TMP0]], <512 x i1> addrspace(258)* [[V]], align 32, [[TBAA6]]
// CHECK-NEXT:    ret void
//
void xloadStore512(volatile __speculate __kvx_x512 *v) {
  v[0] = v[1];
}

// CHECK-LABEL: @xloadStore1024(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <1024 x i1>, <1024 x i1> addrspace(256)* [[V:%.*]], i64 1
// CHECK-NEXT:    [[TMP0:%.*]] = load volatile <1024 x i1>, <1024 x i1> addrspace(256)* [[ARRAYIDX]], align 32, [[TBAA8]]
// CHECK-NEXT:    store volatile <1024 x i1> [[TMP0]], <1024 x i1> addrspace(256)* [[V]], align 32, [[TBAA8]]
// CHECK-NEXT:    ret void
//
void xloadStore1024(volatile __bypass __kvx_x1024 *v) {
  v[0] = v[1];
}

// CHECK-LABEL: @xsendo(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    tail call void @llvm.kvx.xsendo(<256 x i1> [[TMP0]], i32 1) [[ATTR1:#.*]]
// CHECK-NEXT:    [[TMP1:%.*]] = load <256 x i1>, <256 x i1>* [[V]], align 32, [[TBAA2]]
// CHECK-NEXT:    tail call void @llvm.kvx.xsendo(<256 x i1> [[TMP1]], i32 0) [[ATTR1]]
// CHECK-NEXT:    ret void
//
void xsendo(__kvx_x256 *v) {
  __builtin_kvx_xsendo(*v, ".b");
  __builtin_kvx_xsendo(*v, ".f");
}

// CHECK-LABEL: @xrecvo(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <256 x i1> @llvm.kvx.xrecvo(i32 1) [[ATTR1]]
// CHECK-NEXT:    store <256 x i1> [[TMP0]], <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xrecvo(i32 0) [[ATTR1]]
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <256 x i1>, <256 x i1>* [[V]], i64 1
// CHECK-NEXT:    store <256 x i1> [[TMP1]], <256 x i1>* [[ARRAYIDX]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xrecvo(__kvx_x256 *v) {
  *v = __builtin_kvx_xrecvo(".b");
  v[1] = __builtin_kvx_xrecvo(".f");
}

// CHECK-LABEL: @xsendrecvo(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xsendrecvo(<256 x i1> [[TMP0]], i32 1, i32 1) [[ATTR1]]
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <256 x i1>, <256 x i1>* [[V]], i64 2
// CHECK-NEXT:    store <256 x i1> [[TMP1]], <256 x i1>* [[ARRAYIDX]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <256 x i1>, <256 x i1>* [[V]], i64 1
// CHECK-NEXT:    [[TMP2:%.*]] = load <256 x i1>, <256 x i1>* [[ARRAYIDX1]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <256 x i1> @llvm.kvx.xsendrecvo(<256 x i1> [[TMP2]], i32 0, i32 1) [[ATTR1]]
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds <256 x i1>, <256 x i1>* [[V]], i64 4
// CHECK-NEXT:    store <256 x i1> [[TMP3]], <256 x i1>* [[ARRAYIDX2]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP4:%.*]] = load <256 x i1>, <256 x i1>* [[V]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP5:%.*]] = tail call <256 x i1> @llvm.kvx.xsendrecvo(<256 x i1> [[TMP4]], i32 1, i32 0) [[ATTR1]]
// CHECK-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <256 x i1>, <256 x i1>* [[V]], i64 3
// CHECK-NEXT:    store <256 x i1> [[TMP5]], <256 x i1>* [[ARRAYIDX3]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP6:%.*]] = load <256 x i1>, <256 x i1>* [[ARRAYIDX1]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP7:%.*]] = tail call <256 x i1> @llvm.kvx.xsendrecvo(<256 x i1> [[TMP6]], i32 0, i32 0) [[ATTR1]]
// CHECK-NEXT:    [[ARRAYIDX5:%.*]] = getelementptr inbounds <256 x i1>, <256 x i1>* [[V]], i64 5
// CHECK-NEXT:    store <256 x i1> [[TMP7]], <256 x i1>* [[ARRAYIDX5]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xsendrecvo(__kvx_x256 *v) {
  v[2] = __builtin_kvx_xsendrecvo(*v, ".b.b");
  v[4] = __builtin_kvx_xsendrecvo(v[1], ".f.b");
  v[3] = __builtin_kvx_xsendrecvo(*v, ".b.f");
  v[5] = __builtin_kvx_xsendrecvo(v[1], ".f.f");
}

// CHECK-LABEL: @xcopyv(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <1024 x i1>, <1024 x i1>* [[V:%.*]], align 32, [[TBAA8]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <1024 x i1> @llvm.kvx.xcopyv(<1024 x i1> [[TMP0]], i32 0)
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <1024 x i1>, <1024 x i1>* [[V]], i64 3
// CHECK-NEXT:    store <1024 x i1> [[TMP1]], <1024 x i1>* [[ARRAYIDX]], align 32, [[TBAA8]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <1024 x i1> @llvm.kvx.xcopyv(<1024 x i1> [[TMP0]], i32 1)
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <1024 x i1>, <1024 x i1>* [[V]], i64 1
// CHECK-NEXT:    store <1024 x i1> [[TMP2]], <1024 x i1>* [[ARRAYIDX1]], align 32, [[TBAA8]]
// CHECK-NEXT:    ret void
//
void xcopyv(__kvx_x1024 *v) {
  v[3] = __builtin_kvx_xcopyv(*v, "");
  v[1] = __builtin_kvx_xcopyv(*v, ".td");
}

// CHECK-LABEL: @xcopyx(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <512 x i1>, <512 x i1>* [[V:%.*]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <512 x i1> @llvm.kvx.xcopyx(<512 x i1> [[TMP0]], i32 0)
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <512 x i1>, <512 x i1>* [[V]], i64 1
// CHECK-NEXT:    store <512 x i1> [[TMP1]], <512 x i1>* [[ARRAYIDX]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <512 x i1> @llvm.kvx.xcopyx(<512 x i1> [[TMP0]], i32 1)
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <512 x i1>, <512 x i1>* [[V]], i64 2
// CHECK-NEXT:    store <512 x i1> [[TMP2]], <512 x i1>* [[ARRAYIDX1]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <512 x i1> @llvm.kvx.xcopyx(<512 x i1> [[TMP0]], i32 2)
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds <512 x i1>, <512 x i1>* [[V]], i64 3
// CHECK-NEXT:    store <512 x i1> [[TMP3]], <512 x i1>* [[ARRAYIDX2]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <512 x i1> @llvm.kvx.xcopyx(<512 x i1> [[TMP0]], i32 3)
// CHECK-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <512 x i1>, <512 x i1>* [[V]], i64 4
// CHECK-NEXT:    store <512 x i1> [[TMP4]], <512 x i1>* [[ARRAYIDX3]], align 32, [[TBAA6]]
// CHECK-NEXT:    ret void
//
void xcopyx(__kvx_x512 *v) {
  v[1] = __builtin_kvx_xcopyx(*v, "");
  v[2] = __builtin_kvx_xcopyx(*v, ".zd");
  v[3] = __builtin_kvx_xcopyx(*v, ".ud");
  v[4] = __builtin_kvx_xcopyx(*v, ".TQ");
}

// CHECK-LABEL: @xfminmaxhx(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, <256 x i1>* [[V:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <256 x i1>, <256 x i1>* [[V]], i64 1
// CHECK-NEXT:    [[TMP1:%.*]] = load <256 x i1>, <256 x i1>* [[ARRAYIDX1]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <256 x i1> @llvm.kvx.xfmaxhx(<256 x i1> [[TMP0]], <256 x i1> [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <256 x i1> @llvm.kvx.xfminhx(<256 x i1> [[TMP2]], <256 x i1> [[TMP0]])
// CHECK-NEXT:    store <256 x i1> [[TMP3]], <256 x i1>* [[V]], align 32, [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xfminmaxhx(__kvx_x256 *v) {
  v[0] = __builtin_kvx_xfminhx(__builtin_kvx_xfmaxhx(v[0], v[1]), v[0]);
}

// CHECK-LABEL: @xsplatov(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, <256 x i1>* [[C:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <1024 x i1> @llvm.kvx.xsplatov(<256 x i1> [[TMP0]], i32 0)
// CHECK-NEXT:    store <1024 x i1> [[TMP1]], <1024 x i1>* [[V:%.*]], align 32, [[TBAA8]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <1024 x i1> @llvm.kvx.xsplatov(<256 x i1> [[TMP0]], i32 1)
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <1024 x i1>, <1024 x i1>* [[V]], i64 1
// CHECK-NEXT:    store <1024 x i1> [[TMP2]], <1024 x i1>* [[ARRAYIDX1]], align 32, [[TBAA8]]
// CHECK-NEXT:    ret void
//
void xsplatov(__kvx_x1024 *v, __kvx_x256 *c) {
  v[0] = __builtin_kvx_xsplatov(*c, "");
  v[1] = __builtin_kvx_xsplatov(*c, ".td");
}

// CHECK-LABEL: @xsplatox(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, <256 x i1>* [[C:%.*]], align 32, [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <512 x i1> @llvm.kvx.xsplatox(<256 x i1> [[TMP0]], i32 0)
// CHECK-NEXT:    store <512 x i1> [[TMP1]], <512 x i1>* [[V:%.*]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <512 x i1> @llvm.kvx.xsplatox(<256 x i1> [[TMP0]], i32 1)
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <512 x i1>, <512 x i1>* [[V]], i64 1
// CHECK-NEXT:    store <512 x i1> [[TMP2]], <512 x i1>* [[ARRAYIDX1]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <512 x i1> @llvm.kvx.xsplatox(<256 x i1> [[TMP0]], i32 2)
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds <512 x i1>, <512 x i1>* [[V]], i64 2
// CHECK-NEXT:    store <512 x i1> [[TMP3]], <512 x i1>* [[ARRAYIDX2]], align 32, [[TBAA6]]
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <512 x i1> @llvm.kvx.xsplatox(<256 x i1> [[TMP0]], i32 3)
// CHECK-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <512 x i1>, <512 x i1>* [[V]], i64 3
// CHECK-NEXT:    store <512 x i1> [[TMP4]], <512 x i1>* [[ARRAYIDX3]], align 32, [[TBAA6]]
// CHECK-NEXT:    ret void
//
void xsplatox(__kvx_x512 *v, __kvx_x256 *c) {
  v[0] = __builtin_kvx_xsplatox(*c, "");
  v[1] = __builtin_kvx_xsplatox(*c, ".zd");
  v[2] = __builtin_kvx_xsplatox(*c, ".ud");
  v[3] = __builtin_kvx_xsplatox(*c, ".tq");
}
