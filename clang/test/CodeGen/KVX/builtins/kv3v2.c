// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -O2 -triple kvx-kalray-cos -S -emit-llvm -target-cpu kv3-2 -o - -x c %s | FileCheck %s

#include <kvx_builtins.h>

// CHECK-LABEL: @dflushsw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    tail call void @llvm.kvx.dflushsw(i64 [[W:%.*]], i64 [[S:%.*]], i32 0)
// CHECK-NEXT:    tail call void @llvm.kvx.dflushsw(i64 [[W]], i64 [[S]], i32 1)
// CHECK-NEXT:    ret void
//
void dflushsw(unsigned long w, unsigned long s) {
  __builtin_kvx_dflushsw(w, s, ".l1");
  __builtin_kvx_dflushsw(w, s, ".l2");
}

// CHECK-LABEL: @dinvalsw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    tail call void @llvm.kvx.dinvalsw(i64 [[W:%.*]], i64 [[S:%.*]], i32 0)
// CHECK-NEXT:    tail call void @llvm.kvx.dinvalsw(i64 [[W]], i64 [[S]], i32 1)
// CHECK-NEXT:    ret void
//
void dinvalsw(unsigned long w, unsigned long s) {
  __builtin_kvx_dinvalsw(w, s, ".l1");
  __builtin_kvx_dinvalsw(w, s, ".l2");
}

// CHECK-LABEL: @dpurgesw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    tail call void @llvm.kvx.dpurgesw(i64 [[W:%.*]], i64 [[S:%.*]], i32 0)
// CHECK-NEXT:    tail call void @llvm.kvx.dpurgesw(i64 [[W]], i64 [[S]], i32 1)
// CHECK-NEXT:    ret void
//
void dpurgesw(unsigned long w, unsigned long s) {
  __builtin_kvx_dpurgesw(w, s, ".l1");
  __builtin_kvx_dpurgesw(w, s, ".l2");
}

// CHECK-LABEL: @acswapw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.acswapw(ptr [[P:%.*]], i32 [[A:%.*]], i32 [[B:%.*]], i32 1, i32 0)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int acswapw(int *p, int a, int b) {
  return __builtin_kvx_acswapw(p, a, b, "");
}

// CHECK-LABEL: @acswapwv(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.acswapw(ptr [[P:%.*]], i32 [[A:%.*]], i32 [[B:%.*]], i32 1, i32 0)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int acswapwv(int *p, int a, int b) {
  return __builtin_kvx_acswapw(p, a, b);
}

// CHECK-LABEL: @acswapwg(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.acswapw(ptr [[P:%.*]], i32 [[A:%.*]], i32 [[B:%.*]], i32 1, i32 1)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int acswapwg(int *p, int a, int b) {
  return __builtin_kvx_acswapw(p, a, b, ".g");
}

// CHECK-LABEL: @acswapwvg(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.acswapw(ptr [[P:%.*]], i32 [[A:%.*]], i32 [[B:%.*]], i32 0, i32 1)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int acswapwvg(int *p, int a, int b) {
  return __builtin_kvx_acswapw(p, a, b, ".v.g");
}

// CHECK-LABEL: @acswapd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.acswapd(ptr [[P:%.*]], i64 [[A:%.*]], i64 [[B:%.*]], i32 1, i32 0)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long acswapd(long *p, long a, long b) {
  return __builtin_kvx_acswapd(p, a, b, "");
}

// CHECK-LABEL: @acswapdv(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.acswapd(ptr [[P:%.*]], i64 [[A:%.*]], i64 [[B:%.*]], i32 0, i32 0)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long acswapdv(long *p, long a, long b) {
  return __builtin_kvx_acswapd(p, a, b, ".v");
}

// CHECK-LABEL: @acswapdg(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.acswapd(ptr [[P:%.*]], i64 [[A:%.*]], i64 [[B:%.*]], i32 1, i32 1)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long acswapdg(long *p, long a, long b) {
  return __builtin_kvx_acswapd(p, a, b, ".g");
}

// CHECK-LABEL: @acswapdvg(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.acswapd(ptr [[P:%.*]], i64 [[A:%.*]], i64 [[B:%.*]], i32 0, i32 1)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long acswapdvg(long *p, long a, long b) {
  return __builtin_kvx_acswapd(p, a, b, ".v.g");
}

// CHECK-LABEL: @acswapq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i64> @llvm.kvx.acswapq(ptr [[P:%.*]], <2 x i64> [[A:%.*]], <2 x i64> [[B:%.*]], i32 1, i32 0)
// CHECK-NEXT:    ret <2 x i64> [[TMP0]]
//
__kvx_v2di acswapq(__kvx_v2di *p, __kvx_v2di a, __kvx_v2di b) {
  return __builtin_kvx_acswapq(p, a, b, "");
}

// CHECK-LABEL: @acswapqv(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i64> @llvm.kvx.acswapq(ptr [[P:%.*]], <2 x i64> [[A:%.*]], <2 x i64> [[B:%.*]], i32 0, i32 0)
// CHECK-NEXT:    ret <2 x i64> [[TMP0]]
//
__kvx_v2di acswapqv(__kvx_v2di *p, __kvx_v2di a, __kvx_v2di b) {
  return __builtin_kvx_acswapq(p, a, b, ".v");
}

// CHECK-LABEL: @acswapqg(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i64> @llvm.kvx.acswapq(ptr [[P:%.*]], <2 x i64> [[A:%.*]], <2 x i64> [[B:%.*]], i32 1, i32 1)
// CHECK-NEXT:    ret <2 x i64> [[TMP0]]
//
__kvx_v2di acswapqg(__kvx_v2di *p, __kvx_v2di a, __kvx_v2di b) {
  return __builtin_kvx_acswapq(p, a, b, ".g");
}

// CHECK-LABEL: @acswapqvg(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i64> @llvm.kvx.acswapq(ptr [[P:%.*]], <2 x i64> [[A:%.*]], <2 x i64> [[B:%.*]], i32 0, i32 1)
// CHECK-NEXT:    ret <2 x i64> [[TMP0]]
//
__kvx_v2di acswapqvg(__kvx_v2di *p, __kvx_v2di a, __kvx_v2di b) {
  return __builtin_kvx_acswapq(p, a, b, ".v.g");
}

// CHECK-LABEL: @acswapqvgr(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[IDXPROM:%.*]] = sext i32 [[O:%.*]] to i64
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <2 x i64>, ptr [[P:%.*]], i64 [[IDXPROM]]
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i64> @llvm.kvx.acswapq(ptr [[ARRAYIDX]], <2 x i64> [[A:%.*]], <2 x i64> [[B:%.*]], i32 0, i32 1)
// CHECK-NEXT:    ret <2 x i64> [[TMP0]]
//
__kvx_v2di acswapqvgr(__kvx_v2di *p, __kvx_v2di a, __kvx_v2di b, int o) {
  return __builtin_kvx_acswapq(&p[o], a, b, ".v.g");
}

// CHECK-LABEL: @acswapqvgri27(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i8, ptr [[P:%.*]], i64 240
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i64> @llvm.kvx.acswapq(ptr nonnull [[ARRAYIDX]], <2 x i64> [[A:%.*]], <2 x i64> [[B:%.*]], i32 0, i32 1)
// CHECK-NEXT:    ret <2 x i64> [[TMP0]]
//
__kvx_v2di acswapqvgri27(__kvx_v2di *p, __kvx_v2di a, __kvx_v2di b) {
  return __builtin_kvx_acswapq(&p[15], a, b, ".v.g");
}

// CHECK-LABEL: @acswapqvgri54(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i8, ptr [[P:%.*]], i64 68719476784
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i64> @llvm.kvx.acswapq(ptr nonnull [[ARRAYIDX]], <2 x i64> [[A:%.*]], <2 x i64> [[B:%.*]], i32 0, i32 1)
// CHECK-NEXT:    ret <2 x i64> [[TMP0]]
//
__kvx_v2di acswapqvgri54(__kvx_v2di *p, __kvx_v2di a, __kvx_v2di b) {
  return __builtin_kvx_acswapq(&p[4294967299], a, b, ".v.g");
}

// CHECK-LABEL: @alw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.alw(ptr [[V:%.*]], i32 0)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int alw(int *v) {
  return __builtin_kvx_alw(v, "");
}

// CHECK-LABEL: @alw_ri10(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i8, ptr [[V:%.*]], i64 4
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.alw(ptr nonnull [[ARRAYIDX]], i32 0)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int alw_ri10(int *v) {
  return __builtin_kvx_alw(&v[1], "");
}

// CHECK-LABEL: @alw_ri27(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i8, ptr [[V:%.*]], i64 4000
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.alw(ptr nonnull [[ARRAYIDX]], i32 0)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int alw_ri27(int *v) {
  return __builtin_kvx_alw(&v[1000], "");
}

// CHECK-LABEL: @alw_ri64(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i8, ptr [[V:%.*]], i64 36000000000
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.alw(ptr nonnull [[ARRAYIDX]], i32 0)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int alw_ri64(int *v) {
  return __builtin_kvx_alw(&v[9000000000], "");
}

// CHECK-LABEL: @alw_g(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.kvx.alw(ptr [[V:%.*]], i32 1)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int alw_g(int *v) {
  return __builtin_kvx_alw(v, ".g");
}

// CHECK-LABEL: @ald(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.ald(ptr [[V:%.*]], i32 0)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long ald(long *v) {
  return __builtin_kvx_ald(v, "");
}

// CHECK-LABEL: @ald_ri10(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i8, ptr [[V:%.*]], i64 8
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.ald(ptr nonnull [[ARRAYIDX]], i32 0)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long ald_ri10(long *v) {
  return __builtin_kvx_ald(&v[1], "");
}

// CHECK-LABEL: @ald_ri27(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i8, ptr [[V:%.*]], i64 8000
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.ald(ptr nonnull [[ARRAYIDX]], i32 0)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long ald_ri27(long *v) {
  return __builtin_kvx_ald(&v[1000], "");
}

// CHECK-LABEL: @ald_ri64(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i8, ptr [[V:%.*]], i64 72000000000
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.ald(ptr nonnull [[ARRAYIDX]], i32 0)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long ald_ri64(long *v) {
  return __builtin_kvx_ald(&v[9000000000], "");
}

// CHECK-LABEL: @ald_g(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.kvx.ald(ptr [[V:%.*]], i32 1)
// CHECK-NEXT:    ret i64 [[TMP0]]
//
long ald_g(long *v) {
  return __builtin_kvx_ald(v, ".g");
}

// CHECK-LABEL: @asw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    tail call void @llvm.kvx.asw(ptr [[P:%.*]], i32 [[V:%.*]], i32 0)
// CHECK-NEXT:    ret void
//
void asw(int *p, int v) {
  return __builtin_kvx_asw(p, v, "");
}

// CHECK-LABEL: @asw_ri10(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i8, ptr [[P:%.*]], i64 4
// CHECK-NEXT:    tail call void @llvm.kvx.asw(ptr nonnull [[ARRAYIDX]], i32 [[V:%.*]], i32 0)
// CHECK-NEXT:    ret void
//
void asw_ri10(int *p, int v) {
  return __builtin_kvx_asw(&p[1], v, "");
}

// CHECK-LABEL: @asw_ri27(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i8, ptr [[P:%.*]], i64 4000
// CHECK-NEXT:    tail call void @llvm.kvx.asw(ptr nonnull [[ARRAYIDX]], i32 [[V:%.*]], i32 0)
// CHECK-NEXT:    ret void
//
void asw_ri27(int *p, int v) {
  return __builtin_kvx_asw(&p[1000], v, "");
}

// CHECK-LABEL: @asw_ri54(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i8, ptr [[P:%.*]], i64 36000000000
// CHECK-NEXT:    tail call void @llvm.kvx.asw(ptr nonnull [[ARRAYIDX]], i32 [[V:%.*]], i32 0)
// CHECK-NEXT:    ret void
//
void asw_ri54(int *p, int v) {
  return __builtin_kvx_asw(&p[9000000000], v, "");
}

// CHECK-LABEL: @asw_g(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    tail call void @llvm.kvx.asw(ptr [[P:%.*]], i32 [[V:%.*]], i32 1)
// CHECK-NEXT:    ret void
//
void asw_g(int *p, int v) {
  return __builtin_kvx_asw(p, v, ".g");
}

// CHECK-LABEL: @asd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    tail call void @llvm.kvx.asd(ptr [[P:%.*]], i64 [[V:%.*]], i32 0)
// CHECK-NEXT:    ret void
//
void asd(long *p, long v) {
  return __builtin_kvx_asd(p, v, "");
}

// CHECK-LABEL: @asd_ri10(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i8, ptr [[P:%.*]], i64 8
// CHECK-NEXT:    tail call void @llvm.kvx.asd(ptr nonnull [[ARRAYIDX]], i64 [[V:%.*]], i32 0)
// CHECK-NEXT:    ret void
//
void asd_ri10(long *p, long v) {
  return __builtin_kvx_asd(&p[1], v, "");
}

// CHECK-LABEL: @asd_ri27(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i8, ptr [[P:%.*]], i64 8000
// CHECK-NEXT:    tail call void @llvm.kvx.asd(ptr nonnull [[ARRAYIDX]], i64 [[V:%.*]], i32 0)
// CHECK-NEXT:    ret void
//
void asd_ri27(long *p, long v) {
  return __builtin_kvx_asd(&p[1000], v, "");
}

// CHECK-LABEL: @asd_ri54(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i8, ptr [[P:%.*]], i64 72000000000
// CHECK-NEXT:    tail call void @llvm.kvx.asd(ptr nonnull [[ARRAYIDX]], i64 [[V:%.*]], i32 0)
// CHECK-NEXT:    ret void
//
void asd_ri54(long *p, long v) {
  return __builtin_kvx_asd(&p[9000000000], v, "");
}

// CHECK-LABEL: @asd_g(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    tail call void @llvm.kvx.asd(ptr [[P:%.*]], i64 [[V:%.*]], i32 1)
// CHECK-NEXT:    ret void
//
void asd_g(long *p, long v) {
  return __builtin_kvx_asd(p, v, ".g");
}

// CHECK-LABEL: @dpurgel(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    tail call void @llvm.kvx.dpurgel(ptr [[W:%.*]])
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <8 x i64>, ptr [[W]], i64 [[P:%.*]]
// CHECK-NEXT:    tail call void @llvm.kvx.dpurgel(ptr [[ARRAYIDX]])
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds i8, ptr [[W]], i64 320
// CHECK-NEXT:    tail call void @llvm.kvx.dpurgel(ptr nonnull [[ARRAYIDX1]])
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds i8, ptr [[W]], i64 65536
// CHECK-NEXT:    tail call void @llvm.kvx.dpurgel(ptr nonnull [[ARRAYIDX2]])
// CHECK-NEXT:    ret void
//
void dpurgel(__kvx_v8di *w, long p) {
  __builtin_kvx_dpurgel(w);
  __builtin_kvx_dpurgel(&w[p]);
  __builtin_kvx_dpurgel(&w[5]);
  __builtin_kvx_dpurgel(&w[1024]);
}

// CHECK-LABEL: @dflushl(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    tail call void @llvm.kvx.dflushl(ptr [[W:%.*]])
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <8 x i64>, ptr [[W]], i64 [[P:%.*]]
// CHECK-NEXT:    tail call void @llvm.kvx.dflushl(ptr [[ARRAYIDX]])
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds i8, ptr [[W]], i64 320
// CHECK-NEXT:    tail call void @llvm.kvx.dflushl(ptr nonnull [[ARRAYIDX1]])
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds i8, ptr [[W]], i64 65536
// CHECK-NEXT:    tail call void @llvm.kvx.dflushl(ptr nonnull [[ARRAYIDX2]])
// CHECK-NEXT:    ret void
//
void dflushl(__kvx_v8di *w, long p) {
  __builtin_kvx_dflushl(w);
  __builtin_kvx_dflushl(&w[p]);
  __builtin_kvx_dflushl(&w[5]);
  __builtin_kvx_dflushl(&w[1024]);
}

// CHECK-LABEL: @zxlbhq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <4 x i16> @llvm.kvx.zxlbhq(<8 x i8> [[V:%.*]])
// CHECK-NEXT:    ret <4 x i16> [[TMP0]]
//
__kvx_v4hu zxlbhq(__kvx_v8qu v){
  return __builtin_kvx_zxlbhq(v);
}

// CHECK-LABEL: @zxmbhq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <4 x i16> @llvm.kvx.zxmbhq(<8 x i8> [[V:%.*]])
// CHECK-NEXT:    ret <4 x i16> [[TMP0]]
//
__kvx_v4hu zxmbhq(__kvx_v8qu v){
  return __builtin_kvx_zxmbhq(v);
}

// CHECK-LABEL: @zxlhwp(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.kvx.zxlhwp(<4 x i16> [[V:%.*]])
// CHECK-NEXT:    ret <2 x i32> [[TMP0]]
//
__kvx_v2su zxlhwp(__kvx_v4hu v){
  return __builtin_kvx_zxlhwp(v);
}

// CHECK-LABEL: @zxmhwp(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.kvx.zxmhwp(<4 x i16> [[V:%.*]])
// CHECK-NEXT:    ret <2 x i32> [[TMP0]]
//
__kvx_v2su zxmhwp(__kvx_v4hu v){
  return __builtin_kvx_zxmhwp(v);
}

// CHECK-LABEL: @maddwq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[MUL:%.*]] = mul <4 x i32> [[B:%.*]], [[A:%.*]]
// CHECK-NEXT:    [[ADD:%.*]] = add <4 x i32> [[MUL]], [[ACC:%.*]]
// CHECK-NEXT:    ret <4 x i32> [[ADD]]
//
__kvx_v4si maddwq(__kvx_v4si acc, __kvx_v4si a, __kvx_v4si b) {
    return acc + a * b;
}

// CHECK-LABEL: @maddmwq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = sext <4 x i32> [[A:%.*]] to <4 x i64>
// CHECK-NEXT:    [[CONV1_I:%.*]] = sext <4 x i32> [[B:%.*]] to <4 x i64>
// CHECK-NEXT:    [[MUL2_I:%.*]] = mul nsw <4 x i64> [[CONV1_I]], [[CONV_I]]
// CHECK-NEXT:    [[SHR_I:%.*]] = lshr <4 x i64> [[MUL2_I]], <i64 32, i64 32, i64 32, i64 32>
// CHECK-NEXT:    [[CONV3_I:%.*]] = trunc <4 x i64> [[SHR_I]] to <4 x i32>
// CHECK-NEXT:    [[ADD_I:%.*]] = add <4 x i32> [[CONV3_I]], [[ACC:%.*]]
// CHECK-NEXT:    ret <4 x i32> [[ADD_I]]
//
__kvx_v4si maddmwq(__kvx_v4si acc, __kvx_v4si a, __kvx_v4si b) {
    return __builtin_kvx_maddmwq(acc, a, b);
}

// CHECK-LABEL: @maddumwq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext <4 x i32> [[A:%.*]] to <4 x i64>
// CHECK-NEXT:    [[CONV1_I:%.*]] = zext <4 x i32> [[B:%.*]] to <4 x i64>
// CHECK-NEXT:    [[MUL2_I:%.*]] = mul nuw <4 x i64> [[CONV1_I]], [[CONV_I]]
// CHECK-NEXT:    [[SHR_I:%.*]] = lshr <4 x i64> [[MUL2_I]], <i64 32, i64 32, i64 32, i64 32>
// CHECK-NEXT:    [[CONV3_I:%.*]] = trunc <4 x i64> [[SHR_I]] to <4 x i32>
// CHECK-NEXT:    [[ADD_I:%.*]] = add <4 x i32> [[CONV3_I]], [[ACC:%.*]]
// CHECK-NEXT:    ret <4 x i32> [[ADD_I]]
//
__kvx_v4su maddumwq(__kvx_v4su acc, __kvx_v4su a, __kvx_v4su b) {
    return __builtin_kvx_maddumwq(acc, a, b);
}

// CHECK-LABEL: @mulmwq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = sext <4 x i32> [[A:%.*]] to <4 x i64>
// CHECK-NEXT:    [[CONV1_I:%.*]] = sext <4 x i32> [[B:%.*]] to <4 x i64>
// CHECK-NEXT:    [[MUL2_I:%.*]] = mul nsw <4 x i64> [[CONV1_I]], [[CONV_I]]
// CHECK-NEXT:    [[SHR_I:%.*]] = lshr <4 x i64> [[MUL2_I]], <i64 32, i64 32, i64 32, i64 32>
// CHECK-NEXT:    [[CONV3_I:%.*]] = trunc <4 x i64> [[SHR_I]] to <4 x i32>
// CHECK-NEXT:    ret <4 x i32> [[CONV3_I]]
//
__kvx_v4si mulmwq(__kvx_v4si a, __kvx_v4si b) {
    return __builtin_kvx_mulmwq(a, b);
}

// CHECK-LABEL: @mulumwq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext <4 x i32> [[A:%.*]] to <4 x i64>
// CHECK-NEXT:    [[CONV1_I:%.*]] = zext <4 x i32> [[B:%.*]] to <4 x i64>
// CHECK-NEXT:    [[MUL2_I:%.*]] = mul nuw <4 x i64> [[CONV1_I]], [[CONV_I]]
// CHECK-NEXT:    [[SHR_I:%.*]] = lshr <4 x i64> [[MUL2_I]], <i64 32, i64 32, i64 32, i64 32>
// CHECK-NEXT:    [[CONV3_I:%.*]] = trunc <4 x i64> [[SHR_I]] to <4 x i32>
// CHECK-NEXT:    ret <4 x i32> [[CONV3_I]]
//
__kvx_v4su mulumwq(__kvx_v4su a, __kvx_v4su b) {
    return __builtin_kvx_mulumwq(a, b);
}

// CHECK-LABEL: @msbfmwq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = sext <4 x i32> [[A:%.*]] to <4 x i64>
// CHECK-NEXT:    [[CONV1_I:%.*]] = sext <4 x i32> [[B:%.*]] to <4 x i64>
// CHECK-NEXT:    [[MUL2_I:%.*]] = mul nsw <4 x i64> [[CONV1_I]], [[CONV_I]]
// CHECK-NEXT:    [[SHR_I:%.*]] = lshr <4 x i64> [[MUL2_I]], <i64 32, i64 32, i64 32, i64 32>
// CHECK-NEXT:    [[CONV3_I:%.*]] = trunc <4 x i64> [[SHR_I]] to <4 x i32>
// CHECK-NEXT:    [[SUB_I:%.*]] = sub <4 x i32> [[ACC:%.*]], [[CONV3_I]]
// CHECK-NEXT:    ret <4 x i32> [[SUB_I]]
//
__kvx_v4si msbfmwq(__kvx_v4si acc, __kvx_v4si a, __kvx_v4si b) {
    return __builtin_kvx_msbfmwq(acc, a, b);
}

// CHECK-LABEL: @msbfumwq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext <4 x i32> [[A:%.*]] to <4 x i64>
// CHECK-NEXT:    [[CONV1_I:%.*]] = zext <4 x i32> [[B:%.*]] to <4 x i64>
// CHECK-NEXT:    [[MUL2_I:%.*]] = mul nuw <4 x i64> [[CONV1_I]], [[CONV_I]]
// CHECK-NEXT:    [[SHR_I:%.*]] = lshr <4 x i64> [[MUL2_I]], <i64 32, i64 32, i64 32, i64 32>
// CHECK-NEXT:    [[CONV3_I:%.*]] = trunc <4 x i64> [[SHR_I]] to <4 x i32>
// CHECK-NEXT:    [[SUB_I:%.*]] = sub <4 x i32> [[ACC:%.*]], [[CONV3_I]]
// CHECK-NEXT:    ret <4 x i32> [[SUB_I]]
//
__kvx_v4su msbfumwq(__kvx_v4su acc, __kvx_v4su a, __kvx_v4su b) {
    return __builtin_kvx_msbfumwq(acc, a, b);
}

// CHECK-LABEL: @msbfwq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[MUL_I:%.*]] = mul <4 x i32> [[B:%.*]], [[A:%.*]]
// CHECK-NEXT:    [[SUB_I:%.*]] = sub <4 x i32> [[ACC:%.*]], [[MUL_I]]
// CHECK-NEXT:    ret <4 x i32> [[SUB_I]]
//
__kvx_v4si msbfwq(__kvx_v4si acc, __kvx_v4si a, __kvx_v4si b) {
    return __builtin_kvx_msbfwq(acc, a, b);
}

// CHECK-LABEL: @maddsumwq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = sext <4 x i32> [[A:%.*]] to <4 x i64>
// CHECK-NEXT:    [[CONV1_I:%.*]] = zext <4 x i32> [[B:%.*]] to <4 x i64>
// CHECK-NEXT:    [[MUL2_I:%.*]] = mul nsw <4 x i64> [[CONV1_I]], [[CONV_I]]
// CHECK-NEXT:    [[SHR_I:%.*]] = lshr <4 x i64> [[MUL2_I]], <i64 32, i64 32, i64 32, i64 32>
// CHECK-NEXT:    [[CONV3_I:%.*]] = trunc <4 x i64> [[SHR_I]] to <4 x i32>
// CHECK-NEXT:    [[ADD_I:%.*]] = add <4 x i32> [[CONV3_I]], [[ACC:%.*]]
// CHECK-NEXT:    ret <4 x i32> [[ADD_I]]
//
__kvx_v4si maddsumwq(__kvx_v4si acc, __kvx_v4si a, __kvx_v4su b) {
    return __builtin_kvx_maddsumwq(acc, a, b);
}

// CHECK-LABEL: @msbfsumwq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = sext <4 x i32> [[A:%.*]] to <4 x i64>
// CHECK-NEXT:    [[CONV1_I:%.*]] = zext <4 x i32> [[B:%.*]] to <4 x i64>
// CHECK-NEXT:    [[MUL2_I:%.*]] = mul nsw <4 x i64> [[CONV1_I]], [[CONV_I]]
// CHECK-NEXT:    [[SHR_I:%.*]] = lshr <4 x i64> [[MUL2_I]], <i64 32, i64 32, i64 32, i64 32>
// CHECK-NEXT:    [[CONV3_I:%.*]] = trunc <4 x i64> [[SHR_I]] to <4 x i32>
// CHECK-NEXT:    [[SUB_I:%.*]] = sub <4 x i32> [[ACC:%.*]], [[CONV3_I]]
// CHECK-NEXT:    ret <4 x i32> [[SUB_I]]
//
__kvx_v4si msbfsumwq(__kvx_v4si acc, __kvx_v4si a, __kvx_v4su b) {
    return __builtin_kvx_msbfsumwq(acc, a, b);
}

// CHECK-LABEL: @mulsumwq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = sext <4 x i32> [[A:%.*]] to <4 x i64>
// CHECK-NEXT:    [[CONV1_I:%.*]] = zext <4 x i32> [[B:%.*]] to <4 x i64>
// CHECK-NEXT:    [[MUL2_I:%.*]] = mul nsw <4 x i64> [[CONV1_I]], [[CONV_I]]
// CHECK-NEXT:    [[SHR_I:%.*]] = lshr <4 x i64> [[MUL2_I]], <i64 32, i64 32, i64 32, i64 32>
// CHECK-NEXT:    [[CONV3_I:%.*]] = trunc <4 x i64> [[SHR_I]] to <4 x i32>
// CHECK-NEXT:    ret <4 x i32> [[CONV3_I]]
//
__kvx_v4si mulsumwq(__kvx_v4si a, __kvx_v4su b) {
    return __builtin_kvx_mulsumwq(a, b);
}

