// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -triple kvx-kalray-cos -target-cpu kv3-1 -S -O2 -emit-llvm -o - %s | FileCheck %s --check-prefix=ALL
// RUN: %clang_cc1 -triple kvx-kalray-cos -target-cpu kv3-2 -S -O2 -emit-llvm -o - %s | FileCheck %s --check-prefix=ALL

#include <kvx_builtins.h>

// ALL-LABEL: @addrbod(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = sext <8 x i8> [[V:%.*]] to <8 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v8i64(<8 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
long addrbod(__kvx_v8qi V) {
  return __builtin_kvx_addrbod(V);
}

// ALL-LABEL: @addrbpd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = sext <2 x i8> [[V:%.*]] to <2 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v2i64(<2 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
long addrbpd(__kvx_v2qi V) {
  return __builtin_kvx_addrbpd(V);
}

// ALL-LABEL: @addrbqd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = sext <4 x i8> [[V:%.*]] to <4 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v4i64(<4 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
long addrbqd(__kvx_v4qi V) {
  return __builtin_kvx_addrbqd(V);
}

// ALL-LABEL: @addrbvd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = sext <32 x i8> [[V:%.*]] to <32 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v32i64(<32 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
long addrbvd(__kvx_v32qi V) {
  return __builtin_kvx_addrbvd(V);
}

// ALL-LABEL: @addrbxd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = sext <16 x i8> [[V:%.*]] to <16 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v16i64(<16 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
long addrbxd(__kvx_v16qi V) {
  return __builtin_kvx_addrbxd(V);
}

// ALL-LABEL: @addrhod(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = sext <8 x i16> [[V:%.*]] to <8 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v8i64(<8 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
long addrhod(__kvx_v8hi V) {
  return __builtin_kvx_addrhod(V);
}

// ALL-LABEL: @addrhpd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = sext <2 x i16> [[V:%.*]] to <2 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v2i64(<2 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
long addrhpd(__kvx_v2hi V) {
  return __builtin_kvx_addrhpd(V);
}

// ALL-LABEL: @addrhqd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = sext <4 x i16> [[V:%.*]] to <4 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v4i64(<4 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
long addrhqd(__kvx_v4hi V) {
  return __builtin_kvx_addrhqd(V);
}

// ALL-LABEL: @addrhvd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[V:%.*]] = load <32 x i16>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2:![0-9]+]]
// ALL-NEXT:    [[CONV_I:%.*]] = sext <32 x i16> [[V]] to <32 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v32i64(<32 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
long addrhvd(__kvx_v32hi V) {
  return __builtin_kvx_addrhvd(V);
}

// ALL-LABEL: @addrhxd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = sext <16 x i16> [[V:%.*]] to <16 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v16i64(<16 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
long addrhxd(__kvx_v16hi V) {
  return __builtin_kvx_addrhxd(V);
}

// ALL-LABEL: @addrwod(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = sext <8 x i32> [[V:%.*]] to <8 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v8i64(<8 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
long addrwod(__kvx_v8si V) {
  return __builtin_kvx_addrwod(V);
}

// ALL-LABEL: @addrwpd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = sext <2 x i32> [[V:%.*]] to <2 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v2i64(<2 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
long addrwpd(__kvx_v2si V) {
  return __builtin_kvx_addrwpd(V);
}

// ALL-LABEL: @addrwqd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = sext <4 x i32> [[V:%.*]] to <4 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v4i64(<4 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
long addrwqd(__kvx_v4si V) {
  return __builtin_kvx_addrwqd(V);
}

// ALL-LABEL: @addrwvd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[V:%.*]] = load <32 x i32>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2]]
// ALL-NEXT:    [[CONV_I:%.*]] = sext <32 x i32> [[V]] to <32 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v32i64(<32 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
long addrwvd(__kvx_v32si V) {
  return __builtin_kvx_addrwvd(V);
}

// ALL-LABEL: @addrwxd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[V:%.*]] = load <16 x i32>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2]]
// ALL-NEXT:    [[CONV_I:%.*]] = sext <16 x i32> [[V]] to <16 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v16i64(<16 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
long addrwxd(__kvx_v16si V) {
  return __builtin_kvx_addrwxd(V);
}

// ALL-LABEL: @addurbod(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = zext <8 x i8> [[V:%.*]] to <8 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v8i64(<8 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
unsigned long addurbod(__kvx_v8qu V) {
  return __builtin_kvx_addurbod(V);
}

// ALL-LABEL: @addurbpd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = zext <2 x i8> [[V:%.*]] to <2 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v2i64(<2 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
unsigned long addurbpd(__kvx_v2qu V) {
  return __builtin_kvx_addurbpd(V);
}

// ALL-LABEL: @addurbqd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = zext <4 x i8> [[V:%.*]] to <4 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v4i64(<4 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
unsigned long addurbqd(__kvx_v4qu V) {
  return __builtin_kvx_addurbqd(V);
}

// ALL-LABEL: @addurbvd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = zext <32 x i8> [[V:%.*]] to <32 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v32i64(<32 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
unsigned long addurbvd(__kvx_v32qu V) {
  return __builtin_kvx_addurbvd(V);
}

// ALL-LABEL: @addurbxd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = zext <16 x i8> [[V:%.*]] to <16 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v16i64(<16 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
unsigned long addurbxd(__kvx_v16qu V) {
  return __builtin_kvx_addurbxd(V);
}

// ALL-LABEL: @addurhod(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = zext <8 x i16> [[V:%.*]] to <8 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v8i64(<8 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
unsigned long addurhod(__kvx_v8hu V) {
  return __builtin_kvx_addurhod(V);
}

// ALL-LABEL: @addurhpd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = zext <2 x i16> [[V:%.*]] to <2 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v2i64(<2 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
unsigned long addurhpd(__kvx_v2hu V) {
  return __builtin_kvx_addurhpd(V);
}

// ALL-LABEL: @addurhqd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = zext <4 x i16> [[V:%.*]] to <4 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v4i64(<4 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
unsigned long addurhqd(__kvx_v4hu V) {
  return __builtin_kvx_addurhqd(V);
}

// ALL-LABEL: @addurhvd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[V:%.*]] = load <32 x i16>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2]]
// ALL-NEXT:    [[CONV_I:%.*]] = zext <32 x i16> [[V]] to <32 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v32i64(<32 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
unsigned long addurhvd(__kvx_v32hu V) {
  return __builtin_kvx_addurhvd(V);
}

// ALL-LABEL: @addurhxd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = zext <16 x i16> [[V:%.*]] to <16 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v16i64(<16 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
unsigned long addurhxd(__kvx_v16hu V) {
  return __builtin_kvx_addurhxd(V);
}

// ALL-LABEL: @addurwod(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = zext <8 x i32> [[V:%.*]] to <8 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v8i64(<8 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
unsigned long addurwod(__kvx_v8su V) {

  return __builtin_kvx_addurwod(V);
}

// ALL-LABEL: @addurwpd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = zext <2 x i32> [[V:%.*]] to <2 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v2i64(<2 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
unsigned long addurwpd(__kvx_v2su V) {

  return __builtin_kvx_addurwpd(V);
}

// ALL-LABEL: @addurwqd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = zext <4 x i32> [[V:%.*]] to <4 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v4i64(<4 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
unsigned long addurwqd(__kvx_v4su V) {

  return __builtin_kvx_addurwqd(V);
}

// ALL-LABEL: @addurwvd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[V:%.*]] = load <32 x i32>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2]]
// ALL-NEXT:    [[CONV_I:%.*]] = zext <32 x i32> [[V]] to <32 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v32i64(<32 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
unsigned long addurwvd(__kvx_v32su V) {
  return __builtin_kvx_addurwvd(V);
}

// ALL-LABEL: @addurwxd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[V:%.*]] = load <16 x i32>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2]]
// ALL-NEXT:    [[CONV_I:%.*]] = zext <16 x i32> [[V]] to <16 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v16i64(<16 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
unsigned long addurwxd(__kvx_v16su V) {
  return __builtin_kvx_addurwxd(V);
}

// ALL-LABEL: @maxrbod(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i8 @llvm.vector.reduce.smax.v8i8(<8 x i8> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = sext i8 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
long maxrbod(__kvx_v8qi V) {
  return __builtin_kvx_maxrbod(V);
}

// ALL-LABEL: @maxrbpd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i8 @llvm.vector.reduce.smax.v2i8(<2 x i8> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = sext i8 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
long maxrbpd(__kvx_v2qi V) {
  return __builtin_kvx_maxrbpd(V);
}

// ALL-LABEL: @maxrbqd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i8 @llvm.vector.reduce.smax.v4i8(<4 x i8> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = sext i8 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
long maxrbqd(__kvx_v4qi V) {
  return __builtin_kvx_maxrbqd(V);
}

// ALL-LABEL: @maxrbvd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i8 @llvm.vector.reduce.smax.v32i8(<32 x i8> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = sext i8 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
long maxrbvd(__kvx_v32qi V) {
  return __builtin_kvx_maxrbvd(V);
}

// ALL-LABEL: @maxrbxd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i8 @llvm.vector.reduce.smax.v16i8(<16 x i8> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = sext i8 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
long maxrbxd(__kvx_v16qi V) {
  return __builtin_kvx_maxrbxd(V);
}

// ALL-LABEL: @maxrhod(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i16 @llvm.vector.reduce.smax.v8i16(<8 x i16> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = sext i16 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
long maxrhod(__kvx_v8hi V) {
  return __builtin_kvx_maxrhod(V);
}

// ALL-LABEL: @maxrhpd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i16 @llvm.vector.reduce.smax.v2i16(<2 x i16> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = sext i16 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
long maxrhpd(__kvx_v2hi V) {
  return __builtin_kvx_maxrhpd(V);
}

// ALL-LABEL: @maxrhqd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i16 @llvm.vector.reduce.smax.v4i16(<4 x i16> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = sext i16 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
long maxrhqd(__kvx_v4hi V) {
  return __builtin_kvx_maxrhqd(V);
}

// ALL-LABEL: @maxrhvd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[V:%.*]] = load <32 x i16>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2]]
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i16 @llvm.vector.reduce.smax.v32i16(<32 x i16> [[V]])
// ALL-NEXT:    [[CONV_I:%.*]] = sext i16 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
long maxrhvd(__kvx_v32hi V) {
  return __builtin_kvx_maxrhvd(V);
}

// ALL-LABEL: @maxrhxd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i16 @llvm.vector.reduce.smax.v16i16(<16 x i16> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = sext i16 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
long maxrhxd(__kvx_v16hi V) {
  return __builtin_kvx_maxrhxd(V);
}

// ALL-LABEL: @maxrwod(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i32 @llvm.vector.reduce.smax.v8i32(<8 x i32> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = sext i32 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
long maxrwod(__kvx_v8si V) {
  return __builtin_kvx_maxrwod(V);
}

// ALL-LABEL: @maxrwpd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i32 @llvm.vector.reduce.smax.v2i32(<2 x i32> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = sext i32 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
long maxrwpd(__kvx_v2si V) {
  return __builtin_kvx_maxrwpd(V);
}

// ALL-LABEL: @maxrwqd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i32 @llvm.vector.reduce.smax.v4i32(<4 x i32> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = sext i32 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
long maxrwqd(__kvx_v4si V) {
  return __builtin_kvx_maxrwqd(V);
}

// ALL-LABEL: @maxrwvd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[V:%.*]] = load <32 x i32>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2]]
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i32 @llvm.vector.reduce.smax.v32i32(<32 x i32> [[V]])
// ALL-NEXT:    [[CONV_I:%.*]] = sext i32 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
long maxrwvd(__kvx_v32si V) {
  return __builtin_kvx_maxrwvd(V);
}

// ALL-LABEL: @maxrwxd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[V:%.*]] = load <16 x i32>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2]]
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i32 @llvm.vector.reduce.smax.v16i32(<16 x i32> [[V]])
// ALL-NEXT:    [[CONV_I:%.*]] = sext i32 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
long maxrwxd(__kvx_v16si V) {
  return __builtin_kvx_maxrwxd(V);
}

// ALL-LABEL: @maxurbod(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i8 @llvm.vector.reduce.umax.v8i8(<8 x i8> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i8 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long maxurbod(__kvx_v8qu V) {
  return __builtin_kvx_maxurbod(V);
}

// ALL-LABEL: @maxurbpd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i8 @llvm.vector.reduce.umax.v2i8(<2 x i8> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i8 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long maxurbpd(__kvx_v2qu V) {
  return __builtin_kvx_maxurbpd(V);
}

// ALL-LABEL: @maxurbqd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i8 @llvm.vector.reduce.umax.v4i8(<4 x i8> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i8 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long maxurbqd(__kvx_v4qu V) {
  return __builtin_kvx_maxurbqd(V);
}

// ALL-LABEL: @maxurbvd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i8 @llvm.vector.reduce.umax.v32i8(<32 x i8> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i8 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long maxurbvd(__kvx_v32qu V) {
  return __builtin_kvx_maxurbvd(V);
}

// ALL-LABEL: @maxurbxd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i8 @llvm.vector.reduce.umax.v16i8(<16 x i8> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i8 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long maxurbxd(__kvx_v16qu V) {
  return __builtin_kvx_maxurbxd(V);
}

// ALL-LABEL: @maxurhod(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i16 @llvm.vector.reduce.umax.v8i16(<8 x i16> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i16 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long maxurhod(__kvx_v8hu V) {
  return __builtin_kvx_maxurhod(V);
}

// ALL-LABEL: @maxurhpd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i16 @llvm.vector.reduce.umax.v2i16(<2 x i16> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i16 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long maxurhpd(__kvx_v2hu V) {
  return __builtin_kvx_maxurhpd(V);
}

// ALL-LABEL: @maxurhqd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i16 @llvm.vector.reduce.umax.v4i16(<4 x i16> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i16 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long maxurhqd(__kvx_v4hu V) {
  return __builtin_kvx_maxurhqd(V);
}

// ALL-LABEL: @maxurhvd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[V:%.*]] = load <32 x i16>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2]]
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i16 @llvm.vector.reduce.umax.v32i16(<32 x i16> [[V]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i16 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long maxurhvd(__kvx_v32hu V) {
  return __builtin_kvx_maxurhvd(V);
}

// ALL-LABEL: @maxurhxd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i16 @llvm.vector.reduce.umax.v16i16(<16 x i16> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i16 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long maxurhxd(__kvx_v16hu V) {
  return __builtin_kvx_maxurhxd(V);
}

// ALL-LABEL: @maxurwod(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i32 @llvm.vector.reduce.umax.v8i32(<8 x i32> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i32 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long maxurwod(__kvx_v8su V) {

  return __builtin_kvx_maxurwod(V);
}

// ALL-LABEL: @maxurwpd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i32 @llvm.vector.reduce.umax.v2i32(<2 x i32> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i32 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long maxurwpd(__kvx_v2su V) {

  return __builtin_kvx_maxurwpd(V);
}

// ALL-LABEL: @maxurwqd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i32 @llvm.vector.reduce.umax.v4i32(<4 x i32> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i32 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long maxurwqd(__kvx_v4su V) {
  return __builtin_kvx_maxurwqd(V);
}

// ALL-LABEL: @maxurwvd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[V:%.*]] = load <32 x i32>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2]]
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i32 @llvm.vector.reduce.umax.v32i32(<32 x i32> [[V]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i32 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long maxurwvd(__kvx_v32su V) {
  return __builtin_kvx_maxurwvd(V);
}

// ALL-LABEL: @maxurwxd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[V:%.*]] = load <16 x i32>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2]]
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i32 @llvm.vector.reduce.umax.v16i32(<16 x i32> [[V]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i32 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long maxurwxd(__kvx_v16su V) {
  return __builtin_kvx_maxurwxd(V);
}

// ALL-LABEL: @minrbod(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i8 @llvm.vector.reduce.smin.v8i8(<8 x i8> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = sext i8 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
long minrbod(__kvx_v8qi V) {
  return __builtin_kvx_minrbod(V);
}

// ALL-LABEL: @minrbpd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i8 @llvm.vector.reduce.smin.v2i8(<2 x i8> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = sext i8 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
long minrbpd(__kvx_v2qi V) {
  return __builtin_kvx_minrbpd(V);
}

// ALL-LABEL: @minrbqd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i8 @llvm.vector.reduce.smin.v4i8(<4 x i8> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = sext i8 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
long minrbqd(__kvx_v4qi V) {
  return __builtin_kvx_minrbqd(V);
}

// ALL-LABEL: @minrbvd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i8 @llvm.vector.reduce.smin.v32i8(<32 x i8> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = sext i8 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
long minrbvd(__kvx_v32qi V) {
  return __builtin_kvx_minrbvd(V);
}

// ALL-LABEL: @minrbxd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i8 @llvm.vector.reduce.smin.v16i8(<16 x i8> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = sext i8 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
long minrbxd(__kvx_v16qi V) {
  return __builtin_kvx_minrbxd(V);
}

// ALL-LABEL: @minrhod(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i16 @llvm.vector.reduce.smin.v8i16(<8 x i16> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = sext i16 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
long minrhod(__kvx_v8hi V) {
  return __builtin_kvx_minrhod(V);
}

// ALL-LABEL: @minrhpd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i16 @llvm.vector.reduce.smin.v2i16(<2 x i16> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = sext i16 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
long minrhpd(__kvx_v2hi V) {
  return __builtin_kvx_minrhpd(V);
}

// ALL-LABEL: @minrhqd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i16 @llvm.vector.reduce.smin.v4i16(<4 x i16> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = sext i16 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
long minrhqd(__kvx_v4hi V) {
  return __builtin_kvx_minrhqd(V);
}

// ALL-LABEL: @minrhvd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[V:%.*]] = load <32 x i16>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2]]
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i16 @llvm.vector.reduce.smin.v32i16(<32 x i16> [[V]])
// ALL-NEXT:    [[CONV_I:%.*]] = sext i16 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
long minrhvd(__kvx_v32hi V) {
  return __builtin_kvx_minrhvd(V);
}

// ALL-LABEL: @minrhxd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i16 @llvm.vector.reduce.smin.v16i16(<16 x i16> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = sext i16 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
long minrhxd(__kvx_v16hi V) {
  return __builtin_kvx_minrhxd(V);
}

// ALL-LABEL: @minrwod(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i32 @llvm.vector.reduce.smin.v8i32(<8 x i32> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = sext i32 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
long minrwod(__kvx_v8si V) {
  return __builtin_kvx_minrwod(V);
}

// ALL-LABEL: @minrwpd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i32 @llvm.vector.reduce.smin.v2i32(<2 x i32> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = sext i32 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
long minrwpd(__kvx_v2si V) {
  return __builtin_kvx_minrwpd(V);
}

// ALL-LABEL: @minrwqd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i32 @llvm.vector.reduce.smin.v4i32(<4 x i32> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = sext i32 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
long minrwqd(__kvx_v4si V) {
  return __builtin_kvx_minrwqd(V);
}

// ALL-LABEL: @minrwvd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[V:%.*]] = load <32 x i32>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2]]
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i32 @llvm.vector.reduce.smin.v32i32(<32 x i32> [[V]])
// ALL-NEXT:    [[CONV_I:%.*]] = sext i32 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
long minrwvd(__kvx_v32si V) {
  return __builtin_kvx_minrwvd(V);
}

// ALL-LABEL: @minrwxd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[V:%.*]] = load <16 x i32>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2]]
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i32 @llvm.vector.reduce.smin.v16i32(<16 x i32> [[V]])
// ALL-NEXT:    [[CONV_I:%.*]] = sext i32 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
long minrwxd(__kvx_v16si V) {
  return __builtin_kvx_minrwxd(V);
}

// ALL-LABEL: @minurbod(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i8 @llvm.vector.reduce.umin.v8i8(<8 x i8> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i8 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long minurbod(__kvx_v8qu V) {
  return __builtin_kvx_minurbod(V);
}

// ALL-LABEL: @minurbpd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i8 @llvm.vector.reduce.umin.v2i8(<2 x i8> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i8 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long minurbpd(__kvx_v2qu V) {
  return __builtin_kvx_minurbpd(V);
}

// ALL-LABEL: @minurbqd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i8 @llvm.vector.reduce.umin.v4i8(<4 x i8> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i8 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long minurbqd(__kvx_v4qu V) {
  return __builtin_kvx_minurbqd(V);
}

// ALL-LABEL: @minurbvd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i8 @llvm.vector.reduce.umin.v32i8(<32 x i8> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i8 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long minurbvd(__kvx_v32qu V) {
  return __builtin_kvx_minurbvd(V);
}

// ALL-LABEL: @minurbxd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i8 @llvm.vector.reduce.umin.v16i8(<16 x i8> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i8 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long minurbxd(__kvx_v16qu V) {
  return __builtin_kvx_minurbxd(V);
}

// ALL-LABEL: @minurhod(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i16 @llvm.vector.reduce.umin.v8i16(<8 x i16> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i16 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long minurhod(__kvx_v8hu V) {
  return __builtin_kvx_minurhod(V);
}

// ALL-LABEL: @minurhpd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i16 @llvm.vector.reduce.umin.v2i16(<2 x i16> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i16 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long minurhpd(__kvx_v2hu V) {
  return __builtin_kvx_minurhpd(V);
}

// ALL-LABEL: @minurhqd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i16 @llvm.vector.reduce.umin.v4i16(<4 x i16> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i16 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long minurhqd(__kvx_v4hu V) {
  return __builtin_kvx_minurhqd(V);
}

// ALL-LABEL: @minurhvd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[V:%.*]] = load <32 x i16>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2]]
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i16 @llvm.vector.reduce.umin.v32i16(<32 x i16> [[V]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i16 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long minurhvd(__kvx_v32hu V) {
  return __builtin_kvx_minurhvd(V);
}

// ALL-LABEL: @minurhxd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i16 @llvm.vector.reduce.umin.v16i16(<16 x i16> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i16 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long minurhxd(__kvx_v16hu V) {
  return __builtin_kvx_minurhxd(V);
}

// ALL-LABEL: @minurwod(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i32 @llvm.vector.reduce.umin.v8i32(<8 x i32> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i32 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long minurwod(__kvx_v8su V) {

  return __builtin_kvx_minurwod(V);
}

// ALL-LABEL: @minurwpd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i32 @llvm.vector.reduce.umin.v2i32(<2 x i32> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i32 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long minurwpd(__kvx_v2su V) {

  return __builtin_kvx_minurwpd(V);
}

// ALL-LABEL: @minurwqd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i32 @llvm.vector.reduce.umin.v4i32(<4 x i32> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i32 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long minurwqd(__kvx_v4su V) {
  return __builtin_kvx_minurwqd(V);
}

// ALL-LABEL: @minurwvd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[V:%.*]] = load <32 x i32>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2]]
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i32 @llvm.vector.reduce.umin.v32i32(<32 x i32> [[V]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i32 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long minurwvd(__kvx_v32su V) {
  return __builtin_kvx_minurwvd(V);
}

// ALL-LABEL: @minurwxd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[V:%.*]] = load <16 x i32>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2]]
// ALL-NEXT:    [[RDX_MIN_I:%.*]] = tail call i32 @llvm.vector.reduce.umin.v16i32(<16 x i32> [[V]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i32 [[RDX_MIN_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long minurwxd(__kvx_v16su V) {
  return __builtin_kvx_minurwxd(V);
}

// ALL-LABEL: @andrbod(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_AND_I:%.*]] = tail call i8 @llvm.vector.reduce.and.v8i8(<8 x i8> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i8 [[RDX_AND_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long andrbod(__kvx_v8qi V) {
  return __builtin_kvx_andrbod(V);
}

// ALL-LABEL: @andrbpd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_AND_I:%.*]] = tail call i8 @llvm.vector.reduce.and.v2i8(<2 x i8> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i8 [[RDX_AND_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long andrbpd(__kvx_v2qi V) {
  return __builtin_kvx_andrbpd(V);
}

// ALL-LABEL: @andrbqd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_AND_I:%.*]] = tail call i8 @llvm.vector.reduce.and.v4i8(<4 x i8> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i8 [[RDX_AND_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long andrbqd(__kvx_v4qi V) {
  return __builtin_kvx_andrbqd(V);
}

// ALL-LABEL: @andrbvd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_AND_I:%.*]] = tail call i8 @llvm.vector.reduce.and.v32i8(<32 x i8> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i8 [[RDX_AND_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long andrbvd(__kvx_v32qi V) {
  return __builtin_kvx_andrbvd(V);
}

// ALL-LABEL: @andrbxd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_AND_I:%.*]] = tail call i8 @llvm.vector.reduce.and.v16i8(<16 x i8> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i8 [[RDX_AND_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long andrbxd(__kvx_v16qi V) {
  return __builtin_kvx_andrbxd(V);
}

// ALL-LABEL: @andrhod(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_AND_I:%.*]] = tail call i16 @llvm.vector.reduce.and.v8i16(<8 x i16> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i16 [[RDX_AND_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long andrhod(__kvx_v8hi V) {
  return __builtin_kvx_andrhod(V);
}

// ALL-LABEL: @andrhpd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_AND_I:%.*]] = tail call i16 @llvm.vector.reduce.and.v2i16(<2 x i16> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i16 [[RDX_AND_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long andrhpd(__kvx_v2hi V) {
  return __builtin_kvx_andrhpd(V);
}

// ALL-LABEL: @andrhqd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_AND_I:%.*]] = tail call i16 @llvm.vector.reduce.and.v4i16(<4 x i16> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i16 [[RDX_AND_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long andrhqd(__kvx_v4hi V) {
  return __builtin_kvx_andrhqd(V);
}

// ALL-LABEL: @andrhvd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[V:%.*]] = load <32 x i16>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2]]
// ALL-NEXT:    [[RDX_AND_I:%.*]] = tail call i16 @llvm.vector.reduce.and.v32i16(<32 x i16> [[V]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i16 [[RDX_AND_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long andrhvd(__kvx_v32hi V) {
  return __builtin_kvx_andrhvd(V);
}

// ALL-LABEL: @andrhxd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_AND_I:%.*]] = tail call i16 @llvm.vector.reduce.and.v16i16(<16 x i16> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i16 [[RDX_AND_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long andrhxd(__kvx_v16hi V) {
  return __builtin_kvx_andrhxd(V);
}

// ALL-LABEL: @andrwod(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_AND_I:%.*]] = tail call i32 @llvm.vector.reduce.and.v8i32(<8 x i32> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i32 [[RDX_AND_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long andrwod(__kvx_v8si V) {
  return __builtin_kvx_andrwod(V);
}

// ALL-LABEL: @andrwpd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_AND_I:%.*]] = tail call i32 @llvm.vector.reduce.and.v2i32(<2 x i32> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i32 [[RDX_AND_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long andrwpd(__kvx_v2si V) {
  return __builtin_kvx_andrwpd(V);
}

// ALL-LABEL: @andrwqd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_AND_I:%.*]] = tail call i32 @llvm.vector.reduce.and.v4i32(<4 x i32> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i32 [[RDX_AND_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long andrwqd(__kvx_v4si V) {
  return __builtin_kvx_andrwqd(V);
}

// ALL-LABEL: @andrwvd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[V:%.*]] = load <32 x i32>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2]]
// ALL-NEXT:    [[RDX_AND_I:%.*]] = tail call i32 @llvm.vector.reduce.and.v32i32(<32 x i32> [[V]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i32 [[RDX_AND_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long andrwvd(__kvx_v32si V) {
  return __builtin_kvx_andrwvd(V);
}

// ALL-LABEL: @andrwxd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[V:%.*]] = load <16 x i32>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2]]
// ALL-NEXT:    [[RDX_AND_I:%.*]] = tail call i32 @llvm.vector.reduce.and.v16i32(<16 x i32> [[V]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i32 [[RDX_AND_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long andrwxd(__kvx_v16si V) {
  return __builtin_kvx_andrwxd(V);
}

// ALL-LABEL: @orrbod(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_OR_I:%.*]] = tail call i8 @llvm.vector.reduce.or.v8i8(<8 x i8> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i8 [[RDX_OR_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long orrbod(__kvx_v8qi V) {
  return __builtin_kvx_orrbod(V);
}

// ALL-LABEL: @orrbpd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_OR_I:%.*]] = tail call i8 @llvm.vector.reduce.or.v2i8(<2 x i8> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i8 [[RDX_OR_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long orrbpd(__kvx_v2qi V) {
  return __builtin_kvx_orrbpd(V);
}

// ALL-LABEL: @orrbqd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_OR_I:%.*]] = tail call i8 @llvm.vector.reduce.or.v4i8(<4 x i8> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i8 [[RDX_OR_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long orrbqd(__kvx_v4qi V) {
  return __builtin_kvx_orrbqd(V);
}

// ALL-LABEL: @orrbvd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_OR_I:%.*]] = tail call i8 @llvm.vector.reduce.or.v32i8(<32 x i8> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i8 [[RDX_OR_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long orrbvd(__kvx_v32qi V) {
  return __builtin_kvx_orrbvd(V);
}

// ALL-LABEL: @orrbxd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_OR_I:%.*]] = tail call i8 @llvm.vector.reduce.or.v16i8(<16 x i8> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i8 [[RDX_OR_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long orrbxd(__kvx_v16qi V) {
  return __builtin_kvx_orrbxd(V);
}

// ALL-LABEL: @orrhod(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_OR_I:%.*]] = tail call i16 @llvm.vector.reduce.or.v8i16(<8 x i16> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i16 [[RDX_OR_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long orrhod(__kvx_v8hi V) {
  return __builtin_kvx_orrhod(V);
}

// ALL-LABEL: @orrhpd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_OR_I:%.*]] = tail call i16 @llvm.vector.reduce.or.v2i16(<2 x i16> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i16 [[RDX_OR_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long orrhpd(__kvx_v2hi V) {
  return __builtin_kvx_orrhpd(V);
}

// ALL-LABEL: @orrhqd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_OR_I:%.*]] = tail call i16 @llvm.vector.reduce.or.v4i16(<4 x i16> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i16 [[RDX_OR_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long orrhqd(__kvx_v4hi V) {
  return __builtin_kvx_orrhqd(V);
}

// ALL-LABEL: @orrhvd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[V:%.*]] = load <32 x i16>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2]]
// ALL-NEXT:    [[RDX_OR_I:%.*]] = tail call i16 @llvm.vector.reduce.or.v32i16(<32 x i16> [[V]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i16 [[RDX_OR_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long orrhvd(__kvx_v32hi V) {
  return __builtin_kvx_orrhvd(V);
}

// ALL-LABEL: @orrhxd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_OR_I:%.*]] = tail call i16 @llvm.vector.reduce.or.v16i16(<16 x i16> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i16 [[RDX_OR_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long orrhxd(__kvx_v16hi V) {
  return __builtin_kvx_orrhxd(V);
}

// ALL-LABEL: @orrwod(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_OR_I:%.*]] = tail call i32 @llvm.vector.reduce.or.v8i32(<8 x i32> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i32 [[RDX_OR_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long orrwod(__kvx_v8si V) {
  return __builtin_kvx_orrwod(V);
}

// ALL-LABEL: @orrwpd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_OR_I:%.*]] = tail call i32 @llvm.vector.reduce.or.v2i32(<2 x i32> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i32 [[RDX_OR_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long orrwpd(__kvx_v2si V) {
  return __builtin_kvx_orrwpd(V);
}

// ALL-LABEL: @orrwqd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_OR_I:%.*]] = tail call i32 @llvm.vector.reduce.or.v4i32(<4 x i32> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i32 [[RDX_OR_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long orrwqd(__kvx_v4si V) {
  return __builtin_kvx_orrwqd(V);
}

// ALL-LABEL: @orrwvd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[V:%.*]] = load <32 x i32>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2]]
// ALL-NEXT:    [[RDX_OR_I:%.*]] = tail call i32 @llvm.vector.reduce.or.v32i32(<32 x i32> [[V]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i32 [[RDX_OR_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long orrwvd(__kvx_v32si V) {
  return __builtin_kvx_orrwvd(V);
}

// ALL-LABEL: @orrwxd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[V:%.*]] = load <16 x i32>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2]]
// ALL-NEXT:    [[RDX_OR_I:%.*]] = tail call i32 @llvm.vector.reduce.or.v16i32(<16 x i32> [[V]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i32 [[RDX_OR_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long orrwxd(__kvx_v16si V) {
  return __builtin_kvx_orrwxd(V);
}

// ALL-LABEL: @xorrbod(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_XOR_I:%.*]] = tail call i8 @llvm.vector.reduce.xor.v8i8(<8 x i8> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i8 [[RDX_XOR_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long xorrbod(__kvx_v8qi V) {
  return __builtin_kvx_xorrbod(V);
}

// ALL-LABEL: @xorrbpd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_XOR_I:%.*]] = tail call i8 @llvm.vector.reduce.xor.v2i8(<2 x i8> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i8 [[RDX_XOR_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long xorrbpd(__kvx_v2qi V) {
  return __builtin_kvx_xorrbpd(V);
}

// ALL-LABEL: @xorrbqd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_XOR_I:%.*]] = tail call i8 @llvm.vector.reduce.xor.v4i8(<4 x i8> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i8 [[RDX_XOR_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long xorrbqd(__kvx_v4qi V) {
  return __builtin_kvx_xorrbqd(V);
}

// ALL-LABEL: @xorrbvd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_XOR_I:%.*]] = tail call i8 @llvm.vector.reduce.xor.v32i8(<32 x i8> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i8 [[RDX_XOR_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long xorrbvd(__kvx_v32qi V) {
  return __builtin_kvx_xorrbvd(V);
}

// ALL-LABEL: @xorrbxd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_XOR_I:%.*]] = tail call i8 @llvm.vector.reduce.xor.v16i8(<16 x i8> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i8 [[RDX_XOR_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long xorrbxd(__kvx_v16qi V) {
  return __builtin_kvx_xorrbxd(V);
}

// ALL-LABEL: @xorrhod(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_XOR_I:%.*]] = tail call i16 @llvm.vector.reduce.xor.v8i16(<8 x i16> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i16 [[RDX_XOR_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long xorrhod(__kvx_v8hi V) {
  return __builtin_kvx_xorrhod(V);
}

// ALL-LABEL: @xorrhpd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_XOR_I:%.*]] = tail call i16 @llvm.vector.reduce.xor.v2i16(<2 x i16> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i16 [[RDX_XOR_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long xorrhpd(__kvx_v2hi V) {
  return __builtin_kvx_xorrhpd(V);
}

// ALL-LABEL: @xorrhqd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_XOR_I:%.*]] = tail call i16 @llvm.vector.reduce.xor.v4i16(<4 x i16> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i16 [[RDX_XOR_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long xorrhqd(__kvx_v4hi V) {
  return __builtin_kvx_xorrhqd(V);
}

// ALL-LABEL: @xorrhvd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[V:%.*]] = load <32 x i16>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2]]
// ALL-NEXT:    [[RDX_XOR_I:%.*]] = tail call i16 @llvm.vector.reduce.xor.v32i16(<32 x i16> [[V]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i16 [[RDX_XOR_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long xorrhvd(__kvx_v32hi V) {
  return __builtin_kvx_xorrhvd(V);
}

// ALL-LABEL: @xorrhxd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_XOR_I:%.*]] = tail call i16 @llvm.vector.reduce.xor.v16i16(<16 x i16> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i16 [[RDX_XOR_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long xorrhxd(__kvx_v16hi V) {
  return __builtin_kvx_xorrhxd(V);
}

// ALL-LABEL: @xorrwod(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_XOR_I:%.*]] = tail call i32 @llvm.vector.reduce.xor.v8i32(<8 x i32> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i32 [[RDX_XOR_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long xorrwod(__kvx_v8si V) {
  return __builtin_kvx_xorrwod(V);
}

// ALL-LABEL: @xorrwpd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_XOR_I:%.*]] = tail call i32 @llvm.vector.reduce.xor.v2i32(<2 x i32> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i32 [[RDX_XOR_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long xorrwpd(__kvx_v2si V) {
  return __builtin_kvx_xorrwpd(V);
}

// ALL-LABEL: @xorrwqd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[RDX_XOR_I:%.*]] = tail call i32 @llvm.vector.reduce.xor.v4i32(<4 x i32> [[V:%.*]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i32 [[RDX_XOR_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long xorrwqd(__kvx_v4si V) {
  return __builtin_kvx_xorrwqd(V);
}

// ALL-LABEL: @xorrwvd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[V:%.*]] = load <32 x i32>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2]]
// ALL-NEXT:    [[RDX_XOR_I:%.*]] = tail call i32 @llvm.vector.reduce.xor.v32i32(<32 x i32> [[V]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i32 [[RDX_XOR_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long xorrwvd(__kvx_v32si V) {
  return __builtin_kvx_xorrwvd(V);
}

// ALL-LABEL: @xorrwxd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[V:%.*]] = load <16 x i32>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2]]
// ALL-NEXT:    [[RDX_XOR_I:%.*]] = tail call i32 @llvm.vector.reduce.xor.v16i32(<16 x i32> [[V]])
// ALL-NEXT:    [[CONV_I:%.*]] = zext i32 [[RDX_XOR_I]] to i64
// ALL-NEXT:    ret i64 [[CONV_I]]
//
unsigned long xorrwxd(__kvx_v16si V) {
  return __builtin_kvx_xorrwxd(V);
}
