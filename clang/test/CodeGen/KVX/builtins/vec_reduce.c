// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -triple kvx-kalray-cos -target-cpu kv3-1 -S -O2 -emit-llvm -o - %s | FileCheck %s --check-prefix=ALL
// RUN: %clang_cc1 -triple kvx-kalray-cos -target-cpu kv3-2 -S -O2 -emit-llvm -o - %s | FileCheck %s --check-prefix=ALL

#include <kvx_builtins.h>

// ALL-LABEL: @addrbod(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = sext <8 x i8> [[V:%.*]] to <8 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v8i64(<8 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
long addrbod(char __attribute__((vector_size(8 * sizeof(char)))) V) {
  return __builtin_kvx_addrbod(V);
}
// ALL-LABEL: @addrbpd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = sext <2 x i8> [[V:%.*]] to <2 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v2i64(<2 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
long addrbpd(char __attribute__((vector_size(2 * sizeof(char)))) V) {
  return __builtin_kvx_addrbpd(V);
}
// ALL-LABEL: @addrbqd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = sext <4 x i8> [[V:%.*]] to <4 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v4i64(<4 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
long addrbqd(char __attribute__((vector_size(4 * sizeof(char)))) V) {
  return __builtin_kvx_addrbqd(V);
}
// ALL-LABEL: @addrbvd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = sext <32 x i8> [[V:%.*]] to <32 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v32i64(<32 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
long addrbvd(char __attribute__((vector_size(32 * sizeof(char)))) V) {
  return __builtin_kvx_addrbvd(V);
}
// ALL-LABEL: @addrbxd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = sext <16 x i8> [[V:%.*]] to <16 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v16i64(<16 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
long addrbxd(char __attribute__((vector_size(16 * sizeof(char)))) V) {
  return __builtin_kvx_addrbxd(V);
}

// ALL-LABEL: @addrhod(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = sext <8 x i16> [[V:%.*]] to <8 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v8i64(<8 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
long addrhod(short __attribute__((vector_size(8 * sizeof(short)))) V) {
  return __builtin_kvx_addrhod(V);
}
// ALL-LABEL: @addrhpd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = sext <2 x i16> [[V:%.*]] to <2 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v2i64(<2 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
long addrhpd(short __attribute__((vector_size(2 * sizeof(short)))) V) {
  return __builtin_kvx_addrhpd(V);
}
// ALL-LABEL: @addrhqd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = sext <4 x i16> [[V:%.*]] to <4 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v4i64(<4 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
long addrhqd(short __attribute__((vector_size(4 * sizeof(short)))) V) {
  return __builtin_kvx_addrhqd(V);
}
// ALL-LABEL: @addrhvd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[V:%.*]] = load <32 x i16>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2:![0-9]+]]
// ALL-NEXT:    [[CONV_I:%.*]] = sext <32 x i16> [[V]] to <32 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v32i64(<32 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
long addrhvd(short __attribute__((vector_size(32 * sizeof(short)))) V) {
  return __builtin_kvx_addrhvd(V);
}
// ALL-LABEL: @addrhxd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = sext <16 x i16> [[V:%.*]] to <16 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v16i64(<16 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
long addrhxd(short __attribute__((vector_size(16 * sizeof(short)))) V) {
  return __builtin_kvx_addrhxd(V);
}

// ALL-LABEL: @addrwod(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = sext <8 x i32> [[V:%.*]] to <8 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v8i64(<8 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
long addrwod(int __attribute__((vector_size(8 * sizeof(int)))) V) {
  return __builtin_kvx_addrwod(V);
}
// ALL-LABEL: @addrwpd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = sext <2 x i32> [[V:%.*]] to <2 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v2i64(<2 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
long addrwpd(int __attribute__((vector_size(2 * sizeof(int)))) V) {
  return __builtin_kvx_addrwpd(V);
}
// ALL-LABEL: @addrwqd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = sext <4 x i32> [[V:%.*]] to <4 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v4i64(<4 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
long addrwqd(int __attribute__((vector_size(4 * sizeof(int)))) V) {
  return __builtin_kvx_addrwqd(V);
}
// ALL-LABEL: @addrwvd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[V:%.*]] = load <32 x i32>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2]]
// ALL-NEXT:    [[CONV_I:%.*]] = sext <32 x i32> [[V]] to <32 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v32i64(<32 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
long addrwvd(int __attribute__((vector_size(32 * sizeof(int)))) V) {
  return __builtin_kvx_addrwvd(V);
}
// ALL-LABEL: @addrwxd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[V:%.*]] = load <16 x i32>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2]]
// ALL-NEXT:    [[CONV_I:%.*]] = sext <16 x i32> [[V]] to <16 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v16i64(<16 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
long addrwxd(int __attribute__((vector_size(16 * sizeof(int)))) V) {
  return __builtin_kvx_addrwxd(V);
}

unsigned long
// ALL-LABEL: @addurbod(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = zext <8 x i8> [[V:%.*]] to <8 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v8i64(<8 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
addurbod(unsigned char
             __attribute__((vector_size(8 * sizeof(unsigned char)))) V) {
  return __builtin_kvx_addurbod(V);
}
unsigned long
// ALL-LABEL: @addurbpd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = zext <2 x i8> [[V:%.*]] to <2 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v2i64(<2 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
addurbpd(unsigned char
             __attribute__((vector_size(2 * sizeof(unsigned char)))) V) {
  return __builtin_kvx_addurbpd(V);
}
unsigned long
// ALL-LABEL: @addurbqd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = zext <4 x i8> [[V:%.*]] to <4 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v4i64(<4 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
addurbqd(unsigned char
             __attribute__((vector_size(4 * sizeof(unsigned char)))) V) {
  return __builtin_kvx_addurbqd(V);
}
unsigned long
// ALL-LABEL: @addurbvd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = zext <32 x i8> [[V:%.*]] to <32 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v32i64(<32 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
addurbvd(unsigned char
             __attribute__((vector_size(32 * sizeof(unsigned char)))) V) {
  return __builtin_kvx_addurbvd(V);
}
unsigned long
// ALL-LABEL: @addurbxd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = zext <16 x i8> [[V:%.*]] to <16 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v16i64(<16 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
addurbxd(unsigned char
             __attribute__((vector_size(16 * sizeof(unsigned char)))) V) {
  return __builtin_kvx_addurbxd(V);
}

unsigned long
// ALL-LABEL: @addurhod(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = zext <8 x i16> [[V:%.*]] to <8 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v8i64(<8 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
addurhod(unsigned short
             __attribute__((vector_size(8 * sizeof(unsigned short)))) V) {
  return __builtin_kvx_addurhod(V);
}
unsigned long
// ALL-LABEL: @addurhpd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = zext <2 x i16> [[V:%.*]] to <2 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v2i64(<2 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
addurhpd(unsigned short
             __attribute__((vector_size(2 * sizeof(unsigned short)))) V) {
  return __builtin_kvx_addurhpd(V);
}
unsigned long
// ALL-LABEL: @addurhqd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = zext <4 x i16> [[V:%.*]] to <4 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v4i64(<4 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
addurhqd(unsigned short
             __attribute__((vector_size(4 * sizeof(unsigned short)))) V) {
  return __builtin_kvx_addurhqd(V);
}
unsigned long
// ALL-LABEL: @addurhvd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[V:%.*]] = load <32 x i16>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2]]
// ALL-NEXT:    [[CONV_I:%.*]] = zext <32 x i16> [[V]] to <32 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v32i64(<32 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
addurhvd(unsigned short
             __attribute__((vector_size(32 * sizeof(unsigned short)))) V) {
  return __builtin_kvx_addurhvd(V);
}
unsigned long
// ALL-LABEL: @addurhxd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = zext <16 x i16> [[V:%.*]] to <16 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v16i64(<16 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
addurhxd(unsigned short
             __attribute__((vector_size(16 * sizeof(unsigned short)))) V) {
  return __builtin_kvx_addurhxd(V);
}

unsigned long
// ALL-LABEL: @addurwod(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = zext <8 x i32> [[V:%.*]] to <8 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v8i64(<8 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
addurwod(unsigned int __attribute__((vector_size(8 * sizeof(unsigned int))))
             V) {
  return __builtin_kvx_addurwod(V);
}
unsigned long
// ALL-LABEL: @addurwpd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = zext <2 x i32> [[V:%.*]] to <2 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v2i64(<2 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
addurwpd(unsigned int __attribute__((vector_size(2 * sizeof(unsigned int))))
             V) {
  return __builtin_kvx_addurwpd(V);
}
unsigned long
// ALL-LABEL: @addurwqd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[CONV_I:%.*]] = zext <4 x i32> [[V:%.*]] to <4 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v4i64(<4 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
addurwqd(unsigned int __attribute__((vector_size(4 * sizeof(unsigned int))))
             V) {
  return __builtin_kvx_addurwqd(V);
}
unsigned long
// ALL-LABEL: @addurwvd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[V:%.*]] = load <32 x i32>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2]]
// ALL-NEXT:    [[CONV_I:%.*]] = zext <32 x i32> [[V]] to <32 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v32i64(<32 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
addurwvd(unsigned int
             __attribute__((vector_size(32 * sizeof(unsigned int)))) V) {
  return __builtin_kvx_addurwvd(V);
}
unsigned long
// ALL-LABEL: @addurwxd(
// ALL-NEXT:  entry:
// ALL-NEXT:    [[V:%.*]] = load <16 x i32>, ptr [[TMP0:%.*]], align 32, !tbaa [[TBAA2]]
// ALL-NEXT:    [[CONV_I:%.*]] = zext <16 x i32> [[V]] to <16 x i64>
// ALL-NEXT:    [[RDX_ADD_I:%.*]] = tail call i64 @llvm.vector.reduce.add.v16i64(<16 x i64> [[CONV_I]])
// ALL-NEXT:    ret i64 [[RDX_ADD_I]]
//
addurwxd(unsigned int
             __attribute__((vector_size(16 * sizeof(unsigned int)))) V) {
  return __builtin_kvx_addurwxd(V);
}
