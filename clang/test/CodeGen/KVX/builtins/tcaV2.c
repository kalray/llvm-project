// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -O2 -triple kvx-kalray-cos -S -emit-llvm -target-cpu kv3-2 -o - -x c %s | FileCheck %s

// CHECK-LABEL: @xfscalewo_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[V:%.*]], align 32, !tbaa [[TBAA2:![0-9]+]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xfscalewo(<256 x i1> [[TMP0]], i32 [[S:%.*]], i32 7, i32 0)
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <256 x i1> @llvm.kvx.xfscalewo(<256 x i1> [[TMP1]], i32 [[S]], i32 7, i32 1)
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <256 x i1> @llvm.kvx.xfscalewo(<256 x i1> [[TMP2]], i32 [[S]], i32 0, i32 0)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <256 x i1> @llvm.kvx.xfscalewo(<256 x i1> [[TMP3]], i32 [[S]], i32 0, i32 1)
// CHECK-NEXT:    store <256 x i1> [[TMP4]], ptr [[V]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xfscalewo_test(__kvx_x256 *v, int s) {
  __kvx_x256 r = __builtin_kvx_xfscalewo(v[0], s, "");
  r = __builtin_kvx_xfscalewo(r, s, ".s");
  r = __builtin_kvx_xfscalewo(r, s, ".rn");
  v[0] = __builtin_kvx_xfscalewo(r, s, ".rn.s");
}

// CHECK-LABEL: @xclampwo_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[V:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xclampwo(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <256 x i1> [[TMP0]])
// CHECK-NEXT:    store <256 x i1> [[TMP1]], ptr [[V]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xclampwo_test(__kvx_x256 *v) {
  v[0] = __builtin_kvx_xclampwo(v[0], v[0], v[0]);
}

// CHECK-LABEL: @xffma44hw_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[V:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = load <512 x i1>, ptr [[ACC:%.*]], align 32, !tbaa [[TBAA6:![0-9]+]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <512 x i1> @llvm.kvx.xffma44hw(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <512 x i1> [[TMP1]], i32 7, i32 0)
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <512 x i1> @llvm.kvx.xffma44hw(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <512 x i1> [[TMP2]], i32 7, i32 1)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <512 x i1> @llvm.kvx.xffma44hw(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <512 x i1> [[TMP3]], i32 1, i32 0)
// CHECK-NEXT:    [[TMP5:%.*]] = tail call <512 x i1> @llvm.kvx.xffma44hw(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <512 x i1> [[TMP4]], i32 3, i32 1)
// CHECK-NEXT:    store <512 x i1> [[TMP5]], ptr [[ACC]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    ret void
//
void xffma44hw_test(__kvx_x512 *acc, __kvx_x256 *v) {
  __kvx_x256 l = v[0];
  __kvx_x512 r = __builtin_kvx_xffma44hw(l, l, acc[0], "");
  r = __builtin_kvx_xffma44hw(l, l, r, ".s");
  r = __builtin_kvx_xffma44hw(l, l, r, ".rU");
  acc[0] = __builtin_kvx_xffma44hw(l, l, r, ".Rz.S");
}

// CHECK-LABEL: @xfmma484hw_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <512 x i1>, ptr [[ACC:%.*]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <512 x i1> @llvm.kvx.xfmma484hw(<512 x i1> [[TMP0]], <512 x i1> [[TMP0]], <512 x i1> [[TMP0]], i32 7, i32 0)
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <512 x i1> @llvm.kvx.xfmma484hw(<512 x i1> [[TMP1]], <512 x i1> [[TMP1]], <512 x i1> [[TMP1]], i32 7, i32 1)
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <512 x i1> @llvm.kvx.xfmma484hw(<512 x i1> [[TMP2]], <512 x i1> [[TMP2]], <512 x i1> [[TMP2]], i32 0, i32 0)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <512 x i1> @llvm.kvx.xfmma484hw(<512 x i1> [[TMP3]], <512 x i1> [[TMP3]], <512 x i1> [[TMP3]], i32 3, i32 1)
// CHECK-NEXT:    store <512 x i1> [[TMP4]], ptr [[ACC]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    ret void
//
void xfmma484hw_test(__kvx_x512 *acc) {
  __kvx_x512 r = __builtin_kvx_xfmma484hw(acc[0], acc[0], acc[0], "");
  r = __builtin_kvx_xfmma484hw(r, r, r, ".s");
  r = __builtin_kvx_xfmma484hw(r, r, r, ".rn");
  acc[0] = __builtin_kvx_xfmma484hw(r, r, r, ".rz.s");
}

// CHECK-LABEL: @xfnarrow44wh_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <512 x i1>, ptr [[W:%.*]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xfnarrow44wh(<512 x i1> [[TMP0]], i32 7, i32 0)
// CHECK-NEXT:    store <256 x i1> [[TMP1]], ptr [[V:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP2:%.*]] = load <512 x i1>, ptr [[W]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <256 x i1> @llvm.kvx.xfnarrow44wh(<512 x i1> [[TMP2]], i32 7, i32 1)
// CHECK-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <256 x i1>, ptr [[V]], i64 1
// CHECK-NEXT:    store <256 x i1> [[TMP3]], ptr [[ARRAYIDX3]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP4:%.*]] = load <512 x i1>, ptr [[W]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    [[TMP5:%.*]] = tail call <256 x i1> @llvm.kvx.xfnarrow44wh(<512 x i1> [[TMP4]], i32 3, i32 0)
// CHECK-NEXT:    [[ARRAYIDX5:%.*]] = getelementptr inbounds <256 x i1>, ptr [[V]], i64 2
// CHECK-NEXT:    store <256 x i1> [[TMP5]], ptr [[ARRAYIDX5]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP6:%.*]] = load <512 x i1>, ptr [[W]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    [[TMP7:%.*]] = tail call <256 x i1> @llvm.kvx.xfnarrow44wh(<512 x i1> [[TMP6]], i32 1, i32 1)
// CHECK-NEXT:    [[ARRAYIDX7:%.*]] = getelementptr inbounds <256 x i1>, ptr [[V]], i64 3
// CHECK-NEXT:    store <256 x i1> [[TMP7]], ptr [[ARRAYIDX7]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xfnarrow44wh_test(__kvx_x256 *v, __kvx_x512 *w) {
  v[0] = __builtin_kvx_xfnarrow44wh(w[0], "");
  v[1] = __builtin_kvx_xfnarrow44wh(w[0], ".s");
  v[2] = __builtin_kvx_xfnarrow44wh(w[0], ".rz");
  v[3] = __builtin_kvx_xfnarrow44wh(w[0], ".ru.s");
}

// CHECK-LABEL: @xmadd44bw_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[V:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = load <512 x i1>, ptr [[W:%.*]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <512 x i1> @llvm.kvx.xmadd44bw0(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <512 x i1> [[TMP1]], i32 0)
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <512 x i1> @llvm.kvx.xmadd44bw1(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <512 x i1> [[TMP2]], i32 0)
// CHECK-NEXT:    store <512 x i1> [[TMP3]], ptr [[W]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <512 x i1> @llvm.kvx.xmadd44bw0(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <512 x i1> [[TMP2]], i32 0)
// CHECK-NEXT:    store <512 x i1> [[TMP4]], ptr [[W]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    [[TMP5:%.*]] = tail call <512 x i1> @llvm.kvx.xmadd44bw1(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <512 x i1> [[TMP2]], i32 1)
// CHECK-NEXT:    store <512 x i1> [[TMP5]], ptr [[W]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    [[TMP6:%.*]] = tail call <512 x i1> @llvm.kvx.xmadd44bw0(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <512 x i1> [[TMP2]], i32 2)
// CHECK-NEXT:    store <512 x i1> [[TMP6]], ptr [[W]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    [[TMP7:%.*]] = tail call <512 x i1> @llvm.kvx.xmadd44bw1(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <512 x i1> [[TMP2]], i32 1)
// CHECK-NEXT:    store <512 x i1> [[TMP7]], ptr [[W]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    ret void
//
void xmadd44bw_test(__kvx_x256 *v, __kvx_x512 *w) {
  __kvx_x512 r = __builtin_kvx_xmadd44bw0(v[0], v[0], w[0], "");
  w[0] = __builtin_kvx_xmadd44bw1(v[0], v[0], r, ".");
  w[0] = __builtin_kvx_xmadd44bw0(v[0], v[0], r, ".s");
  w[0] = __builtin_kvx_xmadd44bw1(v[0], v[0], r, ".su");
  w[0] = __builtin_kvx_xmadd44bw0(v[0], v[0], r, ".u");
  w[0] = __builtin_kvx_xmadd44bw1(v[0], v[0], r, ".su");
}

// CHECK-LABEL: @xmaddifwo_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[V:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xmaddifwo(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <256 x i1> [[TMP0]], i32 7, i32 0)
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <256 x i1> @llvm.kvx.xmaddifwo(<256 x i1> [[TMP1]], <256 x i1> [[TMP1]], <256 x i1> [[TMP1]], i32 7, i32 1)
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <256 x i1> @llvm.kvx.xmaddifwo(<256 x i1> [[TMP2]], <256 x i1> [[TMP2]], <256 x i1> [[TMP2]], i32 0, i32 0)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <256 x i1> @llvm.kvx.xmaddifwo(<256 x i1> [[TMP3]], <256 x i1> [[TMP3]], <256 x i1> [[TMP3]], i32 3, i32 1)
// CHECK-NEXT:    store <256 x i1> [[TMP4]], ptr [[V]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xmaddifwo_test(__kvx_x256 *v) {
  __kvx_x256 r = __builtin_kvx_xmaddifwo(v[0], v[0], v[0], "");
  r = __builtin_kvx_xmaddifwo(r, r, r, ".s");
  r = __builtin_kvx_xmaddifwo(r, r, r, ".rn");
  v[0] = __builtin_kvx_xmaddifwo(r, r, r, ".rz.s");
}

// CHECK-LABEL: @xmma4164bw_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <512 x i1>, ptr [[W:%.*]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <512 x i1> @llvm.kvx.xmma4164bw(<512 x i1> [[TMP0]], <512 x i1> [[TMP0]], <512 x i1> [[TMP0]], i32 0)
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <512 x i1> @llvm.kvx.xmma4164bw(<512 x i1> [[TMP1]], <512 x i1> [[TMP1]], <512 x i1> [[TMP1]], i32 0)
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <512 x i1> @llvm.kvx.xmma4164bw(<512 x i1> [[TMP2]], <512 x i1> [[TMP2]], <512 x i1> [[TMP2]], i32 1)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <512 x i1> @llvm.kvx.xmma4164bw(<512 x i1> [[TMP3]], <512 x i1> [[TMP3]], <512 x i1> [[TMP3]], i32 2)
// CHECK-NEXT:    [[TMP5:%.*]] = tail call <512 x i1> @llvm.kvx.xmma4164bw(<512 x i1> [[TMP4]], <512 x i1> [[TMP4]], <512 x i1> [[TMP4]], i32 3)
// CHECK-NEXT:    store <512 x i1> [[TMP5]], ptr [[W]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    ret void
//
void xmma4164bw_test(__kvx_x512 *w) {
  __kvx_x512 r =  __builtin_kvx_xmma4164bw(w[0], w[0], w[0], "");
  r =  __builtin_kvx_xmma4164bw(r, r, r, ".s");
  r =  __builtin_kvx_xmma4164bw(r, r, r, ".su");
  r =  __builtin_kvx_xmma4164bw(r, r, r, ".u");
  w[0] =  __builtin_kvx_xmma4164bw(r, r, r, ".us");
}

// CHECK-LABEL: @xmma484bw_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[V:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = load <512 x i1>, ptr [[W:%.*]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <512 x i1> @llvm.kvx.xmma484bw(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <512 x i1> [[TMP1]], i32 0)
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <512 x i1> @llvm.kvx.xmma484bw(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <512 x i1> [[TMP2]], i32 1)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <512 x i1> @llvm.kvx.xmma484bw(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <512 x i1> [[TMP3]], i32 2)
// CHECK-NEXT:    [[TMP5:%.*]] = tail call <512 x i1> @llvm.kvx.xmma484bw(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <512 x i1> [[TMP4]], i32 3)
// CHECK-NEXT:    store <512 x i1> [[TMP5]], ptr [[W]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    ret void
//
void xmma484bw_test(__kvx_x512 *w, __kvx_x256 *v) {
  __kvx_x256 c = v[0];
  __kvx_x512 r =  __builtin_kvx_xmma484bw(c, c, w[0], "");
  r =  __builtin_kvx_xmma484bw(c, c, r, ".su");
  r =  __builtin_kvx_xmma484bw(c, c, r, ".u");
  w[0] =  __builtin_kvx_xmma484bw(c, c, r, ".us");
}

// CHECK-LABEL: @xmsbfifwo_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[V:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xmsbfifwo(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]], <256 x i1> [[TMP0]], i32 7, i32 0)
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <256 x i1> @llvm.kvx.xmsbfifwo(<256 x i1> [[TMP1]], <256 x i1> [[TMP1]], <256 x i1> [[TMP1]], i32 7, i32 1)
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <256 x i1> @llvm.kvx.xmsbfifwo(<256 x i1> [[TMP2]], <256 x i1> [[TMP2]], <256 x i1> [[TMP2]], i32 0, i32 0)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <256 x i1> @llvm.kvx.xmsbfifwo(<256 x i1> [[TMP3]], <256 x i1> [[TMP3]], <256 x i1> [[TMP3]], i32 3, i32 1)
// CHECK-NEXT:    store <256 x i1> [[TMP4]], ptr [[V]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xmsbfifwo_test(__kvx_x256 *v) {
  __kvx_x256 r = __builtin_kvx_xmsbfifwo(v[0], v[0], v[0], "");
  r = __builtin_kvx_xmsbfifwo(r, r, r, ".s");
  r = __builtin_kvx_xmsbfifwo(r, r, r, ".rn");
  v[0] = __builtin_kvx_xmsbfifwo(r, r, r, ".rz.s");
}

// CHECK-LABEL: @xsx48bw_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[V:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <1024 x i1> @llvm.kvx.xsx48bw(<256 x i1> [[TMP0]])
// CHECK-NEXT:    store <1024 x i1> [[TMP1]], ptr [[M:%.*]], align 32, !tbaa [[TBAA8:![0-9]+]]
// CHECK-NEXT:    ret void
//
void xsx48bw_test(__kvx_x1024 *m, __kvx_x256 *v) {
  m[0] = __builtin_kvx_xsx48bw(v[0]);
}

// CHECK-LABEL: @xzx48bw_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[V:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <1024 x i1> @llvm.kvx.xzx48bw(<256 x i1> [[TMP0]])
// CHECK-NEXT:    store <1024 x i1> [[TMP1]], ptr [[M:%.*]], align 32, !tbaa [[TBAA8]]
// CHECK-NEXT:    ret void
//
void xzx48bw_test(__kvx_x1024 *m, __kvx_x256 *v) {
  m[0] = __builtin_kvx_xzx48bw(v[0]);
}

// CHECK-LABEL: @xtrunc48wb_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <1024 x i1>, ptr [[M:%.*]], align 32, !tbaa [[TBAA8]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xtrunc48wb(<1024 x i1> [[TMP0]])
// CHECK-NEXT:    store <256 x i1> [[TMP1]], ptr [[V:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xtrunc48wb_test(__kvx_x1024 *m, __kvx_x256 *v) {
  v[0] = __builtin_kvx_xtrunc48wb(m[0]);
}

// CHECK-LABEL: @xmt44d_test(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <1024 x i1>, ptr [[M:%.*]], align 32, !tbaa [[TBAA8]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <1024 x i1> @llvm.kvx.xmt44d(<1024 x i1> [[TMP0]])
// CHECK-NEXT:    store <1024 x i1> [[TMP1]], ptr [[M]], align 32, !tbaa [[TBAA8]]
// CHECK-NEXT:    ret void
//
void xmt44d_test(__kvx_x1024 *m) {
  m[0] = __builtin_kvx_xmt44d(m[0]);
}

// CHECK-LABEL: @xload256(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <256 x i1>, ptr [[V:%.*]], i64 1
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <256 x i1> @llvm.kvx.xload256(ptr nonnull [[ARRAYIDX]], i32 0)
// CHECK-NEXT:    store <256 x i1> [[TMP0]], ptr [[V]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds <256 x i1>, ptr [[V]], i64 3
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xload256(ptr nonnull [[ARRAYIDX2]], i32 1)
// CHECK-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <256 x i1>, ptr [[V]], i64 2
// CHECK-NEXT:    store <256 x i1> [[TMP1]], ptr [[ARRAYIDX3]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <256 x i1> @llvm.kvx.xload256(ptr nonnull [[ARRAYIDX2]], i32 2)
// CHECK-NEXT:    store <256 x i1> [[TMP2]], ptr [[ARRAYIDX3]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <256 x i1> @llvm.kvx.xload256(ptr nonnull [[ARRAYIDX3]], i32 3)
// CHECK-NEXT:    store <256 x i1> [[TMP3]], ptr [[V]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xload256(__kvx_x256 *v) {
  v[0] = __builtin_kvx_xload256(&v[1], "");
  v[2] = __builtin_kvx_xload256(&v[3], ".s");
  v[2] = __builtin_kvx_xload256(&v[3], ".u");
  v[0] = __builtin_kvx_xload256(&v[2], ".us");
}

// CHECK-LABEL: @xloadc256(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <256 x i1>, ptr [[V:%.*]], i64 1
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <256 x i1> @llvm.kvx.xloadc256(<256 x i1> undef, ptr nonnull [[ARRAYIDX]], i64 [[X:%.*]], i32 0, i32 4)
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <256 x i1>, ptr [[V]], i64 3
// CHECK-NEXT:    [[TOBOOL_NOT:%.*]] = icmp eq i64 [[X]], 0
// CHECK-NEXT:    [[CONV:%.*]] = zext i1 [[TOBOOL_NOT]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xloadc256(<256 x i1> [[TMP0]], ptr nonnull [[ARRAYIDX1]], i64 [[CONV]], i32 1, i32 5)
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <256 x i1> @llvm.kvx.xloadc256(<256 x i1> [[TMP1]], ptr nonnull [[ARRAYIDX1]], i64 0, i32 2, i32 6)
// CHECK-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <256 x i1>, ptr [[V]], i64 2
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <256 x i1> @llvm.kvx.xloadc256(<256 x i1> [[TMP2]], ptr nonnull [[ARRAYIDX3]], i64 1, i32 3, i32 7)
// CHECK-NEXT:    store <256 x i1> [[TMP3]], ptr [[V]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xloadc256(__kvx_x256 *v, long x) {
  __kvx_x256 c;
  c = __builtin_kvx_xloadc256(c, &v[1], x, ".mt");
  c = __builtin_kvx_xloadc256(c, &v[3], !x, ".s.mf");
  c = __builtin_kvx_xloadc256(c, &v[3], 0, ".u.mtc");
  v[0] = __builtin_kvx_xloadc256(c, &v[2], 1, ".us.mfc");
}

// CHECK-LABEL: @xload1024q(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <1024 x i1> @llvm.kvx.xload1024q0(<1024 x i1> undef, ptr [[M:%.*]], i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <1024 x i1> @llvm.kvx.xload1024q1(<1024 x i1> [[TMP0]], ptr [[M]], i32 0)
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <1024 x i1> @llvm.kvx.xload1024q2(<1024 x i1> [[TMP1]], ptr [[M]], i32 1)
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <1024 x i1> @llvm.kvx.xload1024q3(<1024 x i1> [[TMP2]], ptr [[M]], i32 3)
// CHECK-NEXT:    store <1024 x i1> [[TMP3]], ptr [[M]], align 32, !tbaa [[TBAA8]]
// CHECK-NEXT:    ret void
//
void xload1024q(__kvx_x1024 *m) {
  __kvx_x1024 l;
  l = __builtin_kvx_xload1024q0(l, &m[0], "");
  l = __builtin_kvx_xload1024q1(l, &m[0], "");
  l = __builtin_kvx_xload1024q2(l, &m[0], ".s");
  m[0] = __builtin_kvx_xload1024q3(l, &m[0], ".us");
}

// CHECK-LABEL: @xloadc1024q(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <1024 x i1> @llvm.kvx.xloadc1024q0(<1024 x i1> undef, ptr [[M:%.*]], i64 [[X:%.*]], i32 0, i32 4)
// CHECK-NEXT:    [[TOBOOL_NOT:%.*]] = icmp eq i64 [[X]], 0
// CHECK-NEXT:    [[CONV:%.*]] = zext i1 [[TOBOOL_NOT]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <1024 x i1> @llvm.kvx.xloadc1024q1(<1024 x i1> [[TMP0]], ptr [[M]], i64 [[CONV]], i32 1, i32 5)
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <1024 x i1> @llvm.kvx.xloadc1024q2(<1024 x i1> [[TMP1]], ptr [[M]], i64 0, i32 2, i32 6)
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <1024 x i1> @llvm.kvx.xloadc1024q3(<1024 x i1> [[TMP2]], ptr [[M]], i64 1, i32 3, i32 7)
// CHECK-NEXT:    store <1024 x i1> [[TMP3]], ptr [[M]], align 32, !tbaa [[TBAA8]]
// CHECK-NEXT:    ret void
//
void xloadc1024q(__kvx_x1024 *m, long x) {
  __kvx_x1024 l;
  l = __builtin_kvx_xloadc1024q0(l, &m[0], x, ".mt");
  l = __builtin_kvx_xloadc1024q1(l, &m[0], !x, ".s.mf");
  l = __builtin_kvx_xloadc1024q2(l, &m[0], 0, ".u.mtc");
  m[0] = __builtin_kvx_xloadc1024q3(l, &m[0], 1, ".us.mfc");
}

// CHECK-LABEL: @xload512(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <512 x i1> @llvm.kvx.xload512(ptr [[M:%.*]], i32 0)
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <512 x i1>, ptr [[M]], i64 1
// CHECK-NEXT:    store <512 x i1> [[TMP0]], ptr [[ARRAYIDX1]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds <512 x i1>, ptr [[M]], i64 [[X:%.*]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <512 x i1> @llvm.kvx.xload512(ptr [[ARRAYIDX2]], i32 0)
// CHECK-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <512 x i1>, ptr [[M]], i64 2
// CHECK-NEXT:    store <512 x i1> [[TMP1]], ptr [[ARRAYIDX3]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    ret void
//
void xload512(__kvx_x512 *m, long x) {
  __kvx_x512 w = __builtin_kvx_xload512(&m[0], "");
  m[1] = w;
  w = __builtin_kvx_xload512(&m[x], "");
  m[2] = w;
}

// CHECK-LABEL: @xload1024(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <1024 x i1> @llvm.kvx.xload1024(ptr [[M:%.*]], i32 0)
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[M]], i64 1
// CHECK-NEXT:    store <1024 x i1> [[TMP0]], ptr [[ARRAYIDX1]], align 32, !tbaa [[TBAA8]]
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[M]], i64 [[X:%.*]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <1024 x i1> @llvm.kvx.xload1024(ptr [[ARRAYIDX2]], i32 1)
// CHECK-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[M]], i64 2
// CHECK-NEXT:    store <1024 x i1> [[TMP1]], ptr [[ARRAYIDX3]], align 32, !tbaa [[TBAA8]]
// CHECK-NEXT:    ret void
//
void xload1024(__kvx_x1024 *m, long x) {
  m[1] = __builtin_kvx_xload1024(&m[0], "");
  m[2] = __builtin_kvx_xload1024(&m[x], ".s");
}

// CHECK-LABEL: @xloadc512(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <512 x i1>, ptr [[V:%.*]], i64 1
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <512 x i1> @llvm.kvx.xloadc512(<512 x i1> undef, ptr nonnull [[ARRAYIDX]], i64 [[X:%.*]], i32 0, i32 0)
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <512 x i1>, ptr [[V]], i64 3
// CHECK-NEXT:    [[TOBOOL_NOT:%.*]] = icmp eq i64 [[X]], 0
// CHECK-NEXT:    [[CONV:%.*]] = zext i1 [[TOBOOL_NOT]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <512 x i1> @llvm.kvx.xloadc512(<512 x i1> [[TMP0]], ptr nonnull [[ARRAYIDX1]], i64 [[CONV]], i32 1, i32 3)
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <512 x i1> @llvm.kvx.xloadc512(<512 x i1> [[TMP1]], ptr nonnull [[ARRAYIDX1]], i64 0, i32 2, i32 6)
// CHECK-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <512 x i1>, ptr [[V]], i64 2
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <512 x i1> @llvm.kvx.xloadc512(<512 x i1> [[TMP2]], ptr nonnull [[ARRAYIDX3]], i64 1, i32 3, i32 7)
// CHECK-NEXT:    store <512 x i1> [[TMP3]], ptr [[V]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    ret void
//
void xloadc512(__kvx_x512 *v, long x) {
  __kvx_x512 c;
  c = __builtin_kvx_xloadc512(c, &v[1], x, ".dnez");
  c = __builtin_kvx_xloadc512(c, &v[3], !x, ".s.weqz");
  c = __builtin_kvx_xloadc512(c, &v[3], 0, ".u.mtc");
  v[0] = __builtin_kvx_xloadc512(c, &v[2], 1, ".us.mfc");
}

// CHECK-LABEL: @xloadc1024(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[V:%.*]], i64 1
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <1024 x i1> @llvm.kvx.xloadc1024(<1024 x i1> undef, ptr nonnull [[ARRAYIDX]], <4 x i32> [[X:%.*]], i32 0, i32 0)
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[V]], i64 3
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <1024 x i1> @llvm.kvx.xloadc1024(<1024 x i1> [[TMP0]], ptr nonnull [[ARRAYIDX1]], <4 x i32> [[X]], i32 1, i32 3)
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <1024 x i1> @llvm.kvx.xloadc1024(<1024 x i1> [[TMP1]], ptr nonnull [[ARRAYIDX1]], <4 x i32> [[X]], i32 2, i32 6)
// CHECK-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[V]], i64 2
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <1024 x i1> @llvm.kvx.xloadc1024(<1024 x i1> [[TMP2]], ptr nonnull [[ARRAYIDX3]], <4 x i32> [[X]], i32 3, i32 7)
// CHECK-NEXT:    store <1024 x i1> [[TMP3]], ptr [[V]], align 32, !tbaa [[TBAA8]]
// CHECK-NEXT:    ret void
//
void xloadc1024(__kvx_x1024 *v, __kvx_v4si x) {
  __kvx_x1024 c;
  c = __builtin_kvx_xloadc1024(c, &v[1], x, ".dnez");
  c = __builtin_kvx_xloadc1024(c, &v[3], x, ".s.weqz");
  c = __builtin_kvx_xloadc1024(c, &v[3], x, ".u.mtc");
  v[0] = __builtin_kvx_xloadc1024(c, &v[2], x, ".us.mfc");
}

// CHECK-LABEL: @xstore256(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[V:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    tail call void @llvm.kvx.xstore256(<256 x i1> [[TMP0]], ptr [[P:%.*]])
// CHECK-NEXT:    ret void
//
void xstore256(__kvx_x256 *v, void *p) {
  __builtin_kvx_xstore256(*v, p);
}

// CHECK-LABEL: @xstorec256(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[V:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    tail call void @llvm.kvx.xstorec256(<256 x i1> [[TMP0]], ptr [[P:%.*]], i64 1, i32 4)
// CHECK-NEXT:    ret void
//
void xstorec256( __kvx_x256 *v, void *p, long c) {
  __builtin_kvx_xstorec256(*v, p, 1, ".mt");
}

// CHECK-LABEL: @xloadStore256(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <256 x i1>, ptr addrspace(257) [[V:%.*]], i64 1
// CHECK-NEXT:    [[TMP0:%.*]] = load volatile <256 x i1>, ptr addrspace(257) [[ARRAYIDX]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    store volatile <256 x i1> [[TMP0]], ptr addrspace(257) [[V]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xloadStore256(volatile __preload __kvx_x256 *v) {
  v[0] = v[1];
}

// CHECK-LABEL: @xloadStore512(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <512 x i1>, ptr addrspace(258) [[V:%.*]], i64 1
// CHECK-NEXT:    [[TMP0:%.*]] = load volatile <512 x i1>, ptr addrspace(258) [[ARRAYIDX]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    store volatile <512 x i1> [[TMP0]], ptr addrspace(258) [[V]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    ret void
//
void xloadStore512(volatile __speculate __kvx_x512 *v) {
  v[0] = v[1];
}

// CHECK-LABEL: @xloadStore1024(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <1024 x i1>, ptr addrspace(256) [[V:%.*]], i64 1
// CHECK-NEXT:    [[TMP0:%.*]] = load volatile <1024 x i1>, ptr addrspace(256) [[ARRAYIDX]], align 32, !tbaa [[TBAA8]]
// CHECK-NEXT:    store volatile <1024 x i1> [[TMP0]], ptr addrspace(256) [[V]], align 32, !tbaa [[TBAA8]]
// CHECK-NEXT:    ret void
//
void xloadStore1024(volatile __bypass __kvx_x1024 *v) {
  v[0] = v[1];
}

// CHECK-LABEL: @xsendo(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[V:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    tail call void @llvm.kvx.xsendo(<256 x i1> [[TMP0]], i32 1) #[[ATTR1:[0-9]+]]
// CHECK-NEXT:    [[TMP1:%.*]] = load <256 x i1>, ptr [[V]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    tail call void @llvm.kvx.xsendo(<256 x i1> [[TMP1]], i32 0) #[[ATTR1]]
// CHECK-NEXT:    ret void
//
void xsendo(__kvx_x256 *v) {
  __builtin_kvx_xsendo(*v, ".b");
  __builtin_kvx_xsendo(*v, ".f");
}

// CHECK-LABEL: @xrecvo(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <256 x i1> @llvm.kvx.xrecvo(i32 1) #[[ATTR1]]
// CHECK-NEXT:    store <256 x i1> [[TMP0]], ptr [[V:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xrecvo(i32 0) #[[ATTR1]]
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <256 x i1>, ptr [[V]], i64 1
// CHECK-NEXT:    store <256 x i1> [[TMP1]], ptr [[ARRAYIDX]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xrecvo(__kvx_x256 *v) {
  *v = __builtin_kvx_xrecvo(".b");
  v[1] = __builtin_kvx_xrecvo(".f");
}

// CHECK-LABEL: @xsendrecvo(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[V:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xsendrecvo(<256 x i1> [[TMP0]], i32 1, i32 1) #[[ATTR1]]
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <256 x i1>, ptr [[V]], i64 2
// CHECK-NEXT:    store <256 x i1> [[TMP1]], ptr [[ARRAYIDX]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <256 x i1>, ptr [[V]], i64 1
// CHECK-NEXT:    [[TMP2:%.*]] = load <256 x i1>, ptr [[ARRAYIDX1]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <256 x i1> @llvm.kvx.xsendrecvo(<256 x i1> [[TMP2]], i32 0, i32 1) #[[ATTR1]]
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds <256 x i1>, ptr [[V]], i64 4
// CHECK-NEXT:    store <256 x i1> [[TMP3]], ptr [[ARRAYIDX2]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP4:%.*]] = load <256 x i1>, ptr [[V]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP5:%.*]] = tail call <256 x i1> @llvm.kvx.xsendrecvo(<256 x i1> [[TMP4]], i32 1, i32 0) #[[ATTR1]]
// CHECK-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <256 x i1>, ptr [[V]], i64 3
// CHECK-NEXT:    store <256 x i1> [[TMP5]], ptr [[ARRAYIDX3]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP6:%.*]] = load <256 x i1>, ptr [[ARRAYIDX1]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP7:%.*]] = tail call <256 x i1> @llvm.kvx.xsendrecvo(<256 x i1> [[TMP6]], i32 0, i32 0) #[[ATTR1]]
// CHECK-NEXT:    [[ARRAYIDX5:%.*]] = getelementptr inbounds <256 x i1>, ptr [[V]], i64 5
// CHECK-NEXT:    store <256 x i1> [[TMP7]], ptr [[ARRAYIDX5]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xsendrecvo(__kvx_x256 *v) {
  v[2] = __builtin_kvx_xsendrecvo(*v, ".b.b");
  v[4] = __builtin_kvx_xsendrecvo(v[1], ".f.b");
  v[3] = __builtin_kvx_xsendrecvo(*v, ".b.f");
  v[5] = __builtin_kvx_xsendrecvo(v[1], ".f.f");
}

// CHECK-LABEL: @xcopyv(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <1024 x i1>, ptr [[V:%.*]], align 32, !tbaa [[TBAA8]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <1024 x i1> @llvm.kvx.xcopyv(<1024 x i1> [[TMP0]], i32 0)
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[V]], i64 3
// CHECK-NEXT:    store <1024 x i1> [[TMP1]], ptr [[ARRAYIDX]], align 32, !tbaa [[TBAA8]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <1024 x i1> @llvm.kvx.xcopyv(<1024 x i1> [[TMP0]], i32 1)
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[V]], i64 1
// CHECK-NEXT:    store <1024 x i1> [[TMP2]], ptr [[ARRAYIDX1]], align 32, !tbaa [[TBAA8]]
// CHECK-NEXT:    ret void
//
void xcopyv(__kvx_x1024 *v) {
  v[3] = __builtin_kvx_xcopyv(*v, "");
  v[1] = __builtin_kvx_xcopyv(*v, ".td");
}

// CHECK-LABEL: @xcopyx(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <512 x i1>, ptr [[V:%.*]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <512 x i1> @llvm.kvx.xcopyx(<512 x i1> [[TMP0]], i32 0)
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <512 x i1> @llvm.kvx.xcopyx(<512 x i1> [[TMP1]], i32 1)
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <512 x i1> @llvm.kvx.xcopyx(<512 x i1> [[TMP2]], i32 2)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <512 x i1> @llvm.kvx.xcopyx(<512 x i1> [[TMP3]], i32 3)
// CHECK-NEXT:    [[TMP5:%.*]] = tail call <512 x i1> @llvm.kvx.xcopyx(<512 x i1> [[TMP4]], i32 4)
// CHECK-NEXT:    [[TMP6:%.*]] = tail call <512 x i1> @llvm.kvx.xcopyx(<512 x i1> [[TMP5]], i32 5)
// CHECK-NEXT:    [[TMP7:%.*]] = tail call <512 x i1> @llvm.kvx.xcopyx(<512 x i1> [[TMP6]], i32 6)
// CHECK-NEXT:    store <512 x i1> [[TMP7]], ptr [[V]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    ret void
//
void xcopyx(__kvx_x512 *v) {
  v[0] = __builtin_kvx_xcopyx(*v, "");
  v[0] = __builtin_kvx_xcopyx(*v, ".zd");
  v[0] = __builtin_kvx_xcopyx(*v, ".ud");
  v[0] = __builtin_kvx_xcopyx(*v, ".TQ");
  v[0] = __builtin_kvx_xcopyx(*v, ".tW");
  v[0] = __builtin_kvx_xcopyx(*v, ".Zw");
  v[0] = __builtin_kvx_xcopyx(*v, ".uw");
}

// CHECK-LABEL: @xfminmaxhx(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[V:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <256 x i1>, ptr [[V]], i64 1
// CHECK-NEXT:    [[TMP1:%.*]] = load <256 x i1>, ptr [[ARRAYIDX1]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <256 x i1> @llvm.kvx.xfmaxhx(<256 x i1> [[TMP0]], <256 x i1> [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <256 x i1> @llvm.kvx.xfminhx(<256 x i1> [[TMP2]], <256 x i1> [[TMP0]])
// CHECK-NEXT:    store <256 x i1> [[TMP3]], ptr [[V]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xfminmaxhx(__kvx_x256 *v) {
  v[0] = __builtin_kvx_xfminhx(__builtin_kvx_xfmaxhx(v[0], v[1]), v[0]);
}

// CHECK-LABEL: @xsplatov(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[C:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <1024 x i1> @llvm.kvx.xsplatov(<256 x i1> [[TMP0]], i32 0)
// CHECK-NEXT:    store <1024 x i1> [[TMP1]], ptr [[V:%.*]], align 32, !tbaa [[TBAA8]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <1024 x i1> @llvm.kvx.xsplatov(<256 x i1> [[TMP0]], i32 1)
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <1024 x i1>, ptr [[V]], i64 1
// CHECK-NEXT:    store <1024 x i1> [[TMP2]], ptr [[ARRAYIDX1]], align 32, !tbaa [[TBAA8]]
// CHECK-NEXT:    ret void
//
void xsplatov(__kvx_x1024 *v, __kvx_x256 *c) {
  v[0] = __builtin_kvx_xsplatov(*c, "");
  v[1] = __builtin_kvx_xsplatov(*c, ".td");
}

// CHECK-LABEL: @xsplatox(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[C:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <512 x i1> @llvm.kvx.xsplatox(<256 x i1> [[TMP0]], i32 0)
// CHECK-NEXT:    store <512 x i1> [[TMP1]], ptr [[V:%.*]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <512 x i1> @llvm.kvx.xsplatox(<256 x i1> [[TMP0]], i32 1)
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <512 x i1>, ptr [[V]], i64 1
// CHECK-NEXT:    store <512 x i1> [[TMP2]], ptr [[ARRAYIDX1]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <512 x i1> @llvm.kvx.xsplatox(<256 x i1> [[TMP0]], i32 2)
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds <512 x i1>, ptr [[V]], i64 2
// CHECK-NEXT:    store <512 x i1> [[TMP3]], ptr [[ARRAYIDX2]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <512 x i1> @llvm.kvx.xsplatox(<256 x i1> [[TMP0]], i32 3)
// CHECK-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <512 x i1>, ptr [[V]], i64 3
// CHECK-NEXT:    store <512 x i1> [[TMP4]], ptr [[ARRAYIDX3]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    [[TMP5:%.*]] = tail call <512 x i1> @llvm.kvx.xsplatox(<256 x i1> [[TMP0]], i32 4)
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds <512 x i1>, ptr [[V]], i64 4
// CHECK-NEXT:    store <512 x i1> [[TMP5]], ptr [[ARRAYIDX4]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    [[TMP6:%.*]] = tail call <512 x i1> @llvm.kvx.xsplatox(<256 x i1> [[TMP0]], i32 5)
// CHECK-NEXT:    [[ARRAYIDX5:%.*]] = getelementptr inbounds <512 x i1>, ptr [[V]], i64 5
// CHECK-NEXT:    store <512 x i1> [[TMP6]], ptr [[ARRAYIDX5]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    [[TMP7:%.*]] = tail call <512 x i1> @llvm.kvx.xsplatox(<256 x i1> [[TMP0]], i32 6)
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds <512 x i1>, ptr [[V]], i64 6
// CHECK-NEXT:    store <512 x i1> [[TMP7]], ptr [[ARRAYIDX6]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    ret void
//
void xsplatox(__kvx_x512 *v, __kvx_x256 *c) {
  v[0] = __builtin_kvx_xsplatox(*c, "");
  v[1] = __builtin_kvx_xsplatox(*c, ".zd");
  v[2] = __builtin_kvx_xsplatox(*c, ".ud");
  v[3] = __builtin_kvx_xsplatox(*c, ".tq");
  v[4] = __builtin_kvx_xsplatox(*c, ".tw");
  v[5] = __builtin_kvx_xsplatox(*c, ".zw");
  v[6] = __builtin_kvx_xsplatox(*c, ".uw");
}

// CHECK-LABEL: @xsplatdo(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <256 x i1> @llvm.kvx.xsplatdo(i64 511)
// CHECK-NEXT:    store <256 x i1> [[TMP0]], ptr [[V:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xsplatdo(i64 137438953471)
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <256 x i1>, ptr [[V]], i64 1
// CHECK-NEXT:    store <256 x i1> [[TMP1]], ptr [[ARRAYIDX1]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <256 x i1> @llvm.kvx.xsplatdo(i64 27487790694300)
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds <256 x i1>, ptr [[V]], i64 2
// CHECK-NEXT:    store <256 x i1> [[TMP2]], ptr [[ARRAYIDX2]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xsplatdo(__kvx_x256 *v) {
  v[0] = __builtin_kvx_xsplatdo(511);
  v[1] = __builtin_kvx_xsplatdo(137438953471);
  v[2] = __builtin_kvx_xsplatdo(27487790694300);
}

// CHECK-LABEL: @xaligno512(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <512 x i1>, ptr [[W:%.*]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xaligno.v512i1(<512 x i1> [[TMP0]], i64 [[S:%.*]])
// CHECK-NEXT:    store <256 x i1> [[TMP1]], ptr [[V:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xaligno512(__kvx_x256 *v, __kvx_x512 *w, long s) {
  *v = __builtin_kvx_xaligno512(*w, s);
}

// CHECK-LABEL: @xaligno1024(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <1024 x i1>, ptr [[M:%.*]], align 32, !tbaa [[TBAA8]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xaligno.v1024i1(<1024 x i1> [[TMP0]], i64 [[S:%.*]])
// CHECK-NEXT:    store <256 x i1> [[TMP1]], ptr [[V:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xaligno1024(__kvx_x256 *v, __kvx_x1024 *m, long s) {
  *v = __builtin_kvx_xaligno1024(*m, s);
}

// CHECK-LABEL: @xaligno2048(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <256 x i1> @llvm.kvx.xaligno.v2048i1(<2048 x i1> zeroinitializer, i64 [[S:%.*]])
// CHECK-NEXT:    store <256 x i1> [[TMP0]], ptr [[V:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xaligno2048(__kvx_x256 *v, long s) {
  __kvx_x2048 m = __builtin_kvx_xzero2048();
  *v = __builtin_kvx_xaligno2048(m, s);
}

// CHECK-LABEL: @xaligno4096(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <256 x i1> @llvm.kvx.xaligno.v4096i1(<4096 x i1> zeroinitializer, i64 [[S:%.*]])
// CHECK-NEXT:    store <256 x i1> [[TMP0]], ptr [[V:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xaligno4096(__kvx_x256 *v, long s) {
  __kvx_x4096 m = __builtin_kvx_xzero4096();
  *v = __builtin_kvx_xaligno4096(m, s);
}

// CHECK-LABEL: @xpreload512(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <512 x i1> @llvm.kvx.xpreload.v512i1(<512 x i1> undef, ptr [[W:%.*]], i64 [[S:%.*]], i32 0, i32 0)
// CHECK-NEXT:    store <512 x i1> [[TMP0]], ptr [[W]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    ret void
//
void xpreload512(__kvx_x512 *w, long long s) {
  __kvx_x512 f;
  *w = __builtin_kvx_xpreload512(f, w, s, "");
}

// CHECK-LABEL: @xpreload1024(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <1024 x i1> @llvm.kvx.xpreload.v1024i1(<1024 x i1> undef, ptr [[W:%.*]], i64 [[S:%.*]], i32 3, i32 1)
// CHECK-NEXT:    store <1024 x i1> [[TMP0]], ptr [[W]], align 32, !tbaa [[TBAA8]]
// CHECK-NEXT:    ret void
//
void xpreload1024(__kvx_x1024 *w, long long s) {
  __kvx_x1024 f;
  *w = __builtin_kvx_xpreload1024(f, w, s, ".us.q");
}

// CHECK-LABEL: @xpreload2048(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2048 x i1> @llvm.kvx.xpreload.v2048i1(<2048 x i1> undef, ptr [[W:%.*]], i64 [[S:%.*]], i32 1, i32 3)
// CHECK-NEXT:    store <2048 x i1> [[TMP0]], ptr [[W]], align 32, !tbaa [[TBAA10:![0-9]+]]
// CHECK-NEXT:    ret void
//
void xpreload2048(__kvx_x2048 *w, long long s) {
  __kvx_x2048 f;
  *w = __builtin_kvx_xpreload2048(f, w, s, ".s.w");
}

// CHECK-LABEL: @xpreload4096(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <4096 x i1> @llvm.kvx.xpreload.v4096i1(<4096 x i1> undef, ptr [[W:%.*]], i64 [[S:%.*]], i32 2, i32 5)
// CHECK-NEXT:    store <4096 x i1> [[TMP0]], ptr [[W]], align 32, !tbaa [[TBAA12:![0-9]+]]
// CHECK-NEXT:    ret void
//
void xpreload4096(__kvx_x4096 *w, long long s) {
  __kvx_x4096 f;
  *w = __builtin_kvx_xpreload4096(f, w, s, ".u.b");
}

// CHECK-LABEL: @xaccesso512(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <512 x i1>, ptr [[W:%.*]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <4 x i64> @llvm.kvx.xaccesso.v512i1(<512 x i1> [[TMP0]], i64 [[S:%.*]])
// CHECK-NEXT:    ret <4 x i64> [[TMP1]]
//
__kvx_v4di xaccesso512(__kvx_x512 *w, long long s) {
  return __builtin_kvx_xaccesso512(*w, s);
}

// CHECK-LABEL: @xaccesso1024(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <1024 x i1>, ptr [[W:%.*]], align 32, !tbaa [[TBAA8]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <4 x i64> @llvm.kvx.xaccesso.v1024i1(<1024 x i1> [[TMP0]], i64 [[S:%.*]])
// CHECK-NEXT:    ret <4 x i64> [[TMP1]]
//
__kvx_v4di xaccesso1024(__kvx_x1024 *w, long long s) {
  return __builtin_kvx_xaccesso1024(*w, s);
}

// CHECK-LABEL: @xaccesso2048(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <4 x i64> @llvm.kvx.xaccesso.v2048i1(<2048 x i1> zeroinitializer, i64 [[S:%.*]])
// CHECK-NEXT:    ret <4 x i64> [[TMP0]]
//
__kvx_v4di xaccesso2048(long long s) {
  __kvx_x2048 w = __builtin_kvx_xzero2048();
  return __builtin_kvx_xaccesso2048(w, s);
}

// CHECK-LABEL: @xaccesso4096(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <4 x i64> @llvm.kvx.xaccesso.v4096i1(<4096 x i1> zeroinitializer, i64 [[S:%.*]])
// CHECK-NEXT:    ret <4 x i64> [[TMP0]]
//
__kvx_v4di xaccesso4096(long long s) {
  __kvx_x4096 w = __builtin_kvx_xzero4096();
  return __builtin_kvx_xaccesso4096(w, s);
}

// CHECK-LABEL: @binOps(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <256 x i1> @llvm.kvx.xsplat.v256i1(i64 734)
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xandno(<256 x i1> [[TMP0]], <256 x i1> [[TMP0]])
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <256 x i1> @llvm.kvx.xando(<256 x i1> [[TMP1]], <256 x i1> [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <256 x i1> @llvm.kvx.xnando(<256 x i1> [[TMP2]], <256 x i1> [[TMP2]])
// CHECK-NEXT:    [[TMP4:%.*]] = tail call <256 x i1> @llvm.kvx.xnioro(<256 x i1> [[TMP3]], <256 x i1> [[TMP3]])
// CHECK-NEXT:    [[TMP5:%.*]] = tail call <256 x i1> @llvm.kvx.xneoro(<256 x i1> [[TMP4]], <256 x i1> [[TMP4]])
// CHECK-NEXT:    [[TMP6:%.*]] = tail call <256 x i1> @llvm.kvx.xiorno(<256 x i1> [[TMP5]], <256 x i1> [[TMP5]])
// CHECK-NEXT:    [[TMP7:%.*]] = tail call <256 x i1> @llvm.kvx.xioro(<256 x i1> [[TMP6]], <256 x i1> [[TMP6]])
// CHECK-NEXT:    [[TMP8:%.*]] = tail call <256 x i1> @llvm.kvx.xeoro(<256 x i1> [[TMP7]], <256 x i1> [[TMP7]])
// CHECK-NEXT:    store <256 x i1> [[TMP8]], ptr [[P:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
void binOps(__kvx_x256 *p) {
  __kvx_x256 v = __builtin_kvx_xsplatd256(734);
  v = __builtin_kvx_xandno(v, v);
  v = __builtin_kvx_xando(v, v);
  v = __builtin_kvx_xnando(v, v);
  v = __builtin_kvx_xnioro(v, v);
  v = __builtin_kvx_xneoro(v, v);
  v = __builtin_kvx_xiorno(v, v);
  v = __builtin_kvx_xioro(v, v);
  v = __builtin_kvx_xeoro(v, v);
  *p = v;
}

// CHECK-LABEL: @xsbmm8(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <256 x i1> @llvm.kvx.xsplat.v256i1(i64 734)
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> @llvm.kvx.xsplat.v256i1(i64 15)
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <256 x i1> @llvm.kvx.xsbmm8dq(<256 x i1> [[TMP0]], <256 x i1> [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <256 x i1> @llvm.kvx.xsbmmt8dq(<256 x i1> [[TMP1]], <256 x i1> [[TMP2]])
// CHECK-NEXT:    store <256 x i1> [[TMP3]], ptr [[P:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
void xsbmm8(__kvx_x256 *p) {
  __kvx_x256 v = __builtin_kvx_xsplatd256(734);
  __kvx_x256 v1 = __builtin_kvx_xsplatd256(15);
  v = __builtin_kvx_xsbmm8dq(v, v1);
  v = __builtin_kvx_xsbmmt8dq(v1, v);
  *p = v;
}

// CHECK-LABEL: @xcat2048(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2048 x i1> @llvm.kvx.cat.v2048i1(<1024 x i1> zeroinitializer, <1024 x i1> zeroinitializer)
// CHECK-NEXT:    store <2048 x i1> [[TMP0]], ptr [[W:%.*]], align 32, !tbaa [[TBAA10]]
// CHECK-NEXT:    ret void
//
void xcat2048(__kvx_x2048 *W) {
  W[0] = __builtin_kvx_xcat2048(
              __builtin_kvx_xzero1024(),
              __builtin_kvx_xzero1024());
}

// O0-LABEL: @xcat4096(
// O0-NEXT:  entry:
// O0-NEXT:    [[W_ADDR:%.*]] = alloca <4096 x i1>*, align 8
// O0-NEXT:    store <4096 x i1>* [[W:%.*]], <4096 x i1>** [[W_ADDR]], align 8
// O0-NEXT:    [[TMP0:%.*]] = call <4096 x i1> @llvm.kvx.cat.v4096i1(<2048 x i1> zeroinitializer, <2048 x i1> zeroinitializer)
// O0-NEXT:    [[TMP1:%.*]] = load <4096 x i1>*, <4096 x i1>** [[W_ADDR]], align 8
// O0-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <4096 x i1>, <4096 x i1>* [[TMP1]], i64 0
// O0-NEXT:    store <4096 x i1> [[TMP0]], <4096 x i1>* [[ARRAYIDX]], align 32
// O0-NEXT:    ret void
//
// O2-LABEL: @xcat4096(
// O2-NEXT:  entry:
// O2-NEXT:    [[TMP0:%.*]] = call <4096 x i1> @llvm.kvx.cat.v4096i1(<2048 x i1> zeroinitializer, <2048 x i1> zeroinitializer)
// O2-NEXT:    store <4096 x i1> [[TMP0]], <4096 x i1>* [[W:%.*]], align 32, [[TBAA12:!tbaa !.*]]
// O2-NEXT:    ret void
// CHECK-LABEL: @xcat4096(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <4096 x i1> @llvm.kvx.cat.v4096i1(<2048 x i1> zeroinitializer, <2048 x i1> zeroinitializer)
// CHECK-NEXT:    store <4096 x i1> [[TMP0]], ptr [[W:%.*]], align 32, !tbaa [[TBAA12]]
// CHECK-NEXT:    ret void
//
void xcat4096(__kvx_x4096 *W) {
  W[0] = __builtin_kvx_xcat4096(
              __builtin_kvx_xzero2048(),
              __builtin_kvx_xzero2048());
}
