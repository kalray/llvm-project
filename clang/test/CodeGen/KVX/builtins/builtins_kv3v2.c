// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -O2 -triple kvx-kalray-cos -S -emit-llvm -target-cpu kv3-2 -o - -x c %s | FileCheck %s

// CHECK-LABEL: @dflushsw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    tail call void @llvm.kvx.dflushsw(i64 [[W:%.*]], i64 [[S:%.*]], i32 0)
// CHECK-NEXT:    tail call void @llvm.kvx.dflushsw(i64 [[W]], i64 [[S]], i32 1)
// CHECK-NEXT:    ret void
//
void dflushsw(unsigned long w, unsigned long s) {
  __builtin_kvx_dflushsw(w, s, ".l1");
  __builtin_kvx_dflushsw(w, s, ".l2");
}

// CHECK-LABEL: @dinvalsw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    tail call void @llvm.kvx.dinvalsw(i64 [[W:%.*]], i64 [[S:%.*]], i32 0)
// CHECK-NEXT:    tail call void @llvm.kvx.dinvalsw(i64 [[W]], i64 [[S]], i32 1)
// CHECK-NEXT:    ret void
//
void dinvalsw(unsigned long w, unsigned long s) {
  __builtin_kvx_dinvalsw(w, s, ".l1");
  __builtin_kvx_dinvalsw(w, s, ".l2");
}

// CHECK-LABEL: @dpurgesw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    tail call void @llvm.kvx.dpurgesw(i64 [[W:%.*]], i64 [[S:%.*]], i32 0)
// CHECK-NEXT:    tail call void @llvm.kvx.dpurgesw(i64 [[W]], i64 [[S]], i32 1)
// CHECK-NEXT:    ret void
//
void dpurgesw(unsigned long w, unsigned long s) {
  __builtin_kvx_dpurgesw(w, s, ".l1");
  __builtin_kvx_dpurgesw(w, s, ".l2");
}

// CHECK-LABEL: @acswapw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i32* [[P:%.*]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i32 @llvm.kvx.acswapw(i8* [[TMP0]], i32 [[A:%.*]], i32 [[B:%.*]], i32 1, i32 0)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
int acswapw(int *p, int a, int b) {
  return __builtin_kvx_acswapw(p, a, b, "");
}

// CHECK-LABEL: @acswapwv(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i32* [[P:%.*]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i32 @llvm.kvx.acswapw(i8* [[TMP0]], i32 [[A:%.*]], i32 [[B:%.*]], i32 1, i32 0)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
int acswapwv(int *p, int a, int b) {
  return __builtin_kvx_acswapw(p, a, b);
}

//
// int acswapws(int *p, int a, int b) {
//   return __builtin_kvx_acswapw(p, a, b, ".s");
// }

//
// int acswapwvs(int *p, int a, int b) {
//   return __builtin_kvx_acswapw(p, a, b, ".v.s");
// }

// CHECK-LABEL: @acswapwg(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i32* [[P:%.*]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i32 @llvm.kvx.acswapw(i8* [[TMP0]], i32 [[A:%.*]], i32 [[B:%.*]], i32 1, i32 1)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
int acswapwg(int *p, int a, int b) {
  return __builtin_kvx_acswapw(p, a, b, ".g");
}

// CHECK-LABEL: @acswapwvg(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i32* [[P:%.*]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i32 @llvm.kvx.acswapw(i8* [[TMP0]], i32 [[A:%.*]], i32 [[B:%.*]], i32 0, i32 1)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
int acswapwvg(int *p, int a, int b) {
  return __builtin_kvx_acswapw(p, a, b, ".v.g");
}

// CHECK-LABEL: @acswapd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i64* [[P:%.*]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 @llvm.kvx.acswapd(i8* [[TMP0]], i64 [[A:%.*]], i64 [[B:%.*]], i32 1, i32 0)
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long acswapd(long *p, long a, long b) {
  return __builtin_kvx_acswapd(p, a, b, "");
}

// CHECK-LABEL: @acswapdv(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i64* [[P:%.*]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 @llvm.kvx.acswapd(i8* [[TMP0]], i64 [[A:%.*]], i64 [[B:%.*]], i32 0, i32 0)
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long acswapdv(long *p, long a, long b) {
  return __builtin_kvx_acswapd(p, a, b, ".v");
}

//
// long acswapds(long *p, long a, long b) {
//   return __builtin_kvx_acswapd(p, a, b, ".s");
// }

//
// long acswapdvs(long *p, long a, long b) {
//   return __builtin_kvx_acswapd(p, a, b, ".v.s");
// }

// CHECK-LABEL: @acswapdg(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i64* [[P:%.*]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 @llvm.kvx.acswapd(i8* [[TMP0]], i64 [[A:%.*]], i64 [[B:%.*]], i32 1, i32 1)
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long acswapdg(long *p, long a, long b) {
  return __builtin_kvx_acswapd(p, a, b, ".g");
}

// CHECK-LABEL: @acswapdvg(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i64* [[P:%.*]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 @llvm.kvx.acswapd(i8* [[TMP0]], i64 [[A:%.*]], i64 [[B:%.*]], i32 0, i32 1)
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long acswapdvg(long *p, long a, long b) {
  return __builtin_kvx_acswapd(p, a, b, ".v.g");
}

typedef long long2 __attribute__((vector_size(sizeof(long) * 2)));

// CHECK-LABEL: @acswapq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <2 x i64>* [[P:%.*]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <2 x i64> @llvm.kvx.acswapq(i8* [[TMP0]], <2 x i64> [[A:%.*]], <2 x i64> [[B:%.*]], i32 1, i32 0)
// CHECK-NEXT:    ret <2 x i64> [[TMP1]]
//
long2 acswapq(long2 *p, long2 a, long2 b) {
  return __builtin_kvx_acswapq(p, a, b, "");
}

// CHECK-LABEL: @acswapqv(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <2 x i64>* [[P:%.*]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <2 x i64> @llvm.kvx.acswapq(i8* [[TMP0]], <2 x i64> [[A:%.*]], <2 x i64> [[B:%.*]], i32 0, i32 0)
// CHECK-NEXT:    ret <2 x i64> [[TMP1]]
//
long2 acswapqv(long2 *p, long2 a, long2 b) {
  return __builtin_kvx_acswapq(p, a, b, ".v");
}

//
// long2 acswapqs(long2 *p, long2 a, long2 b) {
//   return __builtin_kvx_acswapq(p, a, b, ".s");
// }

//
// long2 acswapqvs(long2 *p, long2 a, long2 b) {
//   return __builtin_kvx_acswapq(p, a, b, ".v.s");
// }

// CHECK-LABEL: @acswapqg(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <2 x i64>* [[P:%.*]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <2 x i64> @llvm.kvx.acswapq(i8* [[TMP0]], <2 x i64> [[A:%.*]], <2 x i64> [[B:%.*]], i32 1, i32 1)
// CHECK-NEXT:    ret <2 x i64> [[TMP1]]
//
long2 acswapqg(long2 *p, long2 a, long2 b) {
  return __builtin_kvx_acswapq(p, a, b, ".g");
}

// CHECK-LABEL: @acswapqvg(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <2 x i64>* [[P:%.*]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <2 x i64> @llvm.kvx.acswapq(i8* [[TMP0]], <2 x i64> [[A:%.*]], <2 x i64> [[B:%.*]], i32 0, i32 1)
// CHECK-NEXT:    ret <2 x i64> [[TMP1]]
//
long2 acswapqvg(long2 *p, long2 a, long2 b) {
  return __builtin_kvx_acswapq(p, a, b, ".v.g");
}

// CHECK-LABEL: @acswapqvgr(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[IDXPROM:%.*]] = sext i32 [[O:%.*]] to i64
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <2 x i64>, <2 x i64>* [[P:%.*]], i64 [[IDXPROM]]
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <2 x i64>* [[ARRAYIDX]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <2 x i64> @llvm.kvx.acswapq(i8* [[TMP0]], <2 x i64> [[A:%.*]], <2 x i64> [[B:%.*]], i32 0, i32 1)
// CHECK-NEXT:    ret <2 x i64> [[TMP1]]
//
long2 acswapqvgr(long2 *p, long2 a, long2 b, int o) {
  return __builtin_kvx_acswapq(&p[o], a, b, ".v.g");
}

// CHECK-LABEL: @acswapqvgri27(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <2 x i64>, <2 x i64>* [[P:%.*]], i64 15
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <2 x i64>* [[ARRAYIDX]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <2 x i64> @llvm.kvx.acswapq(i8* nonnull [[TMP0]], <2 x i64> [[A:%.*]], <2 x i64> [[B:%.*]], i32 0, i32 1)
// CHECK-NEXT:    ret <2 x i64> [[TMP1]]
//
long2 acswapqvgri27(long2 *p, long2 a, long2 b) {
  return __builtin_kvx_acswapq(&p[15], a, b, ".v.g");
}

// CHECK-LABEL: @acswapqvgri54(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <2 x i64>, <2 x i64>* [[P:%.*]], i64 4294967299
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <2 x i64>* [[ARRAYIDX]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <2 x i64> @llvm.kvx.acswapq(i8* nonnull [[TMP0]], <2 x i64> [[A:%.*]], <2 x i64> [[B:%.*]], i32 0, i32 1)
// CHECK-NEXT:    ret <2 x i64> [[TMP1]]
//
long2 acswapqvgri54(long2 *p, long2 a, long2 b) {
  return __builtin_kvx_acswapq(&p[4294967299], a, b, ".v.g");
}

// CHECK-LABEL: @alw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i32* [[V:%.*]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i32 @llvm.kvx.alw(i8* [[TMP0]], i32 0)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
int alw(int *v) {
  return __builtin_kvx_alw(v, "");
}

// CHECK-LABEL: @alw_ri10(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i32, i32* [[V:%.*]], i64 1
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARRAYIDX]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i32 @llvm.kvx.alw(i8* nonnull [[TMP0]], i32 0)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
int alw_ri10(int *v) {
  return __builtin_kvx_alw(&v[1], "");
}

// CHECK-LABEL: @alw_ri27(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i32, i32* [[V:%.*]], i64 1000
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARRAYIDX]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i32 @llvm.kvx.alw(i8* nonnull [[TMP0]], i32 0)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
int alw_ri27(int *v) {
  return __builtin_kvx_alw(&v[1000], "");
}

// CHECK-LABEL: @alw_ri64(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i32, i32* [[V:%.*]], i64 9000000000
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARRAYIDX]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i32 @llvm.kvx.alw(i8* nonnull [[TMP0]], i32 0)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
int alw_ri64(int *v) {
  return __builtin_kvx_alw(&v[9000000000], "");
}

// CHECK-LABEL: @alw_g(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i32* [[V:%.*]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i32 @llvm.kvx.alw(i8* [[TMP0]], i32 1)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
int alw_g(int *v) {
  return __builtin_kvx_alw(v, ".g");
}

// CHECK-LABEL: @ald(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i64* [[V:%.*]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 @llvm.kvx.ald(i8* [[TMP0]], i32 0)
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long ald(long *v) {
  return __builtin_kvx_ald(v, "");
}

// CHECK-LABEL: @ald_ri10(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i64, i64* [[V:%.*]], i64 1
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARRAYIDX]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 @llvm.kvx.ald(i8* nonnull [[TMP0]], i32 0)
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long ald_ri10(long *v) {
  return __builtin_kvx_ald(&v[1], "");
}

// CHECK-LABEL: @ald_ri27(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i64, i64* [[V:%.*]], i64 1000
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARRAYIDX]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 @llvm.kvx.ald(i8* nonnull [[TMP0]], i32 0)
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long ald_ri27(long *v) {
  return __builtin_kvx_ald(&v[1000], "");
}

// CHECK-LABEL: @ald_ri64(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i64, i64* [[V:%.*]], i64 9000000000
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARRAYIDX]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 @llvm.kvx.ald(i8* nonnull [[TMP0]], i32 0)
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long ald_ri64(long *v) {
  return __builtin_kvx_ald(&v[9000000000], "");
}

// CHECK-LABEL: @ald_g(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i64* [[V:%.*]] to i8*
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 @llvm.kvx.ald(i8* [[TMP0]], i32 1)
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long ald_g(long *v) {
  return __builtin_kvx_ald(v, ".g");
}

// CHECK-LABEL: @asw(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i32* [[P:%.*]] to i8*
// CHECK-NEXT:    tail call void @llvm.kvx.asw(i8* [[TMP0]], i32 [[V:%.*]], i32 0)
// CHECK-NEXT:    ret void
//
void asw(int *p, int v) {
  return __builtin_kvx_asw(p, v, "");
}

// CHECK-LABEL: @asw_ri10(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i32, i32* [[P:%.*]], i64 1
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARRAYIDX]] to i8*
// CHECK-NEXT:    tail call void @llvm.kvx.asw(i8* nonnull [[TMP0]], i32 [[V:%.*]], i32 0)
// CHECK-NEXT:    ret void
//
void asw_ri10(int *p, int v) {
  return __builtin_kvx_asw(&p[1], v, "");
}

// CHECK-LABEL: @asw_ri27(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i32, i32* [[P:%.*]], i64 1000
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARRAYIDX]] to i8*
// CHECK-NEXT:    tail call void @llvm.kvx.asw(i8* nonnull [[TMP0]], i32 [[V:%.*]], i32 0)
// CHECK-NEXT:    ret void
//
void asw_ri27(int *p, int v) {
  return __builtin_kvx_asw(&p[1000], v, "");
}

// CHECK-LABEL: @asw_ri54(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i32, i32* [[P:%.*]], i64 9000000000
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARRAYIDX]] to i8*
// CHECK-NEXT:    tail call void @llvm.kvx.asw(i8* nonnull [[TMP0]], i32 [[V:%.*]], i32 0)
// CHECK-NEXT:    ret void
//
void asw_ri54(int *p, int v) {
  return __builtin_kvx_asw(&p[9000000000], v, "");
}

// CHECK-LABEL: @asw_g(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i32* [[P:%.*]] to i8*
// CHECK-NEXT:    tail call void @llvm.kvx.asw(i8* [[TMP0]], i32 [[V:%.*]], i32 1)
// CHECK-NEXT:    ret void
//
void asw_g(int *p, int v) {
  return __builtin_kvx_asw(p, v, ".g");
}

// CHECK-LABEL: @asd(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i64* [[P:%.*]] to i8*
// CHECK-NEXT:    tail call void @llvm.kvx.asd(i8* [[TMP0]], i64 [[V:%.*]], i32 0)
// CHECK-NEXT:    ret void
//
void asd(long *p, long v) {
  return __builtin_kvx_asd(p, v, "");
}

// CHECK-LABEL: @asd_ri10(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i64, i64* [[P:%.*]], i64 1
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARRAYIDX]] to i8*
// CHECK-NEXT:    tail call void @llvm.kvx.asd(i8* nonnull [[TMP0]], i64 [[V:%.*]], i32 0)
// CHECK-NEXT:    ret void
//
void asd_ri10(long *p, long v) {
  return __builtin_kvx_asd(&p[1], v, "");
}

// CHECK-LABEL: @asd_ri27(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i64, i64* [[P:%.*]], i64 1000
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARRAYIDX]] to i8*
// CHECK-NEXT:    tail call void @llvm.kvx.asd(i8* nonnull [[TMP0]], i64 [[V:%.*]], i32 0)
// CHECK-NEXT:    ret void
//
void asd_ri27(long *p, long v) {
  return __builtin_kvx_asd(&p[1000], v, "");
}

// CHECK-LABEL: @asd_ri54(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i64, i64* [[P:%.*]], i64 9000000000
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARRAYIDX]] to i8*
// CHECK-NEXT:    tail call void @llvm.kvx.asd(i8* nonnull [[TMP0]], i64 [[V:%.*]], i32 0)
// CHECK-NEXT:    ret void
//
void asd_ri54(long *p, long v) {
  return __builtin_kvx_asd(&p[9000000000], v, "");
}

// CHECK-LABEL: @asd_g(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i64* [[P:%.*]] to i8*
// CHECK-NEXT:    tail call void @llvm.kvx.asd(i8* [[TMP0]], i64 [[V:%.*]], i32 1)
// CHECK-NEXT:    ret void
//
void asd_g(long *p, long v) {
  return __builtin_kvx_asd(p, v, ".g");
}

//
typedef long long8 __attribute__ ((vector_size (sizeof(long) * 8)));

// CHECK-LABEL: @dpurgel(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <8 x i64>* [[W:%.*]] to i8*
// CHECK-NEXT:    tail call void @llvm.kvx.dpurgel(i8* [[TMP0]])
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <8 x i64>, <8 x i64>* [[W]], i64 [[P:%.*]]
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x i64>* [[ARRAYIDX]] to i8*
// CHECK-NEXT:    tail call void @llvm.kvx.dpurgel(i8* [[TMP1]])
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <8 x i64>, <8 x i64>* [[W]], i64 5
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <8 x i64>* [[ARRAYIDX1]] to i8*
// CHECK-NEXT:    tail call void @llvm.kvx.dpurgel(i8* nonnull [[TMP2]])
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds <8 x i64>, <8 x i64>* [[W]], i64 1024
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x i64>* [[ARRAYIDX2]] to i8*
// CHECK-NEXT:    tail call void @llvm.kvx.dpurgel(i8* nonnull [[TMP3]])
// CHECK-NEXT:    ret void
//
void dpurgel(long8 *w, long p) {
  __builtin_kvx_dpurgel(w);
  __builtin_kvx_dpurgel(&w[p]);
  __builtin_kvx_dpurgel(&w[5]);
  __builtin_kvx_dpurgel(&w[1024]);
}

// CHECK-LABEL: @dflushl(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <8 x i64>* [[W:%.*]] to i8*
// CHECK-NEXT:    tail call void @llvm.kvx.dflushl(i8* [[TMP0]])
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <8 x i64>, <8 x i64>* [[W]], i64 [[P:%.*]]
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x i64>* [[ARRAYIDX]] to i8*
// CHECK-NEXT:    tail call void @llvm.kvx.dflushl(i8* [[TMP1]])
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds <8 x i64>, <8 x i64>* [[W]], i64 5
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <8 x i64>* [[ARRAYIDX1]] to i8*
// CHECK-NEXT:    tail call void @llvm.kvx.dflushl(i8* nonnull [[TMP2]])
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds <8 x i64>, <8 x i64>* [[W]], i64 1024
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x i64>* [[ARRAYIDX2]] to i8*
// CHECK-NEXT:    tail call void @llvm.kvx.dflushl(i8* nonnull [[TMP3]])
// CHECK-NEXT:    ret void
//
void dflushl(long8 *w, long p) {
  __builtin_kvx_dflushl(w);
  __builtin_kvx_dflushl(&w[p]);
  __builtin_kvx_dflushl(&w[5]);
  __builtin_kvx_dflushl(&w[1024]);
}

typedef unsigned char uchar8 __attribute__ ((vector_size (sizeof(char) * 8)));
typedef unsigned short ushort4 __attribute__ ((vector_size (sizeof(short) * 4)));
typedef unsigned int uint2 __attribute__ ((vector_size (sizeof(int) * 2)));

// CHECK-LABEL: @zxlbhq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <4 x i16> @llvm.kvx.zxlbhq(<8 x i8> [[V:%.*]])
// CHECK-NEXT:    ret <4 x i16> [[TMP0]]
//
ushort4 zxlbhq(uchar8 v){
  return __builtin_kvx_zxlbhq(v);
}

// CHECK-LABEL: @zxmbhq(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <4 x i16> @llvm.kvx.zxmbhq(<8 x i8> [[V:%.*]])
// CHECK-NEXT:    ret <4 x i16> [[TMP0]]
//
ushort4 zxmbhq(uchar8 v){
  return __builtin_kvx_zxmbhq(v);
}

// CHECK-LABEL: @zxlhwp(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.kvx.zxlhwp(<4 x i16> [[V:%.*]])
// CHECK-NEXT:    ret <2 x i32> [[TMP0]]
//
uint2 zxlhwp(ushort4 v){
  return __builtin_kvx_zxlhwp(v);
}

// CHECK-LABEL: @zxmhwp(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.kvx.zxmhwp(<4 x i16> [[V:%.*]])
// CHECK-NEXT:    ret <2 x i32> [[TMP0]]
//
uint2 zxmhwp(ushort4 v){
  return __builtin_kvx_zxmhwp(v);
}
