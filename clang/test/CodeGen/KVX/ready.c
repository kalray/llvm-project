// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -triple kvx-kalray-cos -emit-llvm %s -O3 -o - | FileCheck %s

// Generated from base/ready.C

typedef char __attribute__((__vector_size__(2 * sizeof(char)))) v2i8;
typedef char __attribute__((__vector_size__(4 * sizeof(char)))) v4i8;
typedef char __attribute__((__vector_size__(8 * sizeof(char)))) v8i8;

typedef short __attribute__((__vector_size__(2 * sizeof(short)))) v2i16;
typedef short __attribute__((__vector_size__(4 * sizeof(short)))) v4i16;
typedef short __attribute__((__vector_size__(8 * sizeof(short)))) v8i16;

typedef _Float16 __attribute__((__vector_size__(2 * sizeof(_Float16)))) v2f16;
typedef _Float16 __attribute__((__vector_size__(4 * sizeof(_Float16)))) v4f16;

typedef int __attribute__((__vector_size__(2 * sizeof(int)))) v2i32;
typedef int __attribute__((__vector_size__(4 * sizeof(int)))) v4i32;
typedef int __attribute__((__vector_size__(8 * sizeof(int)))) v8i32;

typedef float __attribute__((__vector_size__(2 * sizeof(float)))) v2f32;
typedef float __attribute__((__vector_size__(4 * sizeof(float)))) v4f32;
typedef float __attribute__((__vector_size__(8 * sizeof(float)))) v8f32;

typedef long __attribute__((__vector_size__(2 * sizeof(long)))) v2i64;
typedef long __attribute__((__vector_size__(4 * sizeof(long)))) v4i64;

typedef double __attribute__((__vector_size__(2 * sizeof(double)))) v2f64;
typedef double __attribute__((__vector_size__(4 * sizeof(double)))) v4f64;

// CHECK-LABEL: @ready_int(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, i32* [[ADDR:%.*]], align 4, [[TBAA2:!tbaa !.*]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[TMP0]])
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long ready_int(int *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_long(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i64, i64* [[ADDR:%.*]], align 8, [[TBAA6:!tbaa !.*]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(i64 [[TMP0]])
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long ready_long(long *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_v2i8(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <2 x i8>* [[ADDR:%.*]] to half*
// CHECK-NEXT:    [[TMP1:%.*]] = load half, half* [[TMP0]], align 2, [[TBAA8:!tbaa !.*]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 (...) @llvm.kvx.ready(half [[TMP1]])
// CHECK-NEXT:    ret i64 [[TMP2]]
//
long ready_v2i8(v2i8 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_v2i16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <2 x i16>* [[ADDR:%.*]] to i32*
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, i32* [[TMP0]], align 4, [[TBAA8]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[TMP1]])
// CHECK-NEXT:    ret i64 [[TMP2]]
//
long ready_v2i16(v2i16 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_v2i32(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <2 x i32>* [[ADDR:%.*]] to i64*
// CHECK-NEXT:    [[TMP1:%.*]] = load i64, i64* [[TMP0]], align 8, [[TBAA8]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 (...) @llvm.kvx.ready(i64 [[TMP1]])
// CHECK-NEXT:    ret i64 [[TMP2]]
//
long ready_v2i32(v2i32 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_v2i64(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i64>, <2 x i64>* [[ADDR:%.*]], align 16, [[TBAA8]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(<2 x i64> [[TMP0]])
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long ready_v2i64(v2i64 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_v4i8(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <4 x i8>* [[ADDR:%.*]] to i32*
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, i32* [[TMP0]], align 4, [[TBAA8]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[TMP1]])
// CHECK-NEXT:    ret i64 [[TMP2]]
//
long ready_v4i8(v4i8 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_v4i16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <4 x i16>* [[ADDR:%.*]] to i64*
// CHECK-NEXT:    [[TMP1:%.*]] = load i64, i64* [[TMP0]], align 8, [[TBAA8]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 (...) @llvm.kvx.ready(i64 [[TMP1]])
// CHECK-NEXT:    ret i64 [[TMP2]]
//
long ready_v4i16(v4i16 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_v4i32(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <4 x i32>* [[ADDR:%.*]] to <2 x i64>*
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i64>, <2 x i64>* [[TMP0]], align 16, [[TBAA8]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 (...) @llvm.kvx.ready(<2 x i64> [[TMP1]])
// CHECK-NEXT:    ret i64 [[TMP2]]
//
long ready_v4i32(v4i32 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_v4i64(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i64>, <4 x i64>* [[ADDR:%.*]], align 32, [[TBAA8]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(<4 x i64> [[TMP0]])
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long ready_v4i64(v4i64 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_v8i8(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <8 x i8>* [[ADDR:%.*]] to i64*
// CHECK-NEXT:    [[TMP1:%.*]] = load i64, i64* [[TMP0]], align 8, [[TBAA8]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 (...) @llvm.kvx.ready(i64 [[TMP1]])
// CHECK-NEXT:    ret i64 [[TMP2]]
//
long ready_v8i8(v8i8 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready__Float16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load half, half* [[ADDR:%.*]], align 2, [[TBAA9:!tbaa !.*]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(half [[TMP0]])
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long ready__Float16(_Float16 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_float(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load float, float* [[ADDR:%.*]], align 4, [[TBAA11:!tbaa !.*]]
// CHECK-NEXT:    [[CONV:%.*]] = fpext float [[TMP0]] to double
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast double [[CONV]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 (...) @llvm.kvx.ready(i64 [[TMP1]])
// CHECK-NEXT:    ret i64 [[TMP2]]
//
long ready_float(float *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_double(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast double* [[ADDR:%.*]] to i64*
// CHECK-NEXT:    [[TMP1:%.*]] = load i64, i64* [[TMP0]], align 8, [[TBAA13:!tbaa !.*]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 (...) @llvm.kvx.ready(i64 [[TMP1]])
// CHECK-NEXT:    ret i64 [[TMP2]]
//
long ready_double(double *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_v2f16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <2 x half>* [[ADDR:%.*]] to i32*
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, i32* [[TMP0]], align 4, [[TBAA8]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[TMP1]])
// CHECK-NEXT:    ret i64 [[TMP2]]
//
long ready_v2f16(v2f16 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_v2f32(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <2 x float>* [[ADDR:%.*]] to i64*
// CHECK-NEXT:    [[TMP1:%.*]] = load i64, i64* [[TMP0]], align 8, [[TBAA8]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 (...) @llvm.kvx.ready(i64 [[TMP1]])
// CHECK-NEXT:    ret i64 [[TMP2]]
//
long ready_v2f32(v2f32 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_v2f64(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <2 x double>* [[ADDR:%.*]] to <2 x i64>*
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i64>, <2 x i64>* [[TMP0]], align 16, [[TBAA8]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 (...) @llvm.kvx.ready(<2 x i64> [[TMP1]])
// CHECK-NEXT:    ret i64 [[TMP2]]
//
long ready_v2f64(v2f64 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_v4f16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <4 x half>* [[ADDR:%.*]] to i64*
// CHECK-NEXT:    [[TMP1:%.*]] = load i64, i64* [[TMP0]], align 8, [[TBAA8]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 (...) @llvm.kvx.ready(i64 [[TMP1]])
// CHECK-NEXT:    ret i64 [[TMP2]]
//
long ready_v4f16(v4f16 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_v4f32(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <4 x float>* [[ADDR:%.*]] to <2 x i64>*
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i64>, <2 x i64>* [[TMP0]], align 16, [[TBAA8]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 (...) @llvm.kvx.ready(<2 x i64> [[TMP1]])
// CHECK-NEXT:    ret i64 [[TMP2]]
//
long ready_v4f32(v4f32 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_v4f64(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <4 x double>* [[ADDR:%.*]] to <4 x i64>*
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x i64>, <4 x i64>* [[TMP0]], align 32, [[TBAA8]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 (...) @llvm.kvx.ready(<4 x i64> [[TMP1]])
// CHECK-NEXT:    ret i64 [[TMP2]]
//
long ready_v4f64(v4f64 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready___int128(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i128* [[ADDR:%.*]] to <2 x i64>*
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i64>, <2 x i64>* [[TMP0]], align 16, [[TBAA15:!tbaa !.*]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 (...) @llvm.kvx.ready(<2 x i64> [[TMP1]])
// CHECK-NEXT:    ret i64 [[TMP2]]
//
long ready___int128(__int128 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_v8i16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <8 x i16>* [[ADDR:%.*]] to <2 x i64>*
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i64>, <2 x i64>* [[TMP0]], align 16, [[TBAA8]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 (...) @llvm.kvx.ready(<2 x i64> [[TMP1]])
// CHECK-NEXT:    ret i64 [[TMP2]]
//
long ready_v8i16(v8i16 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_v8i32(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <8 x i32>* [[ADDR:%.*]] to <4 x i64>*
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x i64>, <4 x i64>* [[TMP0]], align 32, [[TBAA8]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 (...) @llvm.kvx.ready(<4 x i64> [[TMP1]])
// CHECK-NEXT:    ret i64 [[TMP2]]
//
long ready_v8i32(v8i32 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_char(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, i8* [[ADDR:%.*]], align 1, [[TBAA8]]
// CHECK-NEXT:    [[CONV:%.*]] = sext i8 [[TMP0]] to i32
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[CONV]])
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long ready_char(char *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_int_v4f32(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, i32* [[ADDR1:%.*]], align 4, [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <4 x float>* [[ADDR2:%.*]] to <2 x i64>*
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x i64>, <2 x i64>* [[TMP1]], align 16, [[TBAA8]]
// CHECK-NEXT:    [[TMP3:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[TMP0]], <2 x i64> [[TMP2]])
// CHECK-NEXT:    ret i64 [[TMP3]]
//
long ready_int_v4f32(int *addr1, v4f32 *addr2) { return __builtin_kvx_ready(*addr1, *addr2); }

// CHECK-LABEL: @ready_long_int(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i64, i64* [[ADDR1:%.*]], align 8, [[TBAA6]]
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, i32* [[ADDR2:%.*]], align 4, [[TBAA2]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 (...) @llvm.kvx.ready(i64 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i64 [[TMP2]]
//
long ready_long_int(long *addr1, int *addr2) { return __builtin_kvx_ready(*addr1, *addr2); }

// CHECK-LABEL: @ready_float_v8i8(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load float, float* [[ADDR1:%.*]], align 4, [[TBAA11]]
// CHECK-NEXT:    [[CONV:%.*]] = fpext float [[TMP0]] to double
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast double [[CONV]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <8 x i8>* [[ADDR2:%.*]] to i64*
// CHECK-NEXT:    [[TMP3:%.*]] = load i64, i64* [[TMP2]], align 8, [[TBAA8]]
// CHECK-NEXT:    [[TMP4:%.*]] = tail call i64 (...) @llvm.kvx.ready(i64 [[TMP1]], i64 [[TMP3]])
// CHECK-NEXT:    ret i64 [[TMP4]]
//
long ready_float_v8i8(float *addr1, v8i8 *addr2) { return __builtin_kvx_ready(*addr1, *addr2); }

// CHECK-LABEL: @ready_int_long_float(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, i32* [[ADDR1:%.*]], align 4, [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = load i64, i64* [[ADDR2:%.*]], align 8, [[TBAA6]]
// CHECK-NEXT:    [[TMP2:%.*]] = load float, float* [[ADDR3:%.*]], align 4, [[TBAA11]]
// CHECK-NEXT:    [[CONV:%.*]] = fpext float [[TMP2]] to double
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast double [[CONV]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[TMP0]], i64 [[TMP1]], i64 [[TMP3]])
// CHECK-NEXT:    ret i64 [[TMP4]]
//
long ready_int_long_float(int *addr1, long *addr2, float *addr3) { return __builtin_kvx_ready(*addr1, *addr2, *addr3); }

// CHECK-LABEL: @ready___int128_v8i8_v2i64(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i128* [[ADDR1:%.*]] to <2 x i64>*
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i64>, <2 x i64>* [[TMP0]], align 16, [[TBAA15]]
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <8 x i8>* [[ADDR2:%.*]] to i64*
// CHECK-NEXT:    [[TMP3:%.*]] = load i64, i64* [[TMP2]], align 8, [[TBAA8]]
// CHECK-NEXT:    [[TMP4:%.*]] = load <2 x i64>, <2 x i64>* [[ADDR3:%.*]], align 16, [[TBAA8]]
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i64 (...) @llvm.kvx.ready(<2 x i64> [[TMP1]], i64 [[TMP3]], <2 x i64> [[TMP4]])
// CHECK-NEXT:    ret i64 [[TMP5]]
//
long ready___int128_v8i8_v2i64(__int128 *addr1, v8i8 *addr2, v2i64 *addr3) { return __builtin_kvx_ready(*addr1, *addr2, *addr3); }

// CHECK-LABEL: @ready_char_short_double(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, i8* [[ADDR1:%.*]], align 1, [[TBAA8]]
// CHECK-NEXT:    [[CONV:%.*]] = sext i8 [[TMP0]] to i32
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, i16* [[ADDR2:%.*]], align 2, [[TBAA17:!tbaa !.*]]
// CHECK-NEXT:    [[CONV1:%.*]] = sext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast double* [[ADDR3:%.*]] to i64*
// CHECK-NEXT:    [[TMP3:%.*]] = load i64, i64* [[TMP2]], align 8, [[TBAA13]]
// CHECK-NEXT:    [[TMP4:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[CONV]], i32 [[CONV1]], i64 [[TMP3]])
// CHECK-NEXT:    ret i64 [[TMP4]]
//
long ready_char_short_double(char *addr1, short *addr2, double *addr3) { return __builtin_kvx_ready(*addr1, *addr2, *addr3); }

// CHECK-LABEL: @ready_char_short_int_long(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, i8* [[ADDR1:%.*]], align 1, [[TBAA8]]
// CHECK-NEXT:    [[CONV:%.*]] = sext i8 [[TMP0]] to i32
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, i16* [[ADDR2:%.*]], align 2, [[TBAA17]]
// CHECK-NEXT:    [[CONV1:%.*]] = sext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, i32* [[ADDR3:%.*]], align 4, [[TBAA2]]
// CHECK-NEXT:    [[TMP3:%.*]] = load i64, i64* [[ADDR4:%.*]], align 8, [[TBAA6]]
// CHECK-NEXT:    [[TMP4:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[CONV]], i32 [[CONV1]], i32 [[TMP2]], i64 [[TMP3]])
// CHECK-NEXT:    ret i64 [[TMP4]]
//
long ready_char_short_int_long(char *addr1, short *addr2, int *addr3, long *addr4) { return __builtin_kvx_ready(*addr1, *addr2, *addr3, *addr4); }

// CHECK-LABEL: @ready__Float16_float_double_v4i64(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load half, half* [[ADDR1:%.*]], align 2, [[TBAA9]]
// CHECK-NEXT:    [[TMP1:%.*]] = load float, float* [[ADDR2:%.*]], align 4, [[TBAA11]]
// CHECK-NEXT:    [[CONV:%.*]] = fpext float [[TMP1]] to double
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast double [[CONV]] to i64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast double* [[ADDR3:%.*]] to i64*
// CHECK-NEXT:    [[TMP4:%.*]] = load i64, i64* [[TMP3]], align 8, [[TBAA13]]
// CHECK-NEXT:    [[TMP5:%.*]] = load <4 x i64>, <4 x i64>* [[ADDR4:%.*]], align 32, [[TBAA8]]
// CHECK-NEXT:    [[TMP6:%.*]] = tail call i64 (...) @llvm.kvx.ready(half [[TMP0]], i64 [[TMP2]], i64 [[TMP4]], <4 x i64> [[TMP5]])
// CHECK-NEXT:    ret i64 [[TMP6]]
//
long ready__Float16_float_double_v4i64(_Float16 *addr1, float *addr2, double *addr3, v4i64 *addr4) { return __builtin_kvx_ready(*addr1, *addr2, *addr3, *addr4); }

// CHECK-LABEL: @ready_v8f32_v4i32___int128_char(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <8 x float>* [[ADDR1:%.*]] to <4 x i64>*
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x i64>, <4 x i64>* [[TMP0]], align 32, [[TBAA8]]
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <4 x i32>* [[ADDR2:%.*]] to <2 x i64>*
// CHECK-NEXT:    [[TMP3:%.*]] = load <2 x i64>, <2 x i64>* [[TMP2]], align 16, [[TBAA8]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i128* [[ADDR3:%.*]] to <2 x i64>*
// CHECK-NEXT:    [[TMP5:%.*]] = load <2 x i64>, <2 x i64>* [[TMP4]], align 16, [[TBAA15]]
// CHECK-NEXT:    [[TMP6:%.*]] = load i8, i8* [[ADDR4:%.*]], align 1, [[TBAA8]]
// CHECK-NEXT:    [[CONV:%.*]] = sext i8 [[TMP6]] to i32
// CHECK-NEXT:    [[TMP7:%.*]] = tail call i64 (...) @llvm.kvx.ready(<4 x i64> [[TMP1]], <2 x i64> [[TMP3]], <2 x i64> [[TMP5]], i32 [[CONV]])
// CHECK-NEXT:    ret i64 [[TMP7]]
//
long ready_v8f32_v4i32___int128_char(v8f32 *addr1, v4i32 *addr2, __int128 *addr3, char *addr4) { return __builtin_kvx_ready(*addr1, *addr2, *addr3, *addr4); }
