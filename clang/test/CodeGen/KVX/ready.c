// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -triple kvx-kalray-cos -emit-llvm %s -O3 -o - | FileCheck %s

// Generated from base/ready.C

typedef char __attribute__((__vector_size__(2 * sizeof(char)))) v2i8;
typedef char __attribute__((__vector_size__(4 * sizeof(char)))) v4i8;
typedef char __attribute__((__vector_size__(8 * sizeof(char)))) v8i8;

typedef short __attribute__((__vector_size__(2 * sizeof(short)))) v2i16;
typedef short __attribute__((__vector_size__(4 * sizeof(short)))) v4i16;
typedef short __attribute__((__vector_size__(8 * sizeof(short)))) v8i16;

typedef _Float16 __attribute__((__vector_size__(2 * sizeof(_Float16)))) v2f16;
typedef _Float16 __attribute__((__vector_size__(4 * sizeof(_Float16)))) v4f16;

typedef int __attribute__((__vector_size__(2 * sizeof(int)))) v2i32;
typedef int __attribute__((__vector_size__(4 * sizeof(int)))) v4i32;
typedef int __attribute__((__vector_size__(8 * sizeof(int)))) v8i32;

typedef float __attribute__((__vector_size__(2 * sizeof(float)))) v2f32;
typedef float __attribute__((__vector_size__(4 * sizeof(float)))) v4f32;
typedef float __attribute__((__vector_size__(8 * sizeof(float)))) v8f32;

typedef long __attribute__((__vector_size__(2 * sizeof(long)))) v2i64;
typedef long __attribute__((__vector_size__(4 * sizeof(long)))) v4i64;

typedef double __attribute__((__vector_size__(2 * sizeof(double)))) v2f64;
typedef double __attribute__((__vector_size__(4 * sizeof(double)))) v4f64;

// CHECK-LABEL: @ready_int(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[ADDR:%.*]], align 4, !tbaa [[TBAA2:![0-9]+]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[TMP0]])
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long ready_int(int *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_long(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i64, ptr [[ADDR:%.*]], align 8, !tbaa [[TBAA6:![0-9]+]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(i64 [[TMP0]])
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long ready_long(long *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_v2i8(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load half, ptr [[ADDR:%.*]], align 2, !tbaa [[TBAA8:![0-9]+]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(half [[TMP0]])
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long ready_v2i8(v2i8 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_v2i16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[ADDR:%.*]], align 4, !tbaa [[TBAA8]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[TMP0]])
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long ready_v2i16(v2i16 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_v2i32(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i64, ptr [[ADDR:%.*]], align 8, !tbaa [[TBAA8]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(i64 [[TMP0]])
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long ready_v2i32(v2i32 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_v2i64(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[ADDR:%.*]], align 16, !tbaa [[TBAA8]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(<2 x i64> [[TMP0]])
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long ready_v2i64(v2i64 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_v4i8(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[ADDR:%.*]], align 4, !tbaa [[TBAA8]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[TMP0]])
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long ready_v4i8(v4i8 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_v4i16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i64, ptr [[ADDR:%.*]], align 8, !tbaa [[TBAA8]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(i64 [[TMP0]])
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long ready_v4i16(v4i16 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_v4i32(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[ADDR:%.*]], align 16, !tbaa [[TBAA8]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(<2 x i64> [[TMP0]])
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long ready_v4i32(v4i32 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_v4i64(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i64>, ptr [[ADDR:%.*]], align 32, !tbaa [[TBAA8]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(<4 x i64> [[TMP0]])
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long ready_v4i64(v4i64 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_v8i8(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i64, ptr [[ADDR:%.*]], align 8, !tbaa [[TBAA8]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(i64 [[TMP0]])
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long ready_v8i8(v8i8 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready__Float16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load half, ptr [[ADDR:%.*]], align 2, !tbaa [[TBAA9:![0-9]+]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(half [[TMP0]])
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long ready__Float16(_Float16 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_float(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load float, ptr [[ADDR:%.*]], align 4, !tbaa [[TBAA11:![0-9]+]]
// CHECK-NEXT:    [[CONV:%.*]] = fpext float [[TMP0]] to double
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast double [[CONV]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 (...) @llvm.kvx.ready(i64 [[TMP1]])
// CHECK-NEXT:    ret i64 [[TMP2]]
//
long ready_float(float *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_double(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i64, ptr [[ADDR:%.*]], align 8, !tbaa [[TBAA13:![0-9]+]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(i64 [[TMP0]])
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long ready_double(double *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_v2f16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[ADDR:%.*]], align 4, !tbaa [[TBAA8]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[TMP0]])
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long ready_v2f16(v2f16 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_v2f32(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i64, ptr [[ADDR:%.*]], align 8, !tbaa [[TBAA8]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(i64 [[TMP0]])
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long ready_v2f32(v2f32 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_v2f64(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[ADDR:%.*]], align 16, !tbaa [[TBAA8]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(<2 x i64> [[TMP0]])
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long ready_v2f64(v2f64 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_v4f16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i64, ptr [[ADDR:%.*]], align 8, !tbaa [[TBAA8]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(i64 [[TMP0]])
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long ready_v4f16(v4f16 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_v4f32(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[ADDR:%.*]], align 16, !tbaa [[TBAA8]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(<2 x i64> [[TMP0]])
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long ready_v4f32(v4f32 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_v4f64(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i64>, ptr [[ADDR:%.*]], align 32, !tbaa [[TBAA8]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(<4 x i64> [[TMP0]])
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long ready_v4f64(v4f64 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready___int128(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[ADDR:%.*]], align 16, !tbaa [[TBAA15:![0-9]+]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(<2 x i64> [[TMP0]])
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long ready___int128(__int128 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_v8i16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[ADDR:%.*]], align 16, !tbaa [[TBAA8]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(<2 x i64> [[TMP0]])
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long ready_v8i16(v8i16 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_v8i32(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i64>, ptr [[ADDR:%.*]], align 32, !tbaa [[TBAA8]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(<4 x i64> [[TMP0]])
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long ready_v8i32(v8i32 *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_char(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[ADDR:%.*]], align 1, !tbaa [[TBAA8]]
// CHECK-NEXT:    [[CONV:%.*]] = sext i8 [[TMP0]] to i32
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[CONV]])
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long ready_char(char *addr) { return __builtin_kvx_ready(*addr); }

// CHECK-LABEL: @ready_int_v4f32(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[ADDR1:%.*]], align 4, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[ADDR2:%.*]], align 16, !tbaa [[TBAA8]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[TMP0]], <2 x i64> [[TMP1]])
// CHECK-NEXT:    ret i64 [[TMP2]]
//
long ready_int_v4f32(int *addr1, v4f32 *addr2) { return __builtin_kvx_ready(*addr1, *addr2); }

// CHECK-LABEL: @ready_long_int(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i64, ptr [[ADDR1:%.*]], align 8, !tbaa [[TBAA6]]
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[ADDR2:%.*]], align 4, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call i64 (...) @llvm.kvx.ready(i64 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i64 [[TMP2]]
//
long ready_long_int(long *addr1, int *addr2) { return __builtin_kvx_ready(*addr1, *addr2); }

// CHECK-LABEL: @ready_float_v8i8(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load float, ptr [[ADDR1:%.*]], align 4, !tbaa [[TBAA11]]
// CHECK-NEXT:    [[CONV:%.*]] = fpext float [[TMP0]] to double
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast double [[CONV]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = load i64, ptr [[ADDR2:%.*]], align 8, !tbaa [[TBAA8]]
// CHECK-NEXT:    [[TMP3:%.*]] = tail call i64 (...) @llvm.kvx.ready(i64 [[TMP1]], i64 [[TMP2]])
// CHECK-NEXT:    ret i64 [[TMP3]]
//
long ready_float_v8i8(float *addr1, v8i8 *addr2) { return __builtin_kvx_ready(*addr1, *addr2); }

// CHECK-LABEL: @ready_int_long_float(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[ADDR1:%.*]], align 4, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = load i64, ptr [[ADDR2:%.*]], align 8, !tbaa [[TBAA6]]
// CHECK-NEXT:    [[TMP2:%.*]] = load float, ptr [[ADDR3:%.*]], align 4, !tbaa [[TBAA11]]
// CHECK-NEXT:    [[CONV:%.*]] = fpext float [[TMP2]] to double
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast double [[CONV]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[TMP0]], i64 [[TMP1]], i64 [[TMP3]])
// CHECK-NEXT:    ret i64 [[TMP4]]
//
long ready_int_long_float(int *addr1, long *addr2, float *addr3) { return __builtin_kvx_ready(*addr1, *addr2, *addr3); }

// CHECK-LABEL: @ready___int128_v8i8_v2i64(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[ADDR1:%.*]], align 16, !tbaa [[TBAA15]]
// CHECK-NEXT:    [[TMP1:%.*]] = load i64, ptr [[ADDR2:%.*]], align 8, !tbaa [[TBAA8]]
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[ADDR3:%.*]], align 16, !tbaa [[TBAA8]]
// CHECK-NEXT:    [[TMP3:%.*]] = tail call i64 (...) @llvm.kvx.ready(<2 x i64> [[TMP0]], i64 [[TMP1]], <2 x i64> [[TMP2]])
// CHECK-NEXT:    ret i64 [[TMP3]]
//
long ready___int128_v8i8_v2i64(__int128 *addr1, v8i8 *addr2, v2i64 *addr3) { return __builtin_kvx_ready(*addr1, *addr2, *addr3); }

// CHECK-LABEL: @ready_char_short_double(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[ADDR1:%.*]], align 1, !tbaa [[TBAA8]]
// CHECK-NEXT:    [[CONV:%.*]] = sext i8 [[TMP0]] to i32
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[ADDR2:%.*]], align 2, !tbaa [[TBAA17:![0-9]+]]
// CHECK-NEXT:    [[CONV1:%.*]] = sext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = load i64, ptr [[ADDR3:%.*]], align 8, !tbaa [[TBAA13]]
// CHECK-NEXT:    [[TMP3:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[CONV]], i32 [[CONV1]], i64 [[TMP2]])
// CHECK-NEXT:    ret i64 [[TMP3]]
//
long ready_char_short_double(char *addr1, short *addr2, double *addr3) { return __builtin_kvx_ready(*addr1, *addr2, *addr3); }

// CHECK-LABEL: @ready_char_short_int_long(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[ADDR1:%.*]], align 1, !tbaa [[TBAA8]]
// CHECK-NEXT:    [[CONV:%.*]] = sext i8 [[TMP0]] to i32
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[ADDR2:%.*]], align 2, !tbaa [[TBAA17]]
// CHECK-NEXT:    [[CONV1:%.*]] = sext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[ADDR3:%.*]], align 4, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP3:%.*]] = load i64, ptr [[ADDR4:%.*]], align 8, !tbaa [[TBAA6]]
// CHECK-NEXT:    [[TMP4:%.*]] = tail call i64 (...) @llvm.kvx.ready(i32 [[CONV]], i32 [[CONV1]], i32 [[TMP2]], i64 [[TMP3]])
// CHECK-NEXT:    ret i64 [[TMP4]]
//
long ready_char_short_int_long(char *addr1, short *addr2, int *addr3, long *addr4) { return __builtin_kvx_ready(*addr1, *addr2, *addr3, *addr4); }

// CHECK-LABEL: @ready__Float16_float_double_v4i64(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load half, ptr [[ADDR1:%.*]], align 2, !tbaa [[TBAA9]]
// CHECK-NEXT:    [[TMP1:%.*]] = load float, ptr [[ADDR2:%.*]], align 4, !tbaa [[TBAA11]]
// CHECK-NEXT:    [[CONV:%.*]] = fpext float [[TMP1]] to double
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast double [[CONV]] to i64
// CHECK-NEXT:    [[TMP3:%.*]] = load i64, ptr [[ADDR3:%.*]], align 8, !tbaa [[TBAA13]]
// CHECK-NEXT:    [[TMP4:%.*]] = load <4 x i64>, ptr [[ADDR4:%.*]], align 32, !tbaa [[TBAA8]]
// CHECK-NEXT:    [[TMP5:%.*]] = tail call i64 (...) @llvm.kvx.ready(half [[TMP0]], i64 [[TMP2]], i64 [[TMP3]], <4 x i64> [[TMP4]])
// CHECK-NEXT:    ret i64 [[TMP5]]
//
long ready__Float16_float_double_v4i64(_Float16 *addr1, float *addr2, double *addr3, v4i64 *addr4) { return __builtin_kvx_ready(*addr1, *addr2, *addr3, *addr4); }

// CHECK-LABEL: @ready_v8f32_v4i32___int128_char(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i64>, ptr [[ADDR1:%.*]], align 32, !tbaa [[TBAA8]]
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[ADDR2:%.*]], align 16, !tbaa [[TBAA8]]
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[ADDR3:%.*]], align 16, !tbaa [[TBAA15]]
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[ADDR4:%.*]], align 1, !tbaa [[TBAA8]]
// CHECK-NEXT:    [[CONV:%.*]] = sext i8 [[TMP3]] to i32
// CHECK-NEXT:    [[TMP4:%.*]] = tail call i64 (...) @llvm.kvx.ready(<4 x i64> [[TMP0]], <2 x i64> [[TMP1]], <2 x i64> [[TMP2]], i32 [[CONV]])
// CHECK-NEXT:    ret i64 [[TMP4]]
//
long ready_v8f32_v4i32___int128_char(v8f32 *addr1, v4i32 *addr2, __int128 *addr3, char *addr4) { return __builtin_kvx_ready(*addr1, *addr2, *addr3, *addr4); }
