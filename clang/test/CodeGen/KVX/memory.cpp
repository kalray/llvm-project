// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -x c++ -triple kvx-kalray-cos -S -O2 -emit-llvm -o - %s | FileCheck %s

// CHECK-LABEL: @_Z7d1invalv(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    tail call void @llvm.kvx.d1inval()
// CHECK-NEXT:    ret void
//
void d1inval() {
  __builtin_kvx_d1inval();
}

// CHECK-LABEL: @_Z7dinvallPc(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    tail call void @llvm.kvx.dinvall(ptr [[P:%.*]])
// CHECK-NEXT:    ret void
//
void dinvall(char *p) {
  __builtin_kvx_dinvall(p);
}

// CHECK-LABEL: @_Z7dtouchlPc(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    tail call void @llvm.kvx.dtouchl(ptr [[P:%.*]])
// CHECK-NEXT:    ret void
//
void dtouchl(char *p) {
  __builtin_kvx_dtouchl(p);
}

// CHECK-LABEL: @_Z6dzerolPc(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    tail call void @llvm.kvx.dzerol(ptr [[P:%.*]])
// CHECK-NEXT:    ret void
//
void dzerol(char *p) {
  __builtin_kvx_dzerol(p);
}

// CHECK-LABEL: @_Z5fencev(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    tail call void @llvm.kvx.fence(i32 0)
// CHECK-NEXT:    ret void
//
void fence(void) {
  __builtin_kvx_fence();
}

// CHECK-LABEL: @_Z7i1invalv(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    tail call void @llvm.kvx.i1inval()
// CHECK-NEXT:    ret void
//
void i1inval(void) {
  __builtin_kvx_i1inval();
}

// CHECK-LABEL: @_Z8i1invalsPc(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    tail call void @llvm.kvx.i1invals(ptr [[P:%.*]])
// CHECK-NEXT:    ret void
//
void i1invals(char *p) {
  __builtin_kvx_i1invals(p);
}

// CHECK-LABEL: @_Z9tlbdinvalv(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    tail call void @llvm.kvx.tlbdinval()
// CHECK-NEXT:    ret void
//
void tlbdinval(void) {
  __builtin_kvx_tlbdinval();
}

// CHECK-LABEL: @_Z9tlbiinvalv(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    tail call void @llvm.kvx.tlbiinval()
// CHECK-NEXT:    ret void
//
void tlbiinval(void) {
  __builtin_kvx_tlbiinval();
}

// CHECK-LABEL: @_Z8tlbprobev(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    tail call void @llvm.kvx.tlbprobe()
// CHECK-NEXT:    ret void
//
void tlbprobe(void) {
  __builtin_kvx_tlbprobe();
}

// CHECK-LABEL: @_Z7tlbreadv(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    tail call void @llvm.kvx.tlbread()
// CHECK-NEXT:    ret void
//
void tlbread(void) {
  __builtin_kvx_tlbread();
}

// CHECK-LABEL: @_Z8tlbwritev(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    tail call void @llvm.kvx.tlbwrite()
// CHECK-NEXT:    ret void
//
void tlbwrite(void) {
  __builtin_kvx_tlbwrite();
}

// CHECK-LABEL: @_Z3lbzPv(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = addrspacecast ptr [[P:%.*]] to ptr addrspace(256)
// CHECK-NEXT:    [[KVXLD:%.*]] = load i8, ptr addrspace(256) [[TMP0]], align 1
// CHECK-NEXT:    ret i8 [[KVXLD]]
//
unsigned char lbz(void *p) {
  return __builtin_kvx_lbz(p, ".u", false);
}

// CHECK-LABEL: @_Z3lbsPv(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = addrspacecast ptr [[P:%.*]] to ptr addrspace(256)
// CHECK-NEXT:    [[KVXLD:%.*]] = load i8, ptr addrspace(256) [[TMP0]], align 1
// CHECK-NEXT:    [[CONV:%.*]] = sext i8 [[KVXLD]] to i32
// CHECK-NEXT:    ret i32 [[CONV]]
//
int lbs(void *p) {
  return __builtin_kvx_lbs(p, ".u", false);
}

// CHECK-LABEL: @_Z3lhzPv(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = addrspacecast ptr [[P:%.*]] to ptr addrspace(256)
// CHECK-NEXT:    [[KVXLD:%.*]] = load i16, ptr addrspace(256) [[TMP0]], align 1
// CHECK-NEXT:    ret i16 [[KVXLD]]
//
unsigned short lhz(void *p) {
  return __builtin_kvx_lhz(p, ".u", false);
}

// CHECK-LABEL: @_Z3lhsPv(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = addrspacecast ptr [[P:%.*]] to ptr addrspace(256)
// CHECK-NEXT:    [[KVXLD:%.*]] = load i16, ptr addrspace(256) [[TMP0]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = sext i16 [[KVXLD]] to i64
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long lhs(void *p) {
  return __builtin_kvx_lhs(p, ".u", false);
}

// CHECK-LABEL: @_Z3lwzPv(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = addrspacecast ptr [[P:%.*]] to ptr addrspace(256)
// CHECK-NEXT:    [[KVXLD:%.*]] = load i32, ptr addrspace(256) [[TMP0]], align 1
// CHECK-NEXT:    ret i32 [[KVXLD]]
//
unsigned int lwz(void *p) {
  return __builtin_kvx_lwz(p, ".u", false);
}

// CHECK-LABEL: @_Z3lwsPv(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = addrspacecast ptr [[P:%.*]] to ptr addrspace(256)
// CHECK-NEXT:    [[KVXLD:%.*]] = load i32, ptr addrspace(256) [[TMP0]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = sext i32 [[KVXLD]] to i64
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long lws(void *p) {
  return __builtin_kvx_lws(p, ".u", false);
}

// CHECK-LABEL: @_Z3lwfPv(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = addrspacecast ptr [[P:%.*]] to ptr addrspace(258)
// CHECK-NEXT:    [[KVXLD:%.*]] = load volatile float, ptr addrspace(258) [[TMP0]], align 1
// CHECK-NEXT:    ret float [[KVXLD]]
//
float lwf(void *p) {
  return __builtin_kvx_lwf(p, ".s", true);
}

// CHECK-LABEL: @_Z2ldPv(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = addrspacecast ptr [[P:%.*]] to ptr addrspace(258)
// CHECK-NEXT:    [[KVXLD:%.*]] = load volatile i64, ptr addrspace(258) [[TMP0]], align 1
// CHECK-NEXT:    ret i64 [[KVXLD]]
//
long ld(void *p) {
  return __builtin_kvx_ld(p, ".s", true);
}

// CHECK-LABEL: @_Z3ldfPv(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = addrspacecast ptr [[P:%.*]] to ptr addrspace(258)
// CHECK-NEXT:    [[KVXLD:%.*]] = load volatile double, ptr addrspace(258) [[TMP0]], align 1
// CHECK-NEXT:    ret double [[KVXLD]]
//
double ldf(void *p) {
  return __builtin_kvx_ldf(p, ".s", true);
}

typedef char int8x8_t __attribute__((__vector_size__(8 * sizeof(char))));

// CHECK-LABEL: @_Z3lboPv(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = addrspacecast ptr [[P:%.*]] to ptr addrspace(258)
// CHECK-NEXT:    [[KVXLD:%.*]] = load volatile <8 x i8>, ptr addrspace(258) [[TMP0]], align 1
// CHECK-NEXT:    ret <8 x i8> [[KVXLD]]
//
int8x8_t lbo(void *p) {
  return __builtin_kvx_lbo(p, ".s", true);
}

typedef short int16x4_t __attribute__((__vector_size__(4 * sizeof(short))));

// CHECK-LABEL: @_Z3lhqPv(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = addrspacecast ptr [[P:%.*]] to ptr addrspace(258)
// CHECK-NEXT:    [[KVXLD:%.*]] = load volatile <4 x i16>, ptr addrspace(258) [[TMP0]], align 1
// CHECK-NEXT:    ret <4 x i16> [[KVXLD]]
//
int16x4_t lhq(void *p) {
  return __builtin_kvx_lhq(p, ".s", true);
}

typedef int int32x2_t __attribute__((__vector_size__(2 * sizeof(int))));

// CHECK-LABEL: @_Z3lwpPv(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = addrspacecast ptr [[P:%.*]] to ptr addrspace(258)
// CHECK-NEXT:    [[KVXLD:%.*]] = load volatile <2 x i32>, ptr addrspace(258) [[TMP0]], align 1
// CHECK-NEXT:    ret <2 x i32> [[KVXLD]]
//
int32x2_t lwp(void *p) {
  return __builtin_kvx_lwp(p, ".s", true);
}

typedef float float32x2_t __attribute__((__vector_size__(2 * sizeof(float))));

// CHECK-LABEL: @_Z4lfwpPv(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = addrspacecast ptr [[P:%.*]] to ptr addrspace(258)
// CHECK-NEXT:    [[KVXLD:%.*]] = load volatile <2 x float>, ptr addrspace(258) [[TMP0]], align 1
// CHECK-NEXT:    ret <2 x float> [[KVXLD]]
//
float32x2_t lfwp(void *p) {
  return __builtin_kvx_lfwp(p, ".s", true);
}
