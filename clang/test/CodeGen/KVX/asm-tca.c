// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -triple kvx-kalray-cos -emit-llvm %s -O2 -o - | FileCheck %s

// CHECK-LABEL: @asm_tca(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ADD:%.*]] = add nsw i64 [[A:%.*]], 1
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <256 x i1>, <256 x i1> } asm sideeffect "xlo.u $0 = $3[$2]\0A\09
// CHECK-NEXT:    [[ASMRESULT:%.*]] = extractvalue { <256 x i1>, <256 x i1> } [[TMP0]], 0
// CHECK-NEXT:    [[ASMRESULT1:%.*]] = extractvalue { <256 x i1>, <256 x i1> } [[TMP0]], 1
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <256 x i1> asm sideeffect "xcopyo $0 = $1\0A\09
// CHECK-NEXT:    ret void
//
void asm_tca(void *v, long A) {
  long B = A + 1;
  __kvx_x256 out1, out2, out3;
  __asm__ volatile("xlo.u %0 = %3[%2]\n\t;;\n\t"
                   "xlo.us %1 = %4[%2]\n\t;;"
                   : "=x"(out1), "=x"(out2)
                   : "r"(v), "r"(A), "r"(B)
                   : "$r0");
  __asm__ volatile("xcopyo %0 = %1\n\t;;\n\t"
                   "xso 0[%3] = %2"
                   : "=x"(out3)
                   : "x"(out1), "x"(out2), "r"(v)
                   :);
}

// CHECK-LABEL: @asm_clobber_vec_vec(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <256 x i1> asm sideeffect "xcopyo $0 = $1", "=x,x,~{$a0}"(<256 x i1> undef) #[[ATTR1:[0-9]+]], !srcloc [[META4:![0-9]+]]
// CHECK-NEXT:    ret void
//
void asm_clobber_vec_vec(long A) {
  __kvx_x256 v4i64;
  __kvx_x256 tcav4i64;
  __asm__ volatile("xcopyo %0 = %1"
                   : "=x"(v4i64)
                   : "x"(tcav4i64)
                   : "$a0");
}

// CHECK-LABEL: @asm_clobber_vec_block(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <256 x i1> asm sideeffect "xcopyo $0 = $1", "=x,x,~{$a0.hi}"(<256 x i1> undef) #[[ATTR1]], !srcloc [[META5:![0-9]+]]
// CHECK-NEXT:    ret void
//
void asm_clobber_vec_block(long A) {
  __kvx_x256 v4i64;
  __kvx_x256 tcav4i64;
  __asm__ volatile("xcopyo %0 = %1"
                   : "=x"(v4i64)
                   : "x"(tcav4i64)
                   : "$a0.hi");
}

// CHECK-LABEL: @asm_clobber_wide_vec(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[A:%.*]], align 32, !tbaa [[TBAA6:![0-9]+]]
// CHECK-NEXT:    tail call void asm sideeffect "xcopyo $0 = $0", "x,~{$r0r1r2r3},~{$a0a1}"(<256 x i1> [[TMP0]]) #[[ATTR1]], !srcloc [[META10:![0-9]+]]
// CHECK-NEXT:    ret void
//
void asm_clobber_wide_vec(__kvx_x256 *a) {
  __asm__ volatile("xcopyo %0 = %0"
                   :
                   : "x"(a[0])
                   : "$r0r1r2r3", "$a0a1");
}

// CHECK-LABEL: @asm_clobber_multiple_quad(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[C:%.*]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call { <256 x i1>, <256 x i1> } asm sideeffect "xcopyo $0 = $1\0A\09
// CHECK-NEXT:    [[ASMRESULT:%.*]] = extractvalue { <256 x i1>, <256 x i1> } [[TMP1]], 0
// CHECK-NEXT:    [[ASMRESULT3:%.*]] = extractvalue { <256 x i1>, <256 x i1> } [[TMP1]], 1
// CHECK-NEXT:    store <256 x i1> [[ASMRESULT]], ptr [[C]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    store <256 x i1> [[ASMRESULT3]], ptr [[B:%.*]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    ret void
//
void asm_clobber_multiple_quad(__kvx_x256 *c, __kvx_x256 *b) {
  __asm__ volatile("xcopyo %0 = %1\n\t;;\n\txcopyo %1 = %0"
                   : "=x"(c[0]), "=x"(b[0])
                   : "x"(c[0])
                   : "$r0r1r2r3", "$a0a1a2a3");
}

// CHECK-LABEL: @asm_clobber_quad_matrix(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <256 x i1>, ptr [[A:%.*]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    tail call void asm sideeffect "xso 0[$$r3] = $0", "x,~{$r0r1r2r3},~{$a0a1a2a3}"(<256 x i1> [[TMP0]]) #[[ATTR1]], !srcloc [[META12:![0-9]+]]
// CHECK-NEXT:    ret ptr [[A]]
//
__kvx_x256 *asm_clobber_quad_matrix(__kvx_x256 *a) {
  __asm__ volatile("xso 0[$r3] = %0"
                   :
                   : "x"(a[0])
                   : "$r0r1r2r3", "$a0a1a2a3");
  return a;
}

// CHECK-LABEL: @use_wide_reg(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <512 x i1>, ptr [[X:%.*]], align 32, !tbaa [[TBAA13:![0-9]+]]
// CHECK-NEXT:    [[TMP1:%.*]] = load <256 x i1>, ptr [[V:%.*]], align 32, !tbaa [[TBAA6]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <512 x i1> asm sideeffect "xmma484bw $0 = $0, $1, $1", "=x,x,0,~{$r0r1r2r3},~{$a0a1a2a3}"(<256 x i1> [[TMP1]], <512 x i1> [[TMP0]]) #[[ATTR1]], !srcloc [[META15:![0-9]+]]
// CHECK-NEXT:    store <512 x i1> [[TMP2]], ptr [[X]], align 32, !tbaa [[TBAA13]]
// CHECK-NEXT:    ret void
//
void use_wide_reg(__kvx_x512 *x, __kvx_x256 *v) {
  __asm__ volatile("xmma484bw %0 = %0, %1, %1"
                   : "+x"(x[0])
                   : "x"(v[0])
                   : "$r0r1r2r3", "$a0a1a2a3");
}

// CHECK-LABEL: @use_matrix_reg(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <1024 x i1>, ptr [[X:%.*]], align 32, !tbaa [[TBAA16:![0-9]+]]
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <1024 x i1> asm sideeffect "xmt44d $0 = $0", "=x,0,~{$r0r1r2r3},~{$a0a1a2a3}"(<1024 x i1> [[TMP0]]) #[[ATTR1]], !srcloc [[META18:![0-9]+]]
// CHECK-NEXT:    store <1024 x i1> [[TMP1]], ptr [[X]], align 32, !tbaa [[TBAA16]]
// CHECK-NEXT:    ret void
//
void use_matrix_reg(__kvx_x1024 *x) {
  __asm__ volatile("xmt44d %0 = %0"
                   : "+x"(x[0])
                   :
                   : "$r0r1r2r3", "$a0a1a2a3");
}
