; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mcpu=kv3-1 -O2 -o - %s | FileCheck %s --check-prefixes=ALL
; RUN: clang -O2 -march=kv3-1 -c -o /dev/null %s

target triple = "kvx-kalray-cos"

define void @convdhv(ptr nocapture %0, ptr nocapture readonly %1) {
; ALL-LABEL: convdhv:
; ALL:       # %bb.0:
; ALL-NEXT:    xlo.u $a3 = 96[$r1]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    xlo.u $a2 = 64[$r1]
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    xlo.u $a1 = 32[$r1]
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    xlo.u $a0 = 0[$r1]
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    xlo.u $a4 = 0[$r0]
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    convdhv1.rn.sat $a4.hi = $a0a1a2a3
; ALL-NEXT:    ;; # (end cycle 6)
; ALL-NEXT:    convdhv0.rn.satu $a4.lo = $a0a1a2a3
; ALL-NEXT:    ;; # (end cycle 7)
; ALL-NEXT:    xso 32[$r0] = $a4
; ALL-NEXT:    ;; # (end cycle 11)
; ALL-NEXT:    xlo.u $a3 = 224[$r1]
; ALL-NEXT:    ;; # (end cycle 12)
; ALL-NEXT:    xlo.u $a2 = 192[$r1]
; ALL-NEXT:    ;; # (end cycle 13)
; ALL-NEXT:    xlo.u $a1 = 160[$r1]
; ALL-NEXT:    ;; # (end cycle 14)
; ALL-NEXT:    xlo.u $a0 = 128[$r1]
; ALL-NEXT:    ;; # (end cycle 15)
; ALL-NEXT:    convdhv0.rz.sat $a4.lo = $a0a1a2a3
; ALL-NEXT:    ;; # (end cycle 18)
; ALL-NEXT:    convdhv1.rz.sat $a4.hi = $a0a1a2a3
; ALL-NEXT:    ;; # (end cycle 19)
; ALL-NEXT:    xso 64[$r0] = $a4
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 23)
  %3 = load <256 x i1>, ptr %0
  %4 = load <1024 x i1>, ptr %1
  %5 = tail call <256 x i1> @llvm.kvx.xconvdhv1(<256 x i1> %3, <1024 x i1> %4, i32 0, i32 0)
  %6 = tail call <256 x i1> @llvm.kvx.xconvdhv0(<256 x i1> %5, <1024 x i1> %4, i32 0, i32 1)
  %7 = getelementptr inbounds <256 x i1>, ptr %0, i64 1
  store <256 x i1> %6, ptr %7
  %8 = getelementptr inbounds <1024 x i1>, ptr %1, i64 1
  %9 = load <1024 x i1>, ptr %8
  %10 = tail call <256 x i1> @llvm.kvx.xconvdhv(<1024 x i1> %9, i32 3, i32 0)
  %11 = getelementptr inbounds <256 x i1>, ptr %0, i64 2
  store <256 x i1> %10, ptr %11
  ret void
}

declare <256 x i1> @llvm.kvx.xconvdhv1(<256 x i1>, <1024 x i1>, i32, i32)

declare <256 x i1> @llvm.kvx.xconvdhv0(<256 x i1>, <1024 x i1>, i32, i32)

declare <256 x i1> @llvm.kvx.xconvdhv(<1024 x i1>, i32, i32)

define void @convwbv(ptr nocapture %0, ptr nocapture readonly %1) {
; ALL-LABEL: convwbv:
; ALL:       # %bb.0:
; ALL-NEXT:    xlo.u $a3 = 96[$r1]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    xlo.u $a2 = 64[$r1]
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    xlo.u $a1 = 32[$r1]
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    xlo.u $a0 = 0[$r1]
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    xlo.u $a4 = 0[$r0]
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    convwbv1.rn.sat $a4.y = $a0a1a2a3
; ALL-NEXT:    ;; # (end cycle 6)
; ALL-NEXT:    convwbv0.rn.satu $a4.x = $a0a1a2a3
; ALL-NEXT:    ;; # (end cycle 7)
; ALL-NEXT:    convwbv2.rd.sat $a4.z = $a0a1a2a3
; ALL-NEXT:    ;; # (end cycle 8)
; ALL-NEXT:    xcopyo $a5 = $a4
; ALL-NEXT:    ;; # (end cycle 12)
; ALL-NEXT:    convwbv3.rhu.satu $a5.t = $a0a1a2a3
; ALL-NEXT:    ;; # (end cycle 13)
; ALL-NEXT:    xso 32[$r0] = $a5
; ALL-NEXT:    convwbv3.rn.sat $a4.t = $a0a1a2a3
; ALL-NEXT:    ;; # (end cycle 17)
; ALL-NEXT:    xso 0[$r0] = $a4
; ALL-NEXT:    ;; # (end cycle 21)
; ALL-NEXT:    xlo.u $a3 = 224[$r1]
; ALL-NEXT:    ;; # (end cycle 22)
; ALL-NEXT:    xlo.u $a2 = 192[$r1]
; ALL-NEXT:    ;; # (end cycle 23)
; ALL-NEXT:    xlo.u $a1 = 160[$r1]
; ALL-NEXT:    ;; # (end cycle 24)
; ALL-NEXT:    xlo.u $a0 = 128[$r1]
; ALL-NEXT:    ;; # (end cycle 25)
; ALL-NEXT:    convwbv0.rz.satu $a4.x = $a0a1a2a3
; ALL-NEXT:    ;; # (end cycle 28)
; ALL-NEXT:    convwbv1.rz.satu $a4.y = $a0a1a2a3
; ALL-NEXT:    ;; # (end cycle 29)
; ALL-NEXT:    convwbv2.rz.satu $a4.z = $a0a1a2a3
; ALL-NEXT:    ;; # (end cycle 30)
; ALL-NEXT:    convwbv3.rz.satu $a4.t = $a0a1a2a3
; ALL-NEXT:    ;; # (end cycle 31)
; ALL-NEXT:    xso 64[$r0] = $a4
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 35)
  %3 = load <256 x i1>, ptr %0
  %4 = load <1024 x i1>, ptr %1
  %5 = tail call <256 x i1> @llvm.kvx.xconvwbv1(<256 x i1> %3, <1024 x i1> %4, i32 0, i32 0)
  %6 = tail call <256 x i1> @llvm.kvx.xconvwbv0(<256 x i1> %5, <1024 x i1> %4, i32 0, i32 1)
  %7 = tail call <256 x i1> @llvm.kvx.xconvwbv2(<256 x i1> %6, <1024 x i1> %4, i32 2, i32 0)
  %8 = tail call <256 x i1> @llvm.kvx.xconvwbv3(<256 x i1> %7, <1024 x i1> %4, i32 4, i32 1)
  %9 = getelementptr inbounds <256 x i1>, ptr %0, i64 1
  store <256 x i1> %8, ptr %9
  %10 = tail call <256 x i1> @llvm.kvx.xconvwbv3(<256 x i1> %7, <1024 x i1> %4, i32 0, i32 0)
  store <256 x i1> %10, ptr %0
  %11 = getelementptr inbounds <1024 x i1>, ptr %1, i64 1
  %12 = load <1024 x i1>, ptr %11
  %13 = tail call <256 x i1> @llvm.kvx.xconvwbv(<1024 x i1> %12, i32 3, i32 1)
  %14 = getelementptr inbounds <256 x i1>, ptr %0, i64 2
  store <256 x i1> %13, ptr %14
  ret void
}

declare <256 x i1> @llvm.kvx.xconvwbv1(<256 x i1>, <1024 x i1>, i32, i32)

declare <256 x i1> @llvm.kvx.xconvwbv0(<256 x i1>, <1024 x i1>, i32, i32)

declare <256 x i1> @llvm.kvx.xconvwbv2(<256 x i1>, <1024 x i1>, i32, i32)

declare <256 x i1> @llvm.kvx.xconvwbv3(<256 x i1>, <1024 x i1>, i32, i32)

declare <256 x i1> @llvm.kvx.xconvwbv(<1024 x i1>, i32, i32)

define void @fmma444hw(ptr nocapture %0, ptr nocapture %1) {
; ALL-LABEL: fmma444hw:
; ALL:       # %bb.0:
; ALL-NEXT:    xlo.u $a3 = 32[$r1]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    xlo.u $a2 = 0[$r1]
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    xlo.u $a4 = 32[$r0]
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    xlo.u $a6 = 64[$r0]
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    xlo.u $a0 = 0[$r0]
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    fmma242hw0 $a0.lo = $a2a3, $a4, $a6
; ALL-NEXT:    ;; # (end cycle 6)
; ALL-NEXT:    fmma242hw1 $a0.hi = $a2a3, $a4, $a6
; ALL-NEXT:    ;; # (end cycle 7)
; ALL-NEXT:    xcopyo $a1 = $a0
; ALL-NEXT:    ;; # (end cycle 13)
; ALL-NEXT:    xcopyo $a5 = $a1
; ALL-NEXT:    ;; # (end cycle 17)
; ALL-NEXT:    fmma242hw2 $a5.lo = $a2a3, $a4, $a6
; ALL-NEXT:    ;; # (end cycle 18)
; ALL-NEXT:    xso 32[$r0] = $a5
; ALL-NEXT:    fmma242hw3 $a1.hi = $a2a3, $a5, $a6
; ALL-NEXT:    ;; # (end cycle 24)
; ALL-NEXT:    xso 0[$r0] = $a0
; ALL-NEXT:    ;; # (end cycle 25)
; ALL-NEXT:    xso 64[$r0] = $a1
; ALL-NEXT:    ;; # (end cycle 30)
; ALL-NEXT:    xlo.u $a3 = 96[$r1]
; ALL-NEXT:    ;; # (end cycle 31)
; ALL-NEXT:    xlo.u $a2 = 64[$r1]
; ALL-NEXT:    ;; # (end cycle 32)
; ALL-NEXT:    fmma242hw0 $a0.lo = $a2a3, $a1, $a5
; ALL-NEXT:    ;; # (end cycle 35)
; ALL-NEXT:    fmma242hw1 $a0.hi = $a2a3, $a1, $a5
; ALL-NEXT:    ;; # (end cycle 36)
; ALL-NEXT:    fmma242hw2 $a7.lo = $a2a3, $a1, $a5
; ALL-NEXT:    ;; # (end cycle 37)
; ALL-NEXT:    fmma242hw3 $a7.hi = $a2a3, $a1, $a5
; ALL-NEXT:    ;; # (end cycle 38)
; ALL-NEXT:    xso 192[$r1] = $a0
; ALL-NEXT:    ;; # (end cycle 42)
; ALL-NEXT:    xso 224[$r1] = $a7
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 44)
  %3 = load <256 x i1>, ptr %0
  %4 = load <512 x i1>, ptr %1
  %5 = getelementptr inbounds <256 x i1>, ptr %0, i64 1
  %6 = load <256 x i1>, ptr %5
  %7 = getelementptr inbounds <256 x i1>, ptr %0, i64 2
  %8 = load <256 x i1>, ptr %7
  %9 = tail call <256 x i1> @llvm.kvx.xfmma242hw0(<256 x i1> %3, <512 x i1> %4, <256 x i1> %6, <256 x i1> %8)
  %10 = tail call <256 x i1> @llvm.kvx.xfmma242hw1(<256 x i1> %9, <512 x i1> %4, <256 x i1> %6, <256 x i1> %8)
  %11 = tail call <256 x i1> @llvm.kvx.xfmma242hw2(<256 x i1> %10, <512 x i1> %4, <256 x i1> %6, <256 x i1> %8)
  store <256 x i1> %11, ptr %5
  %12 = tail call <256 x i1> @llvm.kvx.xfmma242hw3(<256 x i1> %10, <512 x i1> %4, <256 x i1> %11, <256 x i1> %8)
  store <256 x i1> %12, ptr %7
  store <256 x i1> %10, ptr %0
  %13 = getelementptr inbounds <512 x i1>, ptr %1, i64 1
  %14 = load <512 x i1>, ptr %13
  %15 = tail call <512 x i1> @llvm.kvx.xfmma444hw(<256 x i1> %12, <256 x i1> %11, <512 x i1> %14)
  %16 = getelementptr inbounds <512 x i1>, ptr %1, i64 3
  store <512 x i1> %15, ptr %16
  ret void
}

declare <256 x i1> @llvm.kvx.xfmma242hw0(<256 x i1>, <512 x i1>, <256 x i1>, <256 x i1>)

declare <256 x i1> @llvm.kvx.xfmma242hw1(<256 x i1>, <512 x i1>, <256 x i1>, <256 x i1>)

declare <256 x i1> @llvm.kvx.xfmma242hw2(<256 x i1>, <512 x i1>, <256 x i1>, <256 x i1>)

declare <256 x i1> @llvm.kvx.xfmma242hw3(<256 x i1>, <512 x i1>, <256 x i1>, <256 x i1>)

declare <512 x i1> @llvm.kvx.xfmma444hw(<256 x i1>, <256 x i1>, <512 x i1>)

define void @test(ptr nocapture %0) {
; ALL-LABEL: test:
; ALL:       # %bb.0:
; ALL-NEXT:    xlo.u $a0 = 0[$r0]
; ALL-NEXT:    make $r1 = 0
; ALL-NEXT:    make $r2 = 1
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    xmovetq $a0.lo = $r2, $r1
; ALL-NEXT:    xmovetq $a0.hi = $r2, $r1
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    xso 0[$r0] = $a0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 5)
  %2 = load <256 x i1>, ptr %0
  %3 = tail call <256 x i1> @llvm.kvx.xmovetq(<256 x i1> %2, i64 1, i64 0, i32 1)
  %4 = tail call <256 x i1> @llvm.kvx.xmovetq(<256 x i1> %3, i64 1, i64 0, i32 0)
  store <256 x i1> %4, ptr %0
  ret void
}

declare <256 x i1> @llvm.kvx.xmovetq(<256 x i1>, i64, i64, i32)

define void @insertwm(ptr nocapture %0, ptr nocapture readonly %1) {
; ALL-LABEL: insertwm:
; ALL:       # %bb.0:
; ALL-NEXT:    # implicit-def: $x1
; ALL-NEXT:    xlo.u $a0 = 96[$r0]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    xlo.u $a0 = 64[$r0]
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    xlo.u $a1 = 32[$r1]
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    xlo.u $a0 = 0[$r1]
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    xlo.u $a2 = 32[$r0]
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    xlo.u $a2 = 0[$r0]
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    xso 0[$r0] = $a0
; ALL-NEXT:    ;; # (end cycle 6)
; ALL-NEXT:    xso 32[$r0] = $a1
; ALL-NEXT:    ;; # (end cycle 7)
; ALL-NEXT:    xcopyo $a2 = $a0
; ALL-NEXT:    ;; # (end cycle 8)
; ALL-NEXT:    # implicit-def: $w0
; ALL-NEXT:    # implicit-def: $w0
; ALL-NEXT:    xcopyo $a3 = $a1
; ALL-NEXT:    ;; # (end cycle 9)
; ALL-NEXT:    xso 64[$r0] = $a2
; ALL-NEXT:    ;; # (end cycle 12)
; ALL-NEXT:    xso 96[$r0] = $a3
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 13)
  %3 = load <1024 x i1>, ptr %0
  %4 = load <512 x i1>, ptr %1
  %5 = tail call <1024 x i1> @llvm.kvx.xinsertwm(<1024 x i1> %3, <512 x i1> %4, i32 0)
  %6 = tail call <1024 x i1> @llvm.kvx.xinsertwm(<1024 x i1> %5, <512 x i1> %4, i32 1)
  store <1024 x i1> %6, ptr %0
  ret void
}

declare <1024 x i1> @llvm.kvx.xinsertwm(<1024 x i1>, <512 x i1>, i32)

define void @insertvm(ptr nocapture %0, ptr nocapture readonly %1) {
; ALL-LABEL: insertvm:
; ALL:       # %bb.0:
; ALL-NEXT:    # implicit-def: $x1
; ALL-NEXT:    xlo.u $a0 = 96[$r0]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    xlo.u $a0 = 0[$r1]
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    xlo.u $a1 = 64[$r0]
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    xlo.u $a1 = 32[$r0]
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    xlo.u $a1 = 0[$r0]
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    xso 0[$r0] = $a0
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    xcopyo $a1 = $a0
; ALL-NEXT:    ;; # (end cycle 6)
; ALL-NEXT:    xso 32[$r0] = $a1
; ALL-NEXT:    ;; # (end cycle 10)
; ALL-NEXT:    xcopyo $a2 = $a0
; ALL-NEXT:    ;; # (end cycle 11)
; ALL-NEXT:    xso 64[$r0] = $a2
; ALL-NEXT:    ;; # (end cycle 15)
; ALL-NEXT:    # implicit-def: $w0
; ALL-NEXT:    # implicit-def: $w0
; ALL-NEXT:    xcopyo $a3 = $a0
; ALL-NEXT:    ;; # (end cycle 16)
; ALL-NEXT:    xso 96[$r0] = $a3
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 20)
  %3 = load <1024 x i1>, ptr %0
  %4 = load <256 x i1>, ptr %1
  %5 = tail call <1024 x i1> @llvm.kvx.xinsertvm(<1024 x i1> %3, <256 x i1> %4, i32 0)
  %6 = tail call <1024 x i1> @llvm.kvx.xinsertvm(<1024 x i1> %5, <256 x i1> %4, i32 1)
  %7 = tail call <1024 x i1> @llvm.kvx.xinsertvm(<1024 x i1> %6, <256 x i1> %4, i32 2)
  %8 = tail call <1024 x i1> @llvm.kvx.xinsertvm(<1024 x i1> %7, <256 x i1> %4, i32 3)
  store <1024 x i1> %8, ptr %0
  ret void
}

declare <1024 x i1> @llvm.kvx.xinsertvm(<1024 x i1>, <256 x i1>, i32)

define void @insertvw(ptr nocapture %0, ptr nocapture readonly %1) {
; ALL-LABEL: insertvw:
; ALL:       # %bb.0:
; ALL-NEXT:    # implicit-def: $w1
; ALL-NEXT:    xlo.u $a0 = 0[$r1]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    xlo.u $a1 = 32[$r0]
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    xlo.u $a1 = 0[$r0]
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    xso 0[$r0] = $a0
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    xcopyo $a1 = $a0
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    xso 32[$r0] = $a1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 8)
  %3 = load <512 x i1>, ptr %0
  %4 = load <256 x i1>, ptr %1
  %5 = tail call <512 x i1> @llvm.kvx.xinsertvw(<512 x i1> %3, <256 x i1> %4, i32 0)
  %6 = tail call <512 x i1> @llvm.kvx.xinsertvw(<512 x i1> %5, <256 x i1> %4, i32 1)
  store <512 x i1> %6, ptr %0
  ret void
}

declare <512 x i1> @llvm.kvx.xinsertvw(<512 x i1>, <256 x i1>, i32)

define void @movefmw(ptr nocapture %0, ptr nocapture readonly %1) {
; ALL-LABEL: movefmw:
; ALL:       # %bb.0:
; ALL-NEXT:    xlo.u $a3 = 96[$r1]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    xlo.u $a1 = 32[$r1]
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    xlo.u $a0 = 0[$r1]
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    xlo.u $a2 = 64[$r1]
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    xso 32[$r0] = $a1
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    xso 0[$r0] = $a0
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    xso 96[$r0] = $a3
; ALL-NEXT:    ;; # (end cycle 6)
; ALL-NEXT:    xso 64[$r0] = $a2
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 7)
  %3 = load <1024 x i1>, ptr %1
  %4 = tail call <512 x i1> @llvm.kvx.xmovefmw(<1024 x i1> %3, i32 0)
  store <512 x i1> %4, ptr %0
  %5 = tail call <512 x i1> @llvm.kvx.xmovefmw(<1024 x i1> %3, i32 1)
  %6 = getelementptr inbounds <512 x i1>, ptr %0, i64 1
  store <512 x i1> %5, ptr %6
  ret void
}

declare <512 x i1> @llvm.kvx.xmovefmw(<1024 x i1>, i32)

define void @movefmv(ptr nocapture %0, ptr nocapture readonly %1) {
; ALL-LABEL: movefmv:
; ALL:       # %bb.0:
; ALL-NEXT:    xlo.u $a3 = 96[$r1]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    xlo.u $a0 = 0[$r1]
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    xlo.u $a1 = 32[$r1]
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    xlo.u $a2 = 64[$r1]
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    xso 0[$r0] = $a0
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    xso 32[$r0] = $a1
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    xso 64[$r0] = $a2
; ALL-NEXT:    ;; # (end cycle 6)
; ALL-NEXT:    xso 96[$r0] = $a3
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 7)
  %3 = load <1024 x i1>, ptr %1
  %4 = tail call <256 x i1> @llvm.kvx.xmovefmv(<1024 x i1> %3, i32 0)
  store <256 x i1> %4, ptr %0
  %5 = tail call <256 x i1> @llvm.kvx.xmovefmv(<1024 x i1> %3, i32 1)
  %6 = getelementptr inbounds <256 x i1>, ptr %0, i64 1
  store <256 x i1> %5, ptr %6
  %7 = tail call <256 x i1> @llvm.kvx.xmovefmv(<1024 x i1> %3, i32 2)
  %8 = getelementptr inbounds <256 x i1>, ptr %0, i64 2
  store <256 x i1> %7, ptr %8
  %9 = tail call <256 x i1> @llvm.kvx.xmovefmv(<1024 x i1> %3, i32 3)
  %10 = getelementptr inbounds <256 x i1>, ptr %0, i64 3
  store <256 x i1> %9, ptr %10
  ret void
}

declare <256 x i1> @llvm.kvx.xmovefmv(<1024 x i1>, i32)

define void @movefwv(ptr nocapture %0, ptr nocapture readonly %1) {
; ALL-LABEL: movefwv:
; ALL:       # %bb.0:
; ALL-NEXT:    xlo.u $a0 = 0[$r1]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    xlo.u $a1 = 32[$r1]
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    xso 0[$r0] = $a0
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    xso 32[$r0] = $a1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 4)
  %3 = load <512 x i1>, ptr %1
  %4 = tail call <256 x i1> @llvm.kvx.xmovefwv(<512 x i1> %3, i32 0)
  store <256 x i1> %4, ptr %0
  %5 = tail call <256 x i1> @llvm.kvx.xmovefwv(<512 x i1> %3, i32 1)
  %6 = getelementptr inbounds <256 x i1>, ptr %0, i64 1
  store <256 x i1> %5, ptr %6
  ret void
}

declare <256 x i1> @llvm.kvx.xmovefwv(<512 x i1>, i32)

define void @buildfvm(ptr nocapture readonly %0, ptr nocapture %1) {
; ALL-LABEL: buildfvm:
; ALL:       # %bb.0:
; ALL-NEXT:    xlo.u $a1 = 64[$r0]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    xlo.u $a0 = 0[$r0]
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    xlo.u $a3 = 32[$r0]
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    xso 32[$r1] = $a1
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    xso 0[$r1] = $a0
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    xso 96[$r1] = $a3
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    xcopyo $a2 = $a0
; ALL-NEXT:    ;; # (end cycle 6)
; ALL-NEXT:    xso 64[$r1] = $a2
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 10)
  %3 = load <256 x i1>, ptr %0
  %4 = getelementptr inbounds <256 x i1>, ptr %0, i64 2
  %5 = load <256 x i1>, ptr %4
  %6 = getelementptr inbounds <256 x i1>, ptr %0, i64 1
  %7 = load <256 x i1>, ptr %6
  %8 = tail call <1024 x i1> @llvm.kvx.xbuild1024(<256 x i1> %3, <256 x i1> %5, <256 x i1> %3, <256 x i1> %7)
  store <1024 x i1> %8, ptr %1
  ret void
}

declare <1024 x i1> @llvm.kvx.xbuild1024(<256 x i1>, <256 x i1>, <256 x i1>, <256 x i1>)

define void @buildfwm(ptr nocapture readonly %0, ptr nocapture %1) {
; ALL-LABEL: buildfwm:
; ALL:       # %bb.0:
; ALL-NEXT:    xlo.u $a1 = 160[$r0]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    xlo.u $a0 = 128[$r0]
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    xso 160[$r1] = $a1
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    xso 128[$r1] = $a0
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    xcopyo $a2 = $a0
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    xcopyo $a3 = $a1
; ALL-NEXT:    ;; # (end cycle 6)
; ALL-NEXT:    xso 192[$r1] = $a2
; ALL-NEXT:    ;; # (end cycle 9)
; ALL-NEXT:    xso 224[$r1] = $a3
; ALL-NEXT:    ;; # (end cycle 10)
; ALL-NEXT:    xlo.u $a3 = 96[$r0]
; ALL-NEXT:    ;; # (end cycle 11)
; ALL-NEXT:    xlo.u $a2 = 64[$r0]
; ALL-NEXT:    ;; # (end cycle 12)
; ALL-NEXT:    xso 32[$r1] = $a1
; ALL-NEXT:    ;; # (end cycle 13)
; ALL-NEXT:    xso 0[$r1] = $a0
; ALL-NEXT:    ;; # (end cycle 14)
; ALL-NEXT:    xso 96[$r1] = $a3
; ALL-NEXT:    ;; # (end cycle 15)
; ALL-NEXT:    xso 64[$r1] = $a2
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 16)
  %3 = getelementptr inbounds <512 x i1>, ptr %0, i64 2
  %4 = load <512 x i1>, ptr %3
  %5 = tail call <1024 x i1> @llvm.kvx.cat.v1024i1(<512 x i1> %4, <512 x i1> %4)
  %6 = getelementptr inbounds <1024 x i1>, ptr %1, i64 1
  store <1024 x i1> %5, ptr %6
  %7 = getelementptr inbounds <512 x i1>, ptr %0, i64 1
  %8 = load <512 x i1>, ptr %7
  %9 = tail call <1024 x i1> @llvm.kvx.cat.v1024i1(<512 x i1> %4, <512 x i1> %8)
  store <1024 x i1> %9, ptr %1
  ret void
}

declare <1024 x i1> @llvm.kvx.cat.v1024i1(<512 x i1>, <512 x i1>)

define void @buildfvw(ptr nocapture readonly %0, ptr nocapture %1) {
; ALL-LABEL: buildfvw:
; ALL:       # %bb.0:
; ALL-NEXT:    xlo.u $a1 = 64[$r0]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    xlo.u $a0 = 0[$r0]
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    xso 32[$r1] = $a1
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    xso 0[$r1] = $a0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 4)
  %3 = load <256 x i1>, ptr %0
  %4 = getelementptr inbounds <256 x i1>, ptr %0, i64 2
  %5 = load <256 x i1>, ptr %4
  %6 = tail call <512 x i1> @llvm.kvx.cat.v512i1(<256 x i1> %3, <256 x i1> %5)
  store <512 x i1> %6, ptr %1
  ret void
}

declare <512 x i1> @llvm.kvx.cat.v512i1(<256 x i1>, <256 x i1>)

define void @xcat2048(ptr nocapture %0) {
; ALL-LABEL: xcat2048:
; ALL:       # %bb.0:
; ALL-NEXT:    xmt44d $a0a1a2a3 = $a60a61a62a63
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    xso 32[$r0] = $a1
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    xso 0[$r0] = $a0
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    xso 96[$r0] = $a3
; ALL-NEXT:    ;; # (end cycle 6)
; ALL-NEXT:    xso 64[$r0] = $a2
; ALL-NEXT:    ;; # (end cycle 7)
; ALL-NEXT:    xcopyo $a4 = $a0
; ALL-NEXT:    ;; # (end cycle 8)
; ALL-NEXT:    xcopyo $a5 = $a1
; ALL-NEXT:    ;; # (end cycle 9)
; ALL-NEXT:    xcopyo $a6 = $a2
; ALL-NEXT:    ;; # (end cycle 10)
; ALL-NEXT:    xcopyo $a7 = $a3
; ALL-NEXT:    ;; # (end cycle 11)
; ALL-NEXT:    xso 128[$r0] = $a4
; ALL-NEXT:    ;; # (end cycle 12)
; ALL-NEXT:    xso 160[$r0] = $a5
; ALL-NEXT:    ;; # (end cycle 13)
; ALL-NEXT:    xso 192[$r0] = $a6
; ALL-NEXT:    ;; # (end cycle 14)
; ALL-NEXT:    xso 224[$r0] = $a7
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 15)
  %2 = tail call <2048 x i1> @llvm.kvx.cat.v2048i1(<1024 x i1> zeroinitializer, <1024 x i1> zeroinitializer)
  store <2048 x i1> %2, ptr %0
  ret void
}

declare <2048 x i1> @llvm.kvx.cat.v2048i1(<1024 x i1>, <1024 x i1>)

define void @xcat4096(ptr nocapture %0) {
; ALL-LABEL: xcat4096:
; ALL:       # %bb.0:
; ALL-NEXT:    xmt44d $a4a5a6a7 = $a60a61a62a63
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    xmt44d $a0a1a2a3 = $a56a57a58a59
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    xso 160[$r0] = $a5
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    xso 32[$r0] = $a1
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    xso 0[$r0] = $a0
; ALL-NEXT:    ;; # (end cycle 6)
; ALL-NEXT:    xso 96[$r0] = $a3
; ALL-NEXT:    ;; # (end cycle 7)
; ALL-NEXT:    xso 64[$r0] = $a2
; ALL-NEXT:    ;; # (end cycle 8)
; ALL-NEXT:    xso 128[$r0] = $a4
; ALL-NEXT:    ;; # (end cycle 9)
; ALL-NEXT:    xso 224[$r0] = $a7
; ALL-NEXT:    ;; # (end cycle 10)
; ALL-NEXT:    xso 192[$r0] = $a6
; ALL-NEXT:    ;; # (end cycle 11)
; ALL-NEXT:    xcopyo $a8 = $a0
; ALL-NEXT:    ;; # (end cycle 12)
; ALL-NEXT:    xcopyo $a9 = $a1
; ALL-NEXT:    ;; # (end cycle 13)
; ALL-NEXT:    xcopyo $a10 = $a2
; ALL-NEXT:    ;; # (end cycle 14)
; ALL-NEXT:    xcopyo $a11 = $a3
; ALL-NEXT:    ;; # (end cycle 15)
; ALL-NEXT:    xcopyo $a12 = $a4
; ALL-NEXT:    ;; # (end cycle 16)
; ALL-NEXT:    xcopyo $a13 = $a5
; ALL-NEXT:    ;; # (end cycle 17)
; ALL-NEXT:    xcopyo $a14 = $a6
; ALL-NEXT:    ;; # (end cycle 18)
; ALL-NEXT:    xcopyo $a15 = $a7
; ALL-NEXT:    ;; # (end cycle 19)
; ALL-NEXT:    xso 288[$r0] = $a9
; ALL-NEXT:    ;; # (end cycle 20)
; ALL-NEXT:    xso 256[$r0] = $a8
; ALL-NEXT:    ;; # (end cycle 21)
; ALL-NEXT:    xso 352[$r0] = $a11
; ALL-NEXT:    ;; # (end cycle 22)
; ALL-NEXT:    xso 320[$r0] = $a10
; ALL-NEXT:    ;; # (end cycle 23)
; ALL-NEXT:    xso 416[$r0] = $a13
; ALL-NEXT:    ;; # (end cycle 24)
; ALL-NEXT:    xso 384[$r0] = $a12
; ALL-NEXT:    ;; # (end cycle 25)
; ALL-NEXT:    xso 480[$r0] = $a15
; ALL-NEXT:    ;; # (end cycle 26)
; ALL-NEXT:    xso 448[$r0] = $a14
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 27)
  %2 = tail call <4096 x i1> @llvm.kvx.cat.v4096i1(<2048 x i1> zeroinitializer, <2048 x i1> zeroinitializer)
  store <4096 x i1> %2, ptr %0
  ret void
}

declare <4096 x i1> @llvm.kvx.cat.v4096i1(<2048 x i1>, <2048 x i1>)

