; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mcpu=kv3-1 -o - %s -O3 | FileCheck %s --check-prefixes=ALL,CV1
; RUN: llc -mcpu=kv3-2 -o - %s -O3 | FileCheck %s --check-prefixes=ALL,CV2
; RUN: clang -O3 -march=kv3-1 -c -o /dev/null %s
; RUN: clang -O3 -march=kv3-2 -c -o /dev/null %s

target triple = "kvx-kalray-cos"

define void @halfdiv(half %0, half %1, ptr %2) {
; ALL-LABEL: halfdiv:
; ALL:       # %bb.0:
; ALL-NEXT:    fwidenlhw $r0 = $r0
; ALL-NEXT:    fwidenlhw $r1 = $r1
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    frecw $r1 = $r1
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    fmulw $r0 = $r0, $r1
; ALL-NEXT:    ;; # (end cycle 16)
; ALL-NEXT:    fnarrowwh $r0 = $r0
; ALL-NEXT:    ;; # (end cycle 20)
; ALL-NEXT:    sh 0[$r2] = $r0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 21)
  %4 = fdiv half %0, %1
  store half %4, ptr %2, align 2
  ret void
}

define void @half2fdiv(<2 x half> %0, <2 x half> %1, ptr %2) {
; ALL-LABEL: half2fdiv:
; ALL:       # %bb.0:
; ALL-NEXT:    fwidenlhwp $r0 = $r0
; ALL-NEXT:    fwidenlhwp $r1 = $r1
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    srld $r1 = $r1, 32
; ALL-NEXT:    frecw $r3 = $r1
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    frecw $r1 = $r1
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    make $r1 = 0
; ALL-NEXT:    insf $r3 = $r1, 63, 32
; ALL-NEXT:    ;; # (end cycle 17)
; ALL-NEXT:    fmulwp $r0 = $r0, $r3
; ALL-NEXT:    ;; # (end cycle 18)
; ALL-NEXT:    fnarrowwhq $r0 = $r0r1
; ALL-NEXT:    ;; # (end cycle 22)
; ALL-NEXT:    sw 0[$r2] = $r0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 23)
  %4 = fdiv <2 x half> %0, %1
  store <2 x half> %4, ptr %2, align 4
  ret void
}

define void @half4fdiv(<4 x half> %0, <4 x half> %1, ptr %2) {
; CV1-LABEL: half4fdiv:
; CV1:       # %bb.0:
; CV1-NEXT:    fwidenmhwp $r3 = $r1
; CV1-NEXT:    fwidenlhwp $r6 = $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srld $r3 = $r3, 32
; CV1-NEXT:    frecw $r4 = $r3
; CV1-NEXT:    srld $r5 = $r6, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    frecw $r1 = $r3
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    frecw $r3 = $r6
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    frecw $r5 = $r5
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    fwidenmhwp $r1 = $r0
; CV1-NEXT:    insf $r4 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    fwidenlhwp $r0 = $r0
; CV1-NEXT:    fmulwp $r1 = $r1, $r4
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    insf $r3 = $r5, 63, 32
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    fmulwp $r0 = $r0, $r3
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    fnarrowwhq $r0 = $r0r1
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    sd 0[$r2] = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 25)
;
; CV2-LABEL: half4fdiv:
; CV2:       # %bb.0:
; CV2-NEXT:    fwidenmhwp $r3 = $r1
; CV2-NEXT:    fwidenlhwp $r6 = $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srld $r3 = $r3, 32
; CV2-NEXT:    frecw $r4 = $r3
; CV2-NEXT:    srld $r5 = $r6, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    frecw $r1 = $r3
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    frecw $r3 = $r6
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    frecw $r5 = $r5
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    fwidenlhwp $r0 = $r0
; CV2-NEXT:    fwidenmhwp $r1 = $r0
; CV2-NEXT:    insf $r4 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 17)
; CV2-NEXT:    fmulwp $r1 = $r1, $r4
; CV2-NEXT:    ;; # (end cycle 18)
; CV2-NEXT:    insf $r3 = $r5, 63, 32
; CV2-NEXT:    ;; # (end cycle 19)
; CV2-NEXT:    fmulwp $r0 = $r0, $r3
; CV2-NEXT:    ;; # (end cycle 20)
; CV2-NEXT:    fnarrowwhq $r0 = $r0r1
; CV2-NEXT:    ;; # (end cycle 24)
; CV2-NEXT:    sd 0[$r2] = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 25)
  %4 = fdiv <4 x half> %0, %1
  store <4 x half> %4, ptr %2, align 8
  ret void
}

define void @half8fdiv(<8 x half> %0, <8 x half> %1, ptr %2) {
; CV1-LABEL: half8fdiv:
; CV1:       # %bb.0:
; CV1-NEXT:    fwidenmhwp $r5 = $r3
; CV1-NEXT:    fwidenlhwp $r6 = $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    fwidenmhwp $r9 = $r1
; CV1-NEXT:    fwidenmhwp $r11 = $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    fwidenlhwp $r2 = $r2
; CV1-NEXT:    frecw $r3 = $r5
; CV1-NEXT:    srld $r5 = $r5, 32
; CV1-NEXT:    srld $r10 = $r11, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    srld $r6 = $r6, 32
; CV1-NEXT:    frecw $r7 = $r6
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    frecw $r8 = $r11
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    srld $r2 = $r2, 32
; CV1-NEXT:    frecw $r11 = $r2
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    frecw $r5 = $r5
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    frecw $r6 = $r6
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    frecw $r10 = $r10
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    frecw $r2 = $r2
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r3 = $r5, 63, 32
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    fwidenlhwp $r6 = $r1
; CV1-NEXT:    insf $r7 = $r6, 63, 32
; CV1-NEXT:    fmulwp $r17 = $r9, $r3
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    fwidenmhwp $r1 = $r0
; CV1-NEXT:    insf $r8 = $r10, 63, 32
; CV1-NEXT:    fmulwp $r16 = $r6, $r7
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    fwidenlhwp $r0 = $r0
; CV1-NEXT:    fmulwp $r1 = $r1, $r8
; CV1-NEXT:    insf $r11 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    fmulwp $r0 = $r0, $r11
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    fnarrowwhq $r3 = $r16r17
; CV1-NEXT:    ;; # (end cycle 27)
; CV1-NEXT:    fnarrowwhq $r2 = $r0r1
; CV1-NEXT:    ;; # (end cycle 29)
; CV1-NEXT:    sq 0[$r4] = $r2r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 30)
;
; CV2-LABEL: half8fdiv:
; CV2:       # %bb.0:
; CV2-NEXT:    fwidenmhwp $r5 = $r3
; CV2-NEXT:    fwidenlhwp $r6 = $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    fwidenmhwp $r9 = $r1
; CV2-NEXT:    fwidenmhwp $r11 = $r2
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    fwidenlhwp $r2 = $r2
; CV2-NEXT:    frecw $r3 = $r5
; CV2-NEXT:    srld $r5 = $r5, 32
; CV2-NEXT:    srld $r10 = $r11, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    srld $r6 = $r6, 32
; CV2-NEXT:    frecw $r7 = $r6
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    frecw $r8 = $r11
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    srld $r2 = $r2, 32
; CV2-NEXT:    frecw $r11 = $r2
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    frecw $r5 = $r5
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    frecw $r6 = $r6
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    frecw $r10 = $r10
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    frecw $r2 = $r2
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r3 = $r5, 63, 32
; CV2-NEXT:    ;; # (end cycle 21)
; CV2-NEXT:    fwidenmhwp $r1 = $r0
; CV2-NEXT:    fwidenlhwp $r6 = $r1
; CV2-NEXT:    insf $r7 = $r6, 63, 32
; CV2-NEXT:    fmulwp $r17 = $r9, $r3
; CV2-NEXT:    ;; # (end cycle 22)
; CV2-NEXT:    fwidenlhwp $r0 = $r0
; CV2-NEXT:    insf $r8 = $r10, 63, 32
; CV2-NEXT:    fmulwp $r16 = $r6, $r7
; CV2-NEXT:    ;; # (end cycle 23)
; CV2-NEXT:    fmulwp $r1 = $r1, $r8
; CV2-NEXT:    insf $r11 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 24)
; CV2-NEXT:    fmulwp $r0 = $r0, $r11
; CV2-NEXT:    ;; # (end cycle 25)
; CV2-NEXT:    fnarrowwhq $r3 = $r16r17
; CV2-NEXT:    ;; # (end cycle 27)
; CV2-NEXT:    fnarrowwhq $r2 = $r0r1
; CV2-NEXT:    ;; # (end cycle 29)
; CV2-NEXT:    sq 0[$r4] = $r2r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 30)
  %4 = fdiv <8 x half> %0, %1
  store <8 x half> %4, ptr %2, align 16
  ret void
}

define void @half16fdiv(<16 x half> %0, <16 x half> %1, ptr %2) {
; ALL-LABEL: half16fdiv:
; ALL:       # %bb.0:
; ALL-NEXT:    fwidenmhwp $r9 = $r7
; ALL-NEXT:    fwidenlhwp $r16 = $r7
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    fwidenmhwp $r7 = $r3
; ALL-NEXT:    srld $r9 = $r9, 32
; ALL-NEXT:    frecw $r10 = $r9
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    srld $r9 = $r16, 32
; ALL-NEXT:    frecw $r11 = $r9
; ALL-NEXT:    fwidenlhwp $r36 = $r5
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    frecw $r15 = $r16
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    fwidenmhwp $r9 = $r6
; ALL-NEXT:    frecw $r16 = $r9
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    fwidenlhwp $r6 = $r6
; ALL-NEXT:    srld $r17 = $r9, 32
; ALL-NEXT:    frecw $r32 = $r9
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    fwidenmhwp $r9 = $r2
; ALL-NEXT:    srld $r33 = $r6, 32
; ALL-NEXT:    fwidenmhwp $r35 = $r5
; ALL-NEXT:    ;; # (end cycle 6)
; ALL-NEXT:    srld $r34 = $r35, 32
; ALL-NEXT:    fwidenmhwp $r37 = $r1
; ALL-NEXT:    fwidenmhwp $r39 = $r4
; ALL-NEXT:    ;; # (end cycle 7)
; ALL-NEXT:    fwidenlhwp $r4 = $r4
; ALL-NEXT:    frecw $r6 = $r6
; ALL-NEXT:    ;; # (end cycle 8)
; ALL-NEXT:    fwidenlhwp $r2 = $r2
; ALL-NEXT:    frecw $r33 = $r33
; ALL-NEXT:    ;; # (end cycle 9)
; ALL-NEXT:    frecw $r5 = $r34
; ALL-NEXT:    srld $r34 = $r36, 32
; ALL-NEXT:    ;; # (end cycle 10)
; ALL-NEXT:    frecw $r38 = $r39
; ALL-NEXT:    srld $r39 = $r39, 32
; ALL-NEXT:    ;; # (end cycle 11)
; ALL-NEXT:    srld $r4 = $r4, 32
; ALL-NEXT:    frecw $r40 = $r4
; ALL-NEXT:    ;; # (end cycle 12)
; ALL-NEXT:    frecw $r17 = $r17
; ALL-NEXT:    ;; # (end cycle 13)
; ALL-NEXT:    frecw $r35 = $r35
; ALL-NEXT:    ;; # (end cycle 14)
; ALL-NEXT:    frecw $r36 = $r36
; ALL-NEXT:    ;; # (end cycle 15)
; ALL-NEXT:    frecw $r34 = $r34
; ALL-NEXT:    ;; # (end cycle 16)
; ALL-NEXT:    insf $r10 = $r11, 63, 32
; ALL-NEXT:    frecw $r39 = $r39
; ALL-NEXT:    ;; # (end cycle 17)
; ALL-NEXT:    frecw $r4 = $r4
; ALL-NEXT:    fwidenlhwp $r10 = $r3
; ALL-NEXT:    fmulwp $r11 = $r7, $r10
; ALL-NEXT:    ;; # (end cycle 18)
; ALL-NEXT:    insf $r15 = $r16, 63, 32
; ALL-NEXT:    ;; # (end cycle 19)
; ALL-NEXT:    fmulwp $r10 = $r10, $r15
; ALL-NEXT:    ;; # (end cycle 20)
; ALL-NEXT:    insf $r6 = $r33, 63, 32
; ALL-NEXT:    ;; # (end cycle 24)
; ALL-NEXT:    fwidenmhwp $r1 = $r0
; ALL-NEXT:    fmulwp $r2 = $r2, $r6
; ALL-NEXT:    fwidenlhwp $r6 = $r1
; ALL-NEXT:    ;; # (end cycle 25)
; ALL-NEXT:    fwidenlhwp $r0 = $r0
; ALL-NEXT:    ;; # (end cycle 26)
; ALL-NEXT:    insf $r32 = $r17, 63, 32
; ALL-NEXT:    ;; # (end cycle 28)
; ALL-NEXT:    fmulwp $r3 = $r9, $r32
; ALL-NEXT:    insf $r35 = $r5, 63, 32
; ALL-NEXT:    ;; # (end cycle 29)
; ALL-NEXT:    fmulwp $r7 = $r37, $r35
; ALL-NEXT:    fnarrowwhq $r35 = $r10r11
; ALL-NEXT:    ;; # (end cycle 30)
; ALL-NEXT:    insf $r36 = $r34, 63, 32
; ALL-NEXT:    ;; # (end cycle 31)
; ALL-NEXT:    fmulwp $r6 = $r6, $r36
; ALL-NEXT:    insf $r38 = $r39, 63, 32
; ALL-NEXT:    ;; # (end cycle 32)
; ALL-NEXT:    fmulwp $r1 = $r1, $r38
; ALL-NEXT:    fnarrowwhq $r34 = $r2r3
; ALL-NEXT:    insf $r40 = $r4, 63, 32
; ALL-NEXT:    ;; # (end cycle 33)
; ALL-NEXT:    fmulwp $r0 = $r0, $r40
; ALL-NEXT:    ;; # (end cycle 34)
; ALL-NEXT:    fnarrowwhq $r33 = $r6r7
; ALL-NEXT:    ;; # (end cycle 36)
; ALL-NEXT:    fnarrowwhq $r32 = $r0r1
; ALL-NEXT:    ;; # (end cycle 38)
; ALL-NEXT:    so 0[$r8] = $r32r33r34r35
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 39)
  %4 = fdiv <16 x half> %0, %1
  store <16 x half> %4, ptr %2, align 32
  ret void
}

define void @floatdiv(float %0, float %1, ptr %2) {
; ALL-LABEL: floatdiv:
; ALL:       # %bb.0:
; ALL-NEXT:    frecw $r1 = $r1
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    fmulw $r0 = $r0, $r1
; ALL-NEXT:    ;; # (end cycle 15)
; ALL-NEXT:    sw 0[$r2] = $r0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 18)
  %4 = fdiv fast float %0, %1
  store float %4, ptr %2, align 4
  ret void
}

define void @float2fdiv(<2 x float> %0, <2 x float> %1, ptr %2) {
; ALL-LABEL: float2fdiv:
; ALL:       # %bb.0:
; ALL-NEXT:    srld $r1 = $r1, 32
; ALL-NEXT:    frecw $r3 = $r1
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    frecw $r1 = $r1
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    insf $r3 = $r1, 63, 32
; ALL-NEXT:    ;; # (end cycle 16)
; ALL-NEXT:    fmulwp $r0 = $r0, $r3
; ALL-NEXT:    ;; # (end cycle 17)
; ALL-NEXT:    sd 0[$r2] = $r0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 20)
  %4 = fdiv fast <2 x float> %0, %1
  store <2 x float> %4, ptr %2, align 8
  ret void
}

define void @float4fdiv(<4 x float> %0, <4 x float> %1, ptr %2) {
; ALL-LABEL: float4fdiv:
; ALL:       # %bb.0:
; ALL-NEXT:    srld $r3 = $r3, 32
; ALL-NEXT:    frecw $r5 = $r3
; ALL-NEXT:    srld $r6 = $r2, 32
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    frecw $r3 = $r3
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    frecw $r2 = $r2
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    frecw $r6 = $r6
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    insf $r5 = $r3, 63, 32
; ALL-NEXT:    ;; # (end cycle 16)
; ALL-NEXT:    fmulwp $r1 = $r1, $r5
; ALL-NEXT:    ;; # (end cycle 17)
; ALL-NEXT:    insf $r2 = $r6, 63, 32
; ALL-NEXT:    ;; # (end cycle 18)
; ALL-NEXT:    fmulwp $r0 = $r0, $r2
; ALL-NEXT:    ;; # (end cycle 19)
; ALL-NEXT:    sq 0[$r4] = $r0r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 22)
  %4 = fdiv fast <4 x float> %0, %1
  store <4 x float> %4, ptr %2, align 16
  ret void
}

define void @float8fdiv(<8 x float> %0, <8 x float> %1, ptr %2) {
; ALL-LABEL: float8fdiv:
; ALL:       # %bb.0:
; ALL-NEXT:    srld $r7 = $r7, 32
; ALL-NEXT:    frecw $r9 = $r7
; ALL-NEXT:    srld $r11 = $r5, 32
; ALL-NEXT:    srld $r15 = $r4, 32
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    srld $r6 = $r6, 32
; ALL-NEXT:    frecw $r10 = $r6
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    frecw $r7 = $r7
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    frecw $r6 = $r6
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    frecw $r5 = $r5
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    frecw $r11 = $r11
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    frecw $r4 = $r4
; ALL-NEXT:    ;; # (end cycle 6)
; ALL-NEXT:    frecw $r15 = $r15
; ALL-NEXT:    ;; # (end cycle 7)
; ALL-NEXT:    insf $r9 = $r7, 63, 32
; ALL-NEXT:    ;; # (end cycle 17)
; ALL-NEXT:    fmulwp $r3 = $r3, $r9
; ALL-NEXT:    insf $r10 = $r6, 63, 32
; ALL-NEXT:    ;; # (end cycle 18)
; ALL-NEXT:    fmulwp $r2 = $r2, $r10
; ALL-NEXT:    ;; # (end cycle 19)
; ALL-NEXT:    insf $r5 = $r11, 63, 32
; ALL-NEXT:    ;; # (end cycle 20)
; ALL-NEXT:    fmulwp $r1 = $r1, $r5
; ALL-NEXT:    ;; # (end cycle 21)
; ALL-NEXT:    insf $r4 = $r15, 63, 32
; ALL-NEXT:    ;; # (end cycle 22)
; ALL-NEXT:    fmulwp $r0 = $r0, $r4
; ALL-NEXT:    ;; # (end cycle 23)
; ALL-NEXT:    so 0[$r8] = $r0r1r2r3
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 26)
  %4 = fdiv fast <8 x float> %0, %1
  store <8 x float> %4, ptr %2, align 32
  ret void
}

define void @float16fdiv(ptr %0, ptr %1, ptr %2) {
; ALL-LABEL: float16fdiv:
; ALL:       # %bb.0:
; ALL-NEXT:    lo $r4r5r6r7 = 0[$r1]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    lo $r8r9r10r11 = 32[$r1]
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    lo $r36r37r38r39 = 0[$r0]
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    lo $r40r41r42r43 = 32[$r0]
; ALL-NEXT:    srld $r1 = $r5, 32
; ALL-NEXT:    srld $r6 = $r6, 32
; ALL-NEXT:    frecw $r15 = $r6
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    frecw $r3 = $r7
; ALL-NEXT:    srld $r7 = $r7, 32
; ALL-NEXT:    srld $r17 = $r11, 32
; ALL-NEXT:    srld $r32 = $r10, 32
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    frecw $r16 = $r5
; ALL-NEXT:    srld $r33 = $r9, 32
; ALL-NEXT:    srld $r34 = $r8, 32
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    frecw $r5 = $r6
; ALL-NEXT:    ;; # (end cycle 6)
; ALL-NEXT:    srld $r4 = $r4, 32
; ALL-NEXT:    frecw $r6 = $r4
; ALL-NEXT:    ;; # (end cycle 7)
; ALL-NEXT:    frecw $r7 = $r7
; ALL-NEXT:    ;; # (end cycle 8)
; ALL-NEXT:    frecw $r1 = $r1
; ALL-NEXT:    ;; # (end cycle 9)
; ALL-NEXT:    frecw $r11 = $r11
; ALL-NEXT:    ;; # (end cycle 10)
; ALL-NEXT:    frecw $r17 = $r17
; ALL-NEXT:    ;; # (end cycle 11)
; ALL-NEXT:    frecw $r10 = $r10
; ALL-NEXT:    ;; # (end cycle 12)
; ALL-NEXT:    frecw $r32 = $r32
; ALL-NEXT:    ;; # (end cycle 13)
; ALL-NEXT:    frecw $r9 = $r9
; ALL-NEXT:    ;; # (end cycle 14)
; ALL-NEXT:    frecw $r33 = $r33
; ALL-NEXT:    ;; # (end cycle 15)
; ALL-NEXT:    frecw $r8 = $r8
; ALL-NEXT:    ;; # (end cycle 16)
; ALL-NEXT:    frecw $r34 = $r34
; ALL-NEXT:    ;; # (end cycle 17)
; ALL-NEXT:    frecw $r4 = $r4
; ALL-NEXT:    ;; # (end cycle 18)
; ALL-NEXT:    insf $r15 = $r5, 63, 32
; ALL-NEXT:    ;; # (end cycle 21)
; ALL-NEXT:    fmulwp $r38 = $r38, $r15
; ALL-NEXT:    ;; # (end cycle 22)
; ALL-NEXT:    insf $r3 = $r7, 63, 32
; ALL-NEXT:    ;; # (end cycle 23)
; ALL-NEXT:    insf $r16 = $r1, 63, 32
; ALL-NEXT:    fmulwp $r39 = $r39, $r3
; ALL-NEXT:    ;; # (end cycle 24)
; ALL-NEXT:    fmulwp $r37 = $r37, $r16
; ALL-NEXT:    ;; # (end cycle 25)
; ALL-NEXT:    insf $r11 = $r17, 63, 32
; ALL-NEXT:    ;; # (end cycle 26)
; ALL-NEXT:    fmulwp $r11 = $r43, $r11
; ALL-NEXT:    ;; # (end cycle 27)
; ALL-NEXT:    insf $r10 = $r32, 63, 32
; ALL-NEXT:    ;; # (end cycle 28)
; ALL-NEXT:    fmulwp $r10 = $r42, $r10
; ALL-NEXT:    ;; # (end cycle 29)
; ALL-NEXT:    insf $r9 = $r33, 63, 32
; ALL-NEXT:    ;; # (end cycle 30)
; ALL-NEXT:    fmulwp $r9 = $r41, $r9
; ALL-NEXT:    ;; # (end cycle 31)
; ALL-NEXT:    insf $r8 = $r34, 63, 32
; ALL-NEXT:    ;; # (end cycle 32)
; ALL-NEXT:    insf $r6 = $r4, 63, 32
; ALL-NEXT:    fmulwp $r8 = $r40, $r8
; ALL-NEXT:    ;; # (end cycle 33)
; ALL-NEXT:    fmulwp $r36 = $r36, $r6
; ALL-NEXT:    ;; # (end cycle 34)
; ALL-NEXT:    so 32[$r2] = $r8r9r10r11
; ALL-NEXT:    ;; # (end cycle 36)
; ALL-NEXT:    so 0[$r2] = $r36r37r38r39
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 37)
  %4 = load <16 x float>, ptr %0, align 64
  %5 = load <16 x float>, ptr %1, align 64
  %6 = fdiv fast <16 x float> %4, %5
  store <16 x float> %6, ptr %2, align 64
  ret void
}

define void @doublediv(double %0, double %1, ptr %2) {
; ALL-LABEL: doublediv:
; ALL:       # %bb.0:
; ALL-NEXT:    addd $r12 = $r12, -32
; ALL-NEXT:    get $r16 = $ra
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sd 24[$r12] = $r16
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    sd 16[$r12] = $r18
; ALL-NEXT:    copyd $r18 = $r2
; ALL-NEXT:    call __divdf3
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    sd 0[$r18] = $r0
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    ld $r18 = 16[$r12]
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    ld $r16 = 24[$r12]
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    set $ra = $r16
; ALL-NEXT:    addd $r12 = $r12, 32
; ALL-NEXT:    ;; # (end cycle 7)
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %4 = fdiv fast double %0, %1
  store double %4, ptr %2, align 8
  ret void
}

define void @double2fdiv(<2 x double> %0, <2 x double> %1, ptr %2) {
; ALL-LABEL: double2fdiv:
; ALL:       # %bb.0:
; ALL-NEXT:    addd $r12 = $r12, -32
; ALL-NEXT:    get $r16 = $ra
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sd 24[$r12] = $r16
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    sd 16[$r12] = $r18
; ALL-NEXT:    copyd $r18 = $r4
; ALL-NEXT:    call __divv2df3
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    sq 0[$r18] = $r0r1
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    ld $r18 = 16[$r12]
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    ld $r16 = 24[$r12]
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    set $ra = $r16
; ALL-NEXT:    addd $r12 = $r12, 32
; ALL-NEXT:    ;; # (end cycle 7)
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %4 = fdiv fast <2 x double> %0, %1
  store <2 x double> %4, ptr %2, align 16
  ret void
}

define void @double4fdiv(<4 x double> %0, <4 x double> %1, ptr %2) {
; ALL-LABEL: double4fdiv:
; ALL:       # %bb.0:
; ALL-NEXT:    addd $r12 = $r12, -32
; ALL-NEXT:    get $r16 = $ra
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sd 24[$r12] = $r16
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    sd 16[$r12] = $r18
; ALL-NEXT:    copyd $r18 = $r8
; ALL-NEXT:    call __divv4df3
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    so 0[$r18] = $r0r1r2r3
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    ld $r18 = 16[$r12]
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    ld $r16 = 24[$r12]
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    set $ra = $r16
; ALL-NEXT:    addd $r12 = $r12, 32
; ALL-NEXT:    ;; # (end cycle 7)
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %4 = fdiv fast <4 x double> %0, %1
  store <4 x double> %4, ptr %2, align 32
  ret void
}

define void @double8fdiv(ptr %0, ptr %1, ptr %2) {
; ALL-LABEL: double8fdiv:
; ALL:       # %bb.0:
; ALL-NEXT:    addd $r12 = $r12, -128
; ALL-NEXT:    get $r16 = $ra
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sd 120[$r12] = $r16
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    so 88[$r12] = $r28r29r30r31
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    so 56[$r12] = $r24r25r26r27
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    so 24[$r12] = $r20r21r22r23
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    sd 16[$r12] = $r18
; ALL-NEXT:    copyd $r18 = $r2
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    lo $r8r9r10r11 = 32[$r0]
; ALL-NEXT:    ;; # (end cycle 6)
; ALL-NEXT:    lo $r4r5r6r7 = 32[$r1]
; ALL-NEXT:    ;; # (end cycle 7)
; ALL-NEXT:    lo $r24r25r26r27 = 0[$r0]
; ALL-NEXT:    ;; # (end cycle 8)
; ALL-NEXT:    lo $r20r21r22r23 = 0[$r1]
; ALL-NEXT:    copyd $r0 = $r8
; ALL-NEXT:    copyd $r1 = $r9
; ALL-NEXT:    copyd $r2 = $r10
; ALL-NEXT:    ;; # (end cycle 9)
; ALL-NEXT:    copyd $r3 = $r11
; ALL-NEXT:    call __divv4df3
; ALL-NEXT:    ;; # (end cycle 10)
; ALL-NEXT:    copyd $r28 = $r0
; ALL-NEXT:    copyd $r29 = $r1
; ALL-NEXT:    copyd $r30 = $r2
; ALL-NEXT:    copyd $r31 = $r3
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    copyd $r0 = $r24
; ALL-NEXT:    copyd $r1 = $r25
; ALL-NEXT:    copyd $r2 = $r26
; ALL-NEXT:    copyd $r3 = $r27
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    copyd $r4 = $r20
; ALL-NEXT:    copyd $r5 = $r21
; ALL-NEXT:    copyd $r6 = $r22
; ALL-NEXT:    copyd $r7 = $r23
; ALL-NEXT:    call __divv4df3
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    so 32[$r18] = $r28r29r30r31
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    so 0[$r18] = $r0r1r2r3
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    ld $r18 = 16[$r12]
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    lo $r20r21r22r23 = 24[$r12]
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    lo $r24r25r26r27 = 56[$r12]
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    lo $r28r29r30r31 = 88[$r12]
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    ld $r16 = 120[$r12]
; ALL-NEXT:    ;; # (end cycle 6)
; ALL-NEXT:    set $ra = $r16
; ALL-NEXT:    addd $r12 = $r12, 128
; ALL-NEXT:    ;; # (end cycle 11)
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %4 = load <8 x double>, ptr %0, align 64
  %5 = load <8 x double>, ptr %1, align 64
  %6 = fdiv fast <8 x double> %4, %5
  store <8 x double> %6, ptr %2, align 64
  ret void
}

define void @double16fdiv(ptr %0, ptr %1, ptr %2) {
; ALL-LABEL: double16fdiv:
; ALL:       # %bb.0:
; ALL-NEXT:    addd $r12 = $r12, -256
; ALL-NEXT:    get $r16 = $ra
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sd 248[$r12] = $r16
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    so 216[$r12] = $r28r29r30r31
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    so 184[$r12] = $r24r25r26r27
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    so 152[$r12] = $r20r21r22r23
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    sd 144[$r12] = $r18
; ALL-NEXT:    copyd $r18 = $r2
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    lo $r4r5r6r7 = 64[$r0]
; ALL-NEXT:    ;; # (end cycle 6)
; ALL-NEXT:    so 96[$r12] = $r4r5r6r7
; ALL-NEXT:    ;; # (end cycle 8)
; ALL-NEXT:    lo $r4r5r6r7 = 96[$r0]
; ALL-NEXT:    ;; # (end cycle 9)
; ALL-NEXT:    so 32[$r12] = $r4r5r6r7
; ALL-NEXT:    ;; # (end cycle 11)
; ALL-NEXT:    lo $r4r5r6r7 = 64[$r1]
; ALL-NEXT:    ;; # (end cycle 12)
; ALL-NEXT:    lo $r20r21r22r23 = 0[$r0]
; ALL-NEXT:    ;; # (end cycle 13)
; ALL-NEXT:    lo $r8r9r10r11 = 32[$r0]
; ALL-NEXT:    ;; # (end cycle 14)
; ALL-NEXT:    so 64[$r12] = $r4r5r6r7
; ALL-NEXT:    ;; # (end cycle 15)
; ALL-NEXT:    lo $r4r5r6r7 = 96[$r1]
; ALL-NEXT:    ;; # (end cycle 16)
; ALL-NEXT:    so 0[$r12] = $r4r5r6r7
; ALL-NEXT:    ;; # (end cycle 18)
; ALL-NEXT:    lo $r4r5r6r7 = 32[$r1]
; ALL-NEXT:    ;; # (end cycle 19)
; ALL-NEXT:    lo $r24r25r26r27 = 0[$r1]
; ALL-NEXT:    copyd $r0 = $r8
; ALL-NEXT:    copyd $r1 = $r9
; ALL-NEXT:    copyd $r2 = $r10
; ALL-NEXT:    ;; # (end cycle 20)
; ALL-NEXT:    copyd $r3 = $r11
; ALL-NEXT:    call __divv4df3
; ALL-NEXT:    ;; # (end cycle 21)
; ALL-NEXT:    copyd $r28 = $r0
; ALL-NEXT:    copyd $r29 = $r1
; ALL-NEXT:    copyd $r30 = $r2
; ALL-NEXT:    copyd $r31 = $r3
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    copyd $r0 = $r20
; ALL-NEXT:    copyd $r1 = $r21
; ALL-NEXT:    copyd $r2 = $r22
; ALL-NEXT:    copyd $r3 = $r23
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    copyd $r4 = $r24
; ALL-NEXT:    copyd $r5 = $r25
; ALL-NEXT:    copyd $r6 = $r26
; ALL-NEXT:    copyd $r7 = $r27
; ALL-NEXT:    call __divv4df3
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    copyd $r20 = $r0
; ALL-NEXT:    copyd $r21 = $r1
; ALL-NEXT:    copyd $r22 = $r2
; ALL-NEXT:    copyd $r23 = $r3
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    lo $r0r1r2r3 = 32[$r12]
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    lo $r4r5r6r7 = 0[$r12]
; ALL-NEXT:    call __divv4df3
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    copyd $r24 = $r0
; ALL-NEXT:    copyd $r25 = $r1
; ALL-NEXT:    copyd $r26 = $r2
; ALL-NEXT:    copyd $r27 = $r3
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    lo $r0r1r2r3 = 96[$r12]
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    lo $r4r5r6r7 = 64[$r12]
; ALL-NEXT:    call __divv4df3
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    so 96[$r18] = $r24r25r26r27
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    so 0[$r18] = $r20r21r22r23
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    so 64[$r18] = $r0r1r2r3
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    so 32[$r18] = $r28r29r30r31
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    ld $r18 = 144[$r12]
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    lo $r20r21r22r23 = 152[$r12]
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    lo $r24r25r26r27 = 184[$r12]
; ALL-NEXT:    ;; # (end cycle 6)
; ALL-NEXT:    lo $r28r29r30r31 = 216[$r12]
; ALL-NEXT:    ;; # (end cycle 7)
; ALL-NEXT:    ld $r16 = 248[$r12]
; ALL-NEXT:    ;; # (end cycle 8)
; ALL-NEXT:    set $ra = $r16
; ALL-NEXT:    addd $r12 = $r12, 256
; ALL-NEXT:    ;; # (end cycle 13)
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %4 = load <16 x double>, ptr %0, align 128
  %5 = load <16 x double>, ptr %1, align 128
  %6 = fdiv fast <16 x double> %4, %5
  store <16 x double> %6, ptr %2, align 128
  ret void
}

