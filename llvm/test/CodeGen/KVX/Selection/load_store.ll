; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -o - %s | FileCheck %s --check-prefixes=CHECK,CV1
; RUN: llc -mcpu=kv3-2 -o - %s | FileCheck %s --check-prefixes=CHECK,CV2
; RUN: clang -c -o /dev/null %s
; RUN: clang -march=kv3-2 -c -o /dev/null %s

target triple = "kvx-kalray-cos"

define void @so_4xdouble_ri(<4 x double> %0, <4 x double>* %1) {
; CHECK-LABEL: so_4xdouble_ri:
; CHECK:       # %bb.0:
; CHECK-NEXT:    so 32[$r4] = $r0r1r2r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %3 = getelementptr inbounds <4 x double>, <4 x double>* %1, i64 1
  store volatile <4 x double> %0, <4 x double>* %3, align 32
  ret void
}

define void @so_4xdouble_rr(<4 x double> %0, <4 x double>* %1, i32 %2) {
; CHECK-LABEL: so_4xdouble_rr:
; CHECK:       # %bb.0:
; CHECK-NEXT:    sxwd $r5 = $r5
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    so.xs $r5[$r4] = $r0r1r2r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
  %4 = sext i32 %2 to i64
  %5 = getelementptr inbounds <4 x double>, <4 x double>* %1, i64 %4
  store volatile <4 x double> %0, <4 x double>* %5, align 32
  ret void
}

define void @lo_4xdouble_ri(<4 x double>* %0) {
; CHECK-LABEL: lo_4xdouble_ri:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo $r0r1r2r3 = -32[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %2 = getelementptr inbounds <4 x double>, <4 x double>* %0, i64 -1
  %3 = load volatile <4 x double>, <4 x double>* %2, align 32
  ret void
}

define void @lo_4xdouble_rr(<4 x i64>* %0, i32 %1) {
; CHECK-LABEL: lo_4xdouble_rr:
; CHECK:       # %bb.0:
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    lo.xs $r0r1r2r3 = $r1[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
  %3 = sext i32 %1 to i64
  %4 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 %3
  %5 = load volatile <4 x i64>, <4 x i64>* %4, align 32
  ret void
}

define void @lo_4xdouble_ri_p(<4 x double> addrspace(257)* %0) {
; CHECK-LABEL: lo_4xdouble_ri_p:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo.us $r0r1r2r3 = -32[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %2 = getelementptr inbounds <4 x double>, <4 x double> addrspace(257)* %0, i64 -1
  %3 = load volatile <4 x double>, <4 x double> addrspace(257)* %2, align 32
  ret void
}

define void @lo_4xdouble_rr_p(<4 x double> addrspace(257)* %0, i32 %1) {
; CHECK-LABEL: lo_4xdouble_rr_p:
; CHECK:       # %bb.0:
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    lo.us.xs $r0r1r2r3 = $r1[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
  %3 = sext i32 %1 to i64
  %4 = getelementptr inbounds <4 x double>, <4 x double> addrspace(257)* %0, i64 %3
  %5 = load volatile <4 x double>, <4 x double> addrspace(257)* %4, align 32
  ret void
}

define void @lo_4xdouble_ri_b(<4 x double> addrspace(256)* %0) {
; CHECK-LABEL: lo_4xdouble_ri_b:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo.u $r0r1r2r3 = -32[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %2 = getelementptr inbounds <4 x double>, <4 x double> addrspace(256)* %0, i64 -1
  %3 = load volatile <4 x double>, <4 x double> addrspace(256)* %2, align 32
  ret void
}

define void @lo_4xdouble_rr_b(<4 x double> addrspace(256)* %0, i32 %1) {
; CHECK-LABEL: lo_4xdouble_rr_b:
; CHECK:       # %bb.0:
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    lo.u.xs $r0r1r2r3 = $r1[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
  %3 = sext i32 %1 to i64
  %4 = getelementptr inbounds <4 x double>, <4 x double> addrspace(256)* %0, i64 %3
  %5 = load volatile <4 x double>, <4 x double> addrspace(256)* %4, align 32
  ret void
}

define void @lo_4xdouble_ri_s(<4 x double> addrspace(258)* %0) {
; CHECK-LABEL: lo_4xdouble_ri_s:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo.s $r0r1r2r3 = -32[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %2 = getelementptr inbounds <4 x double>, <4 x double> addrspace(258)* %0, i64 -1
  %3 = load volatile <4 x double>, <4 x double> addrspace(258)* %2, align 32
  ret void
}

define void @lo_4xdouble_rr_s(<4 x double> addrspace(258)* %0, i32 %1) {
; CHECK-LABEL: lo_4xdouble_rr_s:
; CHECK:       # %bb.0:
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    lo.s.xs $r0r1r2r3 = $r1[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
  %3 = sext i32 %1 to i64
  %4 = getelementptr inbounds <4 x double>, <4 x double> addrspace(258)* %0, i64 %3
  %5 = load volatile <4 x double>, <4 x double> addrspace(258)* %4, align 32
  ret void
}

define void @so_4xi64_ri(<4 x i64> %0, <4 x i64>* %1) {
; CHECK-LABEL: so_4xi64_ri:
; CHECK:       # %bb.0:
; CHECK-NEXT:    so 32[$r4] = $r0r1r2r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %3 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  store volatile <4 x i64> %0, <4 x i64>* %3, align 32
  ret void
}

define void @so_4xi64_rr(<4 x i64> %0, <4 x i64>* %1, i32 %2) {
; CHECK-LABEL: so_4xi64_rr:
; CHECK:       # %bb.0:
; CHECK-NEXT:    sxwd $r5 = $r5
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    so.xs $r5[$r4] = $r0r1r2r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
  %4 = sext i32 %2 to i64
  %5 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 %4
  store volatile <4 x i64> %0, <4 x i64>* %5, align 32
  ret void
}

define void @lo_4xi64_ri(<4 x i64>* %0) {
; CHECK-LABEL: lo_4xi64_ri:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo $r0r1r2r3 = -32[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %2 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 -1
  %3 = load volatile <4 x i64>, <4 x i64>* %2, align 32
  ret void
}

define void @lo_4xi64_rr(<4 x i64>* %0, i32 %1) {
; CHECK-LABEL: lo_4xi64_rr:
; CHECK:       # %bb.0:
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    lo.xs $r0r1r2r3 = $r1[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
  %3 = sext i32 %1 to i64
  %4 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 %3
  %5 = load volatile <4 x i64>, <4 x i64>* %4, align 32
  ret void
}

define void @lo_4xi64_ri_p(<4 x i64> addrspace(257)* %0) {
; CHECK-LABEL: lo_4xi64_ri_p:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo.us $r0r1r2r3 = -32[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %2 = getelementptr inbounds <4 x i64>, <4 x i64> addrspace(257)* %0, i64 -1
  %3 = load volatile <4 x i64>, <4 x i64> addrspace(257)* %2, align 32
  ret void
}

define void @lo_4xi64_rr_p(<4 x i64> addrspace(257)* %0, i32 %1) {
; CHECK-LABEL: lo_4xi64_rr_p:
; CHECK:       # %bb.0:
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    lo.us.xs $r0r1r2r3 = $r1[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
  %3 = sext i32 %1 to i64
  %4 = getelementptr inbounds <4 x i64>, <4 x i64> addrspace(257)* %0, i64 %3
  %5 = load volatile <4 x i64>, <4 x i64> addrspace(257)* %4, align 32
  ret void
}

define void @lo_4xi64_ri_b(<4 x i64> addrspace(256)* %0) {
; CHECK-LABEL: lo_4xi64_ri_b:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo.u $r0r1r2r3 = -32[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %2 = getelementptr inbounds <4 x i64>, <4 x i64> addrspace(256)* %0, i64 -1
  %3 = load volatile <4 x i64>, <4 x i64> addrspace(256)* %2, align 32
  ret void
}

define void @lo_4xi64_rr_b(<4 x i64> addrspace(256)* %0, i32 %1) {
; CHECK-LABEL: lo_4xi64_rr_b:
; CHECK:       # %bb.0:
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    lo.u.xs $r0r1r2r3 = $r1[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
  %3 = sext i32 %1 to i64
  %4 = getelementptr inbounds <4 x i64>, <4 x i64> addrspace(256)* %0, i64 %3
  %5 = load volatile <4 x i64>, <4 x i64> addrspace(256)* %4, align 32
  ret void
}

define void @lo_4xi64_ri_s(<4 x i64> addrspace(258)* %0) {
; CHECK-LABEL: lo_4xi64_ri_s:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo.s $r0r1r2r3 = -32[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %2 = getelementptr inbounds <4 x i64>, <4 x i64> addrspace(258)* %0, i64 -1
  %3 = load volatile <4 x i64>, <4 x i64> addrspace(258)* %2, align 32
  ret void
}

define void @lo_4xi64_rr_s(<4 x i64> addrspace(258)* %0, i32 %1) {
; CHECK-LABEL: lo_4xi64_rr_s:
; CHECK:       # %bb.0:
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    lo.s.xs $r0r1r2r3 = $r1[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
  %3 = sext i32 %1 to i64
  %4 = getelementptr inbounds <4 x i64>, <4 x i64> addrspace(258)* %0, i64 %3
  %5 = load volatile <4 x i64>, <4 x i64> addrspace(258)* %4, align 32
  ret void
}

define void @lv_s_sv(<256 x i1> addrspace(258)* %0, i64 %1) {
; CV1-LABEL: lv_s_sv:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    xlo.us.xs $a0 = $r1[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    xso 0[$r0] = $a0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: lv_s_sv:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    xlo.s.xs $a0 = $r1[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    xso 0[$r0] = $a0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 4)
entry:
  %2 = getelementptr inbounds <256 x i1>, <256 x i1> addrspace(258)* %0, i64 %1
  %3 = load volatile <256 x i1>, <256 x i1> addrspace(258)* %2, align 32
  store volatile <256 x i1> %3, <256 x i1>addrspace(258)* %0, align 32
  ret void
}

define void @lv_sv_space256(<256 x i1> addrspace(256)* %0, i64 %1) {
; CV1-LABEL: lv_sv_space256:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    xlo.u.xs $a0 = $r1[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    xso 0[$r0] = $a0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: lv_sv_space256:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    xlo.u.xs $a0 = $r1[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    xso 0[$r0] = $a0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 4)
entry:
  %2 = getelementptr inbounds <256 x i1>, <256 x i1> addrspace(256)* %0, i64 %1
  %3 = load volatile <256 x i1>, <256 x i1> addrspace(256)* %2, align 32
  store volatile <256 x i1> %3, <256 x i1>addrspace(256)* %0, align 32
  ret void
}

define void @lv_sv_space257(<256 x i1> addrspace(257)* %0, i64 %1) {
; CV1-LABEL: lv_sv_space257:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    xlo.us.xs $a0 = $r1[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    xso 0[$r0] = $a0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: lv_sv_space257:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    xlo.us.xs $a0 = $r1[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    xso 0[$r0] = $a0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 4)
entry:
  %2 = getelementptr inbounds <256 x i1>, <256 x i1> addrspace(257)* %0, i64 %1
  %3 = load volatile <256 x i1>, <256 x i1> addrspace(257)* %2, align 32
  store volatile <256 x i1> %3, <256 x i1>addrspace(257)* %0, align 32
  ret void
}

define void @lv_sv(<256 x i1> * %0, i64 %1) {
; CV1-LABEL: lv_sv:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    xlo.u.xs $a0 = $r1[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    xso 0[$r0] = $a0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: lv_sv:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    xlo.xs $a0 = $r1[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    xso 0[$r0] = $a0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 4)
entry:
  %2 = getelementptr inbounds <256 x i1>, <256 x i1> * %0, i64 %1
  %3 = load volatile <256 x i1>, <256 x i1>* %2, align 32
  store volatile <256 x i1> %3, <256 x i1>* %0, align 32
  ret void
}

define void @lw_sw(<512 x i1> * %0, i64 %1) {
; CV1-LABEL: lw_sw:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    slld $r1 = $r1, 6
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    xlo.u $a1 = $r1[$r0]
; CV1-NEXT:    addd $r2 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    xlo.u $a0 = 32[$r2]
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    xso 0[$r0] = $a1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    xso 32[$r0] = $a0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 5)
;
; CV2-LABEL: lw_sw:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    slld $r1 = $r1, 6
; CV2-NEXT:    addx64d $r2 = $r1, $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    xlo $a0 = 32[$r2]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    xlo $a1 = $r1[$r0]
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    xso 32[$r0] = $a0
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    xso 0[$r0] = $a1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 6)
entry:
  %2 = getelementptr inbounds <512 x i1>, <512 x i1> * %0, i64 %1
  %3 = load <512 x i1>, <512 x i1>* %2, align 32
  store <512 x i1> %3, <512 x i1>* %0, align 32
  ret void
}

define void @lm_sm(<1024 x i1> * %0, i64 %1) {
; CV1-LABEL: lm_sm:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    slld $r1 = $r1, 7
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    xlo.u $a0 = $r1[$r0]
; CV1-NEXT:    addd $r2 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    xlo.u $a1 = 64[$r2]
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    xlo.u $a2 = 32[$r2]
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    xlo.u $a3 = 96[$r2]
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    xso 0[$r0] = $a0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    xso 32[$r0] = $a2
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    xso 96[$r0] = $a3
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    xso 64[$r0] = $a1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 8)
;
; CV2-LABEL: lm_sm:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    slld $r1 = $r1, 7
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    xlo $a0 = $r1[$r0]
; CV2-NEXT:    addd $r2 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    xlo $a1 = 32[$r2]
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    xlo $a2 = 96[$r2]
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    xlo $a3 = 64[$r2]
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    xso 0[$r0] = $a0
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    xso 32[$r0] = $a1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    xso 96[$r0] = $a2
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    xso 64[$r0] = $a3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
entry:
  %2 = getelementptr inbounds <1024 x i1>, <1024 x i1> * %0, i64 %1
  %3 = load <1024 x i1>, <1024 x i1>* %2, align 32
  store <1024 x i1> %3, <1024 x i1>* %0, align 32
  ret void
}

define void @lo_4xi64_ri_unaligned(<4 x i64>* %0) {
; CHECK-LABEL: lo_4xi64_ri_unaligned:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo $r0r1r2r3 = -32[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %2 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 -1
  %3 = load volatile <4 x i64>, <4 x i64>* %2, align 1
  ret void
}

define void @so_4xdouble_ri_unaligned(<4 x double> %0, <4 x double>* %1) {
; CHECK-LABEL: so_4xdouble_ri_unaligned:
; CHECK:       # %bb.0:
; CHECK-NEXT:    so 32[$r4] = $r0r1r2r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %3 = getelementptr inbounds <4 x double>, <4 x double>* %1, i64 1
  store volatile <4 x double> %0, <4 x double>* %3, align 1
  ret void
}

define <16 x half> @loadhalf16(<16 x half>* nocapture readonly %v) {
; CHECK-LABEL: loadhalf16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    lo $r0r1r2r3 = 96[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <16 x half>, <16 x half>* %v, i64 3
  %0 = load <16 x half>, <16 x half>* %arrayidx, align 32
  ret <16 x half> %0
}

define void @storehalf16(<16 x half> %v, <16 x half>* nocapture %p) {
; CHECK-LABEL: storehalf16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    so 32[$r4] = $r0r1r2r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <16 x half>, <16 x half>* %p, i64 1
  store <16 x half> %v, <16 x half>* %arrayidx, align 32
  ret void
}

define <8 x float> @loadfloat8(<8 x float>* nocapture readonly %v) {
; CHECK-LABEL: loadfloat8:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    lo $r0r1r2r3 = 96[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <8 x float>, <8 x float>* %v, i64 3
  %0 = load <8 x float>, <8 x float>* %arrayidx, align 32
  ret <8 x float> %0
}

define void @storefloat8(<8 x float> %v, <8 x float>* nocapture %p) {
; CHECK-LABEL: storefloat8:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    so 32[$r4] = $r0r1r2r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <8 x float>, <8 x float>* %p, i64 1
  store <8 x float> %v, <8 x float>* %arrayidx, align 32
  ret void
}

define <32 x i8> @loadchar32(<32 x i8>* nocapture readonly %v) {
; CHECK-LABEL: loadchar32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    lo $r0r1r2r3 = 96[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <32 x i8>, <32 x i8>* %v, i64 3
  %0 = load <32 x i8>, <32 x i8>* %arrayidx, align 32
  ret <32 x i8> %0
}

define void @storechar32(<32 x i8> %v, <32 x i8>* nocapture %p) {
; CHECK-LABEL: storechar32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    so 32[$r4] = $r0r1r2r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <32 x i8>, <32 x i8>* %p, i64 1
  store <32 x i8> %v, <32 x i8>* %arrayidx, align 32
  ret void
}

define <32 x i8> @loaduchar32(<32 x i8>* nocapture readonly %v) {
; CHECK-LABEL: loaduchar32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    lo $r0r1r2r3 = 96[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <32 x i8>, <32 x i8>* %v, i64 3
  %0 = load <32 x i8>, <32 x i8>* %arrayidx, align 32
  ret <32 x i8> %0
}

define void @storeuchar32(<32 x i8> %v, <32 x i8>* nocapture %p) {
; CHECK-LABEL: storeuchar32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    so 32[$r4] = $r0r1r2r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <32 x i8>, <32 x i8>* %p, i64 1
  store <32 x i8> %v, <32 x i8>* %arrayidx, align 32
  ret void
}

define <16 x i16> @loadshort16(<16 x i16>* nocapture readonly %v) {
; CHECK-LABEL: loadshort16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    lo $r0r1r2r3 = 96[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <16 x i16>, <16 x i16>* %v, i64 3
  %0 = load <16 x i16>, <16 x i16>* %arrayidx, align 32
  ret <16 x i16> %0
}

define void @storeshort16(<16 x i16> %v, <16 x i16>* nocapture %p) {
; CHECK-LABEL: storeshort16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    so 32[$r4] = $r0r1r2r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <16 x i16>, <16 x i16>* %p, i64 1
  store <16 x i16> %v, <16 x i16>* %arrayidx, align 32
  ret void
}

define <16 x i16> @loadushort16(<16 x i16>* nocapture readonly %v) {
; CHECK-LABEL: loadushort16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    lo $r0r1r2r3 = 96[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <16 x i16>, <16 x i16>* %v, i64 3
  %0 = load <16 x i16>, <16 x i16>* %arrayidx, align 32
  ret <16 x i16> %0
}

define void @storeushort16(<16 x i16> %v, <16 x i16>* nocapture %p) {
; CHECK-LABEL: storeushort16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    so 32[$r4] = $r0r1r2r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <16 x i16>, <16 x i16>* %p, i64 1
  store <16 x i16> %v, <16 x i16>* %arrayidx, align 32
  ret void
}

define <8 x i32> @loadint8(<8 x i32>* nocapture readonly %v) {
; CHECK-LABEL: loadint8:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    lo $r0r1r2r3 = 96[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <8 x i32>, <8 x i32>* %v, i64 3
  %0 = load <8 x i32>, <8 x i32>* %arrayidx, align 32
  ret <8 x i32> %0
}

define void @storeint8(<8 x i32> %v, <8 x i32>* nocapture %p) {
; CHECK-LABEL: storeint8:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    so 32[$r4] = $r0r1r2r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <8 x i32>, <8 x i32>* %p, i64 1
  store <8 x i32> %v, <8 x i32>* %arrayidx, align 32
  ret void
}

define <8 x i32> @loaduint8(<8 x i32>* nocapture readonly %v) {
; CHECK-LABEL: loaduint8:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    lo $r0r1r2r3 = 96[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <8 x i32>, <8 x i32>* %v, i64 3
  %0 = load <8 x i32>, <8 x i32>* %arrayidx, align 32
  ret <8 x i32> %0
}

define void @storeuint8(<8 x i32> %v, <8 x i32>* nocapture %p) {
; CHECK-LABEL: storeuint8:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    so 32[$r4] = $r0r1r2r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <8 x i32>, <8 x i32>* %p, i64 1
  store <8 x i32> %v, <8 x i32>* %arrayidx, align 32
  ret void
}
define <8 x half> @loadhalf8(<8 x half>* nocapture readonly %v) {
; CHECK-LABEL: loadhalf8:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    lq $r0r1 = 48[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <8 x half>, <8 x half>* %v, i64 3
  %0 = load <8 x half>, <8 x half>* %arrayidx, align 16
  ret <8 x half> %0
}

define void @storehalf8(<8 x half> %v, <8 x half>* nocapture %p) {
; CHECK-LABEL: storehalf8:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    sq 16[$r2] = $r0r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <8 x half>, <8 x half>* %p, i64 1
  store <8 x half> %v, <8 x half>* %arrayidx, align 16
  ret void
}

define <4 x float> @loadfloat4(<4 x float>* nocapture readonly %v) {
; CHECK-LABEL: loadfloat4:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    lq $r0r1 = 48[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <4 x float>, <4 x float>* %v, i64 3
  %0 = load <4 x float>, <4 x float>* %arrayidx, align 16
  ret <4 x float> %0
}

define void @storefloat4(<4 x float> %v, <4 x float>* nocapture %p) {
; CHECK-LABEL: storefloat4:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    sq 16[$r2] = $r0r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <4 x float>, <4 x float>* %p, i64 1
  store <4 x float> %v, <4 x float>* %arrayidx, align 16
  ret void
}

define <2 x double> @loaddouble2(<2 x double>* nocapture readonly %v) {
; CHECK-LABEL: loaddouble2:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    lq $r0r1 = 48[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <2 x double>, <2 x double>* %v, i64 3
  %0 = load <2 x double>, <2 x double>* %arrayidx, align 16
  ret <2 x double> %0
}

define void @storedouble2(<2 x double> %v, <2 x double>* nocapture %p) {
; CHECK-LABEL: storedouble2:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    sq 16[$r2] = $r0r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <2 x double>, <2 x double>* %p, i64 1
  store <2 x double> %v, <2 x double>* %arrayidx, align 16
  ret void
}

define <16 x i8> @loadchar16(<16 x i8>* nocapture readonly %v) {
; CHECK-LABEL: loadchar16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    lq $r0r1 = 48[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <16 x i8>, <16 x i8>* %v, i64 3
  %0 = load <16 x i8>, <16 x i8>* %arrayidx, align 16
  ret <16 x i8> %0
}

define void @storechar16(<16 x i8> %v, <16 x i8>* nocapture %p) {
; CHECK-LABEL: storechar16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    sq 16[$r2] = $r0r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <16 x i8>, <16 x i8>* %p, i64 1
  store <16 x i8> %v, <16 x i8>* %arrayidx, align 16
  ret void
}

define <16 x i8> @loaduchar16(<16 x i8>* nocapture readonly %v) {
; CHECK-LABEL: loaduchar16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    lq $r0r1 = 48[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <16 x i8>, <16 x i8>* %v, i64 3
  %0 = load <16 x i8>, <16 x i8>* %arrayidx, align 16
  ret <16 x i8> %0
}

define void @storeuchar16(<16 x i8> %v, <16 x i8>* nocapture %p) {
; CHECK-LABEL: storeuchar16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    sq 16[$r2] = $r0r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <16 x i8>, <16 x i8>* %p, i64 1
  store <16 x i8> %v, <16 x i8>* %arrayidx, align 16
  ret void
}

define <8 x i16> @loadshort8(<8 x i16>* nocapture readonly %v) {
; CHECK-LABEL: loadshort8:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    lq $r0r1 = 48[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <8 x i16>, <8 x i16>* %v, i64 3
  %0 = load <8 x i16>, <8 x i16>* %arrayidx, align 16
  ret <8 x i16> %0
}

define void @storeshort8(<8 x i16> %v, <8 x i16>* nocapture %p) {
; CHECK-LABEL: storeshort8:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    sq 16[$r2] = $r0r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <8 x i16>, <8 x i16>* %p, i64 1
  store <8 x i16> %v, <8 x i16>* %arrayidx, align 16
  ret void
}

define <8 x i16> @loadushort8(<8 x i16>* nocapture readonly %v) {
; CHECK-LABEL: loadushort8:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    lq $r0r1 = 48[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <8 x i16>, <8 x i16>* %v, i64 3
  %0 = load <8 x i16>, <8 x i16>* %arrayidx, align 16
  ret <8 x i16> %0
}

define void @storeushort8(<8 x i16> %v, <8 x i16>* nocapture %p) {
; CHECK-LABEL: storeushort8:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    sq 16[$r2] = $r0r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <8 x i16>, <8 x i16>* %p, i64 1
  store <8 x i16> %v, <8 x i16>* %arrayidx, align 16
  ret void
}

define <4 x i32> @loadint4(<4 x i32>* nocapture readonly %v) {
; CHECK-LABEL: loadint4:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    lq $r0r1 = 48[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <4 x i32>, <4 x i32>* %v, i64 3
  %0 = load <4 x i32>, <4 x i32>* %arrayidx, align 16
  ret <4 x i32> %0
}

define void @storeint4(<4 x i32> %v, <4 x i32>* nocapture %p) {
; CHECK-LABEL: storeint4:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    sq 16[$r2] = $r0r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <4 x i32>, <4 x i32>* %p, i64 1
  store <4 x i32> %v, <4 x i32>* %arrayidx, align 16
  ret void
}

define <4 x i32> @loaduint4(<4 x i32>* nocapture readonly %v) {
; CHECK-LABEL: loaduint4:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    lq $r0r1 = 48[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <4 x i32>, <4 x i32>* %v, i64 3
  %0 = load <4 x i32>, <4 x i32>* %arrayidx, align 16
  ret <4 x i32> %0
}

define void @storeuint4(<4 x i32> %v, <4 x i32>* nocapture %p) {
; CHECK-LABEL: storeuint4:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    sq 16[$r2] = $r0r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <4 x i32>, <4 x i32>* %p, i64 1
  store <4 x i32> %v, <4 x i32>* %arrayidx, align 16
  ret void
}

define <2 x i64> @loadlong2(<2 x i64>* nocapture readonly %v) {
; CHECK-LABEL: loadlong2:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    lq $r0r1 = 48[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <2 x i64>, <2 x i64>* %v, i64 3
  %0 = load <2 x i64>, <2 x i64>* %arrayidx, align 16
  ret <2 x i64> %0
}

define void @storelong2(<2 x i64> %v, <2 x i64>* nocapture %p) {
; CHECK-LABEL: storelong2:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    sq 16[$r2] = $r0r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <2 x i64>, <2 x i64>* %p, i64 1
  store <2 x i64> %v, <2 x i64>* %arrayidx, align 16
  ret void
}

define <2 x i64> @loadulong2(<2 x i64>* nocapture readonly %v) {
; CHECK-LABEL: loadulong2:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    lq $r0r1 = 48[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <2 x i64>, <2 x i64>* %v, i64 3
  %0 = load <2 x i64>, <2 x i64>* %arrayidx, align 16
  ret <2 x i64> %0
}

define void @storeulong2(<2 x i64> %v, <2 x i64>* nocapture %p) {
; CHECK-LABEL: storeulong2:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    sq 16[$r2] = $r0r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %arrayidx = getelementptr inbounds <2 x i64>, <2 x i64>* %p, i64 1
  store <2 x i64> %v, <2 x i64>* %arrayidx, align 16
  ret void
}

define void @lv_sv_space256_512(<512 x i1> addrspace(256)* %0, i64 %1) {
; CV1-LABEL: lv_sv_space256_512:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    slld $r1 = $r1, 6
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    xlo.u $a0 = $r1[$r0]
; CV1-NEXT:    addd $r2 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    xlo.u $a1 = 32[$r2]
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    xso 32[$r0] = $a1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    xso 0[$r0] = $a0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: lv_sv_space256_512:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    addx64d $r1 = $r1, $r0
; CV2-NEXT:    slld $r2 = $r1, 6
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    xlo $a0 = $r2[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    xlo.u $a1 = 32[$r1]
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    xso 32[$r0] = $a1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    xso 0[$r0] = $a0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
entry:
  %2 = getelementptr inbounds <512 x i1>, <512 x i1> addrspace(256)* %0, i64 %1
  %3 = load volatile <512 x i1>, <512 x i1> addrspace(256)* %2, align 32
  store volatile <512 x i1> %3, <512 x i1>addrspace(256)* %0, align 32
  ret void
}

define void @lv_sv_space257_512(<512 x i1> addrspace(257)* %0, i64 %1) {
; CV1-LABEL: lv_sv_space257_512:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    slld $r1 = $r1, 6
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    xlo.u $a0 = $r1[$r0]
; CV1-NEXT:    addd $r2 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    xlo.us $a1 = 32[$r2]
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    xso 32[$r0] = $a1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    xso 0[$r0] = $a0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: lv_sv_space257_512:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    addx64d $r1 = $r1, $r0
; CV2-NEXT:    slld $r2 = $r1, 6
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    xlo $a0 = $r2[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    xlo.us $a1 = 32[$r1]
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    xso 32[$r0] = $a1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    xso 0[$r0] = $a0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
entry:
  %2 = getelementptr inbounds <512 x i1>, <512 x i1> addrspace(257)* %0, i64 %1
  %3 = load volatile <512 x i1>, <512 x i1> addrspace(257)* %2, align 32
  store volatile <512 x i1> %3, <512 x i1>addrspace(257)* %0, align 32
  ret void
}

define void @lv_sv_space258_512(<512 x i1> addrspace(258)* %0, i64 %1) {
; CV1-LABEL: lv_sv_space258_512:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    slld $r1 = $r1, 6
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    xlo.u.xs $a0 = $r1[$r0]
; CV1-NEXT:    addd $r2 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    xlo.us $a1 = 32[$r2]
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    xso 32[$r0] = $a1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    xso 0[$r0] = $a0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: lv_sv_space258_512:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    addx64d $r1 = $r1, $r0
; CV2-NEXT:    slld $r2 = $r1, 6
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    xlo.xs $a0 = $r2[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    xlo.s $a1 = 32[$r1]
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    xso 32[$r0] = $a1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    xso 0[$r0] = $a0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
entry:
  %2 = getelementptr inbounds <512 x i1>, <512 x i1> addrspace(258)* %0, i64 %1
  %3 = load volatile <512 x i1>, <512 x i1> addrspace(258)* %2, align 32
  store volatile <512 x i1> %3, <512 x i1>addrspace(258)* %0, align 32
  ret void
}

define void @lv_sv_space256_1024(<1024 x i1> addrspace(256)* %0, i64 %1) {
; CV1-LABEL: lv_sv_space256_1024:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    slld $r1 = $r1, 7
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    addd $r2 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    xlo.u $a0 = 64[$r2]
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    xlo.u $a1 = 96[$r2]
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    xlo.u $a2 = $r1[$r0]
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    xlo.u $a3 = 32[$r2]
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    xso 32[$r0] = $a3
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    xso 0[$r0] = $a2
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    xso 96[$r0] = $a1
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    xso 64[$r0] = $a0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 11)
;
; CV2-LABEL: lv_sv_space256_1024:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    slld $r1 = $r1, 7
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    addd $r2 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    xlo.u $a0 = 64[$r2]
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    xlo.u $a1 = 96[$r2]
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    xlo $a2 = $r1[$r0]
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    xlo.u $a3 = 32[$r2]
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    xso 32[$r0] = $a3
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    xso 0[$r0] = $a2
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    xso 96[$r0] = $a1
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    xso 64[$r0] = $a0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 12)
entry:
  %2 = getelementptr inbounds <1024 x i1>, <1024 x i1> addrspace(256)* %0, i64 %1
  %3 = load volatile <1024 x i1>, <1024 x i1> addrspace(256)* %2, align 32
  store volatile <1024 x i1> %3, <1024 x i1>addrspace(256)* %0, align 32
  ret void
}

define void @lv_sv_space257_1024(<1024 x i1> addrspace(257)* %0, i64 %1) {
; CV1-LABEL: lv_sv_space257_1024:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    slld $r1 = $r1, 7
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    addd $r2 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    xlo.us $a0 = 64[$r2]
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    xlo.us $a1 = 96[$r2]
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    xlo.u $a2 = $r1[$r0]
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    xlo.us $a3 = 32[$r2]
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    xso 32[$r0] = $a3
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    xso 0[$r0] = $a2
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    xso 96[$r0] = $a1
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    xso 64[$r0] = $a0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 11)
;
; CV2-LABEL: lv_sv_space257_1024:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    slld $r1 = $r1, 7
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    addd $r2 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    xlo.us $a0 = 64[$r2]
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    xlo.us $a1 = 96[$r2]
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    xlo $a2 = $r1[$r0]
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    xlo.us $a3 = 32[$r2]
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    xso 32[$r0] = $a3
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    xso 0[$r0] = $a2
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    xso 96[$r0] = $a1
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    xso 64[$r0] = $a0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 12)
entry:
  %2 = getelementptr inbounds <1024 x i1>, <1024 x i1> addrspace(257)* %0, i64 %1
  %3 = load volatile <1024 x i1>, <1024 x i1> addrspace(257)* %2, align 32
  store volatile <1024 x i1> %3, <1024 x i1>addrspace(257)* %0, align 32
  ret void
}

define void @lv_sv_space258_1024(<1024 x i1> addrspace(258)* %0, i64 %1) {
; CV1-LABEL: lv_sv_space258_1024:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    slld $r1 = $r1, 7
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    addd $r2 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    xlo.us $a0 = 64[$r2]
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    xlo.us $a1 = 96[$r2]
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    xlo.u.xs $a2 = $r1[$r0]
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    xlo.us $a3 = 32[$r2]
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    xso 32[$r0] = $a3
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    xso 0[$r0] = $a2
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    xso 96[$r0] = $a1
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    xso 64[$r0] = $a0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 11)
;
; CV2-LABEL: lv_sv_space258_1024:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    slld $r1 = $r1, 7
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    addd $r2 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    xlo.s $a0 = 64[$r2]
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    xlo.s $a1 = 96[$r2]
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    xlo.xs $a2 = $r1[$r0]
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    xlo.s $a3 = 32[$r2]
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    xso 32[$r0] = $a3
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    xso 0[$r0] = $a2
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    xso 96[$r0] = $a1
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    xso 64[$r0] = $a0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 12)
entry:
  %2 = getelementptr inbounds <1024 x i1>, <1024 x i1> addrspace(258)* %0, i64 %1
  %3 = load volatile <1024 x i1>, <1024 x i1> addrspace(258)* %2, align 32
  store volatile <1024 x i1> %3, <1024 x i1>addrspace(258)* %0, align 32
  ret void
}

define void @ls_2k(<2048 x i1> * %0, i64 %1) {
; CV1-LABEL: ls_2k:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    slld $r1 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    xlo.u $a6 = $r1[$r0]
; CV1-NEXT:    addd $r2 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    xlo.u $a0 = 192[$r2]
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    xlo.u $a1 = 224[$r2]
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    xlo.u $a2 = 128[$r2]
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    xlo.u $a3 = 160[$r2]
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    xlo.u $a4 = 64[$r2]
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    xlo.u $a5 = 32[$r2]
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    xlo.u $a7 = 96[$r2]
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    xso 0[$r0] = $a6
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    xso 32[$r0] = $a5
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    xso 96[$r0] = $a7
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    xso 64[$r0] = $a4
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    xso 160[$r0] = $a3
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    xso 128[$r0] = $a2
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    xso 224[$r0] = $a1
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    xso 192[$r0] = $a0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 16)
;
; CV2-LABEL: ls_2k:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    slld $r1 = $r1, 8
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    xlo $a5 = $r1[$r0]
; CV2-NEXT:    addd $r2 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    xlo $a0 = 192[$r2]
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    xlo $a1 = 224[$r2]
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    xlo $a2 = 128[$r2]
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    xlo $a3 = 160[$r2]
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    xlo $a4 = 32[$r2]
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    xlo $a6 = 96[$r2]
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    xlo $a7 = 64[$r2]
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    xso 0[$r0] = $a5
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    xso 32[$r0] = $a4
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    xso 96[$r0] = $a6
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    xso 64[$r0] = $a7
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    xso 160[$r0] = $a3
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    xso 128[$r0] = $a2
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    xso 224[$r0] = $a1
; CV2-NEXT:    ;; # (end cycle 15)
; CV2-NEXT:    xso 192[$r0] = $a0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 16)
entry:
  %2 = getelementptr inbounds <2048 x i1>, <2048 x i1> * %0, i64 %1
  %3 = load <2048 x i1>, <2048 x i1>* %2, align 32
  store <2048 x i1> %3, <2048 x i1>* %0, align 32
  ret void
}

define void @ls_4k(<4096 x i1> * %0, i64 %1) {
; CV1-LABEL: ls_4k:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    slld $r1 = $r1, 9
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    xlo.u $a14 = $r1[$r0]
; CV1-NEXT:    addd $r2 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    xlo.u $a0 = 448[$r2]
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    xlo.u $a1 = 480[$r2]
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    xlo.u $a2 = 384[$r2]
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    xlo.u $a3 = 416[$r2]
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    xlo.u $a4 = 320[$r2]
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    xlo.u $a5 = 352[$r2]
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    xlo.u $a6 = 256[$r2]
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    xlo.u $a7 = 288[$r2]
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    xlo.u $a8 = 192[$r2]
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    xlo.u $a9 = 224[$r2]
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    xlo.u $a10 = 128[$r2]
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    xlo.u $a11 = 160[$r2]
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    xlo.u $a12 = 64[$r2]
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    xlo.u $a13 = 32[$r2]
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    xlo.u $a15 = 96[$r2]
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    xso 0[$r0] = $a14
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    xso 32[$r0] = $a13
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    xso 96[$r0] = $a15
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    xso 64[$r0] = $a12
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    xso 160[$r0] = $a11
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    xso 128[$r0] = $a10
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    xso 224[$r0] = $a9
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    xso 192[$r0] = $a8
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    xso 288[$r0] = $a7
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    xso 256[$r0] = $a6
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    xso 352[$r0] = $a5
; CV1-NEXT:    ;; # (end cycle 27)
; CV1-NEXT:    xso 320[$r0] = $a4
; CV1-NEXT:    ;; # (end cycle 28)
; CV1-NEXT:    xso 416[$r0] = $a3
; CV1-NEXT:    ;; # (end cycle 29)
; CV1-NEXT:    xso 384[$r0] = $a2
; CV1-NEXT:    ;; # (end cycle 30)
; CV1-NEXT:    xso 480[$r0] = $a1
; CV1-NEXT:    ;; # (end cycle 31)
; CV1-NEXT:    xso 448[$r0] = $a0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 32)
;
; CV2-LABEL: ls_4k:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    slld $r1 = $r1, 9
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    xlo $a13 = $r1[$r0]
; CV2-NEXT:    addd $r2 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    xlo $a0 = 448[$r2]
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    xlo $a1 = 480[$r2]
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    xlo $a2 = 384[$r2]
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    xlo $a3 = 416[$r2]
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    xlo $a4 = 320[$r2]
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    xlo $a5 = 352[$r2]
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    xlo $a6 = 256[$r2]
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    xlo $a7 = 288[$r2]
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    xlo $a8 = 192[$r2]
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    xlo $a9 = 224[$r2]
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    xlo $a10 = 128[$r2]
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    xlo $a11 = 160[$r2]
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    xlo $a12 = 32[$r2]
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    xlo $a14 = 96[$r2]
; CV2-NEXT:    ;; # (end cycle 15)
; CV2-NEXT:    xlo $a15 = 64[$r2]
; CV2-NEXT:    ;; # (end cycle 16)
; CV2-NEXT:    xso 0[$r0] = $a13
; CV2-NEXT:    ;; # (end cycle 17)
; CV2-NEXT:    xso 32[$r0] = $a12
; CV2-NEXT:    ;; # (end cycle 18)
; CV2-NEXT:    xso 96[$r0] = $a14
; CV2-NEXT:    ;; # (end cycle 19)
; CV2-NEXT:    xso 64[$r0] = $a15
; CV2-NEXT:    ;; # (end cycle 20)
; CV2-NEXT:    xso 160[$r0] = $a11
; CV2-NEXT:    ;; # (end cycle 21)
; CV2-NEXT:    xso 128[$r0] = $a10
; CV2-NEXT:    ;; # (end cycle 22)
; CV2-NEXT:    xso 224[$r0] = $a9
; CV2-NEXT:    ;; # (end cycle 23)
; CV2-NEXT:    xso 192[$r0] = $a8
; CV2-NEXT:    ;; # (end cycle 24)
; CV2-NEXT:    xso 288[$r0] = $a7
; CV2-NEXT:    ;; # (end cycle 25)
; CV2-NEXT:    xso 256[$r0] = $a6
; CV2-NEXT:    ;; # (end cycle 26)
; CV2-NEXT:    xso 352[$r0] = $a5
; CV2-NEXT:    ;; # (end cycle 27)
; CV2-NEXT:    xso 320[$r0] = $a4
; CV2-NEXT:    ;; # (end cycle 28)
; CV2-NEXT:    xso 416[$r0] = $a3
; CV2-NEXT:    ;; # (end cycle 29)
; CV2-NEXT:    xso 384[$r0] = $a2
; CV2-NEXT:    ;; # (end cycle 30)
; CV2-NEXT:    xso 480[$r0] = $a1
; CV2-NEXT:    ;; # (end cycle 31)
; CV2-NEXT:    xso 448[$r0] = $a0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 32)
entry:
  %2 = getelementptr inbounds <4096 x i1>, <4096 x i1> * %0, i64 %1
  %3 = load <4096 x i1>, <4096 x i1>* %2, align 32
  store <4096 x i1> %3, <4096 x i1>* %0, align 32
  ret void
}
