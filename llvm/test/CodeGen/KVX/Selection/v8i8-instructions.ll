; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mcpu=kv3-1 -O2 -o - %s | FileCheck %s --check-prefixes=ALL,CV1
; RUN: llc -mcpu=kv3-2 -O2 -o - %s | FileCheck %s --check-prefixes=ALL,CV2
; RUN: clang -O2 -c -o /dev/null %s
; RUN: clang -O2 -march=kv3-2 -c -o /dev/null %s

target triple = "kvx-kalray-cos"

define <8 x i8> @test_ret_const() #0 {
; ALL-LABEL: test_ret_const:
; ALL:       # %bb.0:
; ALL-NEXT:    make $r0 = 0x201020102010201
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  ret <8 x i8> <i8 1, i8 2, i8 1, i8 2, i8 1, i8 2, i8 1, i8 2>
}

define i8 @test_extract_0(<8 x i8> %a) #0 {
; ALL-LABEL: test_extract_0:
; ALL:       # %bb.0:
; ALL-NEXT:    zxbd $r0 = $r0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %e = extractelement <8 x i8> %a, i8 0
  ret i8 %e
}

define i8 @test_extract_1(<8 x i8> %a) #0 {
; ALL-LABEL: test_extract_1:
; ALL:       # %bb.0:
; ALL-NEXT:    extfz $r0 = $r0, 15, 8
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %e = extractelement <8 x i8> %a, i8 1
  ret i8 %e
}

define i8 @test_extract_2(<8 x i8> %a) #0 {
; ALL-LABEL: test_extract_2:
; ALL:       # %bb.0:
; ALL-NEXT:    extfz $r0 = $r0, 23, 16
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %e = extractelement <8 x i8> %a, i8 2
  ret i8 %e
}

define i8 @test_extract_3(<8 x i8> %a) #0 {
; ALL-LABEL: test_extract_3:
; ALL:       # %bb.0:
; ALL-NEXT:    srlw $r0 = $r0, 24
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %e = extractelement <8 x i8> %a, i8 3
  ret i8 %e
}

define <8 x i8> @test_fma(<8 x i8> %a, <8 x i8> %b, <8 x i8> %c) #0 {
; CV1-LABEL: test_fma:
; CV1:       # %bb.0:
; CV1-NEXT:    mulhq $r1 = $r1, $r2
; CV1-NEXT:    srlhqs $r3 = $r2, 8
; CV1-NEXT:    andd $r4 = $r1, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    mulhq $r2 = $r4, $r3
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r1 = $r1, $r2
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    xord $r0 = $r0, $r1
; CV1-NEXT:    andd $r2 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: test_fma:
; CV2:       # %bb.0:
; CV2-NEXT:    mulhq $r1 = $r1, $r2
; CV2-NEXT:    srlhqs $r3 = $r2, 8
; CV2-NEXT:    andd $r4 = $r1, 0xff00ff00.@
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    mulhq $r2 = $r4, $r3
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    andd $r1 = $r1, 0xff00ff.@
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    iord $r1 = $r1, $r2
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 4)
  %m = mul <8 x i8> %b, %c
  %ad = add <8 x i8> %a, %m
  ret <8 x i8> %ad
}

define <8 x i8> @test_fma_imm(<8 x i8> %a, <8 x i8> %b) #0 {
; CV1-LABEL: test_fma_imm:
; CV1:       # %bb.0:
; CV1-NEXT:    make $r2 = 0x301020703010207
; CV1-NEXT:    andd $r4 = $r1, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    mulhq $r1 = $r1, $r2
; CV1-NEXT:    srlhqs $r3 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    mulhq $r2 = $r4, $r3
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    iord $r1 = $r1, $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    xord $r0 = $r0, $r1
; CV1-NEXT:    andd $r2 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: test_fma_imm:
; CV2:       # %bb.0:
; CV2-NEXT:    make $r2 = 0x301020703010207
; CV2-NEXT:    andd $r4 = $r1, 0xff00ff00.@
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    mulhq $r1 = $r1, $r2
; CV2-NEXT:    srlhqs $r3 = $r2, 8
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    mulhq $r2 = $r4, $r3
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    andd $r1 = $r1, 0xff00ff.@
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    iord $r1 = $r1, $r2
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 5)
  %m = mul <8 x i8> <i8 7, i8 2, i8 1, i8 3, i8 7, i8 2, i8 1, i8 3>, %b
  %ad = add <8 x i8> %a, %m
  ret <8 x i8> %ad
}


define <8 x i8> @test_fma_imm_2(<8 x i8> %a, <8 x i8> %b) #0 {
; CV1-LABEL: test_fma_imm_2:
; CV1:       # %bb.0:
; CV1-NEXT:    make $r2 = 0x201020102010201
; CV1-NEXT:    andd $r4 = $r1, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    mulhq $r1 = $r1, $r2
; CV1-NEXT:    srlhqs $r3 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    mulhq $r2 = $r4, $r3
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    iord $r1 = $r1, $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    xord $r0 = $r0, $r1
; CV1-NEXT:    andd $r2 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: test_fma_imm_2:
; CV2:       # %bb.0:
; CV2-NEXT:    make $r2 = 0x201020102010201
; CV2-NEXT:    andd $r4 = $r1, 0xff00ff00.@
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    mulhq $r1 = $r1, $r2
; CV2-NEXT:    srlhqs $r3 = $r2, 8
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    mulhq $r2 = $r4, $r3
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    andd $r1 = $r1, 0xff00ff.@
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    iord $r1 = $r1, $r2
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 5)
  %m = mul <8 x i8> <i8 1, i8 2, i8 1, i8 2, i8 1, i8 2, i8 1, i8 2>, %b
  %ad = add <8 x i8> %a, %m
  ret <8 x i8> %ad
}

define i8 @test_extract_i(<8 x i8> %a, i64 %idx) #0 {
; ALL-LABEL: test_extract_i:
; ALL:       # %bb.0:
; ALL-NEXT:    sllw $r1 = $r1, 3
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    srld $r0 = $r0, $r1
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    zxbd $r0 = $r0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 2)
  %e = extractelement <8 x i8> %a, i64 %idx
  ret i8 %e
}

define <8 x i8> @test_add(<8 x i8> %a, <8 x i8> %b) #0 {
; CV1-LABEL: test_add:
; CV1:       # %bb.0:
; CV1-NEXT:    xord $r0 = $r0, $r1
; CV1-NEXT:    andd $r2 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 2)
;
; CV2-LABEL: test_add:
; CV2:       # %bb.0:
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %r = add <8 x i8> %a, %b
  ret <8 x i8> %r
}

define <8 x i8> @test_add_imm_0(<8 x i8> %a) #0 {
; CV1-LABEL: test_add_imm_0:
; CV1:       # %bb.0:
; CV1-NEXT:    make $r1 = 0x201020102010201
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    xord $r0 = $r0, $r1
; CV1-NEXT:    andd $r2 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: test_add_imm_0:
; CV2:       # %bb.0:
; CV2-NEXT:    addbo $r0 = $r0, 0x2010201.@
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %r = add <8 x i8> <i8 1, i8 2, i8 1, i8 2, i8 1, i8 2, i8 1, i8 2>, %a
  ret <8 x i8> %r
}

define <8 x i8> @test_add_imm_not_at(<8 x i8> %a) #0 {
; CV1-LABEL: test_add_imm_not_at:
; CV1:       # %bb.0:
; CV1-NEXT:    make $r1 = 0x2010201
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    xord $r0 = $r0, $r1
; CV1-NEXT:    andd $r2 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: test_add_imm_not_at:
; CV2:       # %bb.0:
; CV2-NEXT:    addbo $r0 = $r0, 0x2010201
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %r = add <8 x i8> <i8 1, i8 2, i8 1, i8 2, i8 0, i8 0, i8 0, i8 0>, %a
  ret <8 x i8> %r
}

define <8 x i8> @test_add_imm_1(<8 x i8> %a) #0 {
; CV1-LABEL: test_add_imm_1:
; CV1:       # %bb.0:
; CV1-NEXT:    make $r1 = 0x201020102010201
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    xord $r0 = $r0, $r1
; CV1-NEXT:    andd $r2 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: test_add_imm_1:
; CV2:       # %bb.0:
; CV2-NEXT:    addbo $r0 = $r0, 0x2010201.@
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %r = add <8 x i8> %a, <i8 1, i8 2, i8 1, i8 2, i8 1, i8 2, i8 1, i8 2>
  ret <8 x i8> %r
}

define <8 x i8> @test_sub(<8 x i8> %a, <8 x i8> %b) #0 {
; CV1-LABEL: test_sub:
; CV1:       # %bb.0:
; CV1-NEXT:    nxord $r0 = $r0, $r1
; CV1-NEXT:    iord $r2 = $r0, 0x80808080.@
; CV1-NEXT:    andd $r3 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    sbfd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    xord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 2)
;
; CV2-LABEL: test_sub:
; CV2:       # %bb.0:
; CV2-NEXT:    sbfbo $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %r = sub <8 x i8> %a, %b
  ret <8 x i8> %r
}

define <8 x i8> @test_sub_imm(<8 x i8> %a) #0 {
; CV1-LABEL: test_sub_imm:
; CV1:       # %bb.0:
; CV1-NEXT:    make $r1 = 0x201020102010201
; CV1-NEXT:    iord $r2 = $r0, 0x80808080.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    nxord $r0 = $r0, $r1
; CV1-NEXT:    andd $r3 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    sbfd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    xord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: test_sub_imm:
; CV2:       # %bb.0:
; CV2-NEXT:    addbo $r0 = $r0, 0xfefffeff.@
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %r = sub <8 x i8> %a, <i8 1, i8 2, i8 1, i8 2, i8 1, i8 2, i8 1, i8 2>
  ret <8 x i8> %r
}

define <8 x i8> @test_sub_fromimm(<8 x i8> %a) #0 {
; CV1-LABEL: test_sub_fromimm:
; CV1:       # %bb.0:
; CV1-NEXT:    make $r1 = 0x201020102010201
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    nxord $r0 = $r1, $r0
; CV1-NEXT:    iord $r2 = $r1, 0x80808080.@
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    sbfd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    xord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: test_sub_fromimm:
; CV2:       # %bb.0:
; CV2-NEXT:    sbfbo $r0 = $r0, 0x2010201.@
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %r = sub <8 x i8> <i8 1, i8 2, i8 1, i8 2, i8 1, i8 2, i8 1, i8 2>, %a
  ret <8 x i8> %r
}

define <8 x i8> @test_neg(<8 x i8> %a) #0 {
; CV1-LABEL: test_neg:
; CV1:       # %bb.0:
; CV1-NEXT:    make $r1 = 0
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    nxord $r0 = $r1, $r0
; CV1-NEXT:    iord $r2 = $r1, 0x80808080.@
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    sbfd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    xord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: test_neg:
; CV2:       # %bb.0:
; CV2-NEXT:    negbo $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %r = sub <8 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, %a
  ret <8 x i8> %r
}

define <8 x i8> @test_mul(<8 x i8> %a, <8 x i8> %b) #0 {
; ALL-LABEL: test_mul:
; ALL:       # %bb.0:
; ALL-NEXT:    mulhq $r0 = $r0, $r1
; ALL-NEXT:    srlhqs $r2 = $r1, 8
; ALL-NEXT:    andd $r3 = $r0, 0xff00ff00.@
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    mulhq $r1 = $r3, $r2
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    andd $r0 = $r0, 0xff00ff.@
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    iord $r0 = $r0, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 3)
  %r = mul <8 x i8> %a, %b
  ret <8 x i8> %r
}

define <8 x i8> @test_mul_2(<8 x i8> %a, <8 x i8> %b, <8 x i8> %c) #0 {
; ALL-LABEL: test_mul_2:
; ALL:       # %bb.0:
; ALL-NEXT:    mulhq $r0 = $r0, $r1
; ALL-NEXT:    srlhqs $r3 = $r1, 8
; ALL-NEXT:    andd $r4 = $r0, 0xff00ff00.@
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    mulhq $r1 = $r4, $r3
; ALL-NEXT:    srlhqs $r3 = $r2, 8
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    andd $r0 = $r0, 0xff00ff.@
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    iord $r0 = $r0, $r1
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    mulhq $r0 = $r0, $r2
; ALL-NEXT:    andd $r1 = $r0, 0xff00ff00.@
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    mulhq $r1 = $r1, $r3
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    andd $r0 = $r0, 0xff00ff.@
; ALL-NEXT:    ;; # (end cycle 6)
; ALL-NEXT:    iord $r0 = $r0, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 7)
  %r = mul <8 x i8> %a, %b
  %r1 = mul <8 x i8> %r, %c
  ret <8 x i8> %r1
}

define <8 x i8> @test_div(<8 x i8> %a, <8 x i8> %b) #0 {
; CV1-LABEL: test_div:
; CV1:       # %bb.0:
; CV1-NEXT:    addd $r12 = $r12, -64
; CV1-NEXT:    get $r16 = $ra
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sd 56[$r12] = $r16
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sd 48[$r12] = $r22
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sq 32[$r12] = $r20r21
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sq 16[$r12] = $r18r19
; CV1-NEXT:    copyd $r18 = $r1
; CV1-NEXT:    copyd $r19 = $r0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    srld $r0 = $r19, 56
; CV1-NEXT:    srld $r1 = $r18, 56
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    call __divsi3
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    extfz $r0 = $r19, 55, 48
; CV1-NEXT:    extfz $r1 = $r18, 55, 48
; CV1-NEXT:    copyd $r20 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    call __divsi3
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    extfz $r0 = $r19, 47, 40
; CV1-NEXT:    extfz $r1 = $r18, 47, 40
; CV1-NEXT:    copyd $r21 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r21 = $r20, 15, 8
; CV1-NEXT:    call __divsi3
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    extfz $r0 = $r19, 39, 32
; CV1-NEXT:    extfz $r1 = $r18, 39, 32
; CV1-NEXT:    copyd $r20 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    call __divsi3
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srlw $r0 = $r19, 24
; CV1-NEXT:    srlw $r1 = $r18, 24
; CV1-NEXT:    copyd $r22 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r22 = $r20, 15, 8
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r22 = $r21, 31, 16
; CV1-NEXT:    call __divsi3
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    extfz $r0 = $r19, 23, 16
; CV1-NEXT:    extfz $r1 = $r18, 23, 16
; CV1-NEXT:    copyd $r20 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    call __divsi3
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    extfz $r0 = $r19, 15, 8
; CV1-NEXT:    extfz $r1 = $r18, 15, 8
; CV1-NEXT:    copyd $r21 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r21 = $r20, 15, 8
; CV1-NEXT:    call __divsi3
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    zxbd $r0 = $r19
; CV1-NEXT:    zxbd $r1 = $r18
; CV1-NEXT:    copyd $r20 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    call __divsi3
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    lq $r18r19 = 16[$r12]
; CV1-NEXT:    insf $r0 = $r20, 15, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lq $r20r21 = 32[$r12]
; CV1-NEXT:    insf $r0 = $r21, 31, 16
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r0 = $r22, 63, 32
; CV1-NEXT:    ld $r22 = 48[$r12]
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    ld $r16 = 56[$r12]
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    set $ra = $r16
; CV1-NEXT:    addd $r12 = $r12, 64
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: test_div:
; CV2:       # %bb.0:
; CV2-NEXT:    addd $r12 = $r12, -64
; CV2-NEXT:    get $r16 = $ra
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sd 56[$r12] = $r16
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sd 48[$r12] = $r22
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sq 32[$r12] = $r20r21
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sq 16[$r12] = $r18r19
; CV2-NEXT:    copyd $r18 = $r1
; CV2-NEXT:    copyd $r19 = $r0
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    srld $r0 = $r19, 56
; CV2-NEXT:    srld $r1 = $r18, 56
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    sxbd $r1 = $r1
; CV2-NEXT:    call __divsi3
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    extfz $r0 = $r19, 55, 48
; CV2-NEXT:    extfz $r1 = $r18, 55, 48
; CV2-NEXT:    copyd $r20 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    sxbd $r1 = $r1
; CV2-NEXT:    call __divsi3
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    extfz $r0 = $r19, 47, 40
; CV2-NEXT:    extfz $r1 = $r18, 47, 40
; CV2-NEXT:    copyd $r21 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    sxbd $r1 = $r1
; CV2-NEXT:    insf $r21 = $r20, 15, 8
; CV2-NEXT:    call __divsi3
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    extfz $r0 = $r19, 39, 32
; CV2-NEXT:    extfz $r1 = $r18, 39, 32
; CV2-NEXT:    copyd $r20 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    sxbd $r1 = $r1
; CV2-NEXT:    call __divsi3
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    srlw $r0 = $r19, 24
; CV2-NEXT:    srlw $r1 = $r18, 24
; CV2-NEXT:    copyd $r22 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    sxbd $r1 = $r1
; CV2-NEXT:    insf $r22 = $r20, 15, 8
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r22 = $r21, 31, 16
; CV2-NEXT:    call __divsi3
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    extfz $r0 = $r19, 23, 16
; CV2-NEXT:    extfz $r1 = $r18, 23, 16
; CV2-NEXT:    copyd $r20 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    sxbd $r1 = $r1
; CV2-NEXT:    call __divsi3
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    extfz $r0 = $r19, 15, 8
; CV2-NEXT:    extfz $r1 = $r18, 15, 8
; CV2-NEXT:    copyd $r21 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    sxbd $r1 = $r1
; CV2-NEXT:    insf $r21 = $r20, 15, 8
; CV2-NEXT:    call __divsi3
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    zxbd $r0 = $r19
; CV2-NEXT:    zxbd $r1 = $r18
; CV2-NEXT:    copyd $r20 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    sxbd $r1 = $r1
; CV2-NEXT:    call __divsi3
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    lq $r18r19 = 16[$r12]
; CV2-NEXT:    insf $r0 = $r20, 15, 8
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lq $r20r21 = 32[$r12]
; CV2-NEXT:    insf $r0 = $r21, 31, 16
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r0 = $r22, 63, 32
; CV2-NEXT:    ld $r22 = 48[$r12]
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    ld $r16 = 56[$r12]
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    set $ra = $r16
; CV2-NEXT:    addd $r12 = $r12, 64
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %r = sdiv <8 x i8> %a, %b
  ret <8 x i8> %r
}

define <8 x i8> @test_rem(<8 x i8> %a, <8 x i8> %b) #0 {
; CV1-LABEL: test_rem:
; CV1:       # %bb.0:
; CV1-NEXT:    addd $r12 = $r12, -64
; CV1-NEXT:    get $r16 = $ra
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sd 56[$r12] = $r16
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sd 48[$r12] = $r22
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sq 32[$r12] = $r20r21
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sq 16[$r12] = $r18r19
; CV1-NEXT:    copyd $r18 = $r1
; CV1-NEXT:    copyd $r19 = $r0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    srld $r0 = $r19, 56
; CV1-NEXT:    srld $r1 = $r18, 56
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    call __modsi3
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    extfz $r0 = $r19, 55, 48
; CV1-NEXT:    extfz $r1 = $r18, 55, 48
; CV1-NEXT:    copyd $r20 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    call __modsi3
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    extfz $r0 = $r19, 47, 40
; CV1-NEXT:    extfz $r1 = $r18, 47, 40
; CV1-NEXT:    copyd $r21 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r21 = $r20, 15, 8
; CV1-NEXT:    call __modsi3
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    extfz $r0 = $r19, 39, 32
; CV1-NEXT:    extfz $r1 = $r18, 39, 32
; CV1-NEXT:    copyd $r20 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    call __modsi3
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srlw $r0 = $r19, 24
; CV1-NEXT:    srlw $r1 = $r18, 24
; CV1-NEXT:    copyd $r22 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r22 = $r20, 15, 8
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r22 = $r21, 31, 16
; CV1-NEXT:    call __modsi3
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    extfz $r0 = $r19, 23, 16
; CV1-NEXT:    extfz $r1 = $r18, 23, 16
; CV1-NEXT:    copyd $r20 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    call __modsi3
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    extfz $r0 = $r19, 15, 8
; CV1-NEXT:    extfz $r1 = $r18, 15, 8
; CV1-NEXT:    copyd $r21 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r21 = $r20, 15, 8
; CV1-NEXT:    call __modsi3
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    zxbd $r0 = $r19
; CV1-NEXT:    zxbd $r1 = $r18
; CV1-NEXT:    copyd $r20 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    call __modsi3
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    lq $r18r19 = 16[$r12]
; CV1-NEXT:    insf $r0 = $r20, 15, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lq $r20r21 = 32[$r12]
; CV1-NEXT:    insf $r0 = $r21, 31, 16
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r0 = $r22, 63, 32
; CV1-NEXT:    ld $r22 = 48[$r12]
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    ld $r16 = 56[$r12]
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    set $ra = $r16
; CV1-NEXT:    addd $r12 = $r12, 64
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: test_rem:
; CV2:       # %bb.0:
; CV2-NEXT:    addd $r12 = $r12, -64
; CV2-NEXT:    get $r16 = $ra
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sd 56[$r12] = $r16
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sd 48[$r12] = $r22
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sq 32[$r12] = $r20r21
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sq 16[$r12] = $r18r19
; CV2-NEXT:    copyd $r18 = $r1
; CV2-NEXT:    copyd $r19 = $r0
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    srld $r0 = $r19, 56
; CV2-NEXT:    srld $r1 = $r18, 56
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    sxbd $r1 = $r1
; CV2-NEXT:    call __modsi3
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    extfz $r0 = $r19, 55, 48
; CV2-NEXT:    extfz $r1 = $r18, 55, 48
; CV2-NEXT:    copyd $r20 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    sxbd $r1 = $r1
; CV2-NEXT:    call __modsi3
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    extfz $r0 = $r19, 47, 40
; CV2-NEXT:    extfz $r1 = $r18, 47, 40
; CV2-NEXT:    copyd $r21 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    sxbd $r1 = $r1
; CV2-NEXT:    insf $r21 = $r20, 15, 8
; CV2-NEXT:    call __modsi3
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    extfz $r0 = $r19, 39, 32
; CV2-NEXT:    extfz $r1 = $r18, 39, 32
; CV2-NEXT:    copyd $r20 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    sxbd $r1 = $r1
; CV2-NEXT:    call __modsi3
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    srlw $r0 = $r19, 24
; CV2-NEXT:    srlw $r1 = $r18, 24
; CV2-NEXT:    copyd $r22 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    sxbd $r1 = $r1
; CV2-NEXT:    insf $r22 = $r20, 15, 8
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r22 = $r21, 31, 16
; CV2-NEXT:    call __modsi3
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    extfz $r0 = $r19, 23, 16
; CV2-NEXT:    extfz $r1 = $r18, 23, 16
; CV2-NEXT:    copyd $r20 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    sxbd $r1 = $r1
; CV2-NEXT:    call __modsi3
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    extfz $r0 = $r19, 15, 8
; CV2-NEXT:    extfz $r1 = $r18, 15, 8
; CV2-NEXT:    copyd $r21 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    sxbd $r1 = $r1
; CV2-NEXT:    insf $r21 = $r20, 15, 8
; CV2-NEXT:    call __modsi3
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    zxbd $r0 = $r19
; CV2-NEXT:    zxbd $r1 = $r18
; CV2-NEXT:    copyd $r20 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    sxbd $r1 = $r1
; CV2-NEXT:    call __modsi3
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    lq $r18r19 = 16[$r12]
; CV2-NEXT:    insf $r0 = $r20, 15, 8
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lq $r20r21 = 32[$r12]
; CV2-NEXT:    insf $r0 = $r21, 31, 16
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r0 = $r22, 63, 32
; CV2-NEXT:    ld $r22 = 48[$r12]
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    ld $r16 = 56[$r12]
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    set $ra = $r16
; CV2-NEXT:    addd $r12 = $r12, 64
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %r = srem <8 x i8> %a, %b
  ret <8 x i8> %r
}

define void @test_ldst_v8i8(<8 x i8>* %a, <8 x i8>* %b) {
; ALL-LABEL: test_ldst_v8i8:
; ALL:       # %bb.0:
; ALL-NEXT:    ld $r0 = 0[$r0]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sd 0[$r1] = $r0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 2)
  %t1 = load <8 x i8>, <8 x i8>* %a
  store <8 x i8> %t1, <8 x i8>* %b, align 16
  ret void
}

declare <8 x i8> @test_callee(<8 x i8> %a, <8 x i8> %b) #0

define <8 x i8> @test_call(<8 x i8> %a, <8 x i8> %b) #0 {
; ALL-LABEL: test_call:
; ALL:       # %bb.0:
; ALL-NEXT:    addd $r12 = $r12, -32
; ALL-NEXT:    get $r16 = $ra
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sd 24[$r12] = $r16
; ALL-NEXT:    call test_callee
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    ld $r16 = 24[$r12]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    set $ra = $r16
; ALL-NEXT:    addd $r12 = $r12, 32
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %r = call <8 x i8> @test_callee(<8 x i8> %a, <8 x i8> %b)
  ret <8 x i8> %r
}

define <8 x i8> @test_call_flipped(<8 x i8> %a, <8 x i8> %b) #0 {
; ALL-LABEL: test_call_flipped:
; ALL:       # %bb.0:
; ALL-NEXT:    addd $r12 = $r12, -32
; ALL-NEXT:    get $r16 = $ra
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sd 24[$r12] = $r16
; ALL-NEXT:    copyd $r0 = $r1
; ALL-NEXT:    copyd $r1 = $r0
; ALL-NEXT:    call test_callee
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    ld $r16 = 24[$r12]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    set $ra = $r16
; ALL-NEXT:    addd $r12 = $r12, 32
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %r = call <8 x i8> @test_callee(<8 x i8> %b, <8 x i8> %a)
  ret <8 x i8> %r
}

; TODO: Can perform swap in a single bundle
define <8 x i8> @test_tailcall_flipped(<8 x i8> %a, <8 x i8> %b) #0 {
; ALL-LABEL: test_tailcall_flipped:
; ALL:       # %bb.0:
; ALL-NEXT:    copyd $r0 = $r1
; ALL-NEXT:    copyd $r1 = $r0
; ALL-NEXT:    goto test_callee
; ALL-NEXT:    ;; # (end cycle 0)
  %r = tail call <8 x i8> @test_callee(<8 x i8> %b, <8 x i8> %a)
  ret <8 x i8> %r
}

define <8 x i8> @test_select(<8 x i8> %a, <8 x i8> %b, i1 zeroext %c) #0 {
; ALL-LABEL: test_select:
; ALL:       # %bb.0:
; ALL-NEXT:    cmoved.even $r2 ? $r0 = $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %r = select i1 %c, <8 x i8> %a, <8 x i8> %b
  ret <8 x i8> %r
}

define <8 x i8> @test_select_cc(<8 x i8> %a, <8 x i8> %b, <8 x i8> %c, <8 x i8> %d) #0 {
; CV1-LABEL: test_select_cc:
; CV1:       # %bb.0:
; CV1-NEXT:    sxlbhq $r4 = $r3
; CV1-NEXT:    sxlbhq $r5 = $r2
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxmbhq $r2 = $r2
; CV1-NEXT:    sxmbhq $r3 = $r3
; CV1-NEXT:    compnhq.lt $r4 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    compnhq.lt $r2 = $r2, $r3
; CV1-NEXT:    sxmbhq $r3 = $r1
; CV1-NEXT:    andd $r4 = $r4, 0xff00ff.@
; CV1-NEXT:    sxmbhq $r5 = $r0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    cmovehq.even $r4 ? $r0 = $r1
; CV1-NEXT:    cmovehq.even $r2 ? $r5 = $r3
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    sbmm8 $r1 = $r5, 0x40100401
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    insf $r0 = $r1, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: test_select_cc:
; CV2:       # %bb.0:
; CV2-NEXT:    compnbo.lt $r2 = $r2, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    cmovebo.even $r2 ? $r0 = $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 1)
  %cc = icmp slt <8 x i8> %c, %d
  %r = select <8 x i1> %cc, <8 x i8> %a, <8 x i8> %b
  ret <8 x i8> %r
}

define <8 x i32> @test_sext_2xi64(<8 x i8> %a) #0 {
; CV1-LABEL: test_sext_2xi64:
; CV1:       # %bb.0:
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxmbhq $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxlhwp $r2 = $r1
; CV1-NEXT:    sxmhwp $r3 = $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sxlhwp $r0 = $r0
; CV1-NEXT:    sxmhwp $r1 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 2)
;
; CV2-LABEL: test_sext_2xi64:
; CV2:       # %bb.0:
; CV2-NEXT:    sxlbhq $r0 = $r0
; CV2-NEXT:    sxmbhq $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sxlhwp $r0 = $r0
; CV2-NEXT:    sxmhwp $r1 = $r0
; CV2-NEXT:    sxlhwp $r2 = $r1
; CV2-NEXT:    sxmhwp $r3 = $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 1)
  %r = sext <8 x i8> %a to <8 x i32>
  ret <8 x i32> %r
}

declare <8 x i8> @llvm.abs.v8i8(<8 x i8>, i1) #0

define <8 x i8> @test_abs(<8 x i8> %a) #0 {
; CV1-LABEL: test_abs:
; CV1:       # %bb.0:
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxmbhq $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    abshq $r0 = $r0
; CV1-NEXT:    abshq $r1 = $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x40100401
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r0 = $r1, 32, 63
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: test_abs:
; CV2:       # %bb.0:
; CV2-NEXT:    absbo $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %r = call <8 x i8> @llvm.abs.v8i8(<8 x i8> %a, i1 false)
  ret <8 x i8> %r
}

define <8 x i8> @test_insertelement0(<8 x i8> %a, i8 %x) #0 {
; ALL-LABEL: test_insertelement0:
; ALL:       # %bb.0:
; ALL-NEXT:    insf $r0 = $r1, 7, 0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %i = insertelement <8 x i8> %a, i8 %x, i64 0
  ret <8 x i8> %i
}

define <8 x i8> @test_insertelement1(<8 x i8> %a, i8 %x) #0 {
; ALL-LABEL: test_insertelement1:
; ALL:       # %bb.0:
; ALL-NEXT:    insf $r0 = $r1, 15, 8
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %i = insertelement <8 x i8> %a, i8 %x, i64 1
  ret <8 x i8> %i
}

define <8 x i8> @test_insertelement2(<8 x i8> %a, i8 %x) #0 {
; ALL-LABEL: test_insertelement2:
; ALL:       # %bb.0:
; ALL-NEXT:    insf $r0 = $r1, 23, 16
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %i = insertelement <8 x i8> %a, i8 %x, i64 2
  ret <8 x i8> %i
}

define <8 x i8> @test_insertelement3(<8 x i8> %a, i8 %x) #0 {
; ALL-LABEL: test_insertelement3:
; ALL:       # %bb.0:
; ALL-NEXT:    insf $r0 = $r1, 31, 24
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %i = insertelement <8 x i8> %a, i8 %x, i64 3
  ret <8 x i8> %i
}

define <8 x i8> @test_insertelement(<8 x i8> %a, i8 %x, i64 %p) #0 {
; ALL-LABEL: test_insertelement:
; ALL:       # %bb.0:
; ALL-NEXT:    andd $r2 = $r2, 7
; ALL-NEXT:    addd $r12 = $r12, -32
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sd 24[$r12] = $r0
; ALL-NEXT:    addd $r0 = $r12, 24
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    sb $r2[$r0] = $r1
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    ld $r0 = 24[$r12]
; ALL-NEXT:    addd $r12 = $r12, 32
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 3)
  %i = insertelement <8 x i8> %a, i8 %x, i64 %p
  ret <8 x i8> %i
}

define <8 x i8> @mulsub(<8 x i8> %a, <8 x i8> %b, <8 x i8> %c) #0 {
; CV1-LABEL: mulsub:
; CV1:       # %bb.0:
; CV1-NEXT:    mulhq $r1 = $r1, $r2
; CV1-NEXT:    srlhqs $r3 = $r2, 8
; CV1-NEXT:    andd $r4 = $r1, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    mulhq $r2 = $r4, $r3
; CV1-NEXT:    iord $r3 = $r0, 0x80808080.@
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r1 = $r1, $r2
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    nxord $r0 = $r0, $r1
; CV1-NEXT:    andd $r2 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    sbfd $r1 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    xord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: mulsub:
; CV2:       # %bb.0:
; CV2-NEXT:    mulhq $r1 = $r1, $r2
; CV2-NEXT:    srlhqs $r3 = $r2, 8
; CV2-NEXT:    andd $r4 = $r1, 0xff00ff00.@
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    mulhq $r2 = $r4, $r3
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    andd $r1 = $r1, 0xff00ff.@
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    iord $r1 = $r1, $r2
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbfbo $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 4)
  %mul = mul <8 x i8> %b, %c
  %sub = sub <8 x i8> %a, %mul
  ret <8 x i8> %sub
}

define <8 x i8> @vnot(<8 x i8> %a) #0 {
; ALL-LABEL: vnot:
; ALL:       # %bb.0:
; ALL-NEXT:    notd $r0 = $r0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %vnot = xor <8 x i8> %a, <i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1>
  ret <8 x i8> %vnot
}

define <8 x i8> @nandd_rr(<8 x i8> %0, <8 x i8> %1) {
; ALL-LABEL: nandd_rr:
; ALL:       # %bb.0:
; ALL-NEXT:    nandd $r0 = $r1, $r0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %3 = and <8 x i8> %1, %0
  %4 = xor <8 x i8> %3, <i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1>
  ret <8 x i8> %4
}

define <8 x i8> @nandd_ri10(<8 x i8> %0) {
; ALL-LABEL: nandd_ri10:
; ALL:       # %bb.0:
; ALL-NEXT:    nandd $r0 = $r0, 252
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %2 = and <8 x i8> %0, <i8 252, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>
  %3 = xor <8 x i8> %2, <i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1>
  ret <8 x i8> %3
}

define <8 x i8> @nandd_ri37(<8 x i8> %0) {
; ALL-LABEL: nandd_ri37:
; ALL:       # %bb.0:
; ALL-NEXT:    nandd $r0 = $r0, 0x17ffea00df00fc
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %2 = and <8 x i8> %0, <i8 252, i8 0, i8 223, i8 0, i8 234, i8 -1, i8 23, i8 0>
  %3 = xor <8 x i8> %2, <i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1>
  ret <8 x i8> %3
}

define <8 x i8> @nandd_ri37_2(<8 x i8> %0) {
; ALL-LABEL: nandd_ri37_2:
; ALL:       # %bb.0:
; ALL-NEXT:    nandd $r0 = $r0, 0x1700ea00df00fc
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %2 = and <8 x i8> %0, <i8 252, i8 0, i8 223, i8 0, i8 234, i8 0, i8 23, i8 0>
  %3 = xor <8 x i8> %2, <i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1>
  ret <8 x i8> %3
}

define <8 x i8> @concat(<4 x i8> %a) #0 {
; ALL-LABEL: concat:
; ALL:       # %bb.0:
; ALL-NEXT:    insf $r0 = $r0, 63, 32
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %r = shufflevector <4 x i8> %a, <4 x i8> undef,
                        <8 x i32> <i32 0, i32 1, i32 2, i32 3,
                        i32 0, i32 1, i32 2, i32 3>
  ret <8 x i8> %r
}

define <8 x i8> @concat2(<4 x i8> %a, <4 x i8> %b){
; ALL-LABEL: concat2:
; ALL:       # %bb.0:
; ALL-NEXT:    insf $r0 = $r1, 63, 32
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %v = shufflevector <4 x i8> %a, <4 x i8> %b, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i8> %v
}

define <8 x i8> @splat_v8i8(i32 %s) {
; ALL-LABEL: splat_v8i8:
; ALL:       # %bb.0: # %entry
; ALL-NEXT:    sbmm8 $r0 = $r0, 0x1010101.@
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
entry:
  %conv = trunc i32 %s to i8
  %vecinit = insertelement <8 x i8> undef, i8 %conv, i32 0
  %vecinit14 = shufflevector <8 x i8> %vecinit, <8 x i8> undef, <8 x i32> zeroinitializer
  ret <8 x i8> %vecinit14
}

define <8 x i8> @splat_v8i8_ri() {
; ALL-LABEL: splat_v8i8_ri:
; ALL:       # %bb.0: # %entry
; ALL-NEXT:    make $r0 = 0x202020202020202
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
entry:
  ret <8 x i8> <i8 2, i8 2, i8 2, i8 2, i8 2, i8 2, i8 2, i8 2>
}

define <8 x i8> @shl(<8 x i8> %v, i32 %s) {
; CV1-LABEL: shl:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r2 = $r0, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    slld $r0 = $r0, $r1
; CV1-NEXT:    slld $r2 = $r2, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r1 = $r2, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: shl:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    sllbos $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
entry:
  %0 = trunc i32 %s to i8
  %1 = insertelement <8 x i8> undef, i8 %0, i32 0
  %sh_prom = shufflevector <8 x i8> %1, <8 x i8> undef, <8 x i32> zeroinitializer
  %shl = shl <8 x i8> %v, %sh_prom
  ret <8 x i8> %shl
}

define <8 x i8> @lsr(<8 x i8> %v, i32 %s) {
; CV1-LABEL: lsr:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r2 = $r0, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srld $r0 = $r0, $r1
; CV1-NEXT:    srld $r2 = $r2, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r1 = $r2, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: lsr:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    srlbos $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
entry:
  %0 = trunc i32 %s to i8
  %1 = insertelement <8 x i8> undef, i8 %0, i32 0
  %sh_prom = shufflevector <8 x i8> %1, <8 x i8> undef, <8 x i32> zeroinitializer
  %shr = lshr <8 x i8> %v, %sh_prom
  ret <8 x i8> %shr
}

define <8 x i8> @rotl(<8 x i8> %v, i32 %s) {
; CV1-LABEL: rotl:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x8080202008080202
; CV1-NEXT:    sbmm8 $r2 = $r0, 0x4040101004040101
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sllhqs $r0 = $r0, $r1
; CV1-NEXT:    sllhqs $r2 = $r2, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: rotl:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    sllbos $r1 = $r0, $r1
; CV2-NEXT:    sbfw $r2 = $r1, 8
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srlbos $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 2)
entry:
  %0 = trunc i32 %s to i8
  %1 = insertelement <8 x i8> undef, i8 %0, i32 0
  %sh_prom = shufflevector <8 x i8> %1, <8 x i8> undef, <8 x i32> zeroinitializer
  %shl = shl <8 x i8> %v, %sh_prom
  %2 = sub i8 8, %0
  %3 = insertelement <8 x i8> undef, i8 %2, i32 0
  %sh_prom3 = shufflevector <8 x i8> %3, <8 x i8> undef, <8 x i32> zeroinitializer
  %shr = lshr <8 x i8> %v, %sh_prom3
  %or = or <8 x i8> %shr, %shl
  ret <8 x i8> %or
}

define <8 x i8> @rotr(<8 x i8> %v, i32 %s) {
; CV1-LABEL: rotr:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x4040101004040101
; CV1-NEXT:    sbmm8 $r2 = $r0, 0x8080202008080202
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srlhqs $r0 = $r0, $r1
; CV1-NEXT:    srlhqs $r2 = $r2, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff.@
; CV1-NEXT:    sllhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: rotr:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    srlbos $r1 = $r0, $r1
; CV2-NEXT:    sbfw $r2 = $r1, 8
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sllbos $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 2)
entry:
  %0 = trunc i32 %s to i8
  %1 = insertelement <8 x i8> undef, i8 %0, i32 0
  %sh_prom = shufflevector <8 x i8> %1, <8 x i8> undef, <8 x i32> zeroinitializer
  %shr = lshr <8 x i8> %v, %sh_prom
  %2 = sub i8 8, %0
  %3 = insertelement <8 x i8> undef, i8 %2, i32 0
  %sh_prom3 = shufflevector <8 x i8> %3, <8 x i8> undef, <8 x i32> zeroinitializer
  %shl = shl <8 x i8> %v, %sh_prom3
  %or = or <8 x i8> %shl, %shr
  ret <8 x i8> %or
}

define <8 x i8> @abdbo_rr(<8 x i8> %a, <8 x i8> %b) {
; CV1-LABEL: abdbo_rr:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    make $r2 = 0x8000400020001
; CV1-NEXT:    make $r3 = 0x80004000200010
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sbmm8 $r2 = $r0, $r2
; CV1-NEXT:    sbmm8 $r4 = $r1, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r0 = $r0, $r3
; CV1-NEXT:    sbmm8 $r1 = $r1, $r3
; CV1-NEXT:    minuhq $r2 = $r2, $r4
; CV1-NEXT:    maxuhq $r3 = $r2, $r4
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    sbfhq $r1 = $r2, $r3
; CV1-NEXT:    maxuhq $r4 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbfhq $r0 = $r0, $r4
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x40100401
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401.@
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    insf $r0 = $r1, 31, 0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: abdbo_rr:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    abdbo $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
entry:
  %sub = sub nsw <8 x i8> %a, %b
  %0 = tail call <8 x i8> @llvm.abs.v8i8(<8 x i8> %sub, i1 true)
  ret <8 x i8> %0
}

define <8 x i8> @abdbo_ri_(<8 x i8> %0) {
; CV1-LABEL: abdbo_ri_:
; CV1:       # %bb.0:
; CV1-NEXT:    make $r1 = 0x100f100f
; CV1-NEXT:    make $r2 = 0x8000400020001
; CV1-NEXT:    make $r3 = 0x80004000200010
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sbmm8 $r2 = $r1, $r2
; CV1-NEXT:    sbmm8 $r4 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r0 = $r0, $r3
; CV1-NEXT:    sbmm8 $r1 = $r1, $r3
; CV1-NEXT:    minuhq $r2 = $r2, $r4
; CV1-NEXT:    maxuhq $r3 = $r2, $r4
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    minuhq $r0 = $r1, $r0
; CV1-NEXT:    sbfhq $r1 = $r2, $r3
; CV1-NEXT:    maxuhq $r4 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbfhq $r0 = $r0, $r4
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x40100401
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401.@
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    insf $r0 = $r1, 31, 0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: abdbo_ri_:
; CV2:       # %bb.0:
; CV2-NEXT:    abdbo $r0 = $r0, 0x100f100f
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %2 = sub nsw <8 x i8> <i8 15, i8 16, i8 15, i8 16, i8 0, i8 0, i8 0, i8 0>, %0
  %3 = tail call <8 x i8> @llvm.abs.v8i8(<8 x i8> %2, i1 true)
  ret <8 x i8> %3
}

define <8 x i8> @abdbo_ri_at(<8 x i8> %0) {
; CV1-LABEL: abdbo_ri_at:
; CV1:       # %bb.0:
; CV1-NEXT:    make $r1 = 0x100f100f100f100f
; CV1-NEXT:    make $r2 = 0x8000400020001
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    make $r3 = 0x80004000200010
; CV1-NEXT:    sbmm8 $r4 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r0 = $r0, $r3
; CV1-NEXT:    sbmm8 $r2 = $r1, $r2
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sbmm8 $r1 = $r1, $r3
; CV1-NEXT:    minuhq $r2 = $r2, $r4
; CV1-NEXT:    maxuhq $r3 = $r2, $r4
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    minuhq $r0 = $r1, $r0
; CV1-NEXT:    sbfhq $r1 = $r2, $r3
; CV1-NEXT:    maxuhq $r4 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbfhq $r0 = $r0, $r4
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x40100401
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r0 = $r1, 31, 0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: abdbo_ri_at:
; CV2:       # %bb.0:
; CV2-NEXT:    abdbo $r0 = $r0, 0x100f100f.@
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %2 = sub nsw <8 x i8> <i8 15, i8 16, i8 15, i8 16, i8 15, i8 16, i8 15, i8 16>, %0
  %3 = tail call <8 x i8> @llvm.abs.v8i8(<8 x i8> %2, i1 true)
  ret <8 x i8> %3
}

define  <8 x i8> @v4_maxbo_rr_i8(<8 x i8> %a, <8 x i8> %b) {
; CV1-LABEL: v4_maxbo_rr_i8:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r2 = $r1, 8
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    maxhq $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: v4_maxbo_rr_i8:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    maxbo $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
entry:
  %0 = call <8 x i8> @llvm.smax.v8i8(<8 x i8> %a, <8 x i8> %b)
  ret <8 x i8> %0
}

define  <8 x i8> @v4_minbo_rr_i8(<8 x i8> %a, <8 x i8> %b) {
; CV1-LABEL: v4_minbo_rr_i8:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r2 = $r1, 8
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    minhq $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: v4_minbo_rr_i8:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    minbo $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
entry:
  %0 = call <8 x i8> @llvm.smin.v8i8(<8 x i8> %a, <8 x i8> %b)
  ret <8 x i8> %0
}

define  <8 x i8> @v4_umaxbo_rr_i8(<8 x i8> %a, <8 x i8> %b) {
; CV1-LABEL: v4_umaxbo_rr_i8:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r2 = $r1, 8
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    maxuhq $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: v4_umaxbo_rr_i8:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    maxubo $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
entry:
  %0 = call <8 x i8> @llvm.umax.v8i8(<8 x i8> %a, <8 x i8> %b)
  ret <8 x i8> %0
}

define  <8 x i8> @v4_uminbo_rr_i8(<8 x i8> %a, <8 x i8> %b) {
; CV1-LABEL: v4_uminbo_rr_i8:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r2 = $r1, 8
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    minuhq $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: v4_uminbo_rr_i8:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    minubo $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
entry:
  %0 = call <8 x i8> @llvm.umin.v8i8(<8 x i8> %a, <8 x i8> %b)
  ret <8 x i8> %0
}

declare <8 x i8> @llvm.smax.v8i8(<8 x i8> %a, <8 x i8> %b)
declare <8 x i8> @llvm.smin.v8i8(<8 x i8> %a, <8 x i8> %b)
declare <8 x i8> @llvm.umax.v8i8(<8 x i8> %a, <8 x i8> %b)
declare <8 x i8> @llvm.umin.v8i8(<8 x i8> %a, <8 x i8> %b)

attributes #0 = { nounwind }

define <8 x i8> @test_div_4(<8 x i8> %a, <8 x i8> %b) #0 {
; CV1-LABEL: test_div_4:
; CV1:       # %bb.0:
; CV1-NEXT:    srld $r1 = $r0, 56
; CV1-NEXT:    extfz $r2 = $r0, 55, 48
; CV1-NEXT:    extfz $r3 = $r0, 47, 40
; CV1-NEXT:    zxbd $r5 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    extfz $r4 = $r0, 39, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sraw $r1 = $r1, 7
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sraw $r2 = $r2, 7
; CV1-NEXT:    sraw $r3 = $r3, 7
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    extfz $r6 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    sraw $r4 = $r4, 7
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    extfz $r3 = $r0, 23, 16
; CV1-NEXT:    insf $r4 = $r3, 15, 8
; CV1-NEXT:    sraw $r6 = $r6, 7
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    sraw $r1 = $r1, 7
; CV1-NEXT:    sraw $r3 = $r3, 7
; CV1-NEXT:    insf $r4 = $r2, 31, 16
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r3 = $r1, 15, 8
; CV1-NEXT:    sraw $r5 = $r5, 7
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r5 = $r6, 15, 8
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r5 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    insf $r5 = $r4, 63, 32
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    andd $r1 = $r5, 0xff00ff.@
; CV1-NEXT:    andd $r2 = $r5, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    srld $r1 = $r1, 6
; CV1-NEXT:    srld $r2 = $r2, 6
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff.@
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    iord $r1 = $r2, $r1
; CV1-NEXT:    andd $r2 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    xord $r0 = $r0, $r1
; CV1-NEXT:    andd $r3 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    extfz $r1 = $r0, 55, 48
; CV1-NEXT:    srld $r2 = $r0, 56
; CV1-NEXT:    extfz $r3 = $r0, 47, 40
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    extfz $r4 = $r0, 39, 32
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    sraw $r2 = $r2, 2
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    sraw $r1 = $r1, 2
; CV1-NEXT:    sraw $r3 = $r3, 2
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    extfz $r5 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    insf $r1 = $r2, 15, 8
; CV1-NEXT:    srlw $r2 = $r0, 24
; CV1-NEXT:    sraw $r4 = $r4, 2
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    extfz $r3 = $r0, 15, 8
; CV1-NEXT:    insf $r4 = $r3, 15, 8
; CV1-NEXT:    sraw $r5 = $r5, 2
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sraw $r2 = $r2, 2
; CV1-NEXT:    sraw $r3 = $r3, 2
; CV1-NEXT:    insf $r4 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    sraw $r0 = $r0, 2
; CV1-NEXT:    insf $r5 = $r2, 15, 8
; CV1-NEXT:    ;; # (end cycle 27)
; CV1-NEXT:    insf $r0 = $r3, 15, 8
; CV1-NEXT:    ;; # (end cycle 28)
; CV1-NEXT:    insf $r0 = $r5, 31, 16
; CV1-NEXT:    ;; # (end cycle 29)
; CV1-NEXT:    insf $r0 = $r4, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 30)
;
; CV2-LABEL: test_div_4:
; CV2:       # %bb.0:
; CV2-NEXT:    srsbos $r0 = $r0, 2
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %r = sdiv <8 x i8> %a, <i8 4, i8 4, i8 4, i8 4, i8 4, i8 4, i8 4, i8 4>
  ret <8 x i8> %r
}

define <8 x i8> @test_div_32(<8 x i8> %a, <8 x i8> %b) #0 {
; CV1-LABEL: test_div_32:
; CV1:       # %bb.0:
; CV1-NEXT:    srld $r1 = $r0, 56
; CV1-NEXT:    extfz $r2 = $r0, 55, 48
; CV1-NEXT:    extfz $r3 = $r0, 47, 40
; CV1-NEXT:    zxbd $r5 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    extfz $r4 = $r0, 39, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sraw $r1 = $r1, 7
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sraw $r2 = $r2, 7
; CV1-NEXT:    sraw $r3 = $r3, 7
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    extfz $r6 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    sraw $r4 = $r4, 7
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    extfz $r3 = $r0, 23, 16
; CV1-NEXT:    insf $r4 = $r3, 15, 8
; CV1-NEXT:    sraw $r6 = $r6, 7
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    sraw $r1 = $r1, 7
; CV1-NEXT:    sraw $r3 = $r3, 7
; CV1-NEXT:    insf $r4 = $r2, 31, 16
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r3 = $r1, 15, 8
; CV1-NEXT:    sraw $r5 = $r5, 7
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r5 = $r6, 15, 8
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r5 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    insf $r5 = $r4, 63, 32
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    andd $r1 = $r5, 0xff00ff.@
; CV1-NEXT:    andd $r2 = $r5, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    srld $r1 = $r1, 3
; CV1-NEXT:    srld $r2 = $r2, 3
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff.@
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    iord $r1 = $r2, $r1
; CV1-NEXT:    andd $r2 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    xord $r0 = $r0, $r1
; CV1-NEXT:    andd $r3 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    extfz $r1 = $r0, 55, 48
; CV1-NEXT:    srld $r2 = $r0, 56
; CV1-NEXT:    extfz $r3 = $r0, 47, 40
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    extfz $r4 = $r0, 39, 32
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    sraw $r2 = $r2, 5
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    sraw $r1 = $r1, 5
; CV1-NEXT:    sraw $r3 = $r3, 5
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    extfz $r5 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    insf $r1 = $r2, 15, 8
; CV1-NEXT:    srlw $r2 = $r0, 24
; CV1-NEXT:    sraw $r4 = $r4, 5
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    extfz $r3 = $r0, 15, 8
; CV1-NEXT:    insf $r4 = $r3, 15, 8
; CV1-NEXT:    sraw $r5 = $r5, 5
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sraw $r2 = $r2, 5
; CV1-NEXT:    sraw $r3 = $r3, 5
; CV1-NEXT:    insf $r4 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    sraw $r0 = $r0, 5
; CV1-NEXT:    insf $r5 = $r2, 15, 8
; CV1-NEXT:    ;; # (end cycle 27)
; CV1-NEXT:    insf $r0 = $r3, 15, 8
; CV1-NEXT:    ;; # (end cycle 28)
; CV1-NEXT:    insf $r0 = $r5, 31, 16
; CV1-NEXT:    ;; # (end cycle 29)
; CV1-NEXT:    insf $r0 = $r4, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 30)
;
; CV2-LABEL: test_div_32:
; CV2:       # %bb.0:
; CV2-NEXT:    srsbos $r0 = $r0, 5
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %r = sdiv <8 x i8> %a, <i8 32, i8 32, i8 32, i8 32, i8 32, i8 32, i8 32, i8 32>
  ret <8 x i8> %r
}

define <8 x i8> @lshr_cst_splat_w_undefs(<8 x i8> %lhs ) {
; CV1-LABEL: lshr_cst_splat_w_undefs:
; CV1:       # %bb.0:
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r1 = $r0, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srld $r0 = $r0, 2
; CV1-NEXT:    srld $r1 = $r1, 2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: lshr_cst_splat_w_undefs:
; CV2:       # %bb.0:
; CV2-NEXT:    srlbos $r0 = $r0, 2
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %r = lshr <8 x i8> %lhs, <i8 2, i8 undef, i8 undef, i8 undef, i8 2, i8 undef, i8 undef, i8 undef>
  ret <8 x i8> %r
}

define <8 x i8> @lshr_val_splat_w_undefs(<8 x i8> %lhs, i32 %s ) {
; CV1-LABEL: lshr_val_splat_w_undefs:
; CV1:       # %bb.0:
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r2 = $r0, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srld $r0 = $r0, $r1
; CV1-NEXT:    srld $r2 = $r2, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r1 = $r2, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: lshr_val_splat_w_undefs:
; CV2:       # %bb.0:
; CV2-NEXT:    srlbos $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %conv = trunc i32 %s to i8
  %vecinit = insertelement <8 x i8> undef, i8 %conv, i32 0
  %rhs = insertelement <8 x i8> %vecinit, i8 %conv, i32 4

  %r = lshr <8 x i8> %lhs, %rhs
  ret <8 x i8> %r
}

define <8 x i8> @test_select_cmp(<8 x i8> %a, <8 x i8> %b, <8 x i8> %c, <8 x i8> %d) #0 {
; CV1-LABEL: test_select_cmp:
; CV1:       # %bb.0:
; CV1-NEXT:    sbmm8 $r4 = $r3, 0x80004000200010
; CV1-NEXT:    sbmm8 $r5 = $r2, 0x80004000200010
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sbmm8 $r2 = $r2, 0x8000400020001
; CV1-NEXT:    sbmm8 $r3 = $r3, 0x8000400020001
; CV1-NEXT:    compnhq.ne $r4 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    compnhq.ne $r2 = $r2, $r3
; CV1-NEXT:    sbmm8 $r3 = $r4, 0x40100401
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sbmm8 $r2 = $r2, 0x40100401
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    insf $r2 = $r3, 63, 32
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    compd.eq $r2 = $r2, -1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    cmoved.even $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: test_select_cmp:
; CV2:       # %bb.0:
; CV2-NEXT:    compnbo.ne $r2 = $r2, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    compd.eq $r2 = $r2, -1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    cmoved.even $r2 ? $r0 = $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 2)
  %cc = icmp ne <8 x i8> %c, %d
  %bc = bitcast <8 x i1> %cc to i8
  %cmp = icmp eq i8 %bc, -1
  %r = select i1 %cmp, <8 x i8> %a, <8 x i8> %b
  ret <8 x i8> %r
}

define <8 x i8> @fshl_rr(<8 x i8> %a, <8 x i8> %b, i8 %c) {
; CV1-LABEL: fshl_rr:
; CV1:       # %bb.0:
; CV1-NEXT:    andw $r3 = $r2, 7
; CV1-NEXT:    srld $r4 = $r0, 56
; CV1-NEXT:    extfz $r5 = $r0, 55, 48
; CV1-NEXT:    extfz $r6 = $r0, 47, 40
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sbmm8 $r2 = $r2, 0x1010101.@
; CV1-NEXT:    sllw $r4 = $r4, $r3
; CV1-NEXT:    sllw $r5 = $r5, $r3
; CV1-NEXT:    extfz $r7 = $r0, 39, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sllw $r4 = $r6, $r3
; CV1-NEXT:    insf $r5 = $r4, 15, 8
; CV1-NEXT:    sllw $r6 = $r7, $r3
; CV1-NEXT:    zxbd $r9 = $r2
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    srlw $r4 = $r0, 24
; CV1-NEXT:    insf $r6 = $r4, 15, 8
; CV1-NEXT:    extfz $r7 = $r0, 23, 16
; CV1-NEXT:    andw $r9 = $r9, 7
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    extfz $r8 = $r0, 15, 8
; CV1-NEXT:    andd $r10 = $r1, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sllw $r0 = $r0, $r9
; CV1-NEXT:    sllw $r3 = $r8, $r3
; CV1-NEXT:    sllw $r4 = $r4, $r3
; CV1-NEXT:    sllw $r7 = $r7, $r3
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    insf $r0 = $r3, 15, 8
; CV1-NEXT:    srld $r1 = $r1, 1
; CV1-NEXT:    insf $r7 = $r4, 15, 8
; CV1-NEXT:    srld $r8 = $r10, 1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    notd $r2 = $r2
; CV1-NEXT:    andd $r3 = $r8, 0xff00ff.@
; CV1-NEXT:    insf $r6 = $r5, 31, 16
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r0 = $r7, 31, 16
; CV1-NEXT:    iord $r1 = $r1, $r3
; CV1-NEXT:    srld $r4 = $r2, 56
; CV1-NEXT:    extfz $r7 = $r2, 55, 48
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    srld $r3 = $r1, 56
; CV1-NEXT:    andw $r4 = $r4, 7
; CV1-NEXT:    extfz $r5 = $r1, 55, 48
; CV1-NEXT:    extfz $r8 = $r1, 47, 40
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    zxbd $r3 = $r3
; CV1-NEXT:    zxbd $r5 = $r5
; CV1-NEXT:    extfz $r9 = $r2, 47, 40
; CV1-NEXT:    extfz $r10 = $r1, 39, 32
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    andw $r7 = $r7, 7
; CV1-NEXT:    zxbd $r8 = $r8
; CV1-NEXT:    andw $r9 = $r9, 7
; CV1-NEXT:    extfz $r11 = $r2, 39, 32
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    srlw $r3 = $r3, $r4
; CV1-NEXT:    srlw $r4 = $r5, $r7
; CV1-NEXT:    zxbd $r10 = $r10
; CV1-NEXT:    andw $r11 = $r11, 7
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    srlw $r3 = $r1, 24
; CV1-NEXT:    insf $r4 = $r3, 15, 8
; CV1-NEXT:    srlw $r5 = $r8, $r9
; CV1-NEXT:    srlw $r7 = $r10, $r11
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    zxbd $r3 = $r3
; CV1-NEXT:    srlw $r5 = $r2, 24
; CV1-NEXT:    insf $r7 = $r5, 15, 8
; CV1-NEXT:    extfz $r8 = $r1, 23, 16
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    zxbd $r1 = $r1
; CV1-NEXT:    andw $r5 = $r5, 7
; CV1-NEXT:    extfz $r9 = $r2, 23, 16
; CV1-NEXT:    extfz $r10 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    zxbd $r2 = $r2
; CV1-NEXT:    zxbd $r8 = $r8
; CV1-NEXT:    andw $r9 = $r9, 7
; CV1-NEXT:    extfz $r11 = $r2, 15, 8
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    zxbd $r1 = $r1
; CV1-NEXT:    andw $r2 = $r2, 7
; CV1-NEXT:    zxbd $r10 = $r10
; CV1-NEXT:    andw $r11 = $r11, 7
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    srlw $r1 = $r1, $r2
; CV1-NEXT:    srlw $r3 = $r3, $r5
; CV1-NEXT:    srlw $r5 = $r8, $r9
; CV1-NEXT:    srlw $r8 = $r10, $r11
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    insf $r1 = $r8, 15, 8
; CV1-NEXT:    insf $r5 = $r3, 15, 8
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    insf $r1 = $r5, 31, 16
; CV1-NEXT:    insf $r7 = $r4, 31, 16
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    insf $r0 = $r6, 63, 32
; CV1-NEXT:    insf $r1 = $r7, 63, 32
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 22)
;
; CV2-LABEL: fshl_rr:
; CV2:       # %bb.0:
; CV2-NEXT:    srlbos $r1 = $r1, 1
; CV2-NEXT:    sbmm8 $r2 = $r2, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    andd $r2 = $r2, 0x7070707.@
; CV2-NEXT:    andnd $r3 = $r2, 0x7070707.@
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    srlbos $r4 = $r1, $r3
; CV2-NEXT:    extfz $r5 = $r3, 10, 8
; CV2-NEXT:    extfz $r6 = $r3, 18, 16
; CV2-NEXT:    extfz $r8 = $r2, 10, 8
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    srlbos $r5 = $r1, $r5
; CV2-NEXT:    srlbos $r6 = $r1, $r6
; CV2-NEXT:    extfz $r7 = $r3, 26, 24
; CV2-NEXT:    sllbos $r8 = $r0, $r8
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sllbos $r4 = $r0, $r2
; CV2-NEXT:    insf $r5 = $r4, 7, 0
; CV2-NEXT:    srlbos $r7 = $r1, $r7
; CV2-NEXT:    extfz $r9 = $r3, 34, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    extfz $r4 = $r2, 18, 16
; CV2-NEXT:    extfz $r5 = $r2, 26, 24
; CV2-NEXT:    insf $r6 = $r5, 15, 0
; CV2-NEXT:    insf $r8 = $r4, 7, 0
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    sllbos $r4 = $r0, $r4
; CV2-NEXT:    sllbos $r5 = $r0, $r5
; CV2-NEXT:    extfz $r6 = $r2, 34, 32
; CV2-NEXT:    insf $r7 = $r6, 23, 0
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r4 = $r8, 15, 0
; CV2-NEXT:    extfz $r8 = $r3, 42, 40
; CV2-NEXT:    srlbos $r9 = $r1, $r9
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    sllbos $r4 = $r0, $r6
; CV2-NEXT:    insf $r5 = $r4, 23, 0
; CV2-NEXT:    extfz $r6 = $r2, 42, 40
; CV2-NEXT:    insf $r9 = $r7, 31, 0
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    insf $r4 = $r5, 31, 0
; CV2-NEXT:    srlbos $r5 = $r1, $r8
; CV2-NEXT:    sllbos $r6 = $r0, $r6
; CV2-NEXT:    extfz $r7 = $r3, 50, 48
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    extfz $r4 = $r2, 50, 48
; CV2-NEXT:    insf $r5 = $r9, 39, 0
; CV2-NEXT:    insf $r6 = $r4, 39, 0
; CV2-NEXT:    srlbos $r7 = $r1, $r7
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    extfz $r2 = $r2, 58, 56
; CV2-NEXT:    extfz $r3 = $r3, 58, 56
; CV2-NEXT:    sllbos $r4 = $r0, $r4
; CV2-NEXT:    insf $r7 = $r5, 47, 0
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    sllbos $r0 = $r0, $r2
; CV2-NEXT:    srlbos $r1 = $r1, $r3
; CV2-NEXT:    insf $r4 = $r6, 47, 0
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    insf $r0 = $r4, 55, 0
; CV2-NEXT:    insf $r1 = $r7, 55, 0
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 14)
  %i = insertelement <8 x i8> undef, i8 %c, i8 0
  %s = shufflevector <8 x i8> %i, <8 x i8> undef, <8 x i32> zeroinitializer
  %r = call <8 x i8> @llvm.fshl.v8i8(<8 x i8> %a, <8 x i8> %b, <8 x i8> %s)
  ret <8 x i8> %r
}

define <8 x i8> @fshl_ri(<8 x i8> %a, <8 x i8> %b) {
; CV1-LABEL: fshl_ri:
; CV1:       # %bb.0:
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    andd $r2 = $r1, 0xff00ff.@
; CV1-NEXT:    andd $r3 = $r0, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    slld $r0 = $r0, 3
; CV1-NEXT:    srld $r1 = $r1, 1
; CV1-NEXT:    srld $r2 = $r2, 1
; CV1-NEXT:    slld $r3 = $r3, 3
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff.@
; CV1-NEXT:    andd $r3 = $r3, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r3
; CV1-NEXT:    iord $r1 = $r1, $r2
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    andd $r2 = $r1, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    srld $r1 = $r1, 4
; CV1-NEXT:    srld $r2 = $r2, 4
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    iord $r1 = $r1, $r2
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 8)
;
; CV2-LABEL: fshl_ri:
; CV2:       # %bb.0:
; CV2-NEXT:    sllbos $r0 = $r0, 3
; CV2-NEXT:    srlbos $r1 = $r1, 1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srlbos $r1 = $r1, 4
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 2)
  %r = call <8 x i8> @llvm.fshl.v8i8(<8 x i8> %a, <8 x i8> %b, <8 x i8> <i8 3, i8 3, i8 3, i8 3, i8 3, i8 3, i8 3, i8 3>)
  ret <8 x i8> %r
}

define <8 x i8> @fshl_vec(<8 x i8> %a, <8 x i8> %b, <8 x i8> %c) {
; CV1-LABEL: fshl_vec:
; CV1:       # %bb.0:
; CV1-NEXT:    srld $r3 = $r2, 56
; CV1-NEXT:    extfz $r4 = $r2, 55, 48
; CV1-NEXT:    srld $r5 = $r0, 56
; CV1-NEXT:    extfz $r6 = $r0, 55, 48
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andw $r3 = $r3, 7
; CV1-NEXT:    andw $r4 = $r4, 7
; CV1-NEXT:    extfz $r7 = $r2, 47, 40
; CV1-NEXT:    extfz $r8 = $r0, 39, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sllw $r3 = $r5, $r3
; CV1-NEXT:    sllw $r4 = $r6, $r4
; CV1-NEXT:    extfz $r5 = $r2, 39, 32
; CV1-NEXT:    extfz $r6 = $r0, 47, 40
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r4 = $r3, 15, 8
; CV1-NEXT:    andw $r5 = $r5, 7
; CV1-NEXT:    andw $r7 = $r7, 7
; CV1-NEXT:    zxbd $r9 = $r2
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sllw $r3 = $r6, $r7
; CV1-NEXT:    sllw $r5 = $r8, $r5
; CV1-NEXT:    srlw $r6 = $r2, 24
; CV1-NEXT:    extfz $r7 = $r2, 23, 16
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    srlw $r3 = $r0, 24
; CV1-NEXT:    insf $r5 = $r3, 15, 8
; CV1-NEXT:    andw $r6 = $r6, 7
; CV1-NEXT:    andw $r7 = $r7, 7
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sllw $r3 = $r3, $r6
; CV1-NEXT:    extfz $r6 = $r0, 23, 16
; CV1-NEXT:    extfz $r8 = $r2, 15, 8
; CV1-NEXT:    andd $r10 = $r1, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllw $r6 = $r6, $r7
; CV1-NEXT:    extfz $r7 = $r0, 15, 8
; CV1-NEXT:    andw $r8 = $r8, 7
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    sllw $r7 = $r7, $r8
; CV1-NEXT:    srld $r8 = $r10, 1
; CV1-NEXT:    andw $r9 = $r9, 7
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    sllw $r0 = $r0, $r9
; CV1-NEXT:    srld $r1 = $r1, 1
; CV1-NEXT:    andd $r3 = $r8, 0xff00ff.@
; CV1-NEXT:    insf $r6 = $r3, 15, 8
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r0 = $r7, 15, 8
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    notd $r2 = $r2
; CV1-NEXT:    insf $r5 = $r4, 31, 16
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    insf $r0 = $r6, 31, 16
; CV1-NEXT:    iord $r1 = $r1, $r3
; CV1-NEXT:    srld $r4 = $r2, 56
; CV1-NEXT:    extfz $r7 = $r2, 55, 48
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    srld $r3 = $r1, 56
; CV1-NEXT:    andw $r4 = $r4, 7
; CV1-NEXT:    extfz $r6 = $r1, 55, 48
; CV1-NEXT:    extfz $r8 = $r1, 47, 40
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    zxbd $r3 = $r3
; CV1-NEXT:    zxbd $r6 = $r6
; CV1-NEXT:    extfz $r9 = $r2, 47, 40
; CV1-NEXT:    extfz $r10 = $r1, 39, 32
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    andw $r7 = $r7, 7
; CV1-NEXT:    zxbd $r8 = $r8
; CV1-NEXT:    andw $r9 = $r9, 7
; CV1-NEXT:    extfz $r11 = $r2, 39, 32
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    srlw $r3 = $r3, $r4
; CV1-NEXT:    srlw $r4 = $r6, $r7
; CV1-NEXT:    zxbd $r10 = $r10
; CV1-NEXT:    andw $r11 = $r11, 7
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    srlw $r3 = $r1, 24
; CV1-NEXT:    insf $r4 = $r3, 15, 8
; CV1-NEXT:    srlw $r6 = $r8, $r9
; CV1-NEXT:    srlw $r7 = $r10, $r11
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    zxbd $r3 = $r3
; CV1-NEXT:    srlw $r6 = $r2, 24
; CV1-NEXT:    insf $r7 = $r6, 15, 8
; CV1-NEXT:    extfz $r8 = $r1, 23, 16
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    zxbd $r1 = $r1
; CV1-NEXT:    andw $r6 = $r6, 7
; CV1-NEXT:    extfz $r9 = $r2, 23, 16
; CV1-NEXT:    extfz $r10 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    zxbd $r2 = $r2
; CV1-NEXT:    zxbd $r8 = $r8
; CV1-NEXT:    andw $r9 = $r9, 7
; CV1-NEXT:    extfz $r11 = $r2, 15, 8
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    zxbd $r1 = $r1
; CV1-NEXT:    andw $r2 = $r2, 7
; CV1-NEXT:    zxbd $r10 = $r10
; CV1-NEXT:    andw $r11 = $r11, 7
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    srlw $r1 = $r1, $r2
; CV1-NEXT:    srlw $r3 = $r3, $r6
; CV1-NEXT:    srlw $r6 = $r8, $r9
; CV1-NEXT:    srlw $r8 = $r10, $r11
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    insf $r1 = $r8, 15, 8
; CV1-NEXT:    insf $r6 = $r3, 15, 8
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    insf $r1 = $r6, 31, 16
; CV1-NEXT:    insf $r7 = $r4, 31, 16
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    insf $r0 = $r5, 63, 32
; CV1-NEXT:    insf $r1 = $r7, 63, 32
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 25)
;
; CV2-LABEL: fshl_vec:
; CV2:       # %bb.0:
; CV2-NEXT:    srlbos $r1 = $r1, 1
; CV2-NEXT:    andd $r2 = $r2, 0x7070707.@
; CV2-NEXT:    andnd $r3 = $r2, 0x7070707.@
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srlbos $r4 = $r1, $r3
; CV2-NEXT:    extfz $r5 = $r3, 10, 8
; CV2-NEXT:    extfz $r6 = $r3, 18, 16
; CV2-NEXT:    extfz $r7 = $r3, 26, 24
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    srlbos $r5 = $r1, $r5
; CV2-NEXT:    srlbos $r6 = $r1, $r6
; CV2-NEXT:    extfz $r8 = $r3, 34, 32
; CV2-NEXT:    sllbos $r9 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    extfz $r4 = $r2, 10, 8
; CV2-NEXT:    insf $r5 = $r4, 7, 0
; CV2-NEXT:    srlbos $r7 = $r1, $r7
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sllbos $r4 = $r0, $r4
; CV2-NEXT:    extfz $r5 = $r2, 18, 16
; CV2-NEXT:    insf $r6 = $r5, 15, 0
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r4 = $r9, 7, 0
; CV2-NEXT:    sllbos $r5 = $r0, $r5
; CV2-NEXT:    insf $r7 = $r6, 23, 0
; CV2-NEXT:    extfz $r9 = $r2, 26, 24
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    sllbos $r4 = $r0, $r9
; CV2-NEXT:    insf $r5 = $r4, 15, 0
; CV2-NEXT:    srlbos $r6 = $r1, $r8
; CV2-NEXT:    extfz $r8 = $r2, 34, 32
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r4 = $r5, 23, 0
; CV2-NEXT:    sllbos $r5 = $r0, $r8
; CV2-NEXT:    extfz $r8 = $r2, 42, 40
; CV2-NEXT:    extfz $r9 = $r3, 42, 40
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    srlbos $r4 = $r1, $r9
; CV2-NEXT:    insf $r5 = $r4, 31, 0
; CV2-NEXT:    insf $r6 = $r7, 31, 0
; CV2-NEXT:    sllbos $r7 = $r0, $r8
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    insf $r4 = $r6, 39, 0
; CV2-NEXT:    extfz $r5 = $r2, 50, 48
; CV2-NEXT:    extfz $r6 = $r3, 50, 48
; CV2-NEXT:    insf $r7 = $r5, 39, 0
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    extfz $r2 = $r2, 58, 56
; CV2-NEXT:    extfz $r3 = $r3, 58, 56
; CV2-NEXT:    sllbos $r5 = $r0, $r5
; CV2-NEXT:    srlbos $r6 = $r1, $r6
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    sllbos $r0 = $r0, $r2
; CV2-NEXT:    srlbos $r1 = $r1, $r3
; CV2-NEXT:    insf $r5 = $r7, 47, 0
; CV2-NEXT:    insf $r6 = $r4, 47, 0
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    insf $r0 = $r5, 55, 0
; CV2-NEXT:    insf $r1 = $r6, 55, 0
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 13)
  %r = call <8 x i8> @llvm.fshl.v8i8(<8 x i8> %a, <8 x i8> %b, <8 x i8> %c)
  ret <8 x i8> %r
}
define <8 x i8> @fshr_rr(<8 x i8> %a, <8 x i8> %b, i8 %c) {
; CV1-LABEL: fshr_rr:
; CV1:       # %bb.0:
; CV1-NEXT:    srld $r4 = $r1, 56
; CV1-NEXT:    extfz $r5 = $r1, 55, 48
; CV1-NEXT:    extfz $r6 = $r1, 47, 40
; CV1-NEXT:    srlw $r8 = $r1, 24
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andw $r2 = $r2, 7
; CV1-NEXT:    sbmm8 $r3 = $r2, 0x1010101.@
; CV1-NEXT:    zxbd $r4 = $r4
; CV1-NEXT:    extfz $r7 = $r1, 39, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srlw $r4 = $r4, $r2
; CV1-NEXT:    zxbd $r5 = $r5
; CV1-NEXT:    zxbd $r6 = $r6
; CV1-NEXT:    zxbd $r7 = $r7
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    srlw $r5 = $r5, $r2
; CV1-NEXT:    srlw $r6 = $r6, $r2
; CV1-NEXT:    srlw $r7 = $r7, $r2
; CV1-NEXT:    extfz $r9 = $r1, 23, 16
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    zxbd $r4 = $r8
; CV1-NEXT:    insf $r5 = $r4, 15, 8
; CV1-NEXT:    zxbd $r6 = $r9
; CV1-NEXT:    insf $r7 = $r6, 15, 8
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    srlw $r4 = $r4, $r2
; CV1-NEXT:    srlw $r5 = $r6, $r2
; CV1-NEXT:    extfz $r6 = $r1, 15, 8
; CV1-NEXT:    insf $r7 = $r5, 31, 16
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    zxbd $r1 = $r1
; CV1-NEXT:    zxbd $r4 = $r6
; CV1-NEXT:    insf $r5 = $r4, 15, 8
; CV1-NEXT:    zxbd $r6 = $r3
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    zxbd $r1 = $r1
; CV1-NEXT:    andw $r6 = $r6, 7
; CV1-NEXT:    andd $r8 = $r0, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    slld $r0 = $r0, 1
; CV1-NEXT:    srlw $r1 = $r1, $r6
; CV1-NEXT:    srlw $r2 = $r4, $r2
; CV1-NEXT:    slld $r4 = $r8, 1
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r2, 15, 8
; CV1-NEXT:    andd $r2 = $r4, 0xff00ff.@
; CV1-NEXT:    notd $r3 = $r3
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    iord $r0 = $r0, $r2
; CV1-NEXT:    insf $r1 = $r5, 31, 16
; CV1-NEXT:    srld $r2 = $r3, 56
; CV1-NEXT:    extfz $r4 = $r3, 55, 48
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    andw $r2 = $r2, 7
; CV1-NEXT:    andw $r4 = $r4, 7
; CV1-NEXT:    srld $r5 = $r0, 56
; CV1-NEXT:    extfz $r6 = $r0, 55, 48
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    sllw $r2 = $r5, $r2
; CV1-NEXT:    sllw $r4 = $r6, $r4
; CV1-NEXT:    extfz $r5 = $r3, 47, 40
; CV1-NEXT:    extfz $r6 = $r3, 39, 32
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    andw $r5 = $r5, 7
; CV1-NEXT:    andw $r6 = $r6, 7
; CV1-NEXT:    extfz $r8 = $r0, 47, 40
; CV1-NEXT:    extfz $r9 = $r0, 39, 32
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    sllw $r2 = $r8, $r5
; CV1-NEXT:    insf $r4 = $r2, 15, 8
; CV1-NEXT:    sllw $r5 = $r9, $r6
; CV1-NEXT:    srlw $r6 = $r3, 24
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    zxbd $r3 = $r3
; CV1-NEXT:    andw $r6 = $r6, 7
; CV1-NEXT:    extfz $r8 = $r3, 23, 16
; CV1-NEXT:    extfz $r10 = $r3, 15, 8
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    srlw $r2 = $r0, 24
; CV1-NEXT:    insf $r5 = $r2, 15, 8
; CV1-NEXT:    andw $r8 = $r8, 7
; CV1-NEXT:    extfz $r9 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    andw $r3 = $r3, 7
; CV1-NEXT:    andw $r10 = $r10, 7
; CV1-NEXT:    extfz $r11 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    sllw $r0 = $r0, $r3
; CV1-NEXT:    sllw $r2 = $r2, $r6
; CV1-NEXT:    sllw $r6 = $r9, $r8
; CV1-NEXT:    sllw $r8 = $r11, $r10
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    insf $r0 = $r8, 15, 8
; CV1-NEXT:    insf $r6 = $r2, 15, 8
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    insf $r0 = $r6, 31, 16
; CV1-NEXT:    insf $r5 = $r4, 31, 16
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    insf $r0 = $r5, 63, 32
; CV1-NEXT:    insf $r1 = $r7, 63, 32
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 22)
;
; CV2-LABEL: fshr_rr:
; CV2:       # %bb.0:
; CV2-NEXT:    sllbos $r0 = $r0, 1
; CV2-NEXT:    sbmm8 $r2 = $r2, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    andnd $r2 = $r2, 0x7070707.@
; CV2-NEXT:    andd $r3 = $r2, 0x7070707.@
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    srlbos $r4 = $r1, $r3
; CV2-NEXT:    extfz $r5 = $r3, 10, 8
; CV2-NEXT:    extfz $r6 = $r3, 18, 16
; CV2-NEXT:    extfz $r8 = $r2, 10, 8
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    srlbos $r5 = $r1, $r5
; CV2-NEXT:    srlbos $r6 = $r1, $r6
; CV2-NEXT:    extfz $r7 = $r3, 26, 24
; CV2-NEXT:    sllbos $r8 = $r0, $r8
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sllbos $r4 = $r0, $r2
; CV2-NEXT:    insf $r5 = $r4, 7, 0
; CV2-NEXT:    srlbos $r7 = $r1, $r7
; CV2-NEXT:    extfz $r9 = $r3, 34, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    extfz $r4 = $r2, 18, 16
; CV2-NEXT:    extfz $r5 = $r2, 26, 24
; CV2-NEXT:    insf $r6 = $r5, 15, 0
; CV2-NEXT:    insf $r8 = $r4, 7, 0
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    sllbos $r4 = $r0, $r4
; CV2-NEXT:    sllbos $r5 = $r0, $r5
; CV2-NEXT:    extfz $r6 = $r2, 34, 32
; CV2-NEXT:    insf $r7 = $r6, 23, 0
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r4 = $r8, 15, 0
; CV2-NEXT:    extfz $r8 = $r3, 42, 40
; CV2-NEXT:    srlbos $r9 = $r1, $r9
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    sllbos $r4 = $r0, $r6
; CV2-NEXT:    insf $r5 = $r4, 23, 0
; CV2-NEXT:    extfz $r6 = $r2, 42, 40
; CV2-NEXT:    insf $r9 = $r7, 31, 0
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    insf $r4 = $r5, 31, 0
; CV2-NEXT:    srlbos $r5 = $r1, $r8
; CV2-NEXT:    sllbos $r6 = $r0, $r6
; CV2-NEXT:    extfz $r7 = $r3, 50, 48
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    extfz $r4 = $r2, 50, 48
; CV2-NEXT:    insf $r5 = $r9, 39, 0
; CV2-NEXT:    insf $r6 = $r4, 39, 0
; CV2-NEXT:    srlbos $r7 = $r1, $r7
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    extfz $r2 = $r2, 58, 56
; CV2-NEXT:    extfz $r3 = $r3, 58, 56
; CV2-NEXT:    sllbos $r4 = $r0, $r4
; CV2-NEXT:    insf $r7 = $r5, 47, 0
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    sllbos $r0 = $r0, $r2
; CV2-NEXT:    srlbos $r1 = $r1, $r3
; CV2-NEXT:    insf $r4 = $r6, 47, 0
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    insf $r0 = $r4, 55, 0
; CV2-NEXT:    insf $r1 = $r7, 55, 0
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 14)
  %i = insertelement <8 x i8> undef, i8 %c, i8 0
  %s = shufflevector <8 x i8> %i, <8 x i8> undef, <8 x i32> zeroinitializer
  %r = call <8 x i8> @llvm.fshr.v8i8(<8 x i8> %a, <8 x i8> %b, <8 x i8> %s)
  ret <8 x i8> %r
}

define <8 x i8> @fshr_ri(<8 x i8> %a, <8 x i8> %b, i8 %c) {
; CV1-LABEL: fshr_ri:
; CV1:       # %bb.0:
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    andd $r2 = $r0, 0xff00ff.@
; CV1-NEXT:    andd $r3 = $r1, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    slld $r0 = $r0, 1
; CV1-NEXT:    srld $r1 = $r1, 3
; CV1-NEXT:    slld $r2 = $r2, 1
; CV1-NEXT:    srld $r3 = $r3, 3
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff.@
; CV1-NEXT:    andd $r3 = $r3, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r2
; CV1-NEXT:    iord $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r2 = $r0, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    slld $r0 = $r0, 4
; CV1-NEXT:    slld $r2 = $r2, 4
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    iord $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 8)
;
; CV2-LABEL: fshr_ri:
; CV2:       # %bb.0:
; CV2-NEXT:    sllbos $r0 = $r0, 1
; CV2-NEXT:    srlbos $r1 = $r1, 3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sllbos $r0 = $r0, 4
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 2)
  %r = call <8 x i8> @llvm.fshr.v8i8(<8 x i8> %a, <8 x i8> %b, <8 x i8> <i8 3, i8 3, i8 3, i8 3, i8 3, i8 3, i8 3, i8 3>)
  ret <8 x i8> %r
}

define <8 x i8> @fshr_vec(<8 x i8> %a, <8 x i8> %b, <8 x i8> %c) {
; CV1-LABEL: fshr_vec:
; CV1:       # %bb.0:
; CV1-NEXT:    srld $r3 = $r2, 56
; CV1-NEXT:    srld $r4 = $r1, 56
; CV1-NEXT:    extfz $r5 = $r2, 55, 48
; CV1-NEXT:    extfz $r6 = $r1, 55, 48
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andw $r3 = $r3, 7
; CV1-NEXT:    zxbd $r4 = $r4
; CV1-NEXT:    extfz $r7 = $r2, 47, 40
; CV1-NEXT:    extfz $r8 = $r1, 47, 40
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andw $r5 = $r5, 7
; CV1-NEXT:    zxbd $r6 = $r6
; CV1-NEXT:    extfz $r9 = $r2, 39, 32
; CV1-NEXT:    extfz $r10 = $r1, 39, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    andw $r7 = $r7, 7
; CV1-NEXT:    zxbd $r8 = $r8
; CV1-NEXT:    andw $r9 = $r9, 7
; CV1-NEXT:    zxbd $r10 = $r10
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srlw $r3 = $r4, $r3
; CV1-NEXT:    srlw $r4 = $r6, $r5
; CV1-NEXT:    srlw $r5 = $r8, $r7
; CV1-NEXT:    srlw $r6 = $r10, $r9
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    srlw $r7 = $r2, 24
; CV1-NEXT:    srlw $r8 = $r1, 24
; CV1-NEXT:    extfz $r9 = $r2, 23, 16
; CV1-NEXT:    extfz $r10 = $r1, 23, 16
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    andw $r7 = $r7, 7
; CV1-NEXT:    zxbd $r8 = $r8
; CV1-NEXT:    andw $r9 = $r9, 7
; CV1-NEXT:    zxbd $r10 = $r10
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    srlw $r3 = $r8, $r7
; CV1-NEXT:    insf $r4 = $r3, 15, 8
; CV1-NEXT:    srlw $r5 = $r10, $r9
; CV1-NEXT:    insf $r6 = $r5, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r5 = $r3, 15, 8
; CV1-NEXT:    insf $r6 = $r4, 31, 16
; CV1-NEXT:    zxbd $r7 = $r2
; CV1-NEXT:    andd $r8 = $r0, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    zxbd $r1 = $r1
; CV1-NEXT:    extfz $r3 = $r2, 15, 8
; CV1-NEXT:    extfz $r4 = $r1, 15, 8
; CV1-NEXT:    andw $r7 = $r7, 7
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    zxbd $r1 = $r1
; CV1-NEXT:    andw $r3 = $r3, 7
; CV1-NEXT:    zxbd $r4 = $r4
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    slld $r0 = $r0, 1
; CV1-NEXT:    srlw $r1 = $r1, $r7
; CV1-NEXT:    srlw $r3 = $r4, $r3
; CV1-NEXT:    slld $r4 = $r8, 1
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r3, 15, 8
; CV1-NEXT:    notd $r2 = $r2
; CV1-NEXT:    andd $r3 = $r4, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    iord $r0 = $r0, $r3
; CV1-NEXT:    insf $r1 = $r5, 31, 16
; CV1-NEXT:    srld $r3 = $r2, 56
; CV1-NEXT:    extfz $r4 = $r2, 55, 48
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    andw $r3 = $r3, 7
; CV1-NEXT:    andw $r4 = $r4, 7
; CV1-NEXT:    srld $r5 = $r0, 56
; CV1-NEXT:    extfz $r7 = $r0, 55, 48
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    sllw $r3 = $r5, $r3
; CV1-NEXT:    sllw $r4 = $r7, $r4
; CV1-NEXT:    extfz $r5 = $r2, 47, 40
; CV1-NEXT:    extfz $r7 = $r2, 39, 32
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    andw $r5 = $r5, 7
; CV1-NEXT:    andw $r7 = $r7, 7
; CV1-NEXT:    extfz $r8 = $r0, 47, 40
; CV1-NEXT:    extfz $r9 = $r0, 39, 32
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    sllw $r3 = $r8, $r5
; CV1-NEXT:    insf $r4 = $r3, 15, 8
; CV1-NEXT:    sllw $r5 = $r9, $r7
; CV1-NEXT:    srlw $r7 = $r2, 24
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    zxbd $r2 = $r2
; CV1-NEXT:    andw $r7 = $r7, 7
; CV1-NEXT:    extfz $r8 = $r2, 23, 16
; CV1-NEXT:    extfz $r10 = $r2, 15, 8
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    srlw $r3 = $r0, 24
; CV1-NEXT:    insf $r5 = $r3, 15, 8
; CV1-NEXT:    andw $r8 = $r8, 7
; CV1-NEXT:    extfz $r9 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    andw $r2 = $r2, 7
; CV1-NEXT:    andw $r10 = $r10, 7
; CV1-NEXT:    extfz $r11 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    sllw $r0 = $r0, $r2
; CV1-NEXT:    sllw $r3 = $r3, $r7
; CV1-NEXT:    sllw $r7 = $r9, $r8
; CV1-NEXT:    sllw $r8 = $r11, $r10
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    insf $r0 = $r8, 15, 8
; CV1-NEXT:    insf $r7 = $r3, 15, 8
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    insf $r0 = $r7, 31, 16
; CV1-NEXT:    insf $r5 = $r4, 31, 16
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    insf $r0 = $r5, 63, 32
; CV1-NEXT:    insf $r1 = $r6, 63, 32
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 25)
;
; CV2-LABEL: fshr_vec:
; CV2:       # %bb.0:
; CV2-NEXT:    sllbos $r0 = $r0, 1
; CV2-NEXT:    andnd $r2 = $r2, 0x7070707.@
; CV2-NEXT:    andd $r3 = $r2, 0x7070707.@
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srlbos $r4 = $r1, $r3
; CV2-NEXT:    extfz $r5 = $r3, 10, 8
; CV2-NEXT:    extfz $r6 = $r3, 18, 16
; CV2-NEXT:    extfz $r7 = $r3, 26, 24
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    srlbos $r5 = $r1, $r5
; CV2-NEXT:    srlbos $r6 = $r1, $r6
; CV2-NEXT:    extfz $r8 = $r3, 34, 32
; CV2-NEXT:    sllbos $r9 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    extfz $r4 = $r2, 10, 8
; CV2-NEXT:    insf $r5 = $r4, 7, 0
; CV2-NEXT:    srlbos $r7 = $r1, $r7
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sllbos $r4 = $r0, $r4
; CV2-NEXT:    extfz $r5 = $r2, 18, 16
; CV2-NEXT:    insf $r6 = $r5, 15, 0
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r4 = $r9, 7, 0
; CV2-NEXT:    sllbos $r5 = $r0, $r5
; CV2-NEXT:    insf $r7 = $r6, 23, 0
; CV2-NEXT:    extfz $r9 = $r2, 26, 24
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    sllbos $r4 = $r0, $r9
; CV2-NEXT:    insf $r5 = $r4, 15, 0
; CV2-NEXT:    srlbos $r6 = $r1, $r8
; CV2-NEXT:    extfz $r8 = $r2, 34, 32
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r4 = $r5, 23, 0
; CV2-NEXT:    sllbos $r5 = $r0, $r8
; CV2-NEXT:    extfz $r8 = $r2, 42, 40
; CV2-NEXT:    extfz $r9 = $r3, 42, 40
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    srlbos $r4 = $r1, $r9
; CV2-NEXT:    insf $r5 = $r4, 31, 0
; CV2-NEXT:    insf $r6 = $r7, 31, 0
; CV2-NEXT:    sllbos $r7 = $r0, $r8
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    insf $r4 = $r6, 39, 0
; CV2-NEXT:    extfz $r5 = $r2, 50, 48
; CV2-NEXT:    extfz $r6 = $r3, 50, 48
; CV2-NEXT:    insf $r7 = $r5, 39, 0
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    extfz $r2 = $r2, 58, 56
; CV2-NEXT:    extfz $r3 = $r3, 58, 56
; CV2-NEXT:    sllbos $r5 = $r0, $r5
; CV2-NEXT:    srlbos $r6 = $r1, $r6
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    sllbos $r0 = $r0, $r2
; CV2-NEXT:    srlbos $r1 = $r1, $r3
; CV2-NEXT:    insf $r5 = $r7, 47, 0
; CV2-NEXT:    insf $r6 = $r4, 47, 0
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    insf $r0 = $r5, 55, 0
; CV2-NEXT:    insf $r1 = $r6, 55, 0
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 13)
  %r = call <8 x i8> @llvm.fshr.v8i8(<8 x i8> %a, <8 x i8> %b, <8 x i8> %c)
  ret <8 x i8> %r
}

declare <8 x i8> @llvm.fshr.v8i8(<8 x i8>, <8 x i8>, <8 x i8>)
declare <8 x i8> @llvm.fshl.v8i8(<8 x i8>, <8 x i8>, <8 x i8>)
