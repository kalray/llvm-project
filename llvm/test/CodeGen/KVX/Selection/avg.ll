; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mcpu=kv3-1 -O2 -o - %s | FileCheck %s --check-prefixes=CHECK,KV3_1
; RUN: llc -mcpu=kv3-2 -O2 -o - %s | FileCheck %s --check-prefixes=CHECK,KV3_2
; RUN: clang -O2 -c -o /dev/null %s

; Assembly broken, see T19963
; FIXME: clang -O2 -march=kv3-2 -c -o /dev/null %s

target triple = "kvx-kalray-cos"

define i32 @avg_i32(i32 %a, i32 %b) {
; CHECK-LABEL: avg_i32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    avgw $r0 = $r1, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %add = add nsw i32 %b, %a
  %shr = ashr i32 %add, 1
  ret i32 %shr
}

define i32 @ravg_i32(i32 %a, i32 %b) {
; CHECK-LABEL: ravg_i32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    avgrw $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %add = add i32 %a, 1
  %add1 = add i32 %add, %b
  %shr = ashr i32 %add1, 1
  ret i32 %shr
}

define i32 @avg_i32_ri(i32 %a) {
; CHECK-LABEL: avg_i32_ri:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    avgw $r0 = $r0, 0x13880
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %add = add nsw i32 %a, 80000
  %shr = ashr i32 %add, 1
  ret i32 %shr
}

define i32 @avg_i32_ri_n(i32 %a) {
; CHECK-LABEL: avg_i32_ri_n:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    avgw $r0 = $r0, 0xfffec77f
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %sub = add nsw i32 %a, -80001
  %shr = ashr i32 %sub, 1
  ret i32 %shr
}

define i32 @avg_u32_1(i32 %a, i32 %b) {
; CHECK-LABEL: avg_u32_1:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    avguw $r0 = $r1, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %add = add nuw nsw i32 %b, %a
  %shr = lshr i32 %add, 1
  ret i32 %shr
}

define i32 @avg_u32_2(i32 %a, i32 %b) {
; CHECK-LABEL: avg_u32_2:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    avguw $r0 = $r1, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %a1 = zext i32 %a to i64
  %b1 = zext i32 %b to i64
  %add = add i64 %b1, %a1
  %shr = lshr i64 %add, 1
  %r   = trunc i64 %shr to i32
  ret i32 %r
}

define i32 @not_avg_u32(i32 %a, i32 %b) {
; CHECK-LABEL: not_avg_u32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addw $r0 = $r1, $r0
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    srlw $r0 = $r0, 1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
entry:
  %add = add i32 %b, %a
  %shr = lshr i32 %add, 1
  ret i32 %shr
}

define i32 @not_avg_u32_ri(i32 %a) {
; CHECK-LABEL: not_avg_u32_ri:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addw $r0 = $r0, 444
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    srlw $r0 = $r0, 1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
entry:
  %add = add i32 %a, 444
  %shr = lshr i32 %add, 1
  ret i32 %shr
}

define i32 @not_ravg_u32(i32 %a, i32 %b) {
; CHECK-LABEL: not_ravg_u32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addw $r0 = $r0, $r1
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    addw $r0 = $r0, 1
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    srlw $r0 = $r0, 1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 2)
entry:
  %add = add i32 %a, 1
  %add1 = add i32 %add, %b
  %shr = lshr i32 %add1, 1
  ret i32 %shr
}

; FIXME: The nuw flags are removed before isel
; Perhaps they are still there durin dag-combine?
define i32 @ravg_u32(i32 %a, i32 %b) {
; CHECK-LABEL: ravg_u32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addw $r0 = $r0, $r1
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    addw $r0 = $r0, 1
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    srlw $r0 = $r0, 1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 2)
entry:
  %add = add nuw i32 %a, 1
  %add1 = add nuw i32 %add, %b
  %shr = lshr i32 %add1, 1
  ret i32 %shr
}

define i32 @ravg_u32_2(i32 %a, i32 %b) {
; CHECK-LABEL: ravg_u32_2:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    avgruw $r0 = $r1, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %a1 = zext i32 %a to i64
  %b1 = zext i32 %b to i64
  %add = add i64 %b1, %a1
  %add1 = add i64 %add, 1
  %shr = lshr i64 %add1, 1
  %r   = trunc i64 %shr to i32
  ret i32 %r
}

define <2 x i32> @avg_v2i32(<2 x i32> %a, <2 x i32> %b) {
; CHECK-LABEL: avg_v2i32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    avgwp $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %add = add <2 x i32> %b, %a
  %shr = ashr <2 x i32> %add, <i32 1, i32 1>
  ret <2 x i32> %shr
}

define <2 x i32> @avg_v2i32_ri_(<2 x i32> %a) {
; CHECK-LABEL: avg_v2i32_ri_:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    avgwp $r0 = $r0, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %add = add <2 x i32> %a, <i32 32, i32 0>
  %shr = ashr <2 x i32> %add, <i32 1, i32 1>
  ret <2 x i32> %shr
}

define <2 x i32> @avg_v2i32_ri_at(<2 x i32> %a) {
; CHECK-LABEL: avg_v2i32_ri_at:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    avgwp $r0 = $r0, -33.@
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %add = add <2 x i32> %a, <i32 -33, i32 -33>
  %shr = ashr <2 x i32> %add, <i32 1, i32 1>
  ret <2 x i32> %shr
}

define <2 x i32> @ravg_v2i32(<2 x i32> %a, <2 x i32> %b) {
; CHECK-LABEL: ravg_v2i32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    avgrwp $r0 = $r1, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %add = add <2 x i32> %a, <i32 1, i32 1>
  %add1 = add <2 x i32> %add, %b
  %shr = ashr <2 x i32> %add1, <i32 1, i32 1>
  ret <2 x i32> %shr
}

; We check if we can convert the immediate added vector (x, 1) into (x-1, 0).
define <2 x i32> @ravg_v2i32_ri_(<2 x i32> %a) {
; CHECK-LABEL: ravg_v2i32_ri_:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    avgrwp $r0 = $r0, -1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %add1 = add <2 x i32> %a, <i32  0, i32 1>
  %shr = ashr <2 x i32> %add1, <i32 1, i32 1>
  ret <2 x i32> %shr
}

; I don't think we'll ever be able to produce this,
; as the constant values are always merged. Might be
; able to find an limit corner case, where the
; immediate vector has (MAX(i32)+1).
; But I'm not sure the hardware would do the right
; thing anywhay.
define <2 x i32> @ravg_v2i32_ri_at(<2 x i32> %a) {
; CHECK-LABEL: ravg_v2i32_ri_at:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    avgwp $r0 = $r0, -32.@
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %add1 = add <2 x i32> %a, <i32 -32, i32 -32>
  %shr = ashr <2 x i32> %add1, <i32 1, i32 1>
  ret <2 x i32> %shr
}

define <2 x i32> @avg_v2u32(<2 x i32> %a, <2 x i32> %b) {
; CHECK-LABEL: avg_v2u32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    avguwp $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %add = add nuw <2 x i32> %b, %a
  %shr = lshr <2 x i32> %add, <i32 1, i32 1>
  ret <2 x i32> %shr
}

define <2 x i32> @not_avg_v2u32(<2 x i32> %a, <2 x i32> %b) {
; CHECK-LABEL: not_avg_v2u32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addwp $r0 = $r1, $r0
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    srlwps $r0 = $r0, 1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
entry:
  %add = add <2 x i32> %b, %a
  %shr = lshr <2 x i32> %add, <i32 1, i32 1>
  ret <2 x i32> %shr
}

define <2 x i32> @avg_v2u32_ri_(<2 x i32> %a) {
; CHECK-LABEL: avg_v2u32_ri_:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    avguwp $r0 = $r0, 3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %add = add nuw <2 x i32> %a, <i32 3, i32 0>
  %shr = lshr <2 x i32> %add, <i32 1, i32 1>
  ret <2 x i32> %shr
}

define <2 x i32> @not_avg_v2u32_ri_(<2 x i32> %a) {
; CHECK-LABEL: not_avg_v2u32_ri_:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addwp $r0 = $r0, 3
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    srlwps $r0 = $r0, 1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
entry:
  %add = add <2 x i32> %a, <i32 3, i32 0>
  %shr = lshr <2 x i32> %add, <i32 1, i32 1>
  ret <2 x i32> %shr
}

define <2 x i32> @avg_v2u32_ri_at(<2 x i32> %a) {
; CHECK-LABEL: avg_v2u32_ri_at:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    avguwp $r0 = $r0, 3.@
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %add = add nuw <2 x i32> %a, <i32 3, i32 3>
  %shr = lshr <2 x i32> %add, <i32 1, i32 1>
  ret <2 x i32> %shr
}

define <2 x i32> @not_avg_v2u32_ri_at(<2 x i32> %a) {
; CHECK-LABEL: not_avg_v2u32_ri_at:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addwp $r0 = $r0, 3.@
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    srlwps $r0 = $r0, 1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
entry:
  %add = add <2 x i32> %a, <i32 3, i32 3>
  %shr = lshr <2 x i32> %add, <i32 1, i32 1>
  ret <2 x i32> %shr
}

define <2 x i32> @avg_v2u32_rr(<2 x i32> %a) {
; CHECK-LABEL: avg_v2u32_rr:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    make $r1 = 0x300000000
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    avguwp $r0 = $r1, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
entry:
  %add = add nuw <2 x i32> %a, <i32 0, i32 3>
  %shr = lshr <2 x i32> %add, <i32 1, i32 1>
  ret <2 x i32> %shr
}

define <2 x i32> @not_avg_v2u32_rr(<2 x i32> %a) {
; CHECK-LABEL: not_avg_v2u32_rr:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    make $r1 = 0x300000000
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    addwp $r0 = $r0, $r1
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    srlwps $r0 = $r0, 1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 2)
entry:
  %add = add <2 x i32> %a, <i32 0, i32 3>
  %shr = lshr <2 x i32> %add, <i32 1, i32 1>
  ret <2 x i32> %shr
}

; Here the nuw also gets lost.
define <2 x i32> @ravg_v2u32(<2 x i32> %a, <2 x i32> %b) {
; CHECK-LABEL: ravg_v2u32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addwp $r0 = $r0, $r1
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    addwp $r0 = $r0, 1.@
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    srlwps $r0 = $r0, 1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 2)
entry:
  %add = add nuw <2 x i32> %a, <i32 1, i32 1>
  %add1 = add nuw <2 x i32> %add, %b
  %shr = lshr <2 x i32> %add1, <i32 1, i32 1>
  ret <2 x i32> %shr
}

define <2 x i32> @not_ravg_v2u32(<2 x i32> %a, <2 x i32> %b) {
; CHECK-LABEL: not_ravg_v2u32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addwp $r0 = $r0, $r1
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    addwp $r0 = $r0, 1.@
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    srlwps $r0 = $r0, 1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 2)
entry:
  %add = add <2 x i32> %a, <i32 1, i32 1>
  %add1 = add <2 x i32> %add, %b
  %shr = lshr <2 x i32> %add1, <i32 1, i32 1>
  ret <2 x i32> %shr
}

define <2 x i32> @ravg_v2u32_ri_(<2 x i32> %a) {
; CHECK-LABEL: ravg_v2u32_ri_:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    avgruwp $r0 = $r0, 3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %add1 = add nuw <2 x i32> %a, <i32 4, i32 1>
  %shr = lshr <2 x i32> %add1, <i32 1, i32 1>
  ret <2 x i32> %shr
}

define <2 x i32> @not_ravg_v2u32_ri_(<2 x i32> %a) {
; CHECK-LABEL: not_ravg_v2u32_ri_:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    make $r1 = 0x100000004
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    addwp $r0 = $r0, $r1
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    srlwps $r0 = $r0, 1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 2)
entry:
  %add1 = add <2 x i32> %a, <i32 4, i32 1>
  %shr = lshr <2 x i32> %add1, <i32 1, i32 1>
  ret <2 x i32> %shr
}

; Can't generate this one, as immediates merge as well.
define <2 x i32> @ravg_v2u32_ri_at(<2 x i32> %a) {
; CHECK-LABEL: ravg_v2u32_ri_at:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    avguwp $r0 = $r0, 4.@
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %add1 = add nuw <2 x i32> %a, <i32 4, i32 4>
  %shr = lshr <2 x i32> %add1, <i32 1, i32 1>
  ret <2 x i32> %shr
}

define <2 x i16> @avg_v2i16(<2 x i16> %a, <2 x i16> %b) {
; CHECK-LABEL: avg_v2i16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    avghq $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %add = add <2 x i16> %b, %a
  %shr = ashr <2 x i16> %add, <i16 1, i16 1>
  ret <2 x i16> %shr
}

define <2 x i16> @avg_v2i16_ri(<2 x i16> %a) {
; CHECK-LABEL: avg_v2i16_ri:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    avghq $r0 = $r0, 0x30007
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %add = add <2 x i16> %a, <i16 7, i16 3>
  %shr = ashr <2 x i16> %add, <i16 1, i16 1>
  ret <2 x i16> %shr
}

define <2 x i16> @ravg_v2i16(<2 x i16> %a, <2 x i16> %b) {
; CHECK-LABEL: ravg_v2i16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    avgrhq $r0 = $r1, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %add = add <2 x i16> %a, <i16 1, i16 1>
  %add1 = add <2 x i16> %add, %b
  %shr = ashr <2 x i16> %add1, <i16 1, i16 1>
  ret <2 x i16> %shr
}

define <2 x i16> @ravg_v2i16_ri(<2 x i16> %a) {
; CHECK-LABEL: ravg_v2i16_ri:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    avghq $r0 = $r0, 0x818000
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %add1 = add <2 x i16> %a, <i16 -32768, i16 129>
  %shr = ashr <2 x i16> %add1, <i16 1, i16 1>
  ret <2 x i16> %shr
}

define <2 x i16> @avg_v2u16(<2 x i16> %a, <2 x i16> %b) {
; CHECK-LABEL: avg_v2u16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    avguhq $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %add = add nuw <2 x i16> %b, %a
  %shr = lshr <2 x i16> %add, <i16 1, i16 1>
  ret <2 x i16> %shr
}

define <2 x i16> @not_avg_v2u16(<2 x i16> %a, <2 x i16> %b) {
; CHECK-LABEL: not_avg_v2u16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addhq $r0 = $r1, $r0
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    srlhqs $r0 = $r0, 1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
entry:
  %add = add <2 x i16> %b, %a
  %shr = lshr <2 x i16> %add, <i16 1, i16 1>
  ret <2 x i16> %shr
}

define <2 x i16> @avg_v2u16_ri(<2 x i16> %a) {
; CHECK-LABEL: avg_v2u16_ri:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    avguhq $r0 = $r0, 0x80ffff
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %add = add nuw <2 x i16> %a, <i16 -1, i16 128>
  %shr = lshr <2 x i16> %add, <i16 1, i16 1>
  ret <2 x i16> %shr
}
define <2 x i16> @not_avg_v2u16_ri(<2 x i16> %a) {
; CHECK-LABEL: not_avg_v2u16_ri:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addhq $r0 = $r0, 0x80ffff
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    srlhqs $r0 = $r0, 1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
entry:
  %add = add <2 x i16> %a, <i16 -1, i16 128>
  %shr = lshr <2 x i16> %add, <i16 1, i16 1>
  ret <2 x i16> %shr
}

; nuw gets lost here too.
define <2 x i16> @ravg_v2u16(<2 x i16> %a, <2 x i16> %b) {
; CHECK-LABEL: ravg_v2u16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addhq $r0 = $r0, $r1
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    addhq $r0 = $r0, 0x10001
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    srlhqs $r0 = $r0, 1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 2)
entry:
  %add = add nuw <2 x i16> %a, <i16 1, i16 1>
  %add1 = add nuw <2 x i16> %add, %b
  %shr = lshr <2 x i16> %add1, <i16 1, i16 1>
  ret <2 x i16> %shr
}

define <2 x i16> @not_ravg_v2u16(<2 x i16> %a, <2 x i16> %b) {
; CHECK-LABEL: not_ravg_v2u16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addhq $r0 = $r0, $r1
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    addhq $r0 = $r0, 0x10001
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    srlhqs $r0 = $r0, 1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 2)
entry:
  %add = add <2 x i16> %a, <i16 1, i16 1>
  %add1 = add <2 x i16> %add, %b
  %shr = lshr <2 x i16> %add1, <i16 1, i16 1>
  ret <2 x i16> %shr
}

define <2 x i16> @ravg_v2u16_ri(<2 x i16> %a) {
; CHECK-LABEL: ravg_v2u16_ri:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    avguhq $r0 = $r0, 0x810000
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %add1 = add nuw <2 x i16> %a, <i16 0, i16 129>
  %shr = lshr <2 x i16> %add1, <i16 1, i16 1>
  ret <2 x i16> %shr
}

define <4 x i16> @avg_v4i16(<4 x i16> %a, <4 x i16> %b) {
; CHECK-LABEL: avg_v4i16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    avghq $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %add = add <4 x i16> %b, %a
  %shr = ashr <4 x i16> %add, <i16 1, i16 1, i16 1, i16 1>
  ret <4 x i16> %shr
}

define <4 x i16> @avg_v4i16_ri_(<4 x i16> %a) {
; CHECK-LABEL: avg_v4i16_ri_:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    avghq $r0 = $r0, 0x807fff
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %add = add <4 x i16> %a, <i16 32767, i16 128, i16 0, i16 0>
  %shr = ashr <4 x i16> %add, <i16 1, i16 1, i16 1, i16 1>
  ret <4 x i16> %shr
}

define <4 x i16> @avg_v4i16_ri_at(<4 x i16> %a) {
; CHECK-LABEL: avg_v4i16_ri_at:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    avghq $r0 = $r0, 0x807fff.@
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %add = add <4 x i16> %a, <i16 32767, i16 128, i16 32767, i16 128>
  %shr = ashr <4 x i16> %add, <i16 1, i16 1, i16 1, i16 1>
  ret <4 x i16> %shr
}

define <4 x i16> @ravg_v4i16(<4 x i16> %a, <4 x i16> %b) {
; CHECK-LABEL: ravg_v4i16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    avgrhq $r0 = $r1, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %add = add <4 x i16> %a, <i16 1, i16 1, i16 1, i16 1>
  %add1 = add <4 x i16> %add, %b
  %shr = ashr <4 x i16> %add1, <i16 1, i16 1, i16 1, i16 1>
  ret <4 x i16> %shr
}

define <4 x i16> @ravg_v4i16_ri_at(<4 x i16> %a) {
; CHECK-LABEL: ravg_v4i16_ri_at:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    avghq $r0 = $r0, 0x818000.@
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %add1 = add <4 x i16> %a, <i16 -32768, i16 129, i16 -32768, i16 129>
  %shr = ashr <4 x i16> %add1, <i16 1, i16 1, i16 1, i16 1>
  ret <4 x i16> %shr
}

define <4 x i16> @avg_v4u16(<4 x i16> %a, <4 x i16> %b) {
; CHECK-LABEL: avg_v4u16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    avguhq $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %add = add nuw <4 x i16> %b, %a
  %shr = lshr <4 x i16> %add, <i16 1, i16 1, i16 1, i16 1>
  ret <4 x i16> %shr
}

define <4 x i16> @avg_v4u16_ri_(<4 x i16> %a) {
; CHECK-LABEL: avg_v4u16_ri_:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    avguhq $r0 = $r0, 0x80ffff
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %add = add nuw <4 x i16> %a, <i16 -1, i16 128, i16 0, i16 0>
  %shr = lshr <4 x i16> %add, <i16 1, i16 1, i16 1, i16 1>
  ret <4 x i16> %shr
}

define <4 x i16> @avg_v4u16_ri_at(<4 x i16> %a) {
; CHECK-LABEL: avg_v4u16_ri_at:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    avguhq $r0 = $r0, 0x80ffff.@
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %add = add nuw <4 x i16> %a, <i16 -1, i16 128, i16 -1, i16 128>
  %shr = lshr <4 x i16> %add, <i16 1, i16 1, i16 1, i16 1>
  ret <4 x i16> %shr
}

define <4 x i16> @ravg_v4u16(<4 x i16> %a, <4 x i16> %b) {
; CHECK-LABEL: ravg_v4u16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addhq $r0 = $r0, $r1
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    addhq $r0 = $r0, 0x10001.@
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    srlhqs $r0 = $r0, 1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 2)
entry:
  %add = add nuw <4 x i16> %a, <i16 1, i16 1, i16 1, i16 1>
  %add1 = add nuw <4 x i16> %add, %b
  %shr = lshr <4 x i16> %add1, <i16 1, i16 1, i16 1, i16 1>
  ret <4 x i16> %shr
}

; TODO: Create something to subtract (1,1,1,1) from the immediate value.
define <4 x i16> @ravg_v4u16_ri_(<4 x i16> %a) {
; CHECK-LABEL: ravg_v4u16_ri_:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    make $r1 = 0x1000100000081
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    avguhq $r0 = $r1, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
entry:
  %add1 = add nuw <4 x i16> %a, <i16 129, i16 0, i16 1, i16 1>
  %shr = lshr <4 x i16> %add1, <i16 1, i16 1, i16 1, i16 1>
  ret <4 x i16> %shr
}

define <4 x i16> @ravg_v4u16_ri_at(<4 x i16> %a) {
; CHECK-LABEL: ravg_v4u16_ri_at:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    avguhq $r0 = $r0, 0x810000.@
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
entry:
  %add1 = add nuw <4 x i16> %a, <i16 0, i16 129, i16 0, i16 129>
  %shr = lshr <4 x i16> %add1, <i16 1, i16 1, i16 1, i16 1>
  ret <4 x i16> %shr
}

; Check border: -2147483648 is INT_MIN
define <2 x i32> @ravg_v2i32_ri_NOT(<2 x i32> %a) {
; CHECK-LABEL: ravg_v2i32_ri_NOT:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    make $r1 = 0x180000000
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    avgwp $r0 = $r1, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
entry:
  %add1 = add <2 x i32> %a, <i32  -2147483648, i32 1>
  %shr = ashr <2 x i32> %add1, <i32 1, i32 1>
  ret <2 x i32> %shr
}

define <2 x i32> @ravg_v2i32_ri_NOT2(<2 x i32> %a) {
; CHECK-LABEL: ravg_v2i32_ri_NOT2:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    make $r1 = 0x200000001
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    avgwp $r0 = $r1, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
entry:
  %add1 = add <2 x i32> %a, <i32  1, i32 2>
  %shr = ashr <2 x i32> %add1, <i32 1, i32 1>
  ret <2 x i32> %shr
}

define i16 @avg_i16(i16 %a, i16 %b) {
; CHECK-LABEL: avg_i16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    sxhd $r0 = $r0
; CHECK-NEXT:    sxhd $r1 = $r1
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    avgw $r0 = $r1, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
entry:
  %conv = sext i16 %a to i32
  %conv1 = sext i16 %b to i32
  %add = add nsw i32 %conv1, %conv
  %0 = lshr i32 %add, 1
  %conv2 = trunc i32 %0 to i16
  ret i16 %conv2
}

; Here the nsw are lost
define i16 @ravg_i16(i16 %a, i16 %b) {
; CHECK-LABEL: ravg_i16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    sxhd $r0 = $r0
; CHECK-NEXT:    sxhd $r1 = $r1
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    addw $r0 = $r0, $r1
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    addw $r0 = $r0, 1
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    srlw $r0 = $r0, 1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
entry:
  %conv = sext i16 %a to i32
  %conv1 = sext i16 %b to i32
  %add = add nsw i32 %conv, 1
  %add2 = add nsw i32 %add, %conv1
  %0 = lshr i32 %add2, 1
  %conv3 = trunc i32 %0 to i16
  ret i16 %conv3
}

define i16 @avg_i16_ri(i16 %a) {
; CHECK-LABEL: avg_i16_ri:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    sxhd $r0 = $r0
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    avgw $r0 = $r0, 7
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
entry:
  %conv = sext i16 %a to i32
  %add = add nsw i32 %conv, 7
  %0 = lshr i32 %add, 1
  %conv1 = trunc i32 %0 to i16
  ret i16 %conv1
}

define i16 @avg_i16_ri_n(i16 %a) {
; CHECK-LABEL: avg_i16_ri_n:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    sxhd $r0 = $r0
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    avgw $r0 = $r0, 0x1fff9
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
entry:
  %conv = sext i16 %a to i32
  %sub = add nsw i32 %conv, 131065
  %0 = lshr i32 %sub, 1
  %conv1 = trunc i32 %0 to i16
  ret i16 %conv1
}

define i16 @avg_u16(i16 %a, i16 %b) {
; CHECK-LABEL: avg_u16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    zxhd $r0 = $r0
; CHECK-NEXT:    zxhd $r1 = $r1
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    avguw $r0 = $r1, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
entry:
  %conv = zext i16 %a to i32
  %conv1 = zext i16 %b to i32
  %add = add nuw nsw i32 %conv1, %conv
  %0 = lshr i32 %add, 1
  %conv2 = trunc i32 %0 to i16
  ret i16 %conv2
}

; Here the nuw are lost
define i16 @ravg_u16(i16 %a, i16 %b) {
; CHECK-LABEL: ravg_u16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    zxhd $r0 = $r0
; CHECK-NEXT:    zxhd $r1 = $r1
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    addw $r0 = $r0, $r1
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    addw $r0 = $r0, 1
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    srlw $r0 = $r0, 1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
entry:
  %conv = zext i16 %a to i32
  %conv1 = zext i16 %b to i32
  %add = add nuw nsw i32 %conv, 1
  %add2 = add nuw nsw i32 %add, %conv1
  %0 = lshr i32 %add2, 1
  %conv3 = trunc i32 %0 to i16
  ret i16 %conv3
}

define i16 @avg_u16_ri(i16 %a) {
; CHECK-LABEL: avg_u16_ri:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    zxhd $r0 = $r0
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    avguw $r0 = $r0, 140
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
entry:
  %conv = zext i16 %a to i32
  %add = add nuw nsw i32 %conv, 140
  %0 = lshr i32 %add, 1
  %conv1 = trunc i32 %0 to i16
  ret i16 %conv1
}

define <2 x i8> @avg_v2i8(<2 x i8> %a, <2 x i8> %b) {
; KV3_1-LABEL: avg_v2i8:
; KV3_1:       # %bb.0: # %entry
; KV3_1-NEXT:    sxlbhq $r0 = $r0
; KV3_1-NEXT:    sxlbhq $r1 = $r1
; KV3_1-NEXT:    ;; # (end cycle 0)
; KV3_1-NEXT:    addhq $r0 = $r1, $r0
; KV3_1-NEXT:    ;; # (end cycle 1)
; KV3_1-NEXT:    sbmm8 $r0 = $r0, 0x401
; KV3_1-NEXT:    ;; # (end cycle 2)
; KV3_1-NEXT:    zxbd $r0 = $r0
; KV3_1-NEXT:    extfz $r1 = $r0, 15, 8
; KV3_1-NEXT:    ;; # (end cycle 3)
; KV3_1-NEXT:    sxbd $r0 = $r0
; KV3_1-NEXT:    sxbd $r1 = $r1
; KV3_1-NEXT:    ;; # (end cycle 4)
; KV3_1-NEXT:    sraw $r0 = $r0, 1
; KV3_1-NEXT:    sraw $r1 = $r1, 1
; KV3_1-NEXT:    ;; # (end cycle 5)
; KV3_1-NEXT:    insf $r0 = $r1, 15, 8
; KV3_1-NEXT:    ret
; KV3_1-NEXT:    ;; # (end cycle 6)
;
; KV3_2-LABEL: avg_v2i8:
; KV3_2:       # %bb.0: # %entry
; KV3_2-NEXT:    avgbo $r0 = $r0, $r1
; KV3_2-NEXT:    ret
; KV3_2-NEXT:    ;; # (end cycle 0)
entry:
  %add = add <2 x i8> %b, %a
  %shr = ashr <2 x i8> %add, <i8 1, i8 1>
  ret <2 x i8> %shr
}

define <2 x i8> @avg_v2i8_ri(<2 x i8> %a) {
; KV3_1-LABEL: avg_v2i8_ri:
; KV3_1:       # %bb.0: # %entry
; KV3_1-NEXT:    sxlbhq $r0 = $r0
; KV3_1-NEXT:    ;; # (end cycle 0)
; KV3_1-NEXT:    addhq $r0 = $r0, 0x30007
; KV3_1-NEXT:    ;; # (end cycle 1)
; KV3_1-NEXT:    sbmm8 $r0 = $r0, 0x401
; KV3_1-NEXT:    ;; # (end cycle 2)
; KV3_1-NEXT:    zxbd $r0 = $r0
; KV3_1-NEXT:    extfz $r1 = $r0, 15, 8
; KV3_1-NEXT:    ;; # (end cycle 3)
; KV3_1-NEXT:    sxbd $r0 = $r0
; KV3_1-NEXT:    sxbd $r1 = $r1
; KV3_1-NEXT:    ;; # (end cycle 4)
; KV3_1-NEXT:    sraw $r0 = $r0, 1
; KV3_1-NEXT:    sraw $r1 = $r1, 1
; KV3_1-NEXT:    ;; # (end cycle 5)
; KV3_1-NEXT:    insf $r0 = $r1, 15, 8
; KV3_1-NEXT:    ret
; KV3_1-NEXT:    ;; # (end cycle 6)
;
; KV3_2-LABEL: avg_v2i8_ri:
; KV3_2:       # %bb.0: # %entry
; KV3_2-NEXT:    avgbo $r0 = $r0, 775
; KV3_2-NEXT:    ret
; KV3_2-NEXT:    ;; # (end cycle 0)
entry:
  %add = add <2 x i8> %a, <i8 7, i8 3>
  %shr = ashr <2 x i8> %add, <i8 1, i8 1>
  ret <2 x i8> %shr
}

define <2 x i8> @ravg_v2i8(<2 x i8> %a, <2 x i8> %b) {
; KV3_1-LABEL: ravg_v2i8:
; KV3_1:       # %bb.0: # %entry
; KV3_1-NEXT:    sxlbhq $r0 = $r0
; KV3_1-NEXT:    sxlbhq $r1 = $r1
; KV3_1-NEXT:    ;; # (end cycle 0)
; KV3_1-NEXT:    addhq $r0 = $r0, $r1
; KV3_1-NEXT:    ;; # (end cycle 1)
; KV3_1-NEXT:    addhq $r0 = $r0, 0x10001
; KV3_1-NEXT:    ;; # (end cycle 2)
; KV3_1-NEXT:    sbmm8 $r0 = $r0, 0x401
; KV3_1-NEXT:    ;; # (end cycle 3)
; KV3_1-NEXT:    zxbd $r0 = $r0
; KV3_1-NEXT:    extfz $r1 = $r0, 15, 8
; KV3_1-NEXT:    ;; # (end cycle 4)
; KV3_1-NEXT:    sxbd $r0 = $r0
; KV3_1-NEXT:    sxbd $r1 = $r1
; KV3_1-NEXT:    ;; # (end cycle 5)
; KV3_1-NEXT:    sraw $r0 = $r0, 1
; KV3_1-NEXT:    sraw $r1 = $r1, 1
; KV3_1-NEXT:    ;; # (end cycle 6)
; KV3_1-NEXT:    insf $r0 = $r1, 15, 8
; KV3_1-NEXT:    ret
; KV3_1-NEXT:    ;; # (end cycle 7)
;
; KV3_2-LABEL: ravg_v2i8:
; KV3_2:       # %bb.0: # %entry
; KV3_2-NEXT:    avgrbo $r0 = $r1, $r0
; KV3_2-NEXT:    ret
; KV3_2-NEXT:    ;; # (end cycle 0)
entry:
  %add = add <2 x i8> %a, <i8 1, i8 1>
  %add1 = add <2 x i8> %add, %b
  %shr = ashr <2 x i8> %add1, <i8 1, i8 1>
  ret <2 x i8> %shr
}

define <2 x i8> @ravg_v2i8_ri(<2 x i8> %a) {
; KV3_1-LABEL: ravg_v2i8_ri:
; KV3_1:       # %bb.0: # %entry
; KV3_1-NEXT:    sxlbhq $r0 = $r0
; KV3_1-NEXT:    ;; # (end cycle 0)
; KV3_1-NEXT:    addhq $r0 = $r0, 0x810080
; KV3_1-NEXT:    ;; # (end cycle 1)
; KV3_1-NEXT:    sbmm8 $r0 = $r0, 0x401
; KV3_1-NEXT:    ;; # (end cycle 2)
; KV3_1-NEXT:    zxbd $r0 = $r0
; KV3_1-NEXT:    extfz $r1 = $r0, 15, 8
; KV3_1-NEXT:    ;; # (end cycle 3)
; KV3_1-NEXT:    sxbd $r0 = $r0
; KV3_1-NEXT:    sxbd $r1 = $r1
; KV3_1-NEXT:    ;; # (end cycle 4)
; KV3_1-NEXT:    sraw $r0 = $r0, 1
; KV3_1-NEXT:    sraw $r1 = $r1, 1
; KV3_1-NEXT:    ;; # (end cycle 5)
; KV3_1-NEXT:    insf $r0 = $r1, 15, 8
; KV3_1-NEXT:    ret
; KV3_1-NEXT:    ;; # (end cycle 6)
;
; KV3_2-LABEL: ravg_v2i8_ri:
; KV3_2:       # %bb.0: # %entry
; KV3_2-NEXT:    avgbo $r0 = $r0, 0xffff8180
; KV3_2-NEXT:    ret
; KV3_2-NEXT:    ;; # (end cycle 0)
entry:
  %add1 = add <2 x i8> %a, <i8 128, i8 -127>
  %shr = ashr <2 x i8> %add1, <i8 1, i8 1>
  ret <2 x i8> %shr
}

define <2 x i8> @avg_v2u8(<2 x i8> %a, <2 x i8> %b) {
; KV3_1-LABEL: avg_v2u8:
; KV3_1:       # %bb.0: # %entry
; KV3_1-NEXT:    sxlbhq $r0 = $r0
; KV3_1-NEXT:    sxlbhq $r1 = $r1
; KV3_1-NEXT:    ;; # (end cycle 0)
; KV3_1-NEXT:    addhq $r0 = $r1, $r0
; KV3_1-NEXT:    ;; # (end cycle 1)
; KV3_1-NEXT:    sbmm8 $r0 = $r0, 0x401
; KV3_1-NEXT:    ;; # (end cycle 2)
; KV3_1-NEXT:    zxbd $r0 = $r0
; KV3_1-NEXT:    extfz $r1 = $r0, 15, 8
; KV3_1-NEXT:    ;; # (end cycle 3)
; KV3_1-NEXT:    andw $r0 = $r0, 254
; KV3_1-NEXT:    andw $r1 = $r1, 254
; KV3_1-NEXT:    ;; # (end cycle 4)
; KV3_1-NEXT:    srlw $r0 = $r0, 1
; KV3_1-NEXT:    srlw $r1 = $r1, 1
; KV3_1-NEXT:    ;; # (end cycle 5)
; KV3_1-NEXT:    insf $r0 = $r1, 15, 8
; KV3_1-NEXT:    ret
; KV3_1-NEXT:    ;; # (end cycle 6)
;
; KV3_2-LABEL: avg_v2u8:
; KV3_2:       # %bb.0: # %entry
; KV3_2-NEXT:    avgubo $r0 = $r0, $r1
; KV3_2-NEXT:    ret
; KV3_2-NEXT:    ;; # (end cycle 0)
entry:
  %add = add nuw <2 x i8> %b, %a
  %shr = lshr <2 x i8> %add, <i8 1, i8 1>
  ret <2 x i8> %shr
}

define <2 x i8> @not_avg_v2u8(<2 x i8> %a, <2 x i8> %b) {
; KV3_1-LABEL: not_avg_v2u8:
; KV3_1:       # %bb.0: # %entry
; KV3_1-NEXT:    sxlbhq $r0 = $r0
; KV3_1-NEXT:    sxlbhq $r1 = $r1
; KV3_1-NEXT:    ;; # (end cycle 0)
; KV3_1-NEXT:    addhq $r0 = $r1, $r0
; KV3_1-NEXT:    ;; # (end cycle 1)
; KV3_1-NEXT:    sbmm8 $r0 = $r0, 0x401
; KV3_1-NEXT:    ;; # (end cycle 2)
; KV3_1-NEXT:    zxbd $r0 = $r0
; KV3_1-NEXT:    extfz $r1 = $r0, 15, 8
; KV3_1-NEXT:    ;; # (end cycle 3)
; KV3_1-NEXT:    andw $r0 = $r0, 254
; KV3_1-NEXT:    andw $r1 = $r1, 254
; KV3_1-NEXT:    ;; # (end cycle 4)
; KV3_1-NEXT:    srlw $r0 = $r0, 1
; KV3_1-NEXT:    srlw $r1 = $r1, 1
; KV3_1-NEXT:    ;; # (end cycle 5)
; KV3_1-NEXT:    insf $r0 = $r1, 15, 8
; KV3_1-NEXT:    ret
; KV3_1-NEXT:    ;; # (end cycle 6)
;
; KV3_2-LABEL: not_avg_v2u8:
; KV3_2:       # %bb.0: # %entry
; KV3_2-NEXT:    addbo $r0 = $r1, $r0
; KV3_2-NEXT:    ;; # (end cycle 0)
; KV3_2-NEXT:    srlbos $r0 = $r0, 1
; KV3_2-NEXT:    ret
; KV3_2-NEXT:    ;; # (end cycle 1)
entry:
  %add = add <2 x i8> %b, %a
  %shr = lshr <2 x i8> %add, <i8 1, i8 1>
  ret <2 x i8> %shr
}

define <2 x i8> @avg_v2u8_ri(<2 x i8> %a) {
; KV3_1-LABEL: avg_v2u8_ri:
; KV3_1:       # %bb.0: # %entry
; KV3_1-NEXT:    sxlbhq $r0 = $r0
; KV3_1-NEXT:    ;; # (end cycle 0)
; KV3_1-NEXT:    addhq $r0 = $r0, 0x8000ff
; KV3_1-NEXT:    ;; # (end cycle 1)
; KV3_1-NEXT:    sbmm8 $r0 = $r0, 0x401
; KV3_1-NEXT:    ;; # (end cycle 2)
; KV3_1-NEXT:    zxbd $r0 = $r0
; KV3_1-NEXT:    extfz $r1 = $r0, 15, 8
; KV3_1-NEXT:    ;; # (end cycle 3)
; KV3_1-NEXT:    andw $r0 = $r0, 254
; KV3_1-NEXT:    andw $r1 = $r1, 254
; KV3_1-NEXT:    ;; # (end cycle 4)
; KV3_1-NEXT:    srlw $r0 = $r0, 1
; KV3_1-NEXT:    srlw $r1 = $r1, 1
; KV3_1-NEXT:    ;; # (end cycle 5)
; KV3_1-NEXT:    insf $r0 = $r1, 15, 8
; KV3_1-NEXT:    ret
; KV3_1-NEXT:    ;; # (end cycle 6)
;
; KV3_2-LABEL: avg_v2u8_ri:
; KV3_2:       # %bb.0: # %entry
; KV3_2-NEXT:    avgubo $r0 = $r0, 0xffff80ff
; KV3_2-NEXT:    ret
; KV3_2-NEXT:    ;; # (end cycle 0)
entry:
  %add = add nuw <2 x i8> %a, <i8 -1, i8 128>
  %shr = lshr <2 x i8> %add, <i8 1, i8 1>
  ret <2 x i8> %shr
}
define <2 x i8> @not_avg_v2u8_ri(<2 x i8> %a) {
; KV3_1-LABEL: not_avg_v2u8_ri:
; KV3_1:       # %bb.0: # %entry
; KV3_1-NEXT:    sxlbhq $r0 = $r0
; KV3_1-NEXT:    ;; # (end cycle 0)
; KV3_1-NEXT:    addhq $r0 = $r0, 0x8000ff
; KV3_1-NEXT:    ;; # (end cycle 1)
; KV3_1-NEXT:    sbmm8 $r0 = $r0, 0x401
; KV3_1-NEXT:    ;; # (end cycle 2)
; KV3_1-NEXT:    zxbd $r0 = $r0
; KV3_1-NEXT:    extfz $r1 = $r0, 15, 8
; KV3_1-NEXT:    ;; # (end cycle 3)
; KV3_1-NEXT:    andw $r0 = $r0, 254
; KV3_1-NEXT:    andw $r1 = $r1, 254
; KV3_1-NEXT:    ;; # (end cycle 4)
; KV3_1-NEXT:    srlw $r0 = $r0, 1
; KV3_1-NEXT:    srlw $r1 = $r1, 1
; KV3_1-NEXT:    ;; # (end cycle 5)
; KV3_1-NEXT:    insf $r0 = $r1, 15, 8
; KV3_1-NEXT:    ret
; KV3_1-NEXT:    ;; # (end cycle 6)
;
; KV3_2-LABEL: not_avg_v2u8_ri:
; KV3_2:       # %bb.0: # %entry
; KV3_2-NEXT:    addbo $r0 = $r0, 0xffff80ff
; KV3_2-NEXT:    ;; # (end cycle 0)
; KV3_2-NEXT:    srlbos $r0 = $r0, 1
; KV3_2-NEXT:    ret
; KV3_2-NEXT:    ;; # (end cycle 1)
entry:
  %add = add <2 x i8> %a, <i8 -1, i8 128>
  %shr = lshr <2 x i8> %add, <i8 1, i8 1>
  ret <2 x i8> %shr
}

; nuw gets lost here too.
define <2 x i8> @ravg_v2u8(<2 x i8> %a, <2 x i8> %b) {
; KV3_1-LABEL: ravg_v2u8:
; KV3_1:       # %bb.0: # %entry
; KV3_1-NEXT:    sxlbhq $r0 = $r0
; KV3_1-NEXT:    sxlbhq $r1 = $r1
; KV3_1-NEXT:    ;; # (end cycle 0)
; KV3_1-NEXT:    addhq $r0 = $r0, $r1
; KV3_1-NEXT:    ;; # (end cycle 1)
; KV3_1-NEXT:    addhq $r0 = $r0, 0x10001
; KV3_1-NEXT:    ;; # (end cycle 2)
; KV3_1-NEXT:    sbmm8 $r0 = $r0, 0x401
; KV3_1-NEXT:    ;; # (end cycle 3)
; KV3_1-NEXT:    zxbd $r0 = $r0
; KV3_1-NEXT:    extfz $r1 = $r0, 15, 8
; KV3_1-NEXT:    ;; # (end cycle 4)
; KV3_1-NEXT:    andw $r0 = $r0, 254
; KV3_1-NEXT:    andw $r1 = $r1, 254
; KV3_1-NEXT:    ;; # (end cycle 5)
; KV3_1-NEXT:    srlw $r0 = $r0, 1
; KV3_1-NEXT:    srlw $r1 = $r1, 1
; KV3_1-NEXT:    ;; # (end cycle 6)
; KV3_1-NEXT:    insf $r0 = $r1, 15, 8
; KV3_1-NEXT:    ret
; KV3_1-NEXT:    ;; # (end cycle 7)
;
; KV3_2-LABEL: ravg_v2u8:
; KV3_2:       # %bb.0: # %entry
; KV3_2-NEXT:    addbo $r0 = $r0, $r1
; KV3_2-NEXT:    ;; # (end cycle 0)
; KV3_2-NEXT:    addbo $r0 = $r0, 257
; KV3_2-NEXT:    ;; # (end cycle 1)
; KV3_2-NEXT:    srlbos $r0 = $r0, 1
; KV3_2-NEXT:    ret
; KV3_2-NEXT:    ;; # (end cycle 2)
entry:
  %add = add nuw <2 x i8> %a, <i8 1, i8 1>
  %add1 = add nuw <2 x i8> %add, %b
  %shr = lshr <2 x i8> %add1, <i8 1, i8 1>
  ret <2 x i8> %shr
}

define <2 x i8> @not_ravg_v2u8(<2 x i8> %a, <2 x i8> %b) {
; KV3_1-LABEL: not_ravg_v2u8:
; KV3_1:       # %bb.0: # %entry
; KV3_1-NEXT:    sxlbhq $r0 = $r0
; KV3_1-NEXT:    sxlbhq $r1 = $r1
; KV3_1-NEXT:    ;; # (end cycle 0)
; KV3_1-NEXT:    addhq $r0 = $r0, $r1
; KV3_1-NEXT:    ;; # (end cycle 1)
; KV3_1-NEXT:    addhq $r0 = $r0, 0x10001
; KV3_1-NEXT:    ;; # (end cycle 2)
; KV3_1-NEXT:    sbmm8 $r0 = $r0, 0x401
; KV3_1-NEXT:    ;; # (end cycle 3)
; KV3_1-NEXT:    zxbd $r0 = $r0
; KV3_1-NEXT:    extfz $r1 = $r0, 15, 8
; KV3_1-NEXT:    ;; # (end cycle 4)
; KV3_1-NEXT:    andw $r0 = $r0, 254
; KV3_1-NEXT:    andw $r1 = $r1, 254
; KV3_1-NEXT:    ;; # (end cycle 5)
; KV3_1-NEXT:    srlw $r0 = $r0, 1
; KV3_1-NEXT:    srlw $r1 = $r1, 1
; KV3_1-NEXT:    ;; # (end cycle 6)
; KV3_1-NEXT:    insf $r0 = $r1, 15, 8
; KV3_1-NEXT:    ret
; KV3_1-NEXT:    ;; # (end cycle 7)
;
; KV3_2-LABEL: not_ravg_v2u8:
; KV3_2:       # %bb.0: # %entry
; KV3_2-NEXT:    addbo $r0 = $r0, $r1
; KV3_2-NEXT:    ;; # (end cycle 0)
; KV3_2-NEXT:    addbo $r0 = $r0, 257
; KV3_2-NEXT:    ;; # (end cycle 1)
; KV3_2-NEXT:    srlbos $r0 = $r0, 1
; KV3_2-NEXT:    ret
; KV3_2-NEXT:    ;; # (end cycle 2)
entry:
  %add = add <2 x i8> %a, <i8 1, i8 1>
  %add1 = add <2 x i8> %add, %b
  %shr = lshr <2 x i8> %add1, <i8 1, i8 1>
  ret <2 x i8> %shr
}

; Again, can't produce the R_RI
define <2 x i8> @ravg_v2u8_ri(<2 x i8> %a) {
; KV3_1-LABEL: ravg_v2u8_ri:
; KV3_1:       # %bb.0: # %entry
; KV3_1-NEXT:    sxlbhq $r0 = $r0
; KV3_1-NEXT:    ;; # (end cycle 0)
; KV3_1-NEXT:    addhq $r0 = $r0, 0x810000
; KV3_1-NEXT:    ;; # (end cycle 1)
; KV3_1-NEXT:    sbmm8 $r0 = $r0, 0x401
; KV3_1-NEXT:    ;; # (end cycle 2)
; KV3_1-NEXT:    zxbd $r0 = $r0
; KV3_1-NEXT:    extfz $r1 = $r0, 15, 8
; KV3_1-NEXT:    ;; # (end cycle 3)
; KV3_1-NEXT:    andw $r0 = $r0, 254
; KV3_1-NEXT:    andw $r1 = $r1, 254
; KV3_1-NEXT:    ;; # (end cycle 4)
; KV3_1-NEXT:    srlw $r0 = $r0, 1
; KV3_1-NEXT:    srlw $r1 = $r1, 1
; KV3_1-NEXT:    ;; # (end cycle 5)
; KV3_1-NEXT:    insf $r0 = $r1, 15, 8
; KV3_1-NEXT:    ret
; KV3_1-NEXT:    ;; # (end cycle 6)
;
; KV3_2-LABEL: ravg_v2u8_ri:
; KV3_2:       # %bb.0: # %entry
; KV3_2-NEXT:    avgubo $r0 = $r0, 0xffff8100
; KV3_2-NEXT:    ret
; KV3_2-NEXT:    ;; # (end cycle 0)
entry:
  %add1 = add nuw <2 x i8> %a, <i8 0, i8 129>
  %shr = lshr <2 x i8> %add1, <i8 1, i8 1>
  ret <2 x i8> %shr
}

define <4 x i8> @avg_v4i8(<4 x i8> %a, <4 x i8> %b) {
; KV3_1-LABEL: avg_v4i8:
; KV3_1:       # %bb.0: # %entry
; KV3_1-NEXT:    sxlbhq $r0 = $r0
; KV3_1-NEXT:    sxlbhq $r1 = $r1
; KV3_1-NEXT:    ;; # (end cycle 0)
; KV3_1-NEXT:    addhq $r0 = $r1, $r0
; KV3_1-NEXT:    ;; # (end cycle 1)
; KV3_1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; KV3_1-NEXT:    ;; # (end cycle 2)
; KV3_1-NEXT:    sxlbhq $r0 = $r0
; KV3_1-NEXT:    ;; # (end cycle 3)
; KV3_1-NEXT:    srahqs $r0 = $r0, 1
; KV3_1-NEXT:    ;; # (end cycle 4)
; KV3_1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; KV3_1-NEXT:    ret
; KV3_1-NEXT:    ;; # (end cycle 5)
;
; KV3_2-LABEL: avg_v4i8:
; KV3_2:       # %bb.0: # %entry
; KV3_2-NEXT:    avgbo $r0 = $r0, $r1
; KV3_2-NEXT:    ret
; KV3_2-NEXT:    ;; # (end cycle 0)
entry:
  %add = add <4 x i8> %b, %a
  %shr = ashr <4 x i8> %add, <i8 1, i8 1, i8 1, i8 1>
  ret <4 x i8> %shr
}

define <4 x i8> @avg_v4i8_ri_(<4 x i8> %a) {
; KV3_1-LABEL: avg_v4i8_ri_:
; KV3_1:       # %bb.0: # %entry
; KV3_1-NEXT:    sxlbhq $r0 = $r0
; KV3_1-NEXT:    ;; # (end cycle 0)
; KV3_1-NEXT:    addhq $r0 = $r0, 0x40007f
; KV3_1-NEXT:    ;; # (end cycle 1)
; KV3_1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; KV3_1-NEXT:    ;; # (end cycle 2)
; KV3_1-NEXT:    sxlbhq $r0 = $r0
; KV3_1-NEXT:    ;; # (end cycle 3)
; KV3_1-NEXT:    srahqs $r0 = $r0, 1
; KV3_1-NEXT:    ;; # (end cycle 4)
; KV3_1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; KV3_1-NEXT:    ret
; KV3_1-NEXT:    ;; # (end cycle 5)
;
; KV3_2-LABEL: avg_v4i8_ri_:
; KV3_2:       # %bb.0: # %entry
; KV3_2-NEXT:    avgbo $r0 = $r0, 0x407f
; KV3_2-NEXT:    ret
; KV3_2-NEXT:    ;; # (end cycle 0)
entry:
  %add = add <4 x i8> %a, <i8 127, i8 64, i8 0, i8 0>
  %shr = ashr <4 x i8> %add, <i8 1, i8 1, i8 1, i8 1>
  ret <4 x i8> %shr
}

define <4 x i8> @avg_v4i8_ri_2(<4 x i8> %a) {
; KV3_1-LABEL: avg_v4i8_ri_2:
; KV3_1:       # %bb.0: # %entry
; KV3_1-NEXT:    sxlbhq $r0 = $r0
; KV3_1-NEXT:    ;; # (end cycle 0)
; KV3_1-NEXT:    addhq $r0 = $r0, 0x7f001f.@
; KV3_1-NEXT:    ;; # (end cycle 1)
; KV3_1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; KV3_1-NEXT:    ;; # (end cycle 2)
; KV3_1-NEXT:    sxlbhq $r0 = $r0
; KV3_1-NEXT:    ;; # (end cycle 3)
; KV3_1-NEXT:    srahqs $r0 = $r0, 1
; KV3_1-NEXT:    ;; # (end cycle 4)
; KV3_1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; KV3_1-NEXT:    ret
; KV3_1-NEXT:    ;; # (end cycle 5)
;
; KV3_2-LABEL: avg_v4i8_ri_2:
; KV3_2:       # %bb.0: # %entry
; KV3_2-NEXT:    avgbo $r0 = $r0, 0x7f1f7f1f
; KV3_2-NEXT:    ret
; KV3_2-NEXT:    ;; # (end cycle 0)
entry:
  %add = add <4 x i8> %a, <i8 31, i8 127, i8 31, i8 127>
  %shr = ashr <4 x i8> %add, <i8 1, i8 1, i8 1, i8 1>
  ret <4 x i8> %shr
}

define <4 x i8> @ravg_v4i8(<4 x i8> %a, <4 x i8> %b) {
; KV3_1-LABEL: ravg_v4i8:
; KV3_1:       # %bb.0: # %entry
; KV3_1-NEXT:    sxlbhq $r0 = $r0
; KV3_1-NEXT:    sxlbhq $r1 = $r1
; KV3_1-NEXT:    ;; # (end cycle 0)
; KV3_1-NEXT:    addhq $r0 = $r0, $r1
; KV3_1-NEXT:    ;; # (end cycle 1)
; KV3_1-NEXT:    addhq $r0 = $r0, 0x10001.@
; KV3_1-NEXT:    ;; # (end cycle 2)
; KV3_1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; KV3_1-NEXT:    ;; # (end cycle 3)
; KV3_1-NEXT:    sxlbhq $r0 = $r0
; KV3_1-NEXT:    ;; # (end cycle 4)
; KV3_1-NEXT:    srahqs $r0 = $r0, 1
; KV3_1-NEXT:    ;; # (end cycle 5)
; KV3_1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; KV3_1-NEXT:    ret
; KV3_1-NEXT:    ;; # (end cycle 6)
;
; KV3_2-LABEL: ravg_v4i8:
; KV3_2:       # %bb.0: # %entry
; KV3_2-NEXT:    avgrbo $r0 = $r1, $r0
; KV3_2-NEXT:    ret
; KV3_2-NEXT:    ;; # (end cycle 0)
entry:
  %add = add <4 x i8> %a, <i8 1, i8 1, i8 1, i8 1>
  %add1 = add <4 x i8> %add, %b
  %shr = ashr <4 x i8> %add1, <i8 1, i8 1, i8 1, i8 1>
  ret <4 x i8> %shr
}

define <4 x i8> @ravg_v4i8_ri_at(<4 x i8> %a) {
; KV3_1-LABEL: ravg_v4i8_ri_at:
; KV3_1:       # %bb.0: # %entry
; KV3_1-NEXT:    sxlbhq $r0 = $r0
; KV3_1-NEXT:    ;; # (end cycle 0)
; KV3_1-NEXT:    addhq $r0 = $r0, 0x800081.@
; KV3_1-NEXT:    ;; # (end cycle 1)
; KV3_1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; KV3_1-NEXT:    ;; # (end cycle 2)
; KV3_1-NEXT:    sxlbhq $r0 = $r0
; KV3_1-NEXT:    ;; # (end cycle 3)
; KV3_1-NEXT:    srahqs $r0 = $r0, 1
; KV3_1-NEXT:    ;; # (end cycle 4)
; KV3_1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; KV3_1-NEXT:    ret
; KV3_1-NEXT:    ;; # (end cycle 5)
;
; KV3_2-LABEL: ravg_v4i8_ri_at:
; KV3_2:       # %bb.0: # %entry
; KV3_2-NEXT:    avgbo $r0 = $r0, 0x80818081
; KV3_2-NEXT:    ret
; KV3_2-NEXT:    ;; # (end cycle 0)
entry:
  %add1 = add <4 x i8> %a, <i8 -127, i8 128, i8 -127, i8 128>
  %shr = ashr <4 x i8> %add1, <i8 1, i8 1, i8 1, i8 1>
  ret <4 x i8> %shr
}

define <4 x i8> @avg_v4u8(<4 x i8> %a, <4 x i8> %b) {
; KV3_1-LABEL: avg_v4u8:
; KV3_1:       # %bb.0: # %entry
; KV3_1-NEXT:    sxlbhq $r0 = $r0
; KV3_1-NEXT:    sxlbhq $r1 = $r1
; KV3_1-NEXT:    ;; # (end cycle 0)
; KV3_1-NEXT:    addhq $r0 = $r1, $r0
; KV3_1-NEXT:    ;; # (end cycle 1)
; KV3_1-NEXT:    andd $r0 = $r0, 0xff00ff.@
; KV3_1-NEXT:    ;; # (end cycle 2)
; KV3_1-NEXT:    srlhqs $r0 = $r0, 1
; KV3_1-NEXT:    ;; # (end cycle 3)
; KV3_1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; KV3_1-NEXT:    ret
; KV3_1-NEXT:    ;; # (end cycle 4)
;
; KV3_2-LABEL: avg_v4u8:
; KV3_2:       # %bb.0: # %entry
; KV3_2-NEXT:    avgubo $r0 = $r0, $r1
; KV3_2-NEXT:    ret
; KV3_2-NEXT:    ;; # (end cycle 0)
entry:
  %add = add nuw <4 x i8> %b, %a
  %shr = lshr <4 x i8> %add, <i8 1, i8 1, i8 1, i8 1>
  ret <4 x i8> %shr
}

define <4 x i8> @avg_v4u8_ri_(<4 x i8> %a) {
; KV3_1-LABEL: avg_v4u8_ri_:
; KV3_1:       # %bb.0: # %entry
; KV3_1-NEXT:    sxlbhq $r0 = $r0
; KV3_1-NEXT:    ;; # (end cycle 0)
; KV3_1-NEXT:    addhq $r0 = $r0, 0x8000ff
; KV3_1-NEXT:    ;; # (end cycle 1)
; KV3_1-NEXT:    andd $r0 = $r0, 0xff00ff.@
; KV3_1-NEXT:    ;; # (end cycle 2)
; KV3_1-NEXT:    srlhqs $r0 = $r0, 1
; KV3_1-NEXT:    ;; # (end cycle 3)
; KV3_1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; KV3_1-NEXT:    ret
; KV3_1-NEXT:    ;; # (end cycle 4)
;
; KV3_2-LABEL: avg_v4u8_ri_:
; KV3_2:       # %bb.0: # %entry
; KV3_2-NEXT:    avgubo $r0 = $r0, 0x80ff
; KV3_2-NEXT:    ret
; KV3_2-NEXT:    ;; # (end cycle 0)
entry:
  %add = add nuw <4 x i8> %a, <i8 -1, i8 128, i8 0, i8 0>
  %shr = lshr <4 x i8> %add, <i8 1, i8 1, i8 1, i8 1>
  ret <4 x i8> %shr
}

define <4 x i8> @avg_v4u8_ri_at(<4 x i8> %a) {
; KV3_1-LABEL: avg_v4u8_ri_at:
; KV3_1:       # %bb.0: # %entry
; KV3_1-NEXT:    sxlbhq $r0 = $r0
; KV3_1-NEXT:    ;; # (end cycle 0)
; KV3_1-NEXT:    addhq $r0 = $r0, 0x8000ff.@
; KV3_1-NEXT:    ;; # (end cycle 1)
; KV3_1-NEXT:    andd $r0 = $r0, 0xff00ff.@
; KV3_1-NEXT:    ;; # (end cycle 2)
; KV3_1-NEXT:    srlhqs $r0 = $r0, 1
; KV3_1-NEXT:    ;; # (end cycle 3)
; KV3_1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; KV3_1-NEXT:    ret
; KV3_1-NEXT:    ;; # (end cycle 4)
;
; KV3_2-LABEL: avg_v4u8_ri_at:
; KV3_2:       # %bb.0: # %entry
; KV3_2-NEXT:    avgubo $r0 = $r0, 0x80ff80ff
; KV3_2-NEXT:    ret
; KV3_2-NEXT:    ;; # (end cycle 0)
entry:
  %add = add nuw <4 x i8> %a, <i8 -1, i8 128, i8 -1, i8 128>
  %shr = lshr <4 x i8> %add, <i8 1, i8 1, i8 1, i8 1>
  ret <4 x i8> %shr
}

; FIXME: nuw get's lost
define <4 x i8> @ravg_v4u8(<4 x i8> %a, <4 x i8> %b) {
; KV3_1-LABEL: ravg_v4u8:
; KV3_1:       # %bb.0: # %entry
; KV3_1-NEXT:    sxlbhq $r0 = $r0
; KV3_1-NEXT:    sxlbhq $r1 = $r1
; KV3_1-NEXT:    ;; # (end cycle 0)
; KV3_1-NEXT:    addhq $r0 = $r0, $r1
; KV3_1-NEXT:    ;; # (end cycle 1)
; KV3_1-NEXT:    addhq $r0 = $r0, 0x10001.@
; KV3_1-NEXT:    ;; # (end cycle 2)
; KV3_1-NEXT:    andd $r0 = $r0, 0xff00ff.@
; KV3_1-NEXT:    ;; # (end cycle 3)
; KV3_1-NEXT:    srlhqs $r0 = $r0, 1
; KV3_1-NEXT:    ;; # (end cycle 4)
; KV3_1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; KV3_1-NEXT:    ret
; KV3_1-NEXT:    ;; # (end cycle 5)
;
; KV3_2-LABEL: ravg_v4u8:
; KV3_2:       # %bb.0: # %entry
; KV3_2-NEXT:    addbo $r0 = $r0, $r1
; KV3_2-NEXT:    ;; # (end cycle 0)
; KV3_2-NEXT:    addbo $r0 = $r0, 0x1010101
; KV3_2-NEXT:    ;; # (end cycle 1)
; KV3_2-NEXT:    srlbos $r0 = $r0, 1
; KV3_2-NEXT:    ret
; KV3_2-NEXT:    ;; # (end cycle 2)
entry:
  %add = add nuw <4 x i8> %a, <i8 1, i8 1, i8 1, i8 1>
  %add1 = add nuw <4 x i8> %add, %b
  %shr = lshr <4 x i8> %add1, <i8 1, i8 1, i8 1, i8 1>
  ret <4 x i8> %shr
}

define <4 x i8> @ravg_v4u8_ri_(<4 x i8> %a) {
; KV3_1-LABEL: ravg_v4u8_ri_:
; KV3_1:       # %bb.0: # %entry
; KV3_1-NEXT:    sxlbhq $r0 = $r0
; KV3_1-NEXT:    make $r1 = 0x1000100000081
; KV3_1-NEXT:    ;; # (end cycle 0)
; KV3_1-NEXT:    addhq $r0 = $r0, $r1
; KV3_1-NEXT:    ;; # (end cycle 1)
; KV3_1-NEXT:    andd $r0 = $r0, 0xff00ff.@
; KV3_1-NEXT:    ;; # (end cycle 2)
; KV3_1-NEXT:    srlhqs $r0 = $r0, 1
; KV3_1-NEXT:    ;; # (end cycle 3)
; KV3_1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; KV3_1-NEXT:    ret
; KV3_1-NEXT:    ;; # (end cycle 4)
;
; KV3_2-LABEL: ravg_v4u8_ri_:
; KV3_2:       # %bb.0: # %entry
; KV3_2-NEXT:    avgubo $r0 = $r0, 0x1010081
; KV3_2-NEXT:    ret
; KV3_2-NEXT:    ;; # (end cycle 0)
entry:
  %add1 = add nuw <4 x i8> %a, <i8 129, i8 0, i8 1, i8 1>
  %shr = lshr <4 x i8> %add1, <i8 1, i8 1, i8 1, i8 1>
  ret <4 x i8> %shr
}

define <4 x i8> @ravg_v4u8_ri_2(<4 x i8> %a) {
; KV3_1-LABEL: ravg_v4u8_ri_2:
; KV3_1:       # %bb.0: # %entry
; KV3_1-NEXT:    sxlbhq $r0 = $r0
; KV3_1-NEXT:    ;; # (end cycle 0)
; KV3_1-NEXT:    addhq $r0 = $r0, 0x810000.@
; KV3_1-NEXT:    ;; # (end cycle 1)
; KV3_1-NEXT:    andd $r0 = $r0, 0xff00ff.@
; KV3_1-NEXT:    ;; # (end cycle 2)
; KV3_1-NEXT:    srlhqs $r0 = $r0, 1
; KV3_1-NEXT:    ;; # (end cycle 3)
; KV3_1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; KV3_1-NEXT:    ret
; KV3_1-NEXT:    ;; # (end cycle 4)
;
; KV3_2-LABEL: ravg_v4u8_ri_2:
; KV3_2:       # %bb.0: # %entry
; KV3_2-NEXT:    avgubo $r0 = $r0, 0x81008100
; KV3_2-NEXT:    ret
; KV3_2-NEXT:    ;; # (end cycle 0)
entry:
  %add1 = add nuw <4 x i8> %a, <i8 0, i8 129, i8 0, i8 129>
  %shr = lshr <4 x i8> %add1, <i8 1, i8 1, i8 1, i8 1>
  ret <4 x i8> %shr
}


define <8 x i8> @avg_v8i8(<8 x i8> %a, <8 x i8> %b) {
; KV3_1-LABEL: avg_v8i8:
; KV3_1:       # %bb.0: # %entry
; KV3_1-NEXT:    xord $r0 = $r1, $r0
; KV3_1-NEXT:    andd $r2 = $r0, 0x7f7f7f7f.@
; KV3_1-NEXT:    andd $r3 = $r1, 0x7f7f7f7f.@
; KV3_1-NEXT:    ;; # (end cycle 0)
; KV3_1-NEXT:    andd $r0 = $r0, 0x80808080.@
; KV3_1-NEXT:    addd $r1 = $r3, $r2
; KV3_1-NEXT:    ;; # (end cycle 1)
; KV3_1-NEXT:    xord $r0 = $r1, $r0
; KV3_1-NEXT:    ;; # (end cycle 2)
; KV3_1-NEXT:    extfz $r1 = $r0, 55, 48
; KV3_1-NEXT:    extfz $r2 = $r0, 47, 40
; KV3_1-NEXT:    srld $r3 = $r0, 56
; KV3_1-NEXT:    srlw $r4 = $r0, 24
; KV3_1-NEXT:    ;; # (end cycle 3)
; KV3_1-NEXT:    sxbd $r1 = $r1
; KV3_1-NEXT:    sxbd $r3 = $r3
; KV3_1-NEXT:    zxbd $r5 = $r0
; KV3_1-NEXT:    ;; # (end cycle 4)
; KV3_1-NEXT:    sraw $r1 = $r1, 1
; KV3_1-NEXT:    sxbd $r2 = $r2
; KV3_1-NEXT:    sraw $r3 = $r3, 1
; KV3_1-NEXT:    extfz $r6 = $r0, 39, 32
; KV3_1-NEXT:    ;; # (end cycle 5)
; KV3_1-NEXT:    insf $r1 = $r3, 15, 8
; KV3_1-NEXT:    sraw $r2 = $r2, 1
; KV3_1-NEXT:    sxbd $r3 = $r6
; KV3_1-NEXT:    ;; # (end cycle 6)
; KV3_1-NEXT:    sraw $r3 = $r3, 1
; KV3_1-NEXT:    sxbd $r4 = $r4
; KV3_1-NEXT:    sxbd $r5 = $r5
; KV3_1-NEXT:    ;; # (end cycle 7)
; KV3_1-NEXT:    extfz $r2 = $r0, 23, 16
; KV3_1-NEXT:    insf $r3 = $r2, 15, 8
; KV3_1-NEXT:    sraw $r4 = $r4, 1
; KV3_1-NEXT:    ;; # (end cycle 8)
; KV3_1-NEXT:    extfz $r0 = $r0, 15, 8
; KV3_1-NEXT:    sxbd $r2 = $r2
; KV3_1-NEXT:    ;; # (end cycle 9)
; KV3_1-NEXT:    sxbd $r0 = $r0
; KV3_1-NEXT:    sraw $r2 = $r2, 1
; KV3_1-NEXT:    insf $r3 = $r1, 31, 16
; KV3_1-NEXT:    ;; # (end cycle 10)
; KV3_1-NEXT:    sraw $r0 = $r5, 1
; KV3_1-NEXT:    insf $r2 = $r4, 15, 8
; KV3_1-NEXT:    sraw $r6 = $r0, 1
; KV3_1-NEXT:    ;; # (end cycle 11)
; KV3_1-NEXT:    insf $r0 = $r6, 15, 8
; KV3_1-NEXT:    ;; # (end cycle 12)
; KV3_1-NEXT:    insf $r0 = $r2, 31, 16
; KV3_1-NEXT:    ;; # (end cycle 13)
; KV3_1-NEXT:    insf $r0 = $r3, 63, 32
; KV3_1-NEXT:    ret
; KV3_1-NEXT:    ;; # (end cycle 14)
;
; KV3_2-LABEL: avg_v8i8:
; KV3_2:       # %bb.0: # %entry
; KV3_2-NEXT:    avgbo $r0 = $r0, $r1
; KV3_2-NEXT:    ret
; KV3_2-NEXT:    ;; # (end cycle 0)
entry:
  %add = add <8 x i8> %b, %a
  %shr = ashr <8 x i8> %add, <i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1>
  ret <8 x i8> %shr
}

define <8 x i8> @avg_v8i8_ri_(<8 x i8> %a) {
; KV3_1-LABEL: avg_v8i8_ri_:
; KV3_1:       # %bb.0: # %entry
; KV3_1-NEXT:    make $r1 = 0x7f437f43
; KV3_1-NEXT:    andd $r2 = $r0, 0x7f7f7f7f.@
; KV3_1-NEXT:    ;; # (end cycle 0)
; KV3_1-NEXT:    xord $r0 = $r0, $r1
; KV3_1-NEXT:    andd $r3 = $r1, 0x7f7f7f7f.@
; KV3_1-NEXT:    ;; # (end cycle 1)
; KV3_1-NEXT:    andd $r0 = $r0, 0x80808080.@
; KV3_1-NEXT:    addd $r1 = $r2, $r3
; KV3_1-NEXT:    ;; # (end cycle 2)
; KV3_1-NEXT:    xord $r0 = $r1, $r0
; KV3_1-NEXT:    ;; # (end cycle 3)
; KV3_1-NEXT:    extfz $r1 = $r0, 55, 48
; KV3_1-NEXT:    extfz $r2 = $r0, 47, 40
; KV3_1-NEXT:    srld $r3 = $r0, 56
; KV3_1-NEXT:    srlw $r4 = $r0, 24
; KV3_1-NEXT:    ;; # (end cycle 4)
; KV3_1-NEXT:    sxbd $r1 = $r1
; KV3_1-NEXT:    sxbd $r3 = $r3
; KV3_1-NEXT:    zxbd $r5 = $r0
; KV3_1-NEXT:    ;; # (end cycle 5)
; KV3_1-NEXT:    sraw $r1 = $r1, 1
; KV3_1-NEXT:    sxbd $r2 = $r2
; KV3_1-NEXT:    sraw $r3 = $r3, 1
; KV3_1-NEXT:    extfz $r6 = $r0, 39, 32
; KV3_1-NEXT:    ;; # (end cycle 6)
; KV3_1-NEXT:    insf $r1 = $r3, 15, 8
; KV3_1-NEXT:    sraw $r2 = $r2, 1
; KV3_1-NEXT:    sxbd $r3 = $r6
; KV3_1-NEXT:    ;; # (end cycle 7)
; KV3_1-NEXT:    sraw $r3 = $r3, 1
; KV3_1-NEXT:    sxbd $r4 = $r4
; KV3_1-NEXT:    sxbd $r5 = $r5
; KV3_1-NEXT:    ;; # (end cycle 8)
; KV3_1-NEXT:    extfz $r2 = $r0, 23, 16
; KV3_1-NEXT:    insf $r3 = $r2, 15, 8
; KV3_1-NEXT:    sraw $r4 = $r4, 1
; KV3_1-NEXT:    ;; # (end cycle 9)
; KV3_1-NEXT:    extfz $r0 = $r0, 15, 8
; KV3_1-NEXT:    sxbd $r2 = $r2
; KV3_1-NEXT:    ;; # (end cycle 10)
; KV3_1-NEXT:    sxbd $r0 = $r0
; KV3_1-NEXT:    sraw $r2 = $r2, 1
; KV3_1-NEXT:    insf $r3 = $r1, 31, 16
; KV3_1-NEXT:    ;; # (end cycle 11)
; KV3_1-NEXT:    sraw $r0 = $r5, 1
; KV3_1-NEXT:    insf $r2 = $r4, 15, 8
; KV3_1-NEXT:    sraw $r6 = $r0, 1
; KV3_1-NEXT:    ;; # (end cycle 12)
; KV3_1-NEXT:    insf $r0 = $r6, 15, 8
; KV3_1-NEXT:    ;; # (end cycle 13)
; KV3_1-NEXT:    insf $r0 = $r2, 31, 16
; KV3_1-NEXT:    ;; # (end cycle 14)
; KV3_1-NEXT:    insf $r0 = $r3, 63, 32
; KV3_1-NEXT:    ret
; KV3_1-NEXT:    ;; # (end cycle 15)
;
; KV3_2-LABEL: avg_v8i8_ri_:
; KV3_2:       # %bb.0: # %entry
; KV3_2-NEXT:    avgbo $r0 = $r0, 0x7f437f43
; KV3_2-NEXT:    ret
; KV3_2-NEXT:    ;; # (end cycle 0)
entry:
  %add = add <8 x i8> %a, <i8 67, i8 127, i8 67, i8 127, i8 0, i8 0, i8 0, i8 0>
  %shr = ashr <8 x i8> %add, <i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1>
  ret <8 x i8> %shr
}

define <8 x i8> @avg_v8i8_ri_at(<8 x i8> %a) {
; KV3_1-LABEL: avg_v8i8_ri_at:
; KV3_1:       # %bb.0: # %entry
; KV3_1-NEXT:    make $r1 = 0x7f437f437f437f43
; KV3_1-NEXT:    andd $r2 = $r0, 0x7f7f7f7f.@
; KV3_1-NEXT:    ;; # (end cycle 0)
; KV3_1-NEXT:    xord $r0 = $r0, $r1
; KV3_1-NEXT:    andd $r3 = $r1, 0x7f7f7f7f.@
; KV3_1-NEXT:    ;; # (end cycle 1)
; KV3_1-NEXT:    andd $r0 = $r0, 0x80808080.@
; KV3_1-NEXT:    addd $r1 = $r2, $r3
; KV3_1-NEXT:    ;; # (end cycle 2)
; KV3_1-NEXT:    xord $r0 = $r1, $r0
; KV3_1-NEXT:    ;; # (end cycle 3)
; KV3_1-NEXT:    extfz $r1 = $r0, 55, 48
; KV3_1-NEXT:    extfz $r2 = $r0, 47, 40
; KV3_1-NEXT:    srld $r3 = $r0, 56
; KV3_1-NEXT:    srlw $r4 = $r0, 24
; KV3_1-NEXT:    ;; # (end cycle 4)
; KV3_1-NEXT:    sxbd $r1 = $r1
; KV3_1-NEXT:    sxbd $r3 = $r3
; KV3_1-NEXT:    zxbd $r5 = $r0
; KV3_1-NEXT:    ;; # (end cycle 5)
; KV3_1-NEXT:    sraw $r1 = $r1, 1
; KV3_1-NEXT:    sxbd $r2 = $r2
; KV3_1-NEXT:    sraw $r3 = $r3, 1
; KV3_1-NEXT:    extfz $r6 = $r0, 39, 32
; KV3_1-NEXT:    ;; # (end cycle 6)
; KV3_1-NEXT:    insf $r1 = $r3, 15, 8
; KV3_1-NEXT:    sraw $r2 = $r2, 1
; KV3_1-NEXT:    sxbd $r3 = $r6
; KV3_1-NEXT:    ;; # (end cycle 7)
; KV3_1-NEXT:    sraw $r3 = $r3, 1
; KV3_1-NEXT:    sxbd $r4 = $r4
; KV3_1-NEXT:    sxbd $r5 = $r5
; KV3_1-NEXT:    ;; # (end cycle 8)
; KV3_1-NEXT:    extfz $r2 = $r0, 23, 16
; KV3_1-NEXT:    insf $r3 = $r2, 15, 8
; KV3_1-NEXT:    sraw $r4 = $r4, 1
; KV3_1-NEXT:    ;; # (end cycle 9)
; KV3_1-NEXT:    extfz $r0 = $r0, 15, 8
; KV3_1-NEXT:    sxbd $r2 = $r2
; KV3_1-NEXT:    ;; # (end cycle 10)
; KV3_1-NEXT:    sxbd $r0 = $r0
; KV3_1-NEXT:    sraw $r2 = $r2, 1
; KV3_1-NEXT:    insf $r3 = $r1, 31, 16
; KV3_1-NEXT:    ;; # (end cycle 11)
; KV3_1-NEXT:    sraw $r0 = $r5, 1
; KV3_1-NEXT:    insf $r2 = $r4, 15, 8
; KV3_1-NEXT:    sraw $r6 = $r0, 1
; KV3_1-NEXT:    ;; # (end cycle 12)
; KV3_1-NEXT:    insf $r0 = $r6, 15, 8
; KV3_1-NEXT:    ;; # (end cycle 13)
; KV3_1-NEXT:    insf $r0 = $r2, 31, 16
; KV3_1-NEXT:    ;; # (end cycle 14)
; KV3_1-NEXT:    insf $r0 = $r3, 63, 32
; KV3_1-NEXT:    ret
; KV3_1-NEXT:    ;; # (end cycle 15)
;
; KV3_2-LABEL: avg_v8i8_ri_at:
; KV3_2:       # %bb.0: # %entry
; KV3_2-NEXT:    avgbo $r0 = $r0, 0x7f437f43.@
; KV3_2-NEXT:    ret
; KV3_2-NEXT:    ;; # (end cycle 0)
entry:
  %add = add <8 x i8> %a, <i8 67, i8 127, i8 67, i8 127, i8 67, i8 127, i8 67, i8 127>
  %shr = ashr <8 x i8> %add, <i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1>
  ret <8 x i8> %shr
}

define <8 x i8> @ravg_v8i8(<8 x i8> %a, <8 x i8> %b) {
; KV3_1-LABEL: ravg_v8i8:
; KV3_1:       # %bb.0: # %entry
; KV3_1-NEXT:    xord $r0 = $r0, $r1
; KV3_1-NEXT:    andd $r2 = $r1, 0x7f7f7f7f.@
; KV3_1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; KV3_1-NEXT:    ;; # (end cycle 0)
; KV3_1-NEXT:    andd $r0 = $r0, 0x80808080.@
; KV3_1-NEXT:    addd $r1 = $r3, $r2
; KV3_1-NEXT:    make $r2 = 0x101010101010101
; KV3_1-NEXT:    ;; # (end cycle 1)
; KV3_1-NEXT:    xord $r0 = $r1, $r0
; KV3_1-NEXT:    andd $r3 = $r2, 0x7f7f7f7f.@
; KV3_1-NEXT:    ;; # (end cycle 2)
; KV3_1-NEXT:    xord $r0 = $r0, $r2
; KV3_1-NEXT:    andd $r1 = $r0, 0x7f7f7f7f.@
; KV3_1-NEXT:    ;; # (end cycle 3)
; KV3_1-NEXT:    andd $r0 = $r0, 0x80808080.@
; KV3_1-NEXT:    addd $r1 = $r1, $r3
; KV3_1-NEXT:    ;; # (end cycle 4)
; KV3_1-NEXT:    xord $r0 = $r1, $r0
; KV3_1-NEXT:    ;; # (end cycle 5)
; KV3_1-NEXT:    extfz $r1 = $r0, 55, 48
; KV3_1-NEXT:    extfz $r2 = $r0, 47, 40
; KV3_1-NEXT:    srld $r3 = $r0, 56
; KV3_1-NEXT:    srlw $r4 = $r0, 24
; KV3_1-NEXT:    ;; # (end cycle 6)
; KV3_1-NEXT:    sxbd $r3 = $r3
; KV3_1-NEXT:    extfz $r5 = $r0, 39, 32
; KV3_1-NEXT:    ;; # (end cycle 7)
; KV3_1-NEXT:    sxbd $r1 = $r1
; KV3_1-NEXT:    sxbd $r2 = $r2
; KV3_1-NEXT:    sraw $r3 = $r3, 1
; KV3_1-NEXT:    ;; # (end cycle 8)
; KV3_1-NEXT:    sraw $r1 = $r1, 1
; KV3_1-NEXT:    sraw $r2 = $r2, 1
; KV3_1-NEXT:    sxbd $r4 = $r4
; KV3_1-NEXT:    sxbd $r5 = $r5
; KV3_1-NEXT:    ;; # (end cycle 9)
; KV3_1-NEXT:    insf $r1 = $r3, 15, 8
; KV3_1-NEXT:    extfz $r3 = $r0, 23, 16
; KV3_1-NEXT:    sraw $r4 = $r4, 1
; KV3_1-NEXT:    sraw $r5 = $r5, 1
; KV3_1-NEXT:    ;; # (end cycle 10)
; KV3_1-NEXT:    zxbd $r0 = $r0
; KV3_1-NEXT:    extfz $r2 = $r0, 15, 8
; KV3_1-NEXT:    insf $r5 = $r2, 15, 8
; KV3_1-NEXT:    ;; # (end cycle 11)
; KV3_1-NEXT:    sxbd $r2 = $r2
; KV3_1-NEXT:    sxbd $r3 = $r3
; KV3_1-NEXT:    ;; # (end cycle 12)
; KV3_1-NEXT:    sxbd $r0 = $r0
; KV3_1-NEXT:    sraw $r2 = $r2, 1
; KV3_1-NEXT:    sraw $r3 = $r3, 1
; KV3_1-NEXT:    insf $r5 = $r1, 31, 16
; KV3_1-NEXT:    ;; # (end cycle 13)
; KV3_1-NEXT:    sraw $r0 = $r0, 1
; KV3_1-NEXT:    insf $r3 = $r4, 15, 8
; KV3_1-NEXT:    ;; # (end cycle 14)
; KV3_1-NEXT:    insf $r0 = $r2, 15, 8
; KV3_1-NEXT:    ;; # (end cycle 15)
; KV3_1-NEXT:    insf $r0 = $r3, 31, 16
; KV3_1-NEXT:    ;; # (end cycle 16)
; KV3_1-NEXT:    insf $r0 = $r5, 63, 32
; KV3_1-NEXT:    ret
; KV3_1-NEXT:    ;; # (end cycle 17)
;
; KV3_2-LABEL: ravg_v8i8:
; KV3_2:       # %bb.0: # %entry
; KV3_2-NEXT:    avgrbo $r0 = $r1, $r0
; KV3_2-NEXT:    ret
; KV3_2-NEXT:    ;; # (end cycle 0)
entry:
  %add = add <8 x i8> %a, <i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1>
  %add1 = add <8 x i8> %add, %b
  %shr = ashr <8 x i8> %add1, <i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1>
  ret <8 x i8> %shr
}

define <8 x i8> @ravg_v8i8_ri_at(<8 x i8> %a) {
; KV3_1-LABEL: ravg_v8i8_ri_at:
; KV3_1:       # %bb.0: # %entry
; KV3_1-NEXT:    make $r1 = 0x81e081e081e081e0
; KV3_1-NEXT:    andd $r2 = $r0, 0x7f7f7f7f.@
; KV3_1-NEXT:    ;; # (end cycle 0)
; KV3_1-NEXT:    xord $r0 = $r0, $r1
; KV3_1-NEXT:    andd $r3 = $r1, 0x7f7f7f7f.@
; KV3_1-NEXT:    ;; # (end cycle 1)
; KV3_1-NEXT:    andd $r0 = $r0, 0x80808080.@
; KV3_1-NEXT:    addd $r1 = $r2, $r3
; KV3_1-NEXT:    ;; # (end cycle 2)
; KV3_1-NEXT:    xord $r0 = $r1, $r0
; KV3_1-NEXT:    ;; # (end cycle 3)
; KV3_1-NEXT:    extfz $r1 = $r0, 55, 48
; KV3_1-NEXT:    extfz $r2 = $r0, 47, 40
; KV3_1-NEXT:    srld $r3 = $r0, 56
; KV3_1-NEXT:    srlw $r4 = $r0, 24
; KV3_1-NEXT:    ;; # (end cycle 4)
; KV3_1-NEXT:    sxbd $r1 = $r1
; KV3_1-NEXT:    sxbd $r3 = $r3
; KV3_1-NEXT:    zxbd $r5 = $r0
; KV3_1-NEXT:    ;; # (end cycle 5)
; KV3_1-NEXT:    sraw $r1 = $r1, 1
; KV3_1-NEXT:    sxbd $r2 = $r2
; KV3_1-NEXT:    sraw $r3 = $r3, 1
; KV3_1-NEXT:    extfz $r6 = $r0, 39, 32
; KV3_1-NEXT:    ;; # (end cycle 6)
; KV3_1-NEXT:    insf $r1 = $r3, 15, 8
; KV3_1-NEXT:    sraw $r2 = $r2, 1
; KV3_1-NEXT:    sxbd $r3 = $r6
; KV3_1-NEXT:    ;; # (end cycle 7)
; KV3_1-NEXT:    sraw $r3 = $r3, 1
; KV3_1-NEXT:    sxbd $r4 = $r4
; KV3_1-NEXT:    sxbd $r5 = $r5
; KV3_1-NEXT:    ;; # (end cycle 8)
; KV3_1-NEXT:    extfz $r2 = $r0, 23, 16
; KV3_1-NEXT:    insf $r3 = $r2, 15, 8
; KV3_1-NEXT:    sraw $r4 = $r4, 1
; KV3_1-NEXT:    ;; # (end cycle 9)
; KV3_1-NEXT:    extfz $r0 = $r0, 15, 8
; KV3_1-NEXT:    sxbd $r2 = $r2
; KV3_1-NEXT:    ;; # (end cycle 10)
; KV3_1-NEXT:    sxbd $r0 = $r0
; KV3_1-NEXT:    sraw $r2 = $r2, 1
; KV3_1-NEXT:    insf $r3 = $r1, 31, 16
; KV3_1-NEXT:    ;; # (end cycle 11)
; KV3_1-NEXT:    sraw $r0 = $r5, 1
; KV3_1-NEXT:    insf $r2 = $r4, 15, 8
; KV3_1-NEXT:    sraw $r6 = $r0, 1
; KV3_1-NEXT:    ;; # (end cycle 12)
; KV3_1-NEXT:    insf $r0 = $r6, 15, 8
; KV3_1-NEXT:    ;; # (end cycle 13)
; KV3_1-NEXT:    insf $r0 = $r2, 31, 16
; KV3_1-NEXT:    ;; # (end cycle 14)
; KV3_1-NEXT:    insf $r0 = $r3, 63, 32
; KV3_1-NEXT:    ret
; KV3_1-NEXT:    ;; # (end cycle 15)
;
; KV3_2-LABEL: ravg_v8i8_ri_at:
; KV3_2:       # %bb.0: # %entry
; KV3_2-NEXT:    avgbo $r0 = $r0, 0x81e081e0.@
; KV3_2-NEXT:    ret
; KV3_2-NEXT:    ;; # (end cycle 0)
entry:
  %add1 = add <8 x i8> %a, <i8 -32, i8 129, i8 -32, i8 129, i8 -32, i8 129, i8 -32, i8 129>
  %shr = ashr <8 x i8> %add1, <i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1>
  ret <8 x i8> %shr
}

define <8 x i8> @avg_v8u8(<8 x i8> %a, <8 x i8> %b) {
; KV3_1-LABEL: avg_v8u8:
; KV3_1:       # %bb.0: # %entry
; KV3_1-NEXT:    xord $r0 = $r1, $r0
; KV3_1-NEXT:    andd $r2 = $r0, 0x7f7f7f7f.@
; KV3_1-NEXT:    andd $r3 = $r1, 0x7f7f7f7f.@
; KV3_1-NEXT:    ;; # (end cycle 0)
; KV3_1-NEXT:    andd $r0 = $r0, 0x80808080.@
; KV3_1-NEXT:    addd $r1 = $r3, $r2
; KV3_1-NEXT:    ;; # (end cycle 1)
; KV3_1-NEXT:    xord $r0 = $r1, $r0
; KV3_1-NEXT:    ;; # (end cycle 2)
; KV3_1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; KV3_1-NEXT:    andd $r1 = $r0, 0xff00ff.@
; KV3_1-NEXT:    ;; # (end cycle 3)
; KV3_1-NEXT:    srld $r0 = $r0, 1
; KV3_1-NEXT:    srld $r1 = $r1, 1
; KV3_1-NEXT:    ;; # (end cycle 4)
; KV3_1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; KV3_1-NEXT:    andd $r1 = $r1, 0xff00ff.@
; KV3_1-NEXT:    ;; # (end cycle 5)
; KV3_1-NEXT:    ord $r0 = $r0, $r1
; KV3_1-NEXT:    ret
; KV3_1-NEXT:    ;; # (end cycle 6)
;
; KV3_2-LABEL: avg_v8u8:
; KV3_2:       # %bb.0: # %entry
; KV3_2-NEXT:    avgubo $r0 = $r0, $r1
; KV3_2-NEXT:    ret
; KV3_2-NEXT:    ;; # (end cycle 0)
entry:
  %add = add nuw <8 x i8> %b, %a
  %shr = lshr <8 x i8> %add, <i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1>
  ret <8 x i8> %shr
}

define <8 x i8> @avg_v8u8_ri_(<8 x i8> %a) {
; KV3_1-LABEL: avg_v8u8_ri_:
; KV3_1:       # %bb.0: # %entry
; KV3_1-NEXT:    make $r1 = 0x7fff7fff
; KV3_1-NEXT:    andd $r2 = $r0, 0x7f7f7f7f.@
; KV3_1-NEXT:    ;; # (end cycle 0)
; KV3_1-NEXT:    xord $r0 = $r0, $r1
; KV3_1-NEXT:    andd $r3 = $r1, 0x7f7f7f7f.@
; KV3_1-NEXT:    ;; # (end cycle 1)
; KV3_1-NEXT:    andd $r0 = $r0, 0x80808080.@
; KV3_1-NEXT:    addd $r1 = $r2, $r3
; KV3_1-NEXT:    ;; # (end cycle 2)
; KV3_1-NEXT:    xord $r0 = $r1, $r0
; KV3_1-NEXT:    ;; # (end cycle 3)
; KV3_1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; KV3_1-NEXT:    andd $r1 = $r0, 0xff00ff.@
; KV3_1-NEXT:    ;; # (end cycle 4)
; KV3_1-NEXT:    srld $r0 = $r0, 1
; KV3_1-NEXT:    srld $r1 = $r1, 1
; KV3_1-NEXT:    ;; # (end cycle 5)
; KV3_1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; KV3_1-NEXT:    andd $r1 = $r1, 0xff00ff.@
; KV3_1-NEXT:    ;; # (end cycle 6)
; KV3_1-NEXT:    ord $r0 = $r0, $r1
; KV3_1-NEXT:    ret
; KV3_1-NEXT:    ;; # (end cycle 7)
;
; KV3_2-LABEL: avg_v8u8_ri_:
; KV3_2:       # %bb.0: # %entry
; KV3_2-NEXT:    avgubo $r0 = $r0, 0x7fff7fff
; KV3_2-NEXT:    ret
; KV3_2-NEXT:    ;; # (end cycle 0)
entry:
  %add = add nuw <8 x i8> %a, <i8 -1, i8 127, i8 -1, i8 127, i8 0, i8 0, i8 0, i8 0>
  %shr = lshr <8 x i8> %add, <i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1>
  ret <8 x i8> %shr
}

define <8 x i8> @avg_v8u8_ri_at(<8 x i8> %a) {
; KV3_1-LABEL: avg_v8u8_ri_at:
; KV3_1:       # %bb.0: # %entry
; KV3_1-NEXT:    make $r1 = 0x7f017f017f017f01
; KV3_1-NEXT:    andd $r2 = $r0, 0x7f7f7f7f.@
; KV3_1-NEXT:    ;; # (end cycle 0)
; KV3_1-NEXT:    xord $r0 = $r0, $r1
; KV3_1-NEXT:    andd $r3 = $r1, 0x7f7f7f7f.@
; KV3_1-NEXT:    ;; # (end cycle 1)
; KV3_1-NEXT:    andd $r0 = $r0, 0x80808080.@
; KV3_1-NEXT:    addd $r1 = $r2, $r3
; KV3_1-NEXT:    ;; # (end cycle 2)
; KV3_1-NEXT:    xord $r0 = $r1, $r0
; KV3_1-NEXT:    ;; # (end cycle 3)
; KV3_1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; KV3_1-NEXT:    andd $r1 = $r0, 0xff00ff.@
; KV3_1-NEXT:    ;; # (end cycle 4)
; KV3_1-NEXT:    srld $r0 = $r0, 1
; KV3_1-NEXT:    srld $r1 = $r1, 1
; KV3_1-NEXT:    ;; # (end cycle 5)
; KV3_1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; KV3_1-NEXT:    andd $r1 = $r1, 0xff00ff.@
; KV3_1-NEXT:    ;; # (end cycle 6)
; KV3_1-NEXT:    ord $r0 = $r0, $r1
; KV3_1-NEXT:    ret
; KV3_1-NEXT:    ;; # (end cycle 7)
;
; KV3_2-LABEL: avg_v8u8_ri_at:
; KV3_2:       # %bb.0: # %entry
; KV3_2-NEXT:    avgubo $r0 = $r0, 0x7f017f01.@
; KV3_2-NEXT:    ret
; KV3_2-NEXT:    ;; # (end cycle 0)
entry:
  %add = add nuw <8 x i8> %a, <i8 1, i8 127, i8 1, i8 127, i8 1, i8 127, i8 1, i8 127>
  %shr = lshr <8 x i8> %add, <i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1>
  ret <8 x i8> %shr
}

; FIXME: nuw get's lost
define <8 x i8> @ravg_v8u8(<8 x i8> %a, <8 x i8> %b) {
; KV3_1-LABEL: ravg_v8u8:
; KV3_1:       # %bb.0: # %entry
; KV3_1-NEXT:    xord $r0 = $r0, $r1
; KV3_1-NEXT:    andd $r2 = $r1, 0x7f7f7f7f.@
; KV3_1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; KV3_1-NEXT:    ;; # (end cycle 0)
; KV3_1-NEXT:    andd $r0 = $r0, 0x80808080.@
; KV3_1-NEXT:    addd $r1 = $r3, $r2
; KV3_1-NEXT:    make $r2 = 0x101010101010101
; KV3_1-NEXT:    ;; # (end cycle 1)
; KV3_1-NEXT:    xord $r0 = $r1, $r0
; KV3_1-NEXT:    andd $r3 = $r2, 0x7f7f7f7f.@
; KV3_1-NEXT:    ;; # (end cycle 2)
; KV3_1-NEXT:    xord $r0 = $r0, $r2
; KV3_1-NEXT:    andd $r1 = $r0, 0x7f7f7f7f.@
; KV3_1-NEXT:    ;; # (end cycle 3)
; KV3_1-NEXT:    andd $r0 = $r0, 0x80808080.@
; KV3_1-NEXT:    addd $r1 = $r1, $r3
; KV3_1-NEXT:    ;; # (end cycle 4)
; KV3_1-NEXT:    xord $r0 = $r1, $r0
; KV3_1-NEXT:    ;; # (end cycle 5)
; KV3_1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; KV3_1-NEXT:    andd $r1 = $r0, 0xff00ff.@
; KV3_1-NEXT:    ;; # (end cycle 6)
; KV3_1-NEXT:    srld $r0 = $r0, 1
; KV3_1-NEXT:    srld $r1 = $r1, 1
; KV3_1-NEXT:    ;; # (end cycle 7)
; KV3_1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; KV3_1-NEXT:    andd $r1 = $r1, 0xff00ff.@
; KV3_1-NEXT:    ;; # (end cycle 8)
; KV3_1-NEXT:    ord $r0 = $r0, $r1
; KV3_1-NEXT:    ret
; KV3_1-NEXT:    ;; # (end cycle 9)
;
; KV3_2-LABEL: ravg_v8u8:
; KV3_2:       # %bb.0: # %entry
; KV3_2-NEXT:    addbo $r0 = $r0, $r1
; KV3_2-NEXT:    ;; # (end cycle 0)
; KV3_2-NEXT:    addbo $r0 = $r0, 0x1010101.@
; KV3_2-NEXT:    ;; # (end cycle 1)
; KV3_2-NEXT:    srlbos $r0 = $r0, 1
; KV3_2-NEXT:    ret
; KV3_2-NEXT:    ;; # (end cycle 2)
entry:
  %add = add nuw <8 x i8> %a, <i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1>
  %add1 = add nuw <8 x i8> %add, %b
  %shr = lshr <8 x i8> %add1, <i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1>
  ret <8 x i8> %shr
}

; TODO: Create something to subtract 1 from all immediate values.
define <8 x i8> @ravg_v8u8_ri_(<8 x i8> %a) {
; KV3_1-LABEL: ravg_v8u8_ri_:
; KV3_1:       # %bb.0: # %entry
; KV3_1-NEXT:    make $r1 = 0x101010101010281
; KV3_1-NEXT:    andd $r2 = $r0, 0x7f7f7f7f.@
; KV3_1-NEXT:    ;; # (end cycle 0)
; KV3_1-NEXT:    xord $r0 = $r0, $r1
; KV3_1-NEXT:    andd $r3 = $r1, 0x7f7f7f7f.@
; KV3_1-NEXT:    ;; # (end cycle 1)
; KV3_1-NEXT:    andd $r0 = $r0, 0x80808080.@
; KV3_1-NEXT:    addd $r1 = $r2, $r3
; KV3_1-NEXT:    ;; # (end cycle 2)
; KV3_1-NEXT:    xord $r0 = $r1, $r0
; KV3_1-NEXT:    ;; # (end cycle 3)
; KV3_1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; KV3_1-NEXT:    andd $r1 = $r0, 0xff00ff.@
; KV3_1-NEXT:    ;; # (end cycle 4)
; KV3_1-NEXT:    srld $r0 = $r0, 1
; KV3_1-NEXT:    srld $r1 = $r1, 1
; KV3_1-NEXT:    ;; # (end cycle 5)
; KV3_1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; KV3_1-NEXT:    andd $r1 = $r1, 0xff00ff.@
; KV3_1-NEXT:    ;; # (end cycle 6)
; KV3_1-NEXT:    ord $r0 = $r0, $r1
; KV3_1-NEXT:    ret
; KV3_1-NEXT:    ;; # (end cycle 7)
;
; KV3_2-LABEL: ravg_v8u8_ri_:
; KV3_2:       # %bb.0: # %entry
; KV3_2-NEXT:    make $r1 = 0x101010101010281
; KV3_2-NEXT:    ;; # (end cycle 0)
; KV3_2-NEXT:    avgubo $r0 = $r1, $r0
; KV3_2-NEXT:    ret
; KV3_2-NEXT:    ;; # (end cycle 1)
entry:
  %add1 = add nuw <8 x i8> %a, <i8 129, i8 2, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1>
  %shr = lshr <8 x i8> %add1, <i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1>
  ret <8 x i8> %shr
}

define <8 x i8> @ravg_v8u8_ri_at(<8 x i8> %a) {
; KV3_1-LABEL: ravg_v8u8_ri_at:
; KV3_1:       # %bb.0: # %entry
; KV3_1-NEXT:    make $r1 = 0x8100810081008100
; KV3_1-NEXT:    andd $r2 = $r0, 0x7f7f7f7f.@
; KV3_1-NEXT:    ;; # (end cycle 0)
; KV3_1-NEXT:    xord $r0 = $r0, $r1
; KV3_1-NEXT:    andd $r3 = $r1, 0x7f7f7f7f.@
; KV3_1-NEXT:    ;; # (end cycle 1)
; KV3_1-NEXT:    andd $r0 = $r0, 0x80808080.@
; KV3_1-NEXT:    addd $r1 = $r2, $r3
; KV3_1-NEXT:    ;; # (end cycle 2)
; KV3_1-NEXT:    xord $r0 = $r1, $r0
; KV3_1-NEXT:    ;; # (end cycle 3)
; KV3_1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; KV3_1-NEXT:    andd $r1 = $r0, 0xff00ff.@
; KV3_1-NEXT:    ;; # (end cycle 4)
; KV3_1-NEXT:    srld $r0 = $r0, 1
; KV3_1-NEXT:    srld $r1 = $r1, 1
; KV3_1-NEXT:    ;; # (end cycle 5)
; KV3_1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; KV3_1-NEXT:    andd $r1 = $r1, 0xff00ff.@
; KV3_1-NEXT:    ;; # (end cycle 6)
; KV3_1-NEXT:    ord $r0 = $r0, $r1
; KV3_1-NEXT:    ret
; KV3_1-NEXT:    ;; # (end cycle 7)
;
; KV3_2-LABEL: ravg_v8u8_ri_at:
; KV3_2:       # %bb.0: # %entry
; KV3_2-NEXT:    avgubo $r0 = $r0, 0x81008100.@
; KV3_2-NEXT:    ret
; KV3_2-NEXT:    ;; # (end cycle 0)
entry:
  %add1 = add nuw <8 x i8> %a, <i8 0, i8 129, i8 0, i8 129, i8 0, i8 129, i8 0, i8 129>
  %shr = lshr <8 x i8> %add1, <i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1>
  ret <8 x i8> %shr
}
