; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mcpu=kv3-1 -O2 -o - %s | FileCheck %s --check-prefixes=ALL,CV1
; RUN: llc -mcpu=kv3-2 -O2 -o - %s | FileCheck %s --check-prefixes=ALL,CV2
; RUN: clang -O2 -c -o /dev/null %s
; RUN: clang -O2 -march=kv3-2 -c -o /dev/null %s

target triple = "kvx-kalray-cos"

define <2 x i8> @srs_v2i8(<2 x i8> %0) {
; CV1-LABEL: srs_v2i8:
; CV1:       # %bb.0:
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    zxbd $r2 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    extfz $r1 = $r1, 14, 11
; CV1-NEXT:    extfz $r2 = $r2, 14, 11
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sxlbhq $r1 = $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x401
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    extfs $r0 = $r0, 7, 4
; CV1-NEXT:    extfs $r1 = $r1, 7, 4
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r0 = $r1, 15, 8
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 9)
;
; CV2-LABEL: srs_v2i8:
; CV2:       # %bb.0:
; CV2-NEXT:    srsbos $r0 = $r0, 4
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %2 = sdiv <2 x i8> %0, <i8 16, i8 16>
  ret <2 x i8> %2
}

define <4 x i8> @srs_v4i8(<4 x i8> %0) {
; CV1-LABEL: srs_v4i8:
; CV1:       # %bb.0:
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srahqs $r1 = $r0, 7
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    srlhqs $r1 = $r1, 4
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    srahqs $r0 = $r0, 4
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 8)
;
; CV2-LABEL: srs_v4i8:
; CV2:       # %bb.0:
; CV2-NEXT:    srsbos $r0 = $r0, 4
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %2 = sdiv <4 x i8> %0, <i8 16, i8 16, i8 16, i8 16>
  ret <4 x i8> %2
}

define <8 x i8> @srs_v8i8(<8 x i8> %0) {
; CV1-LABEL: srs_v8i8:
; CV1:       # %bb.0:
; CV1-NEXT:    srld $r1 = $r0, 56
; CV1-NEXT:    extfz $r2 = $r0, 55, 48
; CV1-NEXT:    extfz $r3 = $r0, 47, 40
; CV1-NEXT:    zxbd $r6 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    extfs $r1 = $r1, 7, 7
; CV1-NEXT:    extfz $r4 = $r0, 39, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    extfs $r2 = $r2, 7, 7
; CV1-NEXT:    extfs $r3 = $r3, 7, 7
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    extfs $r4 = $r4, 7, 7
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    extfz $r3 = $r0, 23, 16
; CV1-NEXT:    insf $r4 = $r3, 15, 8
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    extfs $r1 = $r1, 7, 7
; CV1-NEXT:    extfz $r5 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    extfs $r3 = $r3, 7, 7
; CV1-NEXT:    extfs $r5 = $r5, 7, 7
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r3 = $r1, 15, 8
; CV1-NEXT:    extfs $r6 = $r6, 7, 7
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r4 = $r2, 31, 16
; CV1-NEXT:    insf $r6 = $r5, 15, 8
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r6 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r6 = $r4, 63, 32
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    andd $r1 = $r6, 0xff00ff.@
; CV1-NEXT:    andd $r2 = $r6, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    srld $r1 = $r1, 4
; CV1-NEXT:    srld $r2 = $r2, 4
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff.@
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    iord $r1 = $r2, $r1
; CV1-NEXT:    andd $r2 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    andd $r3 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    extfz $r1 = $r0, 55, 48
; CV1-NEXT:    srld $r2 = $r0, 56
; CV1-NEXT:    extfz $r3 = $r0, 47, 40
; CV1-NEXT:    srlw $r5 = $r0, 24
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    extfs $r2 = $r2, 7, 4
; CV1-NEXT:    extfz $r4 = $r0, 39, 32
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    extfs $r1 = $r1, 7, 4
; CV1-NEXT:    extfs $r3 = $r3, 7, 4
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    insf $r1 = $r2, 15, 8
; CV1-NEXT:    extfs $r4 = $r4, 7, 4
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    insf $r4 = $r3, 15, 8
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    extfz $r3 = $r0, 15, 8
; CV1-NEXT:    extfs $r5 = $r5, 7, 4
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    extfs $r2 = $r2, 7, 4
; CV1-NEXT:    extfs $r3 = $r3, 7, 4
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    extfs $r0 = $r0, 7, 4
; CV1-NEXT:    insf $r2 = $r5, 15, 8
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    insf $r0 = $r3, 15, 8
; CV1-NEXT:    insf $r4 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    insf $r0 = $r2, 31, 16
; CV1-NEXT:    ;; # (end cycle 27)
; CV1-NEXT:    insf $r0 = $r4, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 28)
;
; CV2-LABEL: srs_v8i8:
; CV2:       # %bb.0:
; CV2-NEXT:    srsbos $r0 = $r0, 4
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %2 = sdiv <8 x i8> %0, <i8 16, i8 16, i8 16, i8 16, i8 16, i8 16, i8 16, i8 16>
  ret <8 x i8> %2
}

define <2 x i16> @srs_v2i16(<2 x i16> %0) {
; ALL-LABEL: srs_v2i16:
; ALL:       # %bb.0:
; ALL-NEXT:    srshqs $r0 = $r0, 4
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %2 = sdiv <2 x i16> %0, <i16 16, i16 16>
  ret <2 x i16> %2
}

define <4 x i16> @srs_v4i16(<4 x i16> %0) {
; ALL-LABEL: srs_v4i16:
; ALL:       # %bb.0:
; ALL-NEXT:    srshqs $r0 = $r0, 4
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %2 = sdiv <4 x i16> %0, <i16 16, i16 16, i16 16, i16 16>
  ret <4 x i16> %2
}

; TODO: This should be a simple sequence of 4 srshqs
; and 3 insf
define <4 x i16> @diff_srs_v4i16(<4 x i16> %0) {
; CV1-LABEL: diff_srs_v4i16:
; CV1:       # %bb.0:
; CV1-NEXT:    srahqs $r1 = $r0, 15
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srlhqs $r2 = $r1, 12
; CV1-NEXT:    srlhqs $r3 = $r1, 13
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srlhqs $r2 = $r1, 14
; CV1-NEXT:    insf $r3 = $r2, 15, 0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    srlhqs $r1 = $r1, 15
; CV1-NEXT:    insf $r2 = $r3, 31, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    insf $r1 = $r2, 47, 0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srahqs $r1 = $r0, 4
; CV1-NEXT:    srahqs $r2 = $r0, 3
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    srahqs $r1 = $r0, 2
; CV1-NEXT:    insf $r2 = $r1, 15, 0
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    srahqs $r0 = $r0, 1
; CV1-NEXT:    insf $r1 = $r2, 31, 0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r0 = $r1, 47, 0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 9)
;
; CV2-LABEL: diff_srs_v4i16:
; CV2:       # %bb.0:
; CV2-NEXT:    srahqs $r1 = $r0, 15
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srlhqs $r2 = $r1, 12
; CV2-NEXT:    srlhqs $r3 = $r1, 13
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    srlhqs $r1 = $r1, 15
; CV2-NEXT:    srlhqs $r2 = $r1, 14
; CV2-NEXT:    insf $r3 = $r2, 15, 0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r2 = $r3, 31, 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    insf $r1 = $r2, 47, 0
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    addhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    srahqs $r1 = $r0, 4
; CV2-NEXT:    srahqs $r2 = $r0, 3
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    srahqs $r0 = $r0, 1
; CV2-NEXT:    srahqs $r1 = $r0, 2
; CV2-NEXT:    insf $r2 = $r1, 15, 0
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r1 = $r2, 31, 0
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    insf $r0 = $r1, 47, 0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 9)
  %2 = sdiv <4 x i16> %0, <i16 16, i16 8, i16 4, i16 2>
  ret <4 x i16> %2
}

; TODO: This should simply be 4 srshqs
define <16 x i16> @diff_srs16(<16 x i16> %0) {
; CV1-LABEL: diff_srs16:
; CV1:       # %bb.0:
; CV1-NEXT:    srahqs $r5 = $r0, 15
; CV1-NEXT:    srahqs $r6 = $r1, 15
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srlhqs $r5 = $r5, 12
; CV1-NEXT:    srahqs $r7 = $r2, 15
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    addhq $r0 = $r0, $r5
; CV1-NEXT:    srahqs $r4 = $r3, 15
; CV1-NEXT:    srlhqs $r5 = $r6, 13
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    addhq $r1 = $r1, $r5
; CV1-NEXT:    srlhqs $r4 = $r4, 15
; CV1-NEXT:    srlhqs $r6 = $r7, 14
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srahqs $r0 = $r0, 4
; CV1-NEXT:    addhq $r2 = $r2, $r6
; CV1-NEXT:    avghq $r3 = $r4, $r3
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    srahqs $r1 = $r1, 3
; CV1-NEXT:    srahqs $r2 = $r2, 2
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 5)
;
; CV2-LABEL: diff_srs16:
; CV2:       # %bb.0:
; CV2-NEXT:    srahqs $r4 = $r0, 15
; CV2-NEXT:    srahqs $r5 = $r1, 15
; CV2-NEXT:    srahqs $r6 = $r2, 15
; CV2-NEXT:    srahqs $r7 = $r3, 15
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srlhqs $r4 = $r4, 12
; CV2-NEXT:    srlhqs $r5 = $r5, 13
; CV2-NEXT:    srlhqs $r6 = $r6, 14
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    addhq $r0 = $r0, $r4
; CV2-NEXT:    addhq $r1 = $r1, $r5
; CV2-NEXT:    addhq $r2 = $r2, $r6
; CV2-NEXT:    srlhqs $r4 = $r7, 15
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    srahqs $r0 = $r0, 4
; CV2-NEXT:    srahqs $r1 = $r1, 3
; CV2-NEXT:    srahqs $r2 = $r2, 2
; CV2-NEXT:    avghq $r3 = $r4, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 3)
  %2 = sdiv <16 x i16> %0, <i16 16, i16 16, i16 16, i16 16, i16 8, i16 8, i16 8, i16 8, i16 4, i16 4, i16 4, i16 4, i16 2, i16 2, i16 2, i16 2>
  ret <16 x i16> %2
}

define <2 x i32> @srs_v2i32(<2 x i32> %0) {
; ALL-LABEL: srs_v2i32:
; ALL:       # %bb.0:
; ALL-NEXT:    srswps $r0 = $r0, 4
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %2 = sdiv <2 x i32> %0, <i32 16, i32 16>
  ret <2 x i32> %2
}

define i32 @srs_i32(i32 %0) {
; ALL-LABEL: srs_i32:
; ALL:       # %bb.0:
; ALL-NEXT:    srsw $r0 = $r0, 4
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %2 = sdiv i32 %0, 16
  ret i32 %2
}

define i64 @srs_i64(i64 %0) {
; ALL-LABEL: srs_i64:
; ALL:       # %bb.0:
; ALL-NEXT:    srsd $r0 = $r0, 4
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %2 = sdiv i64 %0, 16
  ret i64 %2
}

define <16 x i8> @srs_v16i8(<16 x i8> %0) {
; CV1-LABEL: srs_v16i8:
; CV1:       # %bb.0:
; CV1-NEXT:    srld $r2 = $r0, 56
; CV1-NEXT:    extfz $r3 = $r0, 55, 48
; CV1-NEXT:    extfz $r4 = $r0, 47, 40
; CV1-NEXT:    zxbd $r7 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    extfs $r2 = $r2, 7, 7
; CV1-NEXT:    extfz $r5 = $r0, 39, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    extfs $r3 = $r3, 7, 7
; CV1-NEXT:    extfs $r4 = $r4, 7, 7
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    srlw $r2 = $r0, 24
; CV1-NEXT:    insf $r3 = $r2, 15, 8
; CV1-NEXT:    extfs $r5 = $r5, 7, 7
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    extfz $r4 = $r0, 23, 16
; CV1-NEXT:    insf $r5 = $r4, 15, 8
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    extfs $r2 = $r2, 7, 7
; CV1-NEXT:    extfz $r6 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    extfs $r4 = $r4, 7, 7
; CV1-NEXT:    extfs $r6 = $r6, 7, 7
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r4 = $r2, 15, 8
; CV1-NEXT:    extfs $r7 = $r7, 7, 7
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r5 = $r3, 31, 16
; CV1-NEXT:    insf $r7 = $r6, 15, 8
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    srld $r4 = $r1, 56
; CV1-NEXT:    insf $r7 = $r4, 31, 16
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    extfs $r4 = $r4, 7, 7
; CV1-NEXT:    insf $r7 = $r5, 63, 32
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    andd $r2 = $r7, 0xff00ff.@
; CV1-NEXT:    andd $r3 = $r7, 0xff00ff00.@
; CV1-NEXT:    extfz $r7 = $r1, 39, 32
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    srld $r2 = $r2, 4
; CV1-NEXT:    srld $r3 = $r3, 4
; CV1-NEXT:    extfs $r7 = $r7, 7, 7
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff.@
; CV1-NEXT:    andd $r3 = $r3, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    iord $r2 = $r3, $r2
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    extfz $r2 = $r1, 55, 48
; CV1-NEXT:    andd $r5 = $r2, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    extfs $r2 = $r2, 7, 7
; CV1-NEXT:    addd $r3 = $r3, $r5
; CV1-NEXT:    extfz $r5 = $r1, 47, 40
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    eord $r0 = $r3, $r0
; CV1-NEXT:    insf $r2 = $r4, 15, 8
; CV1-NEXT:    extfs $r3 = $r5, 7, 7
; CV1-NEXT:    zxbd $r4 = $r1
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    extfz $r3 = $r1, 23, 16
; CV1-NEXT:    srld $r5 = $r0, 56
; CV1-NEXT:    insf $r7 = $r3, 15, 8
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    srlw $r2 = $r1, 24
; CV1-NEXT:    extfs $r3 = $r3, 7, 7
; CV1-NEXT:    insf $r7 = $r2, 31, 16
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    extfs $r2 = $r2, 7, 7
; CV1-NEXT:    extfs $r4 = $r4, 7, 7
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    extfz $r2 = $r1, 15, 8
; CV1-NEXT:    insf $r3 = $r2, 15, 8
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    extfs $r2 = $r2, 7, 7
; CV1-NEXT:    extfz $r6 = $r0, 55, 48
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    extfz $r2 = $r0, 47, 40
; CV1-NEXT:    insf $r4 = $r2, 15, 8
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    extfz $r3 = $r0, 39, 32
; CV1-NEXT:    insf $r4 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    extfs $r2 = $r2, 7, 4
; CV1-NEXT:    extfs $r3 = $r3, 7, 4
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    insf $r3 = $r2, 15, 8
; CV1-NEXT:    insf $r4 = $r7, 63, 32
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    andd $r2 = $r4, 0xff00ff.@
; CV1-NEXT:    andd $r4 = $r4, 0xff00ff00.@
; CV1-NEXT:    extfs $r5 = $r5, 7, 4
; CV1-NEXT:    extfs $r6 = $r6, 7, 4
; CV1-NEXT:    ;; # (end cycle 27)
; CV1-NEXT:    srld $r2 = $r2, 4
; CV1-NEXT:    srld $r4 = $r4, 4
; CV1-NEXT:    srlw $r5 = $r0, 24
; CV1-NEXT:    insf $r6 = $r5, 15, 8
; CV1-NEXT:    ;; # (end cycle 28)
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff.@
; CV1-NEXT:    insf $r3 = $r6, 31, 16
; CV1-NEXT:    andd $r4 = $r4, 0xff00ff00.@
; CV1-NEXT:    extfz $r6 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 29)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    iord $r2 = $r4, $r2
; CV1-NEXT:    extfs $r5 = $r5, 7, 4
; CV1-NEXT:    extfz $r7 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 30)
; CV1-NEXT:    andd $r1 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    eord $r2 = $r1, $r2
; CV1-NEXT:    andd $r4 = $r2, 0x7f7f7f7f.@
; CV1-NEXT:    extfs $r6 = $r6, 7, 4
; CV1-NEXT:    ;; # (end cycle 31)
; CV1-NEXT:    extfs $r0 = $r0, 7, 4
; CV1-NEXT:    addd $r1 = $r1, $r4
; CV1-NEXT:    andd $r2 = $r2, 0x80808080.@
; CV1-NEXT:    extfs $r7 = $r7, 7, 4
; CV1-NEXT:    ;; # (end cycle 32)
; CV1-NEXT:    insf $r0 = $r7, 15, 8
; CV1-NEXT:    eord $r1 = $r1, $r2
; CV1-NEXT:    insf $r6 = $r5, 15, 8
; CV1-NEXT:    ;; # (end cycle 33)
; CV1-NEXT:    insf $r0 = $r6, 31, 16
; CV1-NEXT:    srld $r2 = $r1, 56
; CV1-NEXT:    extfz $r4 = $r1, 55, 48
; CV1-NEXT:    ;; # (end cycle 34)
; CV1-NEXT:    extfz $r5 = $r1, 47, 40
; CV1-NEXT:    extfz $r6 = $r1, 39, 32
; CV1-NEXT:    ;; # (end cycle 35)
; CV1-NEXT:    extfs $r2 = $r2, 7, 4
; CV1-NEXT:    extfs $r4 = $r4, 7, 4
; CV1-NEXT:    ;; # (end cycle 36)
; CV1-NEXT:    extfs $r5 = $r5, 7, 4
; CV1-NEXT:    extfs $r6 = $r6, 7, 4
; CV1-NEXT:    ;; # (end cycle 37)
; CV1-NEXT:    srlw $r2 = $r1, 24
; CV1-NEXT:    insf $r4 = $r2, 15, 8
; CV1-NEXT:    insf $r6 = $r5, 15, 8
; CV1-NEXT:    ;; # (end cycle 38)
; CV1-NEXT:    zxbd $r1 = $r1
; CV1-NEXT:    extfz $r5 = $r1, 23, 16
; CV1-NEXT:    extfz $r7 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 39)
; CV1-NEXT:    extfs $r2 = $r2, 7, 4
; CV1-NEXT:    extfs $r5 = $r5, 7, 4
; CV1-NEXT:    ;; # (end cycle 40)
; CV1-NEXT:    extfs $r1 = $r1, 7, 4
; CV1-NEXT:    extfs $r7 = $r7, 7, 4
; CV1-NEXT:    ;; # (end cycle 41)
; CV1-NEXT:    insf $r1 = $r7, 15, 8
; CV1-NEXT:    insf $r5 = $r2, 15, 8
; CV1-NEXT:    ;; # (end cycle 42)
; CV1-NEXT:    insf $r1 = $r5, 31, 16
; CV1-NEXT:    insf $r6 = $r4, 31, 16
; CV1-NEXT:    ;; # (end cycle 43)
; CV1-NEXT:    insf $r0 = $r3, 63, 32
; CV1-NEXT:    insf $r1 = $r6, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 44)
;
; CV2-LABEL: srs_v16i8:
; CV2:       # %bb.0:
; CV2-NEXT:    srabos $r2 = $r0, 7
; CV2-NEXT:    srabos $r3 = $r1, 7
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srlbos $r2 = $r2, 4
; CV2-NEXT:    srlbos $r3 = $r3, 4
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    addbo $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    srabos $r0 = $r0, 4
; CV2-NEXT:    srabos $r1 = $r1, 4
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 3)
  %2 = sdiv <16 x i8> %0, <i8 16, i8 16, i8 16, i8 16, i8 16, i8 16, i8 16, i8 16, i8 16, i8 16, i8 16, i8 16, i8 16, i8 16, i8 16, i8 16>
  ret <16 x i8> %2
}

define <32 x i8> @srs_v32i8(<32 x i8> %0) {
; CV1-LABEL: srs_v32i8:
; CV1:       # %bb.0:
; CV1-NEXT:    srld $r4 = $r0, 56
; CV1-NEXT:    extfz $r5 = $r0, 55, 48
; CV1-NEXT:    extfz $r6 = $r0, 47, 40
; CV1-NEXT:    zxbd $r9 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    extfs $r4 = $r4, 7, 7
; CV1-NEXT:    extfz $r7 = $r0, 39, 32
; CV1-NEXT:    zxbd $r10 = $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    extfs $r5 = $r5, 7, 7
; CV1-NEXT:    extfs $r6 = $r6, 7, 7
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    srlw $r4 = $r0, 24
; CV1-NEXT:    insf $r5 = $r4, 15, 8
; CV1-NEXT:    extfs $r7 = $r7, 7, 7
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    extfz $r6 = $r0, 23, 16
; CV1-NEXT:    insf $r7 = $r6, 15, 8
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    extfs $r4 = $r4, 7, 7
; CV1-NEXT:    extfz $r8 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    extfs $r6 = $r6, 7, 7
; CV1-NEXT:    extfs $r8 = $r8, 7, 7
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r6 = $r4, 15, 8
; CV1-NEXT:    extfs $r9 = $r9, 7, 7
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r7 = $r5, 31, 16
; CV1-NEXT:    insf $r9 = $r8, 15, 8
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    extfz $r8 = $r1, 47, 40
; CV1-NEXT:    insf $r9 = $r6, 31, 16
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    extfz $r7 = $r1, 55, 48
; CV1-NEXT:    insf $r9 = $r7, 63, 32
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    andd $r4 = $r9, 0xff00ff.@
; CV1-NEXT:    andd $r5 = $r9, 0xff00ff00.@
; CV1-NEXT:    extfs $r8 = $r8, 7, 7
; CV1-NEXT:    extfz $r9 = $r1, 23, 16
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    srld $r4 = $r4, 3
; CV1-NEXT:    srld $r5 = $r5, 3
; CV1-NEXT:    extfs $r9 = $r9, 7, 7
; CV1-NEXT:    extfs $r10 = $r10, 7, 7
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    andd $r4 = $r4, 0xff00ff.@
; CV1-NEXT:    andd $r5 = $r5, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    iord $r4 = $r5, $r4
; CV1-NEXT:    andd $r5 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    eord $r0 = $r0, $r4
; CV1-NEXT:    srld $r4 = $r1, 56
; CV1-NEXT:    andd $r6 = $r4, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    extfs $r4 = $r4, 7, 7
; CV1-NEXT:    addd $r5 = $r5, $r6
; CV1-NEXT:    extfs $r6 = $r7, 7, 7
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    eord $r0 = $r5, $r0
; CV1-NEXT:    insf $r6 = $r4, 15, 8
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    srld $r4 = $r0, 56
; CV1-NEXT:    extfz $r5 = $r0, 55, 48
; CV1-NEXT:    extfz $r7 = $r0, 47, 40
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    extfs $r4 = $r4, 7, 5
; CV1-NEXT:    extfs $r5 = $r5, 7, 5
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    extfz $r4 = $r1, 39, 32
; CV1-NEXT:    insf $r5 = $r4, 15, 8
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    extfs $r4 = $r4, 7, 7
; CV1-NEXT:    extfs $r7 = $r7, 7, 5
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    insf $r4 = $r8, 15, 8
; CV1-NEXT:    srlw $r8 = $r1, 24
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    insf $r4 = $r6, 31, 16
; CV1-NEXT:    extfs $r8 = $r8, 7, 7
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    extfz $r8 = $r1, 15, 8
; CV1-NEXT:    insf $r9 = $r8, 15, 8
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    extfz $r6 = $r0, 39, 32
; CV1-NEXT:    extfs $r8 = $r8, 7, 7
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    insf $r10 = $r8, 15, 8
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    insf $r10 = $r9, 31, 16
; CV1-NEXT:    ;; # (end cycle 27)
; CV1-NEXT:    extfs $r4 = $r6, 7, 5
; CV1-NEXT:    srlw $r6 = $r0, 24
; CV1-NEXT:    insf $r10 = $r4, 63, 32
; CV1-NEXT:    ;; # (end cycle 28)
; CV1-NEXT:    insf $r4 = $r7, 15, 8
; CV1-NEXT:    andd $r8 = $r10, 0xff00ff.@
; CV1-NEXT:    andd $r9 = $r10, 0xff00ff00.@
; CV1-NEXT:    extfz $r10 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 29)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    srld $r7 = $r8, 3
; CV1-NEXT:    srld $r8 = $r9, 3
; CV1-NEXT:    extfz $r9 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 30)
; CV1-NEXT:    extfs $r6 = $r6, 7, 5
; CV1-NEXT:    andd $r7 = $r7, 0xff00ff.@
; CV1-NEXT:    andd $r8 = $r8, 0xff00ff00.@
; CV1-NEXT:    extfs $r9 = $r9, 7, 5
; CV1-NEXT:    ;; # (end cycle 31)
; CV1-NEXT:    extfs $r0 = $r0, 7, 5
; CV1-NEXT:    iord $r7 = $r8, $r7
; CV1-NEXT:    extfs $r10 = $r10, 7, 5
; CV1-NEXT:    ;; # (end cycle 32)
; CV1-NEXT:    andd $r1 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    eord $r7 = $r1, $r7
; CV1-NEXT:    andd $r8 = $r7, 0x7f7f7f7f.@
; CV1-NEXT:    insf $r9 = $r6, 15, 8
; CV1-NEXT:    ;; # (end cycle 33)
; CV1-NEXT:    insf $r0 = $r10, 15, 8
; CV1-NEXT:    addd $r1 = $r1, $r8
; CV1-NEXT:    insf $r4 = $r5, 31, 16
; CV1-NEXT:    andd $r6 = $r7, 0x80808080.@
; CV1-NEXT:    ;; # (end cycle 34)
; CV1-NEXT:    insf $r0 = $r9, 31, 16
; CV1-NEXT:    eord $r1 = $r1, $r6
; CV1-NEXT:    srld $r6 = $r2, 56
; CV1-NEXT:    extfz $r7 = $r2, 55, 48
; CV1-NEXT:    ;; # (end cycle 35)
; CV1-NEXT:    insf $r0 = $r4, 63, 32
; CV1-NEXT:    srld $r4 = $r1, 56
; CV1-NEXT:    extfz $r5 = $r1, 55, 48
; CV1-NEXT:    zxbd $r9 = $r2
; CV1-NEXT:    ;; # (end cycle 36)
; CV1-NEXT:    extfs $r4 = $r4, 7, 5
; CV1-NEXT:    extfs $r5 = $r5, 7, 5
; CV1-NEXT:    ;; # (end cycle 37)
; CV1-NEXT:    extfs $r6 = $r6, 7, 7
; CV1-NEXT:    extfs $r7 = $r7, 7, 7
; CV1-NEXT:    ;; # (end cycle 38)
; CV1-NEXT:    insf $r5 = $r4, 15, 8
; CV1-NEXT:    insf $r7 = $r6, 15, 8
; CV1-NEXT:    ;; # (end cycle 39)
; CV1-NEXT:    extfz $r4 = $r2, 47, 40
; CV1-NEXT:    extfz $r6 = $r2, 39, 32
; CV1-NEXT:    ;; # (end cycle 40)
; CV1-NEXT:    extfs $r4 = $r4, 7, 7
; CV1-NEXT:    extfs $r6 = $r6, 7, 7
; CV1-NEXT:    ;; # (end cycle 41)
; CV1-NEXT:    srlw $r4 = $r2, 24
; CV1-NEXT:    insf $r6 = $r4, 15, 8
; CV1-NEXT:    extfz $r8 = $r2, 23, 16
; CV1-NEXT:    ;; # (end cycle 42)
; CV1-NEXT:    extfs $r4 = $r4, 7, 7
; CV1-NEXT:    extfs $r8 = $r8, 7, 7
; CV1-NEXT:    ;; # (end cycle 43)
; CV1-NEXT:    extfz $r4 = $r2, 15, 8
; CV1-NEXT:    insf $r8 = $r4, 15, 8
; CV1-NEXT:    ;; # (end cycle 44)
; CV1-NEXT:    extfs $r4 = $r4, 7, 7
; CV1-NEXT:    extfs $r9 = $r9, 7, 7
; CV1-NEXT:    ;; # (end cycle 45)
; CV1-NEXT:    insf $r6 = $r7, 31, 16
; CV1-NEXT:    insf $r9 = $r4, 15, 8
; CV1-NEXT:    ;; # (end cycle 46)
; CV1-NEXT:    insf $r9 = $r8, 31, 16
; CV1-NEXT:    extfz $r10 = $r1, 47, 40
; CV1-NEXT:    ;; # (end cycle 47)
; CV1-NEXT:    extfz $r4 = $r1, 39, 32
; CV1-NEXT:    insf $r9 = $r6, 63, 32
; CV1-NEXT:    ;; # (end cycle 48)
; CV1-NEXT:    extfs $r4 = $r4, 7, 5
; CV1-NEXT:    extfs $r6 = $r10, 7, 5
; CV1-NEXT:    andd $r7 = $r9, 0xff00ff.@
; CV1-NEXT:    andd $r8 = $r9, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 49)
; CV1-NEXT:    insf $r4 = $r6, 15, 8
; CV1-NEXT:    srlw $r6 = $r1, 24
; CV1-NEXT:    srld $r7 = $r7, 3
; CV1-NEXT:    srld $r8 = $r8, 3
; CV1-NEXT:    ;; # (end cycle 50)
; CV1-NEXT:    andd $r7 = $r7, 0xff00ff.@
; CV1-NEXT:    andd $r8 = $r8, 0xff00ff00.@
; CV1-NEXT:    extfz $r9 = $r1, 23, 16
; CV1-NEXT:    extfz $r10 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 51)
; CV1-NEXT:    zxbd $r1 = $r1
; CV1-NEXT:    extfs $r6 = $r6, 7, 5
; CV1-NEXT:    iord $r7 = $r8, $r7
; CV1-NEXT:    extfs $r9 = $r9, 7, 5
; CV1-NEXT:    ;; # (end cycle 52)
; CV1-NEXT:    extfs $r1 = $r1, 7, 5
; CV1-NEXT:    andd $r2 = $r2, 0x7f7f7f7f.@
; CV1-NEXT:    eord $r7 = $r2, $r7
; CV1-NEXT:    andd $r8 = $r7, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 53)
; CV1-NEXT:    addd $r2 = $r2, $r8
; CV1-NEXT:    andd $r7 = $r7, 0x80808080.@
; CV1-NEXT:    extfs $r8 = $r10, 7, 5
; CV1-NEXT:    insf $r9 = $r6, 15, 8
; CV1-NEXT:    ;; # (end cycle 54)
; CV1-NEXT:    insf $r1 = $r8, 15, 8
; CV1-NEXT:    eord $r2 = $r2, $r7
; CV1-NEXT:    srld $r6 = $r3, 56
; CV1-NEXT:    extfz $r7 = $r3, 55, 48
; CV1-NEXT:    ;; # (end cycle 55)
; CV1-NEXT:    insf $r1 = $r9, 31, 16
; CV1-NEXT:    extfz $r8 = $r3, 47, 40
; CV1-NEXT:    ;; # (end cycle 56)
; CV1-NEXT:    extfs $r6 = $r6, 7, 7
; CV1-NEXT:    extfz $r9 = $r3, 39, 32
; CV1-NEXT:    ;; # (end cycle 57)
; CV1-NEXT:    extfs $r7 = $r7, 7, 7
; CV1-NEXT:    extfs $r8 = $r8, 7, 7
; CV1-NEXT:    ;; # (end cycle 58)
; CV1-NEXT:    srlw $r6 = $r3, 24
; CV1-NEXT:    insf $r7 = $r6, 15, 8
; CV1-NEXT:    extfs $r9 = $r9, 7, 7
; CV1-NEXT:    ;; # (end cycle 59)
; CV1-NEXT:    extfs $r6 = $r6, 7, 7
; CV1-NEXT:    zxbd $r8 = $r3
; CV1-NEXT:    insf $r9 = $r8, 15, 8
; CV1-NEXT:    ;; # (end cycle 60)
; CV1-NEXT:    extfz $r7 = $r3, 23, 16
; CV1-NEXT:    insf $r9 = $r7, 31, 16
; CV1-NEXT:    ;; # (end cycle 61)
; CV1-NEXT:    extfs $r7 = $r7, 7, 7
; CV1-NEXT:    extfs $r8 = $r8, 7, 7
; CV1-NEXT:    ;; # (end cycle 62)
; CV1-NEXT:    extfz $r6 = $r3, 15, 8
; CV1-NEXT:    insf $r7 = $r6, 15, 8
; CV1-NEXT:    ;; # (end cycle 63)
; CV1-NEXT:    insf $r4 = $r5, 31, 16
; CV1-NEXT:    srld $r5 = $r2, 56
; CV1-NEXT:    extfs $r6 = $r6, 7, 7
; CV1-NEXT:    ;; # (end cycle 64)
; CV1-NEXT:    insf $r1 = $r4, 63, 32
; CV1-NEXT:    insf $r8 = $r6, 15, 8
; CV1-NEXT:    ;; # (end cycle 65)
; CV1-NEXT:    extfs $r4 = $r5, 7, 5
; CV1-NEXT:    extfz $r5 = $r2, 55, 48
; CV1-NEXT:    ;; # (end cycle 66)
; CV1-NEXT:    extfz $r6 = $r2, 47, 40
; CV1-NEXT:    insf $r8 = $r7, 31, 16
; CV1-NEXT:    ;; # (end cycle 67)
; CV1-NEXT:    extfs $r5 = $r5, 7, 5
; CV1-NEXT:    extfz $r7 = $r2, 39, 32
; CV1-NEXT:    ;; # (end cycle 68)
; CV1-NEXT:    extfs $r6 = $r6, 7, 5
; CV1-NEXT:    extfs $r7 = $r7, 7, 5
; CV1-NEXT:    ;; # (end cycle 69)
; CV1-NEXT:    insf $r5 = $r4, 15, 8
; CV1-NEXT:    insf $r8 = $r9, 63, 32
; CV1-NEXT:    ;; # (end cycle 70)
; CV1-NEXT:    andd $r4 = $r8, 0xff00ff.@
; CV1-NEXT:    andd $r6 = $r8, 0xff00ff00.@
; CV1-NEXT:    insf $r7 = $r6, 15, 8
; CV1-NEXT:    extfz $r8 = $r2, 23, 16
; CV1-NEXT:    ;; # (end cycle 71)
; CV1-NEXT:    srld $r4 = $r4, 3
; CV1-NEXT:    srlw $r5 = $r2, 24
; CV1-NEXT:    srld $r6 = $r6, 3
; CV1-NEXT:    insf $r7 = $r5, 31, 16
; CV1-NEXT:    ;; # (end cycle 72)
; CV1-NEXT:    zxbd $r2 = $r2
; CV1-NEXT:    andd $r4 = $r4, 0xff00ff.@
; CV1-NEXT:    andd $r6 = $r6, 0xff00ff00.@
; CV1-NEXT:    extfz $r9 = $r2, 15, 8
; CV1-NEXT:    ;; # (end cycle 73)
; CV1-NEXT:    iord $r4 = $r6, $r4
; CV1-NEXT:    extfs $r5 = $r5, 7, 5
; CV1-NEXT:    extfs $r8 = $r8, 7, 5
; CV1-NEXT:    ;; # (end cycle 74)
; CV1-NEXT:    andd $r3 = $r3, 0x7f7f7f7f.@
; CV1-NEXT:    eord $r4 = $r3, $r4
; CV1-NEXT:    andd $r6 = $r4, 0x7f7f7f7f.@
; CV1-NEXT:    extfs $r9 = $r9, 7, 5
; CV1-NEXT:    ;; # (end cycle 75)
; CV1-NEXT:    extfs $r2 = $r2, 7, 5
; CV1-NEXT:    addd $r3 = $r3, $r6
; CV1-NEXT:    andd $r4 = $r4, 0x80808080.@
; CV1-NEXT:    insf $r8 = $r5, 15, 8
; CV1-NEXT:    ;; # (end cycle 76)
; CV1-NEXT:    insf $r2 = $r9, 15, 8
; CV1-NEXT:    eord $r3 = $r3, $r4
; CV1-NEXT:    ;; # (end cycle 77)
; CV1-NEXT:    insf $r2 = $r8, 31, 16
; CV1-NEXT:    srld $r4 = $r3, 56
; CV1-NEXT:    extfz $r5 = $r3, 55, 48
; CV1-NEXT:    ;; # (end cycle 78)
; CV1-NEXT:    extfz $r6 = $r3, 47, 40
; CV1-NEXT:    extfz $r8 = $r3, 39, 32
; CV1-NEXT:    ;; # (end cycle 79)
; CV1-NEXT:    extfs $r4 = $r4, 7, 5
; CV1-NEXT:    extfs $r5 = $r5, 7, 5
; CV1-NEXT:    ;; # (end cycle 80)
; CV1-NEXT:    extfs $r6 = $r6, 7, 5
; CV1-NEXT:    extfs $r8 = $r8, 7, 5
; CV1-NEXT:    ;; # (end cycle 81)
; CV1-NEXT:    srlw $r4 = $r3, 24
; CV1-NEXT:    insf $r5 = $r4, 15, 8
; CV1-NEXT:    insf $r8 = $r6, 15, 8
; CV1-NEXT:    ;; # (end cycle 82)
; CV1-NEXT:    zxbd $r3 = $r3
; CV1-NEXT:    extfz $r6 = $r3, 23, 16
; CV1-NEXT:    extfz $r9 = $r3, 15, 8
; CV1-NEXT:    ;; # (end cycle 83)
; CV1-NEXT:    extfs $r4 = $r4, 7, 5
; CV1-NEXT:    extfs $r6 = $r6, 7, 5
; CV1-NEXT:    ;; # (end cycle 84)
; CV1-NEXT:    extfs $r3 = $r3, 7, 5
; CV1-NEXT:    extfs $r9 = $r9, 7, 5
; CV1-NEXT:    ;; # (end cycle 85)
; CV1-NEXT:    insf $r3 = $r9, 15, 8
; CV1-NEXT:    insf $r6 = $r4, 15, 8
; CV1-NEXT:    ;; # (end cycle 86)
; CV1-NEXT:    insf $r3 = $r6, 31, 16
; CV1-NEXT:    insf $r8 = $r5, 31, 16
; CV1-NEXT:    ;; # (end cycle 87)
; CV1-NEXT:    insf $r2 = $r7, 63, 32
; CV1-NEXT:    insf $r3 = $r8, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 88)
;
; CV2-LABEL: srs_v32i8:
; CV2:       # %bb.0:
; CV2-NEXT:    srabos $r4 = $r0, 7
; CV2-NEXT:    srabos $r5 = $r1, 7
; CV2-NEXT:    srabos $r6 = $r2, 7
; CV2-NEXT:    srabos $r7 = $r3, 7
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srlbos $r4 = $r4, 3
; CV2-NEXT:    srlbos $r5 = $r5, 3
; CV2-NEXT:    srlbos $r6 = $r6, 3
; CV2-NEXT:    srlbos $r7 = $r7, 3
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    addbo $r0 = $r0, $r4
; CV2-NEXT:    addbo $r1 = $r1, $r5
; CV2-NEXT:    addbo $r2 = $r2, $r6
; CV2-NEXT:    addbo $r3 = $r3, $r7
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    srabos $r0 = $r0, 5
; CV2-NEXT:    srabos $r1 = $r1, 5
; CV2-NEXT:    srabos $r2 = $r2, 5
; CV2-NEXT:    srabos $r3 = $r3, 5
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 3)
  %2 = sdiv <32 x i8> %0, <i8 32, i8 32, i8 32, i8 32, i8 32, i8 32, i8 32, i8 32, i8 32, i8 32, i8 32, i8 32, i8 32, i8 32, i8 32, i8 32, i8 32, i8 32, i8 32, i8 32, i8 32, i8 32, i8 32, i8 32, i8 32, i8 32, i8 32, i8 32, i8 32, i8 32, i8 32, i8 32>
  ret <32 x i8> %2
}

define <8 x i16> @srs_v8i16(<8 x i16> %0) {
; ALL-LABEL: srs_v8i16:
; ALL:       # %bb.0:
; ALL-NEXT:    srahqs $r2 = $r0, 15
; ALL-NEXT:    srahqs $r3 = $r1, 15
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    srlhqs $r2 = $r2, 12
; ALL-NEXT:    srlhqs $r3 = $r3, 12
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    addhq $r0 = $r0, $r2
; ALL-NEXT:    addhq $r1 = $r1, $r3
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    srahqs $r0 = $r0, 4
; ALL-NEXT:    srahqs $r1 = $r1, 4
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 3)
  %2 = sdiv <8 x i16> %0, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  ret <8 x i16> %2
}

define <16 x i16> @srs_v16i16(<16 x i16> %0) {
; CV1-LABEL: srs_v16i16:
; CV1:       # %bb.0:
; CV1-NEXT:    srahqs $r4 = $r0, 15
; CV1-NEXT:    srahqs $r5 = $r1, 15
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srlhqs $r4 = $r4, 12
; CV1-NEXT:    srlhqs $r5 = $r5, 12
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    addhq $r0 = $r0, $r4
; CV1-NEXT:    addhq $r1 = $r1, $r5
; CV1-NEXT:    srahqs $r6 = $r2, 15
; CV1-NEXT:    srahqs $r7 = $r3, 15
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    srlhqs $r4 = $r6, 12
; CV1-NEXT:    srlhqs $r5 = $r7, 12
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srahqs $r0 = $r0, 4
; CV1-NEXT:    srahqs $r1 = $r1, 4
; CV1-NEXT:    addhq $r2 = $r2, $r4
; CV1-NEXT:    addhq $r3 = $r3, $r5
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    srahqs $r2 = $r2, 4
; CV1-NEXT:    srahqs $r3 = $r3, 4
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 5)
;
; CV2-LABEL: srs_v16i16:
; CV2:       # %bb.0:
; CV2-NEXT:    srahqs $r4 = $r0, 15
; CV2-NEXT:    srahqs $r5 = $r1, 15
; CV2-NEXT:    srahqs $r6 = $r2, 15
; CV2-NEXT:    srahqs $r7 = $r3, 15
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srlhqs $r4 = $r4, 12
; CV2-NEXT:    srlhqs $r5 = $r5, 12
; CV2-NEXT:    srlhqs $r6 = $r6, 12
; CV2-NEXT:    srlhqs $r7 = $r7, 12
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    addhq $r0 = $r0, $r4
; CV2-NEXT:    addhq $r1 = $r1, $r5
; CV2-NEXT:    addhq $r2 = $r2, $r6
; CV2-NEXT:    addhq $r3 = $r3, $r7
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    srahqs $r0 = $r0, 4
; CV2-NEXT:    srahqs $r1 = $r1, 4
; CV2-NEXT:    srahqs $r2 = $r2, 4
; CV2-NEXT:    srahqs $r3 = $r3, 4
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 3)
  %2 = sdiv <16 x i16> %0, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  ret <16 x i16> %2
}

define i32 @srs_i322(i32 %0) {
; ALL-LABEL: srs_i322:
; ALL:       # %bb.0:
; ALL-NEXT:    mulwd $r1 = $r0, 0xe070381d
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    srad $r1 = $r1, 32
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    addw $r0 = $r1, $r0
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    sraw $r0 = $r0, 6
; ALL-NEXT:    srlw $r1 = $r0, 31
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    addw $r0 = $r0, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 5)
  %2 = sdiv i32 %0, 73
  ret i32 %2
}
