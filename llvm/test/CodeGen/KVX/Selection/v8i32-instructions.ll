; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mcpu=kv3-1 -O2 -o - %s | FileCheck --check-prefixes=ALL,V1 %s
; RUN: llc -mcpu=kv3-2 -O2 -o - %s | FileCheck --check-prefixes=ALL,V2 %s
; RUN: clang -O2 -march=kv3-1 -c -o /dev/null %s
; RUN: clang -O2 -march=kv3-2 -c -o /dev/null %s

target triple = "kvx-kalray-cos"

define <8 x i32> @test_ret_const() {
; ALL-LABEL: test_ret_const:
; ALL:       # %bb.0:
; ALL-NEXT:    make $r0 = 0x200000001
; ALL-NEXT:    make $r1 = 0x200000001
; ALL-NEXT:    make $r2 = 0x200000001
; ALL-NEXT:    make $r3 = 0x200000001
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  ret <8 x i32> <i32 1, i32 2, i32 1, i32 2, i32 1, i32 2, i32 1, i32 2>
}

define i32 @test_extract_0(<8 x i32> %a) {
; ALL-LABEL: test_extract_0:
; ALL:       # %bb.0:
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %e = extractelement <8 x i32> %a, i32 0
  ret i32 %e
}

define i32 @test_extract_1(<8 x i32> %a) {
; ALL-LABEL: test_extract_1:
; ALL:       # %bb.0:
; ALL-NEXT:    srad $r0 = $r0, 32
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %e = extractelement <8 x i32> %a, i32 1
  ret i32 %e
}

define i32 @test_extract_2(<8 x i32> %a) {
; ALL-LABEL: test_extract_2:
; ALL:       # %bb.0:
; ALL-NEXT:    copyd $r0 = $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %e = extractelement <8 x i32> %a, i32 2
  ret i32 %e
}

define i32 @test_extract_3(<8 x i32> %a) {
; ALL-LABEL: test_extract_3:
; ALL:       # %bb.0:
; ALL-NEXT:    srad $r0 = $r1, 32
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %e = extractelement <8 x i32> %a, i32 3
  ret i32 %e
}

define <8 x i32> @test_fma(<8 x i32> %a, <8 x i32> %b, <8 x i32> %c) {
; V1-LABEL: test_fma:
; V1:       # %bb.0:
; V1-NEXT:    maddwp $r2 = $r6, $r10
; V1-NEXT:    ;; # (end cycle 0)
; V1-NEXT:    maddwp $r3 = $r7, $r11
; V1-NEXT:    ;; # (end cycle 1)
; V1-NEXT:    maddwp $r0 = $r4, $r8
; V1-NEXT:    ;; # (end cycle 2)
; V1-NEXT:    maddwp $r1 = $r5, $r9
; V1-NEXT:    ret
; V1-NEXT:    ;; # (end cycle 3)
;
; V2-LABEL: test_fma:
; V2:       # %bb.0:
; V2-NEXT:    maddwq $r2r3 = $r6r7, $r10r11
; V2-NEXT:    ;; # (end cycle 0)
; V2-NEXT:    maddwq $r0r1 = $r4r5, $r8r9
; V2-NEXT:    ret
; V2-NEXT:    ;; # (end cycle 1)
  %m = mul <8 x i32> %b, %c
  %ad = add <8 x i32> %a, %m
  ret <8 x i32> %ad
}

define <8 x i32> @test_fma_imm(<8 x i32> %a, <8 x i32> %b) {
; V1-LABEL: test_fma_imm:
; V1:       # %bb.0:
; V1-NEXT:    make $r8 = 0x200000007
; V1-NEXT:    make $r9 = 0x300000001
; V1-NEXT:    ;; # (end cycle 0)
; V1-NEXT:    maddwp $r2 = $r6, $r8
; V1-NEXT:    ;; # (end cycle 1)
; V1-NEXT:    maddwp $r3 = $r7, $r9
; V1-NEXT:    ;; # (end cycle 2)
; V1-NEXT:    maddwp $r0 = $r4, $r8
; V1-NEXT:    ;; # (end cycle 3)
; V1-NEXT:    maddwp $r1 = $r5, $r9
; V1-NEXT:    ret
; V1-NEXT:    ;; # (end cycle 4)
;
; V2-LABEL: test_fma_imm:
; V2:       # %bb.0:
; V2-NEXT:    make $r8 = 0x200000007
; V2-NEXT:    make $r9 = 0x300000001
; V2-NEXT:    ;; # (end cycle 0)
; V2-NEXT:    maddwq $r2r3 = $r6r7, $r8r9
; V2-NEXT:    ;; # (end cycle 1)
; V2-NEXT:    maddwq $r0r1 = $r4r5, $r8r9
; V2-NEXT:    ret
; V2-NEXT:    ;; # (end cycle 2)
  %m = mul <8 x i32> <i32 7, i32 2, i32 1, i32 3, i32 7, i32 2, i32 1, i32 3>, %b
  %ad = add <8 x i32> %a, %m
  ret <8 x i32> %ad
}


define <8 x i32> @test_fma_imm_2(<8 x i32> %a, <8 x i32> %b) {
; V1-LABEL: test_fma_imm_2:
; V1:       # %bb.0:
; V1-NEXT:    make $r9 = 0x200000001
; V1-NEXT:    ;; # (end cycle 0)
; V1-NEXT:    maddwp $r3 = $r7, $r9
; V1-NEXT:    copyd $r8 = $r9
; V1-NEXT:    ;; # (end cycle 1)
; V1-NEXT:    maddwp $r1 = $r5, $r9
; V1-NEXT:    ;; # (end cycle 2)
; V1-NEXT:    maddwp $r2 = $r6, $r8
; V1-NEXT:    ;; # (end cycle 3)
; V1-NEXT:    maddwp $r0 = $r4, $r8
; V1-NEXT:    ret
; V1-NEXT:    ;; # (end cycle 4)
;
; V2-LABEL: test_fma_imm_2:
; V2:       # %bb.0:
; V2-NEXT:    make $r9 = 0x200000001
; V2-NEXT:    ;; # (end cycle 0)
; V2-NEXT:    copyd $r8 = $r9
; V2-NEXT:    ;; # (end cycle 1)
; V2-NEXT:    maddwq $r2r3 = $r6r7, $r8r9
; V2-NEXT:    ;; # (end cycle 2)
; V2-NEXT:    maddwq $r0r1 = $r4r5, $r8r9
; V2-NEXT:    ret
; V2-NEXT:    ;; # (end cycle 3)
  %m = mul <8 x i32> <i32 1, i32 2, i32 1, i32 2, i32 1, i32 2, i32 1, i32 2>, %b
  %ad = add <8 x i32> %a, %m
  ret <8 x i32> %ad
}

define i32 @test_extract_i(<8 x i32> %a, i64 %idx) #0 {
; ALL-LABEL: test_extract_i:
; ALL:       # %bb.0:
; ALL-NEXT:    andw $r4 = $r4, 7
; ALL-NEXT:    addd $r12 = $r12, -32
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sq 16[$r12] = $r2r3
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    sq 0[$r12] = $r0r1
; ALL-NEXT:    addd $r0 = $r12, 0
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    lwz.xs $r0 = $r4[$r0]
; ALL-NEXT:    addd $r12 = $r12, 32
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 3)
  %e = extractelement <8 x i32> %a, i64 %idx
  ret i32 %e
}

define <8 x i32> @test_add(<8 x i32> %a, <8 x i32> %b) {
; ALL-LABEL: test_add:
; ALL:       # %bb.0:
; ALL-NEXT:    addwp $r0 = $r0, $r4
; ALL-NEXT:    addwp $r1 = $r1, $r5
; ALL-NEXT:    addwp $r2 = $r2, $r6
; ALL-NEXT:    addwp $r3 = $r3, $r7
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %r = add <8 x i32> %a, %b
  ret <8 x i32> %r
}

define <8 x i32> @test_add_imm_0(<8 x i32> %a) {
; ALL-LABEL: test_add_imm_0:
; ALL:       # %bb.0:
; ALL-NEXT:    make $r5 = 0x200000001
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    addwp $r1 = $r1, $r5
; ALL-NEXT:    addwp $r3 = $r3, $r5
; ALL-NEXT:    copyd $r4 = $r5
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    addwp $r0 = $r0, $r4
; ALL-NEXT:    addwp $r2 = $r2, $r4
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 2)
  %r = add <8 x i32> <i32 1, i32 2, i32 1, i32 2, i32 1, i32 2, i32 1, i32 2>, %a
  ret <8 x i32> %r
}

define <8 x i32> @test_add_imm_1(<8 x i32> %a) {
; ALL-LABEL: test_add_imm_1:
; ALL:       # %bb.0:
; ALL-NEXT:    make $r5 = 0x200000001
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    addwp $r1 = $r1, $r5
; ALL-NEXT:    addwp $r3 = $r3, $r5
; ALL-NEXT:    copyd $r4 = $r5
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    addwp $r0 = $r0, $r4
; ALL-NEXT:    addwp $r2 = $r2, $r4
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 2)
  %r = add <8 x i32> %a, <i32 1, i32 2, i32 1, i32 2, i32 1, i32 2, i32 1, i32 2>
  ret <8 x i32> %r
}

define <8 x i32> @test_sub(<8 x i32> %a, <8 x i32> %b) {
; ALL-LABEL: test_sub:
; ALL:       # %bb.0:
; ALL-NEXT:    sbfwp $r0 = $r4, $r0
; ALL-NEXT:    sbfwp $r1 = $r5, $r1
; ALL-NEXT:    sbfwp $r2 = $r6, $r2
; ALL-NEXT:    sbfwp $r3 = $r7, $r3
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %r = sub <8 x i32> %a, %b
  ret <8 x i32> %r
}

define <8 x i32> @test_sub_imm(<8 x i32> %a) {
; ALL-LABEL: test_sub_imm:
; ALL:       # %bb.0:
; ALL-NEXT:    make $r5 = 0x200000001
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sbfwp $r1 = $r5, $r1
; ALL-NEXT:    sbfwp $r3 = $r5, $r3
; ALL-NEXT:    copyd $r4 = $r5
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    sbfwp $r0 = $r4, $r0
; ALL-NEXT:    sbfwp $r2 = $r4, $r2
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 2)
  %r = sub <8 x i32> %a, <i32 1, i32 2, i32 1, i32 2, i32 1, i32 2, i32 1, i32 2>
  ret <8 x i32> %r
}

define <8 x i32> @test_sub_fromimm(<8 x i32> %a) {
; ALL-LABEL: test_sub_fromimm:
; ALL:       # %bb.0:
; ALL-NEXT:    make $r5 = 0x200000001
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sbfwp $r1 = $r1, $r5
; ALL-NEXT:    sbfwp $r3 = $r3, $r5
; ALL-NEXT:    copyd $r4 = $r5
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    sbfwp $r0 = $r0, $r4
; ALL-NEXT:    sbfwp $r2 = $r2, $r4
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 2)
  %r = sub <8 x i32> <i32 1, i32 2, i32 1, i32 2, i32 1, i32 2, i32 1, i32 2>, %a
  ret <8 x i32> %r
}

define <8 x i32> @test_neg(<8 x i32> %a) {
; ALL-LABEL: test_neg:
; ALL:       # %bb.0:
; ALL-NEXT:    make $r4 = 0
; ALL-NEXT:    make $r5 = 0
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sbfwp $r0 = $r0, $r4
; ALL-NEXT:    sbfwp $r1 = $r1, $r5
; ALL-NEXT:    sbfwp $r2 = $r2, $r4
; ALL-NEXT:    sbfwp $r3 = $r3, $r5
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 1)
  %r = sub <8 x i32> zeroinitializer, %a
  ret <8 x i32> %r
}

define <8 x i32> @test_mul(<8 x i32> %a, <8 x i32> %b) {
; ALL-LABEL: test_mul:
; ALL:       # %bb.0:
; ALL-NEXT:    mulwp $r0 = $r0, $r4
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    mulwp $r1 = $r1, $r5
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    mulwp $r2 = $r2, $r6
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    mulwp $r3 = $r3, $r7
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 3)
  %r = mul <8 x i32> %a, %b
  ret <8 x i32> %r
}

define <8 x i32> @test_mul_2(<8 x i32> %a, <8 x i32> %b, <8 x i32> %c) {
; ALL-LABEL: test_mul_2:
; ALL:       # %bb.0:
; ALL-NEXT:    mulwq $r2r3 = $r2r3, $r6r7
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    mulwq $r0r1 = $r0r1, $r4r5
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    mulwq $r2r3 = $r2r3, $r10r11
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    mulwq $r0r1 = $r0r1, $r8r9
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 3)
  %r = mul <8 x i32> %a, %b
  %r1 = mul <8 x i32> %r, %c
  ret <8 x i32> %r1
}

define <8 x i32> @test_div(<8 x i32> %a, <8 x i32> %b) #0 {
; ALL-LABEL: test_div:
; ALL:       # %bb.0:
; ALL-NEXT:    addd $r12 = $r12, -32
; ALL-NEXT:    get $r16 = $ra
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sd 24[$r12] = $r16
; ALL-NEXT:    call __divv8si3
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    ld $r16 = 24[$r12]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    set $ra = $r16
; ALL-NEXT:    addd $r12 = $r12, 32
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %r = sdiv <8 x i32> %a, %b
  ret <8 x i32> %r
}

define <8 x i32> @test_rem(<8 x i32> %a, <8 x i32> %b) #0 {
; ALL-LABEL: test_rem:
; ALL:       # %bb.0:
; ALL-NEXT:    addd $r12 = $r12, -32
; ALL-NEXT:    get $r16 = $ra
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sd 24[$r12] = $r16
; ALL-NEXT:    call __modv8si3
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    ld $r16 = 24[$r12]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    set $ra = $r16
; ALL-NEXT:    addd $r12 = $r12, 32
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %r = srem <8 x i32> %a, %b
  ret <8 x i32> %r
}

define void @test_ldst_v4i32(ptr %a, ptr %b) {
; ALL-LABEL: test_ldst_v4i32:
; ALL:       # %bb.0:
; ALL-NEXT:    lo $r4r5r6r7 = 0[$r0]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    so 0[$r1] = $r4r5r6r7
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 2)
  %t1 = load <8 x i32>, ptr %a
  store <8 x i32> %t1, ptr %b, align 16
  ret void
}

declare <8 x i32> @test_callee(<8 x i32> %a, <8 x i32> %b)

define <8 x i32> @test_call(<8 x i32> %a, <8 x i32> %b) #0 {
; ALL-LABEL: test_call:
; ALL:       # %bb.0:
; ALL-NEXT:    addd $r12 = $r12, -32
; ALL-NEXT:    get $r16 = $ra
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sd 24[$r12] = $r16
; ALL-NEXT:    call test_callee
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    ld $r16 = 24[$r12]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    set $ra = $r16
; ALL-NEXT:    addd $r12 = $r12, 32
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %r = call <8 x i32> @test_callee(<8 x i32> %a, <8 x i32> %b)
  ret <8 x i32> %r
}

define <8 x i32> @test_call_flipped(<8 x i32> %a, <8 x i32> %b) #0 {
; ALL-LABEL: test_call_flipped:
; ALL:       # %bb.0:
; ALL-NEXT:    addd $r12 = $r12, -32
; ALL-NEXT:    get $r16 = $ra
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sd 24[$r12] = $r16
; ALL-NEXT:    copyd $r0 = $r4
; ALL-NEXT:    copyd $r4 = $r0
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    copyd $r1 = $r5
; ALL-NEXT:    copyd $r2 = $r6
; ALL-NEXT:    copyd $r5 = $r1
; ALL-NEXT:    copyd $r6 = $r2
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    copyd $r3 = $r7
; ALL-NEXT:    copyd $r7 = $r3
; ALL-NEXT:    call test_callee
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    ld $r16 = 24[$r12]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    set $ra = $r16
; ALL-NEXT:    addd $r12 = $r12, 32
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %r = call <8 x i32> @test_callee(<8 x i32> %b, <8 x i32> %a)
  ret <8 x i32> %r
}

; Can perform swap in a single bundle
define <8 x i32> @test_tailcall_flipped(<8 x i32> %a, <8 x i32> %b) {
; ALL-LABEL: test_tailcall_flipped:
; ALL:       # %bb.0:
; ALL-NEXT:    copyd $r0 = $r4
; ALL-NEXT:    copyd $r1 = $r5
; ALL-NEXT:    copyd $r4 = $r0
; ALL-NEXT:    copyd $r5 = $r1
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    copyd $r2 = $r6
; ALL-NEXT:    copyd $r3 = $r7
; ALL-NEXT:    copyd $r6 = $r2
; ALL-NEXT:    copyd $r7 = $r3
; ALL-NEXT:    goto test_callee
; ALL-NEXT:    ;; # (end cycle 1)
  %r = tail call <8 x i32> @test_callee(<8 x i32> %b, <8 x i32> %a)
  ret <8 x i32> %r
}

define <8 x i32> @test_select(<8 x i32> %a, <8 x i32> %b, i1 zeroext %c) {
; V1-LABEL: test_select:
; V1:       # %bb.0:
; V1-NEXT:    cmoved.even $r8 ? $r2 = $r6
; V1-NEXT:    cmoved.even $r8 ? $r3 = $r7
; V1-NEXT:    ;; # (end cycle 0)
; V1-NEXT:    cmoved.even $r8 ? $r0 = $r4
; V1-NEXT:    cmoved.even $r8 ? $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;; # (end cycle 1)
;
; V2-LABEL: test_select:
; V2:       # %bb.0:
; V2-NEXT:    cmoved.even $r8 ? $r0 = $r4
; V2-NEXT:    cmoved.even $r8 ? $r1 = $r5
; V2-NEXT:    cmoved.even $r8 ? $r2 = $r6
; V2-NEXT:    cmoved.even $r8 ? $r3 = $r7
; V2-NEXT:    ret
; V2-NEXT:    ;; # (end cycle 0)
  %r = select i1 %c, <8 x i32> %a, <8 x i32> %b
  ret <8 x i32> %r
}

define <8 x i32> @test_select_cc(<8 x i32> %a, <8 x i32> %b, <8 x i32> %c, <8 x i32> %d) {
; ALL-LABEL: test_select_cc:
; ALL:       # %bb.0:
; ALL-NEXT:    ld $r16 = 16[$r12]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    ld $r17 = 24[$r12]
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    ld $r32 = 0[$r12]
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    compnwp.lt $r10 = $r10, $r16
; ALL-NEXT:    ld $r33 = 8[$r12]
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    cmovewp.even $r10 ? $r2 = $r6
; ALL-NEXT:    compnwp.lt $r11 = $r11, $r17
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    cmovewp.even $r11 ? $r3 = $r7
; ALL-NEXT:    compnwp.lt $r8 = $r8, $r32
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    cmovewp.even $r8 ? $r0 = $r4
; ALL-NEXT:    compnwp.lt $r9 = $r9, $r33
; ALL-NEXT:    ;; # (end cycle 6)
; ALL-NEXT:    cmovewp.even $r9 ? $r1 = $r5
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 7)
  %cc = icmp slt <8 x i32> %c, %d
  %r = select <8 x i1> %cc, <8 x i32> %a, <8 x i32> %b
  ret <8 x i32> %r
}

define <8 x i1> @test_icmp_ule(<8 x i32> %a, <8 x i32> %b) {
; V1-LABEL: test_icmp_ule:
; V1:       # %bb.0:
; V1-NEXT:    compnwp.leu $r0 = $r0, $r4
; V1-NEXT:    compnwp.leu $r1 = $r1, $r5
; V1-NEXT:    compnwp.leu $r2 = $r2, $r6
; V1-NEXT:    compnwp.leu $r3 = $r3, $r7
; V1-NEXT:    ;; # (end cycle 0)
; V1-NEXT:    sbmm8 $r2 = $r2, 0x1001
; V1-NEXT:    sbmm8 $r3 = $r3, 0x1001
; V1-NEXT:    ;; # (end cycle 1)
; V1-NEXT:    sbmm8 $r0 = $r0, 0x1001
; V1-NEXT:    sbmm8 $r1 = $r1, 0x1001
; V1-NEXT:    ;; # (end cycle 2)
; V1-NEXT:    insf $r0 = $r1, 31, 16
; V1-NEXT:    insf $r2 = $r3, 31, 16
; V1-NEXT:    ;; # (end cycle 3)
; V1-NEXT:    insf $r0 = $r2, 63, 32
; V1-NEXT:    ret
; V1-NEXT:    ;; # (end cycle 4)
;
; V2-LABEL: test_icmp_ule:
; V2:       # %bb.0:
; V2-NEXT:    compnwp.leu $r0 = $r0, $r4
; V2-NEXT:    compnwp.leu $r1 = $r1, $r5
; V2-NEXT:    compnwp.leu $r2 = $r2, $r6
; V2-NEXT:    compnwp.leu $r3 = $r3, $r7
; V2-NEXT:    ;; # (end cycle 0)
; V2-NEXT:    sbmm8 $r0 = $r0, 0x1001
; V2-NEXT:    sbmm8 $r1 = $r1, 0x1001
; V2-NEXT:    sbmm8 $r2 = $r2, 0x1001
; V2-NEXT:    sbmm8 $r3 = $r3, 0x1001
; V2-NEXT:    ;; # (end cycle 1)
; V2-NEXT:    insf $r0 = $r1, 31, 16
; V2-NEXT:    insf $r2 = $r3, 31, 16
; V2-NEXT:    ;; # (end cycle 2)
; V2-NEXT:    insf $r0 = $r2, 63, 32
; V2-NEXT:    ret
; V2-NEXT:    ;; # (end cycle 3)
  %r = icmp ule <8 x i32> %a, %b
  ret <8 x i1> %r
}

define <8 x i1> @test_icmp_slt(<8 x i32> %a, <8 x i32> %b) {
; V1-LABEL: test_icmp_slt:
; V1:       # %bb.0:
; V1-NEXT:    compnwp.lt $r0 = $r0, $r4
; V1-NEXT:    compnwp.lt $r1 = $r1, $r5
; V1-NEXT:    compnwp.lt $r2 = $r2, $r6
; V1-NEXT:    compnwp.lt $r3 = $r3, $r7
; V1-NEXT:    ;; # (end cycle 0)
; V1-NEXT:    sbmm8 $r2 = $r2, 0x1001
; V1-NEXT:    sbmm8 $r3 = $r3, 0x1001
; V1-NEXT:    ;; # (end cycle 1)
; V1-NEXT:    sbmm8 $r0 = $r0, 0x1001
; V1-NEXT:    sbmm8 $r1 = $r1, 0x1001
; V1-NEXT:    ;; # (end cycle 2)
; V1-NEXT:    insf $r0 = $r1, 31, 16
; V1-NEXT:    insf $r2 = $r3, 31, 16
; V1-NEXT:    ;; # (end cycle 3)
; V1-NEXT:    insf $r0 = $r2, 63, 32
; V1-NEXT:    ret
; V1-NEXT:    ;; # (end cycle 4)
;
; V2-LABEL: test_icmp_slt:
; V2:       # %bb.0:
; V2-NEXT:    compnwp.lt $r0 = $r0, $r4
; V2-NEXT:    compnwp.lt $r1 = $r1, $r5
; V2-NEXT:    compnwp.lt $r2 = $r2, $r6
; V2-NEXT:    compnwp.lt $r3 = $r3, $r7
; V2-NEXT:    ;; # (end cycle 0)
; V2-NEXT:    sbmm8 $r0 = $r0, 0x1001
; V2-NEXT:    sbmm8 $r1 = $r1, 0x1001
; V2-NEXT:    sbmm8 $r2 = $r2, 0x1001
; V2-NEXT:    sbmm8 $r3 = $r3, 0x1001
; V2-NEXT:    ;; # (end cycle 1)
; V2-NEXT:    insf $r0 = $r1, 31, 16
; V2-NEXT:    insf $r2 = $r3, 31, 16
; V2-NEXT:    ;; # (end cycle 2)
; V2-NEXT:    insf $r0 = $r2, 63, 32
; V2-NEXT:    ret
; V2-NEXT:    ;; # (end cycle 3)
  %r = icmp slt <8 x i32> %a, %b
  ret <8 x i1> %r
}

define <8 x i1> @test_icmp_ugt(<8 x i32> %a, <8 x i32> %b) {
; V1-LABEL: test_icmp_ugt:
; V1:       # %bb.0:
; V1-NEXT:    compnwp.gtu $r0 = $r0, $r4
; V1-NEXT:    compnwp.gtu $r1 = $r1, $r5
; V1-NEXT:    compnwp.gtu $r2 = $r2, $r6
; V1-NEXT:    compnwp.gtu $r3 = $r3, $r7
; V1-NEXT:    ;; # (end cycle 0)
; V1-NEXT:    sbmm8 $r2 = $r2, 0x1001
; V1-NEXT:    sbmm8 $r3 = $r3, 0x1001
; V1-NEXT:    ;; # (end cycle 1)
; V1-NEXT:    sbmm8 $r0 = $r0, 0x1001
; V1-NEXT:    sbmm8 $r1 = $r1, 0x1001
; V1-NEXT:    ;; # (end cycle 2)
; V1-NEXT:    insf $r0 = $r1, 31, 16
; V1-NEXT:    insf $r2 = $r3, 31, 16
; V1-NEXT:    ;; # (end cycle 3)
; V1-NEXT:    insf $r0 = $r2, 63, 32
; V1-NEXT:    ret
; V1-NEXT:    ;; # (end cycle 4)
;
; V2-LABEL: test_icmp_ugt:
; V2:       # %bb.0:
; V2-NEXT:    compnwp.gtu $r0 = $r0, $r4
; V2-NEXT:    compnwp.gtu $r1 = $r1, $r5
; V2-NEXT:    compnwp.gtu $r2 = $r2, $r6
; V2-NEXT:    compnwp.gtu $r3 = $r3, $r7
; V2-NEXT:    ;; # (end cycle 0)
; V2-NEXT:    sbmm8 $r0 = $r0, 0x1001
; V2-NEXT:    sbmm8 $r1 = $r1, 0x1001
; V2-NEXT:    sbmm8 $r2 = $r2, 0x1001
; V2-NEXT:    sbmm8 $r3 = $r3, 0x1001
; V2-NEXT:    ;; # (end cycle 1)
; V2-NEXT:    insf $r0 = $r1, 31, 16
; V2-NEXT:    insf $r2 = $r3, 31, 16
; V2-NEXT:    ;; # (end cycle 2)
; V2-NEXT:    insf $r0 = $r2, 63, 32
; V2-NEXT:    ret
; V2-NEXT:    ;; # (end cycle 3)
  %r = icmp ugt <8 x i32> %a, %b
  ret <8 x i1> %r
}

define <8 x i1> @test_icmp_uge(<8 x i32> %a, <8 x i32> %b) {
; V1-LABEL: test_icmp_uge:
; V1:       # %bb.0:
; V1-NEXT:    compnwp.geu $r0 = $r0, $r4
; V1-NEXT:    compnwp.geu $r1 = $r1, $r5
; V1-NEXT:    compnwp.geu $r2 = $r2, $r6
; V1-NEXT:    compnwp.geu $r3 = $r3, $r7
; V1-NEXT:    ;; # (end cycle 0)
; V1-NEXT:    sbmm8 $r2 = $r2, 0x1001
; V1-NEXT:    sbmm8 $r3 = $r3, 0x1001
; V1-NEXT:    ;; # (end cycle 1)
; V1-NEXT:    sbmm8 $r0 = $r0, 0x1001
; V1-NEXT:    sbmm8 $r1 = $r1, 0x1001
; V1-NEXT:    ;; # (end cycle 2)
; V1-NEXT:    insf $r0 = $r1, 31, 16
; V1-NEXT:    insf $r2 = $r3, 31, 16
; V1-NEXT:    ;; # (end cycle 3)
; V1-NEXT:    insf $r0 = $r2, 63, 32
; V1-NEXT:    ret
; V1-NEXT:    ;; # (end cycle 4)
;
; V2-LABEL: test_icmp_uge:
; V2:       # %bb.0:
; V2-NEXT:    compnwp.geu $r0 = $r0, $r4
; V2-NEXT:    compnwp.geu $r1 = $r1, $r5
; V2-NEXT:    compnwp.geu $r2 = $r2, $r6
; V2-NEXT:    compnwp.geu $r3 = $r3, $r7
; V2-NEXT:    ;; # (end cycle 0)
; V2-NEXT:    sbmm8 $r0 = $r0, 0x1001
; V2-NEXT:    sbmm8 $r1 = $r1, 0x1001
; V2-NEXT:    sbmm8 $r2 = $r2, 0x1001
; V2-NEXT:    sbmm8 $r3 = $r3, 0x1001
; V2-NEXT:    ;; # (end cycle 1)
; V2-NEXT:    insf $r0 = $r1, 31, 16
; V2-NEXT:    insf $r2 = $r3, 31, 16
; V2-NEXT:    ;; # (end cycle 2)
; V2-NEXT:    insf $r0 = $r2, 63, 32
; V2-NEXT:    ret
; V2-NEXT:    ;; # (end cycle 3)
  %r = icmp uge <8 x i32> %a, %b
  ret <8 x i1> %r
}

define <8 x i1> @test_icmp_ult(<8 x i32> %a, <8 x i32> %b) {
; V1-LABEL: test_icmp_ult:
; V1:       # %bb.0:
; V1-NEXT:    compnwp.ltu $r0 = $r0, $r4
; V1-NEXT:    compnwp.ltu $r1 = $r1, $r5
; V1-NEXT:    compnwp.ltu $r2 = $r2, $r6
; V1-NEXT:    compnwp.ltu $r3 = $r3, $r7
; V1-NEXT:    ;; # (end cycle 0)
; V1-NEXT:    sbmm8 $r2 = $r2, 0x1001
; V1-NEXT:    sbmm8 $r3 = $r3, 0x1001
; V1-NEXT:    ;; # (end cycle 1)
; V1-NEXT:    sbmm8 $r0 = $r0, 0x1001
; V1-NEXT:    sbmm8 $r1 = $r1, 0x1001
; V1-NEXT:    ;; # (end cycle 2)
; V1-NEXT:    insf $r0 = $r1, 31, 16
; V1-NEXT:    insf $r2 = $r3, 31, 16
; V1-NEXT:    ;; # (end cycle 3)
; V1-NEXT:    insf $r0 = $r2, 63, 32
; V1-NEXT:    ret
; V1-NEXT:    ;; # (end cycle 4)
;
; V2-LABEL: test_icmp_ult:
; V2:       # %bb.0:
; V2-NEXT:    compnwp.ltu $r0 = $r0, $r4
; V2-NEXT:    compnwp.ltu $r1 = $r1, $r5
; V2-NEXT:    compnwp.ltu $r2 = $r2, $r6
; V2-NEXT:    compnwp.ltu $r3 = $r3, $r7
; V2-NEXT:    ;; # (end cycle 0)
; V2-NEXT:    sbmm8 $r0 = $r0, 0x1001
; V2-NEXT:    sbmm8 $r1 = $r1, 0x1001
; V2-NEXT:    sbmm8 $r2 = $r2, 0x1001
; V2-NEXT:    sbmm8 $r3 = $r3, 0x1001
; V2-NEXT:    ;; # (end cycle 1)
; V2-NEXT:    insf $r0 = $r1, 31, 16
; V2-NEXT:    insf $r2 = $r3, 31, 16
; V2-NEXT:    ;; # (end cycle 2)
; V2-NEXT:    insf $r0 = $r2, 63, 32
; V2-NEXT:    ret
; V2-NEXT:    ;; # (end cycle 3)
  %r = icmp ult <8 x i32> %a, %b
  ret <8 x i1> %r
}

define <8 x i8> @trunc_to_v4i8(<8 x i32> %a) {
; V1-LABEL: trunc_to_v4i8:
; V1:       # %bb.0:
; V1-NEXT:    sbmm8 $r2 = $r2, 0x1001
; V1-NEXT:    sbmm8 $r3 = $r3, 0x1001
; V1-NEXT:    ;; # (end cycle 0)
; V1-NEXT:    sbmm8 $r0 = $r0, 0x1001
; V1-NEXT:    sbmm8 $r1 = $r1, 0x1001
; V1-NEXT:    ;; # (end cycle 1)
; V1-NEXT:    insf $r0 = $r1, 31, 16
; V1-NEXT:    insf $r2 = $r3, 31, 16
; V1-NEXT:    ;; # (end cycle 2)
; V1-NEXT:    insf $r0 = $r2, 63, 32
; V1-NEXT:    ret
; V1-NEXT:    ;; # (end cycle 3)
;
; V2-LABEL: trunc_to_v4i8:
; V2:       # %bb.0:
; V2-NEXT:    sbmm8 $r0 = $r0, 0x1001
; V2-NEXT:    sbmm8 $r1 = $r1, 0x1001
; V2-NEXT:    sbmm8 $r2 = $r2, 0x1001
; V2-NEXT:    sbmm8 $r3 = $r3, 0x1001
; V2-NEXT:    ;; # (end cycle 0)
; V2-NEXT:    insf $r0 = $r1, 31, 16
; V2-NEXT:    insf $r2 = $r3, 31, 16
; V2-NEXT:    ;; # (end cycle 1)
; V2-NEXT:    insf $r0 = $r2, 63, 32
; V2-NEXT:    ret
; V2-NEXT:    ;; # (end cycle 2)
  %r = trunc <8 x i32> %a to <8 x i8>
  ret <8 x i8> %r
}

define <8 x i8> @trunc_to_v4i8_buildvector(i32 %arg1, i32 %arg2, i32 %arg3, i32 %arg4) {
; ALL-LABEL: trunc_to_v4i8_buildvector:
; ALL:       # %bb.0:
; ALL-NEXT:    insf $r0 = $r1, 15, 8
; ALL-NEXT:    insf $r2 = $r3, 15, 8
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    insf $r0 = $r2, 31, 16
; ALL-NEXT:    insf $r1 = $r0, 15, 8
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    insf $r1 = $r1, 31, 16
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    insf $r0 = $r1, 63, 32
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 3)
  %v0 = insertelement <8 x i32> undef, i32 %arg1, i32 0
  %v1 = insertelement <8 x i32> %v0, i32 %arg2, i32 1
  %v2 = insertelement <8 x i32> %v1, i32 %arg3, i32 2
  %v3 = insertelement <8 x i32> %v2, i32 %arg4, i32 3
  %conv = trunc <8 x i32> %v3 to <8 x i8>
  ret <8 x i8> %conv
}

define <8 x i32> @concat(<4 x i32> %a, <4 x i32> %b){
; ALL-LABEL: concat:
; ALL:       # %bb.0:
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %v = shufflevector <4 x i32> %a, <4 x i32> %b, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i32> %v
}

declare <8 x i32> @llvm.abs.v4i32(<8 x i32>, i1) #0

define <8 x i32> @test_abs(<8 x i32> %a) {
; V1-LABEL: test_abs:
; V1:       # %bb.0:
; V1-NEXT:    abswp $r2 = $r2
; V1-NEXT:    abswp $r3 = $r3
; V1-NEXT:    ;; # (end cycle 0)
; V1-NEXT:    abswp $r0 = $r0
; V1-NEXT:    abswp $r1 = $r1
; V1-NEXT:    ret
; V1-NEXT:    ;; # (end cycle 1)
;
; V2-LABEL: test_abs:
; V2:       # %bb.0:
; V2-NEXT:    abswp $r0 = $r0
; V2-NEXT:    abswp $r1 = $r1
; V2-NEXT:    abswp $r2 = $r2
; V2-NEXT:    abswp $r3 = $r3
; V2-NEXT:    ;; # (end cycle 0)
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %r = call <8 x i32> @llvm.abs.v4i32(<8 x i32> %a, i1 false)
  ret <8 x i32> %r
}

define <8 x i32> @test_insertelement0(<8 x i32> %a, i32 %x) {
; ALL-LABEL: test_insertelement0:
; ALL:       # %bb.0:
; ALL-NEXT:    insf $r0 = $r4, 31, 0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %i = insertelement <8 x i32> %a, i32 %x, i64 0
  ret <8 x i32> %i
}

define <8 x i32> @test_insertelement1(<8 x i32> %a, i32 %x) {
; ALL-LABEL: test_insertelement1:
; ALL:       # %bb.0:
; ALL-NEXT:    insf $r0 = $r4, 63, 32
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %i = insertelement <8 x i32> %a, i32 %x, i64 1
  ret <8 x i32> %i
}

define <8 x i32> @test_insertelement2(<8 x i32> %a, i32 %x) {
; ALL-LABEL: test_insertelement2:
; ALL:       # %bb.0:
; ALL-NEXT:    insf $r1 = $r4, 31, 0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %i = insertelement <8 x i32> %a, i32 %x, i64 2
  ret <8 x i32> %i
}

define <8 x i32> @test_insertelement3(<8 x i32> %a, i32 %x) {
; ALL-LABEL: test_insertelement3:
; ALL:       # %bb.0:
; ALL-NEXT:    insf $r1 = $r4, 63, 32
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %i = insertelement <8 x i32> %a, i32 %x, i64 3
  ret <8 x i32> %i
}

define <8 x i32> @test_insertelement(<8 x i32> %a, i32 %x, i64 %p) {
; ALL-LABEL: test_insertelement:
; ALL:       # %bb.0:
; ALL-NEXT:    andw $r5 = $r5, 7
; ALL-NEXT:    addd $r12 = $r12, -32
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sq 16[$r12] = $r2r3
; ALL-NEXT:    addd $r2 = $r12, 0
; ALL-NEXT:    muluwd $r5 = $r5, 4
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    sq 0[$r12] = $r0r1
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    sw $r5[$r2] = $r4
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    lq $r2r3 = 16[$r12]
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    lq $r0r1 = 0[$r12]
; ALL-NEXT:    addd $r12 = $r12, 32
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 5)
  %i = insertelement <8 x i32> %a, i32 %x, i64 %p
  ret <8 x i32> %i
}

define <8 x i32> @mulsub(<8 x i32> %a, <8 x i32> %b, <8 x i32> %c) {
; V1-LABEL: mulsub:
; V1:       # %bb.0:
; V1-NEXT:    msbfwp $r2 = $r6, $r10
; V1-NEXT:    ;; # (end cycle 0)
; V1-NEXT:    msbfwp $r3 = $r7, $r11
; V1-NEXT:    ;; # (end cycle 1)
; V1-NEXT:    msbfwp $r0 = $r4, $r8
; V1-NEXT:    ;; # (end cycle 2)
; V1-NEXT:    msbfwp $r1 = $r5, $r9
; V1-NEXT:    ret
; V1-NEXT:    ;; # (end cycle 3)
;
; V2-LABEL: mulsub:
; V2:       # %bb.0:
; V2-NEXT:    msbfwq $r2r3 = $r6r7, $r10r11
; V2-NEXT:    ;; # (end cycle 0)
; V2-NEXT:    msbfwq $r0r1 = $r4r5, $r8r9
; V2-NEXT:    ret
; V2-NEXT:    ;; # (end cycle 1)
  %mul = mul <8 x i32> %b, %c
  %sub = sub <8 x i32> %a, %mul
  ret <8 x i32> %sub
}

define <8 x i32> @vnot(<8 x i32> %a) {
; ALL-LABEL: vnot:
; ALL:       # %bb.0:
; ALL-NEXT:    notd $r0 = $r0
; ALL-NEXT:    notd $r1 = $r1
; ALL-NEXT:    notd $r2 = $r2
; ALL-NEXT:    notd $r3 = $r3
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %vnot = xor <8 x i32> %a, <i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1>
  ret <8 x i32> %vnot
}

define <8 x i32> @lnand(<8 x i32> %0, <8 x i32> %1) {
; V1-LABEL: lnand:
; V1:       # %bb.0:
; V1-NEXT:    make $r8 = 0
; V1-NEXT:    make $r9 = 0
; V1-NEXT:    ;; # (end cycle 0)
; V1-NEXT:    compnwp.eq $r0 = $r0, $r8
; V1-NEXT:    compnwp.eq $r1 = $r1, $r9
; V1-NEXT:    compnwp.eq $r2 = $r2, $r8
; V1-NEXT:    compnwp.eq $r3 = $r3, $r9
; V1-NEXT:    ;; # (end cycle 1)
; V1-NEXT:    compnwp.eq $r4 = $r4, $r8
; V1-NEXT:    compnwp.eq $r5 = $r5, $r9
; V1-NEXT:    compnwp.eq $r6 = $r6, $r8
; V1-NEXT:    compnwp.eq $r7 = $r7, $r9
; V1-NEXT:    ;; # (end cycle 2)
; V1-NEXT:    iord $r0 = $r4, $r0
; V1-NEXT:    iord $r1 = $r5, $r1
; V1-NEXT:    iord $r2 = $r6, $r2
; V1-NEXT:    iord $r3 = $r7, $r3
; V1-NEXT:    ;; # (end cycle 3)
; V1-NEXT:    make $r4 = 0x100000001
; V1-NEXT:    make $r5 = 0x100000001
; V1-NEXT:    ;; # (end cycle 4)
; V1-NEXT:    andd $r0 = $r0, $r4
; V1-NEXT:    andd $r1 = $r1, $r5
; V1-NEXT:    andd $r2 = $r2, $r4
; V1-NEXT:    andd $r3 = $r3, $r5
; V1-NEXT:    ret
; V1-NEXT:    ;; # (end cycle 5)
;
; V2-LABEL: lnand:
; V2:       # %bb.0:
; V2-NEXT:    make $r8 = 0
; V2-NEXT:    make $r9 = 0
; V2-NEXT:    ;; # (end cycle 0)
; V2-NEXT:    compnwp.eq $r0 = $r0, $r8
; V2-NEXT:    compnwp.eq $r1 = $r1, $r9
; V2-NEXT:    compnwp.eq $r2 = $r2, $r8
; V2-NEXT:    compnwp.eq $r3 = $r3, $r9
; V2-NEXT:    ;; # (end cycle 1)
; V2-NEXT:    compnwp.eq $r4 = $r4, $r8
; V2-NEXT:    compnwp.eq $r5 = $r5, $r9
; V2-NEXT:    compnwp.eq $r6 = $r6, $r8
; V2-NEXT:    compnwp.eq $r7 = $r7, $r9
; V2-NEXT:    ;; # (end cycle 2)
; V2-NEXT:    sbmm8 $r0 = $r0, 0x1001
; V2-NEXT:    sbmm8 $r1 = $r1, 0x1001
; V2-NEXT:    sbmm8 $r2 = $r2, 0x1001
; V2-NEXT:    sbmm8 $r3 = $r3, 0x1001
; V2-NEXT:    ;; # (end cycle 3)
; V2-NEXT:    sbmm8 $r4 = $r4, 0x1001
; V2-NEXT:    sbmm8 $r5 = $r5, 0x1001
; V2-NEXT:    sbmm8 $r6 = $r6, 0x1001
; V2-NEXT:    sbmm8 $r7 = $r7, 0x1001
; V2-NEXT:    ;; # (end cycle 4)
; V2-NEXT:    insf $r0 = $r1, 31, 16
; V2-NEXT:    insf $r2 = $r3, 31, 16
; V2-NEXT:    insf $r4 = $r5, 31, 16
; V2-NEXT:    insf $r6 = $r7, 31, 16
; V2-NEXT:    ;; # (end cycle 5)
; V2-NEXT:    insf $r0 = $r2, 63, 32
; V2-NEXT:    insf $r4 = $r6, 63, 32
; V2-NEXT:    ;; # (end cycle 6)
; V2-NEXT:    iord $r0 = $r4, $r0
; V2-NEXT:    ;; # (end cycle 7)
; V2-NEXT:    srld $r1 = $r0, 56
; V2-NEXT:    extfz $r2 = $r0, 55, 48
; V2-NEXT:    extfz $r3 = $r0, 47, 40
; V2-NEXT:    extfz $r4 = $r0, 39, 32
; V2-NEXT:    ;; # (end cycle 8)
; V2-NEXT:    srlw $r1 = $r0, 24
; V2-NEXT:    insf $r2 = $r1, 15, 8
; V2-NEXT:    extfz $r3 = $r0, 23, 16
; V2-NEXT:    insf $r4 = $r3, 15, 8
; V2-NEXT:    ;; # (end cycle 9)
; V2-NEXT:    zxbd $r0 = $r0
; V2-NEXT:    insf $r3 = $r1, 15, 8
; V2-NEXT:    insf $r4 = $r2, 31, 16
; V2-NEXT:    extfz $r5 = $r0, 15, 8
; V2-NEXT:    ;; # (end cycle 10)
; V2-NEXT:    insf $r0 = $r5, 15, 8
; V2-NEXT:    sbmm8 $r1 = $r4, 0x800000004
; V2-NEXT:    sbmm8 $r2 = $r4, 0x200000001
; V2-NEXT:    make $r5 = 0x100000001
; V2-NEXT:    ;; # (end cycle 11)
; V2-NEXT:    insf $r0 = $r3, 31, 16
; V2-NEXT:    andd $r3 = $r1, $r5
; V2-NEXT:    make $r4 = 0x100000001
; V2-NEXT:    ;; # (end cycle 12)
; V2-NEXT:    sbmm8 $r0 = $r0, 0x200000001
; V2-NEXT:    andd $r2 = $r2, $r4
; V2-NEXT:    sbmm8 $r7 = $r0, 0x800000004
; V2-NEXT:    ;; # (end cycle 13)
; V2-NEXT:    andd $r0 = $r0, $r4
; V2-NEXT:    andd $r1 = $r7, $r5
; V2-NEXT:    ret
; V2-NEXT:    ;; # (end cycle 14)
  %3 = icmp eq <8 x i32> %0, zeroinitializer
  %4 = icmp eq <8 x i32> %1, zeroinitializer
  %5 = or <8 x i1> %4, %3
  %6 = zext <8 x i1> %5 to <8 x i32>
  ret <8 x i32> %6
}

define <8 x i32> @lnandn(<8 x i32> %0, <8 x i32> %1) {
; V1-LABEL: lnandn:
; V1:       # %bb.0:
; V1-NEXT:    make $r8 = 0
; V1-NEXT:    make $r9 = 0
; V1-NEXT:    ;; # (end cycle 0)
; V1-NEXT:    compnwp.eq $r0 = $r0, $r8
; V1-NEXT:    compnwp.eq $r1 = $r1, $r9
; V1-NEXT:    compnwp.eq $r2 = $r2, $r8
; V1-NEXT:    compnwp.eq $r3 = $r3, $r9
; V1-NEXT:    ;; # (end cycle 1)
; V1-NEXT:    compnwp.eq $r4 = $r4, $r8
; V1-NEXT:    compnwp.eq $r5 = $r5, $r9
; V1-NEXT:    compnwp.eq $r6 = $r6, $r8
; V1-NEXT:    compnwp.eq $r7 = $r7, $r9
; V1-NEXT:    ;; # (end cycle 2)
; V1-NEXT:    iord $r0 = $r4, $r0
; V1-NEXT:    iord $r1 = $r5, $r1
; V1-NEXT:    iord $r2 = $r6, $r2
; V1-NEXT:    iord $r3 = $r7, $r3
; V1-NEXT:    ret
; V1-NEXT:    ;; # (end cycle 3)
;
; V2-LABEL: lnandn:
; V2:       # %bb.0:
; V2-NEXT:    make $r8 = 0
; V2-NEXT:    make $r9 = 0
; V2-NEXT:    ;; # (end cycle 0)
; V2-NEXT:    compnwp.eq $r0 = $r0, $r8
; V2-NEXT:    compnwp.eq $r1 = $r1, $r9
; V2-NEXT:    compnwp.eq $r2 = $r2, $r8
; V2-NEXT:    compnwp.eq $r3 = $r3, $r9
; V2-NEXT:    ;; # (end cycle 1)
; V2-NEXT:    compnwp.eq $r4 = $r4, $r8
; V2-NEXT:    compnwp.eq $r5 = $r5, $r9
; V2-NEXT:    compnwp.eq $r6 = $r6, $r8
; V2-NEXT:    compnwp.eq $r7 = $r7, $r9
; V2-NEXT:    ;; # (end cycle 2)
; V2-NEXT:    sbmm8 $r0 = $r0, 0x1001
; V2-NEXT:    sbmm8 $r1 = $r1, 0x1001
; V2-NEXT:    sbmm8 $r2 = $r2, 0x1001
; V2-NEXT:    sbmm8 $r3 = $r3, 0x1001
; V2-NEXT:    ;; # (end cycle 3)
; V2-NEXT:    sbmm8 $r4 = $r4, 0x1001
; V2-NEXT:    sbmm8 $r5 = $r5, 0x1001
; V2-NEXT:    sbmm8 $r6 = $r6, 0x1001
; V2-NEXT:    sbmm8 $r7 = $r7, 0x1001
; V2-NEXT:    ;; # (end cycle 4)
; V2-NEXT:    insf $r0 = $r1, 31, 16
; V2-NEXT:    insf $r2 = $r3, 31, 16
; V2-NEXT:    insf $r4 = $r5, 31, 16
; V2-NEXT:    insf $r6 = $r7, 31, 16
; V2-NEXT:    ;; # (end cycle 5)
; V2-NEXT:    insf $r0 = $r2, 63, 32
; V2-NEXT:    insf $r4 = $r6, 63, 32
; V2-NEXT:    ;; # (end cycle 6)
; V2-NEXT:    iord $r0 = $r4, $r0
; V2-NEXT:    ;; # (end cycle 7)
; V2-NEXT:    srld $r1 = $r0, 56
; V2-NEXT:    extfz $r2 = $r0, 55, 48
; V2-NEXT:    extfz $r3 = $r0, 47, 40
; V2-NEXT:    extfz $r4 = $r0, 39, 32
; V2-NEXT:    ;; # (end cycle 8)
; V2-NEXT:    srlw $r1 = $r0, 24
; V2-NEXT:    insf $r2 = $r1, 15, 8
; V2-NEXT:    extfz $r3 = $r0, 15, 8
; V2-NEXT:    insf $r4 = $r3, 15, 8
; V2-NEXT:    ;; # (end cycle 9)
; V2-NEXT:    zxbd $r0 = $r0
; V2-NEXT:    extfz $r2 = $r0, 23, 16
; V2-NEXT:    insf $r4 = $r2, 31, 16
; V2-NEXT:    ;; # (end cycle 10)
; V2-NEXT:    insf $r0 = $r3, 15, 8
; V2-NEXT:    insf $r2 = $r1, 15, 8
; V2-NEXT:    sbmm8 $r4 = $r4, 0x200000001
; V2-NEXT:    sbmm8 $r5 = $r4, 0x800000004
; V2-NEXT:    ;; # (end cycle 11)
; V2-NEXT:    insf $r0 = $r2, 31, 16
; V2-NEXT:    srld $r1 = $r5, 32
; V2-NEXT:    extfs $r2 = $r4, 0, 0
; V2-NEXT:    extfs $r3 = $r5, 0, 0
; V2-NEXT:    ;; # (end cycle 12)
; V2-NEXT:    sbmm8 $r0 = $r0, 0x200000001
; V2-NEXT:    srld $r1 = $r4, 32
; V2-NEXT:    extfs $r5 = $r1, 0, 0
; V2-NEXT:    sbmm8 $r7 = $r0, 0x800000004
; V2-NEXT:    ;; # (end cycle 13)
; V2-NEXT:    extfs $r1 = $r7, 0, 0
; V2-NEXT:    extfs $r4 = $r1, 0, 0
; V2-NEXT:    srld $r6 = $r7, 32
; V2-NEXT:    srld $r8 = $r0, 32
; V2-NEXT:    ;; # (end cycle 14)
; V2-NEXT:    extfs $r0 = $r0, 0, 0
; V2-NEXT:    insf $r2 = $r4, 63, 32
; V2-NEXT:    extfs $r6 = $r6, 0, 0
; V2-NEXT:    extfs $r7 = $r8, 0, 0
; V2-NEXT:    ;; # (end cycle 15)
; V2-NEXT:    insf $r0 = $r7, 63, 32
; V2-NEXT:    insf $r1 = $r6, 63, 32
; V2-NEXT:    insf $r3 = $r5, 63, 32
; V2-NEXT:    ret
; V2-NEXT:    ;; # (end cycle 16)
  %3 = icmp eq <8 x i32> %0, zeroinitializer
  %4 = icmp eq <8 x i32> %1, zeroinitializer
  %5 = or <8 x i1> %4, %3
  %6 = sext <8 x i1> %5 to <8 x i32>
  ret <8 x i32> %6
}

define <8 x i32> @lor(<8 x i32> %0, <8 x i32> %1) {
; ALL-LABEL: lor:
; ALL:       # %bb.0:
; ALL-NEXT:    iord $r2 = $r6, $r2
; ALL-NEXT:    iord $r3 = $r7, $r3
; ALL-NEXT:    make $r6 = 0
; ALL-NEXT:    make $r7 = 0
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    iord $r0 = $r4, $r0
; ALL-NEXT:    iord $r1 = $r5, $r1
; ALL-NEXT:    compnwp.ne $r2 = $r2, $r6
; ALL-NEXT:    compnwp.ne $r3 = $r3, $r7
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    compnwp.ne $r0 = $r0, $r6
; ALL-NEXT:    compnwp.ne $r1 = $r1, $r7
; ALL-NEXT:    make $r4 = 0x100000001
; ALL-NEXT:    make $r5 = 0x100000001
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    andd $r0 = $r0, $r4
; ALL-NEXT:    andd $r1 = $r1, $r5
; ALL-NEXT:    andd $r2 = $r2, $r4
; ALL-NEXT:    andd $r3 = $r3, $r5
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 3)
  %3 = or <8 x i32> %1, %0
  %4 = icmp ne <8 x i32> %3, zeroinitializer
  %5 = zext <8 x i1> %4 to <8 x i32>
  ret <8 x i32> %5
}

; Not sure this is better than a (compnhq.ne (ord), (make 0))
define <8 x i32> @lorneg(<8 x i32> %0, <8 x i32> %1) {
; ALL-LABEL: lorneg:
; ALL:       # %bb.0:
; ALL-NEXT:    iord $r0 = $r4, $r0
; ALL-NEXT:    iord $r1 = $r5, $r1
; ALL-NEXT:    iord $r2 = $r6, $r2
; ALL-NEXT:    iord $r3 = $r7, $r3
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    make $r4 = 0
; ALL-NEXT:    make $r5 = 0
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    compnwp.ne $r0 = $r0, $r4
; ALL-NEXT:    compnwp.ne $r1 = $r1, $r5
; ALL-NEXT:    compnwp.ne $r2 = $r2, $r4
; ALL-NEXT:    compnwp.ne $r3 = $r3, $r5
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 2)
  %3 = or <8 x i32> %1, %0
  %4 = icmp ne <8 x i32> %3, zeroinitializer
  %5 = sext <8 x i1> %4 to <8 x i32>
  ret <8 x i32> %5
}

define <8 x i32> @lnor(<8 x i32> %0, <8 x i32> %1) {
; ALL-LABEL: lnor:
; ALL:       # %bb.0:
; ALL-NEXT:    iord $r2 = $r6, $r2
; ALL-NEXT:    iord $r3 = $r7, $r3
; ALL-NEXT:    make $r6 = 0
; ALL-NEXT:    make $r7 = 0
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    iord $r0 = $r4, $r0
; ALL-NEXT:    iord $r1 = $r5, $r1
; ALL-NEXT:    compnwp.eq $r2 = $r2, $r6
; ALL-NEXT:    compnwp.eq $r3 = $r3, $r7
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    compnwp.eq $r0 = $r0, $r6
; ALL-NEXT:    compnwp.eq $r1 = $r1, $r7
; ALL-NEXT:    make $r4 = 0x100000001
; ALL-NEXT:    make $r5 = 0x100000001
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    andd $r0 = $r0, $r4
; ALL-NEXT:    andd $r1 = $r1, $r5
; ALL-NEXT:    andd $r2 = $r2, $r4
; ALL-NEXT:    andd $r3 = $r3, $r5
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 3)
  %3 = or <8 x i32> %1, %0
  %4 = icmp eq <8 x i32> %3, zeroinitializer
  %5 = zext <8 x i1> %4 to <8 x i32>
  ret <8 x i32> %5
}

; Not sure this is better than a (compnhq.eq (ord), (make 0))
define <8 x i32> @lnorneg(<8 x i32> %0, <8 x i32> %1) {
; ALL-LABEL: lnorneg:
; ALL:       # %bb.0:
; ALL-NEXT:    iord $r0 = $r4, $r0
; ALL-NEXT:    iord $r1 = $r5, $r1
; ALL-NEXT:    iord $r2 = $r6, $r2
; ALL-NEXT:    iord $r3 = $r7, $r3
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    make $r4 = 0
; ALL-NEXT:    make $r5 = 0
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    compnwp.eq $r0 = $r0, $r4
; ALL-NEXT:    compnwp.eq $r1 = $r1, $r5
; ALL-NEXT:    compnwp.eq $r2 = $r2, $r4
; ALL-NEXT:    compnwp.eq $r3 = $r3, $r5
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 2)
  %3 = or <8 x i32> %1, %0
  %4 = icmp eq <8 x i32> %3, zeroinitializer
  %5 = sext <8 x i1> %4 to <8 x i32>
  ret <8 x i32> %5
}


define <8 x i32> @abdhq_rr(<8 x i32> %a, <8 x i32> %b) {
; V1-LABEL: abdhq_rr:
; V1:       # %bb.0: # %entry
; V1-NEXT:    sbfwp $r0 = $r4, $r0
; V1-NEXT:    sbfwp $r1 = $r5, $r1
; V1-NEXT:    sbfwp $r2 = $r6, $r2
; V1-NEXT:    sbfwp $r3 = $r7, $r3
; V1-NEXT:    ;; # (end cycle 0)
; V1-NEXT:    abswp $r2 = $r2
; V1-NEXT:    abswp $r3 = $r3
; V1-NEXT:    ;; # (end cycle 1)
; V1-NEXT:    abswp $r0 = $r0
; V1-NEXT:    abswp $r1 = $r1
; V1-NEXT:    ret
; V1-NEXT:    ;; # (end cycle 2)
;
; V2-LABEL: abdhq_rr:
; V2:       # %bb.0: # %entry
; V2-NEXT:    sbfwp $r0 = $r4, $r0
; V2-NEXT:    sbfwp $r1 = $r5, $r1
; V2-NEXT:    sbfwp $r2 = $r6, $r2
; V2-NEXT:    sbfwp $r3 = $r7, $r3
; V2-NEXT:    ;; # (end cycle 0)
; V2-NEXT:    abswp $r0 = $r0
; V2-NEXT:    abswp $r1 = $r1
; V2-NEXT:    abswp $r2 = $r2
; V2-NEXT:    abswp $r3 = $r3
; V2-NEXT:    ;; # (end cycle 1)
; V2-NEXT:    ret
; V2-NEXT:    ;;
entry:
  %sub = sub nsw <8 x i32> %a, %b
  %0 = tail call <8 x i32> @llvm.abs.v4i32(<8 x i32> %sub, i1 true)
  ret <8 x i32> %0
}

define <8 x i32> @abdhq_not_ri(<8 x i32> %0) {
; V1-LABEL: abdhq_not_ri:
; V1:       # %bb.0:
; V1-NEXT:    make $r4 = 0x1000000012
; V1-NEXT:    make $r5 = 0x100000000f
; V1-NEXT:    ;; # (end cycle 0)
; V1-NEXT:    sbfwp $r0 = $r0, $r4
; V1-NEXT:    sbfwp $r1 = $r1, $r5
; V1-NEXT:    sbfwp $r2 = $r2, $r4
; V1-NEXT:    sbfwp $r3 = $r3, $r5
; V1-NEXT:    ;; # (end cycle 1)
; V1-NEXT:    abswp $r2 = $r2
; V1-NEXT:    abswp $r3 = $r3
; V1-NEXT:    ;; # (end cycle 2)
; V1-NEXT:    abswp $r0 = $r0
; V1-NEXT:    abswp $r1 = $r1
; V1-NEXT:    ret
; V1-NEXT:    ;; # (end cycle 3)
;
; V2-LABEL: abdhq_not_ri:
; V2:       # %bb.0:
; V2-NEXT:    make $r4 = 0x1000000012
; V2-NEXT:    make $r5 = 0x100000000f
; V2-NEXT:    ;; # (end cycle 0)
; V2-NEXT:    sbfwp $r0 = $r0, $r4
; V2-NEXT:    sbfwp $r1 = $r1, $r5
; V2-NEXT:    sbfwp $r2 = $r2, $r4
; V2-NEXT:    sbfwp $r3 = $r3, $r5
; V2-NEXT:    ;; # (end cycle 1)
; V2-NEXT:    abswp $r0 = $r0
; V2-NEXT:    abswp $r1 = $r1
; V2-NEXT:    abswp $r2 = $r2
; V2-NEXT:    abswp $r3 = $r3
; V2-NEXT:    ;; # (end cycle 2)
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %2 = sub nsw <8 x i32> <i32 18, i32 16, i32 15, i32 16, i32 18, i32 16, i32 15, i32 16>, %0
  %3 = tail call <8 x i32> @llvm.abs.v4i32(<8 x i32> %2, i1 true)
  ret <8 x i32> %3
}

define <8 x i32> @abdhq_ri_(<8 x i32> %0) {
; V1-LABEL: abdhq_ri_:
; V1:       # %bb.0:
; V1-NEXT:    make $r4 = 0x100000000f
; V1-NEXT:    make $r5 = 0
; V1-NEXT:    ;; # (end cycle 0)
; V1-NEXT:    sbfwp $r0 = $r0, $r4
; V1-NEXT:    sbfwp $r1 = $r1, $r5
; V1-NEXT:    sbfwp $r2 = $r2, $r4
; V1-NEXT:    sbfwp $r3 = $r3, $r5
; V1-NEXT:    ;; # (end cycle 1)
; V1-NEXT:    abswp $r2 = $r2
; V1-NEXT:    abswp $r3 = $r3
; V1-NEXT:    ;; # (end cycle 2)
; V1-NEXT:    abswp $r0 = $r0
; V1-NEXT:    abswp $r1 = $r1
; V1-NEXT:    ret
; V1-NEXT:    ;; # (end cycle 3)
;
; V2-LABEL: abdhq_ri_:
; V2:       # %bb.0:
; V2-NEXT:    make $r4 = 0x100000000f
; V2-NEXT:    make $r5 = 0
; V2-NEXT:    ;; # (end cycle 0)
; V2-NEXT:    sbfwp $r0 = $r0, $r4
; V2-NEXT:    sbfwp $r1 = $r1, $r5
; V2-NEXT:    sbfwp $r2 = $r2, $r4
; V2-NEXT:    sbfwp $r3 = $r3, $r5
; V2-NEXT:    ;; # (end cycle 1)
; V2-NEXT:    abswp $r0 = $r0
; V2-NEXT:    abswp $r1 = $r1
; V2-NEXT:    abswp $r2 = $r2
; V2-NEXT:    abswp $r3 = $r3
; V2-NEXT:    ;; # (end cycle 2)
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %2 = sub nsw <8 x i32> <i32 15, i32 16, i32 0, i32 0, i32 15, i32 16, i32 0, i32 0>, %0
  %3 = tail call <8 x i32> @llvm.abs.v4i32(<8 x i32> %2, i1 true)
  ret <8 x i32> %3
}

define <8 x i32> @abdhq_ri_at(<8 x i32> %0) {
; ALL-LABEL: abdhq_ri_at:
; ALL:       # %bb.0:
; ALL-NEXT:    make $r5 = 0x100000000f
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sbfwp $r1 = $r1, $r5
; ALL-NEXT:    sbfwp $r3 = $r3, $r5
; ALL-NEXT:    copyd $r4 = $r5
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    sbfwp $r0 = $r0, $r4
; ALL-NEXT:    abswp $r1 = $r1
; ALL-NEXT:    sbfwp $r2 = $r2, $r4
; ALL-NEXT:    abswp $r3 = $r3
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    abswp $r0 = $r0
; ALL-NEXT:    abswp $r2 = $r2
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 3)
  %2 = sub nsw <8 x i32> <i32 15, i32 16, i32 15, i32 16, i32 15, i32 16, i32 15, i32 16>, %0
  %3 = tail call <8 x i32> @llvm.abs.v4i32(<8 x i32> %2, i1 true)
  ret <8 x i32> %3
}

define <8 x i32> @nandd_v4i32_rr(<8 x i32> %0, <8 x i32> %1) {
; ALL-LABEL: nandd_v4i32_rr:
; ALL:       # %bb.0:
; ALL-NEXT:    andd $r0 = $r4, $r0
; ALL-NEXT:    andd $r1 = $r5, $r1
; ALL-NEXT:    andd $r2 = $r6, $r2
; ALL-NEXT:    andd $r3 = $r7, $r3
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    make $r4 = -1
; ALL-NEXT:    make $r5 = -1
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    eord $r0 = $r0, $r4
; ALL-NEXT:    eord $r1 = $r1, $r5
; ALL-NEXT:    eord $r2 = $r2, $r4
; ALL-NEXT:    eord $r3 = $r3, $r5
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 2)
  %3 = and <8 x i32> %1, %0
  %4 = xor <8 x i32> %3, <i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1>
  ret <8 x i32> %4
}

define <8 x i32> @nandd_v4i32_ri10(<8 x i32> %0) {
; ALL-LABEL: nandd_v4i32_ri10:
; ALL:       # %bb.0:
; ALL-NEXT:    make $r1 = 0
; ALL-NEXT:    make $r4 = 1023
; ALL-NEXT:    make $r7 = -1
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    andd $r0 = $r0, $r4
; ALL-NEXT:    andd $r2 = $r2, $r4
; ALL-NEXT:    make $r4 = -1
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    eord $r0 = $r0, $r4
; ALL-NEXT:    andd $r1 = $r0, $r1
; ALL-NEXT:    eord $r2 = $r2, $r4
; ALL-NEXT:    andd $r5 = $r0, $r1
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    eord $r1 = $r5, $r7
; ALL-NEXT:    eord $r3 = $r1, $r7
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 3)
  %2 = and <8 x i32> %0, <i32 1023, i32 0, i32 0, i32 0, i32 1023, i32 0, i32 0, i32 0>
  %3 = xor <8 x i32> %2, <i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1>
  ret <8 x i32> %3
}

define <8 x i32> @nandd_v4i32_ri37_0(<8 x i32> %0) {
; ALL-LABEL: nandd_v4i32_ri37_0:
; ALL:       # %bb.0:
; ALL-NEXT:    make $r1 = 0
; ALL-NEXT:    make $r4 = 0xfffffffd00000400
; ALL-NEXT:    make $r7 = -1
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    andd $r0 = $r0, $r4
; ALL-NEXT:    andd $r2 = $r2, $r4
; ALL-NEXT:    make $r4 = -1
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    eord $r0 = $r0, $r4
; ALL-NEXT:    andd $r1 = $r0, $r1
; ALL-NEXT:    eord $r2 = $r2, $r4
; ALL-NEXT:    andd $r5 = $r0, $r1
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    eord $r1 = $r5, $r7
; ALL-NEXT:    eord $r3 = $r1, $r7
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 3)
  %2 = and <8 x i32> %0, <i32 1024, i32 -3, i32 0, i32 0, i32 1024, i32 -3, i32 0, i32 0>
  %3 = xor <8 x i32> %2, <i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1>
  ret <8 x i32> %3
}

define <8 x i32> @nandd_v4i32_ri37_1(<8 x i32> %0) {
; ALL-LABEL: nandd_v4i32_ri37_1:
; ALL:       # %bb.0:
; ALL-NEXT:    make $r1 = 0
; ALL-NEXT:    make $r4 = 0xfffffffd00000400
; ALL-NEXT:    make $r7 = -1
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    andd $r0 = $r0, $r4
; ALL-NEXT:    andd $r2 = $r2, $r4
; ALL-NEXT:    make $r4 = -1
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    eord $r0 = $r0, $r4
; ALL-NEXT:    andd $r1 = $r0, $r1
; ALL-NEXT:    eord $r2 = $r2, $r4
; ALL-NEXT:    andd $r5 = $r0, $r1
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    eord $r1 = $r5, $r7
; ALL-NEXT:    eord $r3 = $r1, $r7
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 3)
  %2 = and <8 x i32> %0, <i32 1024, i32 -3, i32 0, i32 0, i32 1024, i32 -3, i32 0, i32 0>
  %3 = xor <8 x i32> %2, <i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1>
  ret <8 x i32> %3
}

define <8 x i32> @nandd_v4i32_ri37_2(<8 x i32> %0) {
; ALL-LABEL: nandd_v4i32_ri37_2:
; ALL:       # %bb.0:
; ALL-NEXT:    make $r4 = 0xfffffffd00000400
; ALL-NEXT:    make $r5 = 31
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    andd $r0 = $r0, $r4
; ALL-NEXT:    andd $r1 = $r1, $r5
; ALL-NEXT:    andd $r2 = $r2, $r4
; ALL-NEXT:    andd $r3 = $r3, $r5
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    make $r4 = -1
; ALL-NEXT:    make $r5 = -1
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    eord $r0 = $r0, $r4
; ALL-NEXT:    eord $r1 = $r1, $r5
; ALL-NEXT:    eord $r2 = $r2, $r4
; ALL-NEXT:    eord $r3 = $r3, $r5
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 3)
  %2 = and <8 x i32> %0, <i32 1024, i32 -3, i32 31, i32 0, i32 1024, i32 -3, i32 31, i32 0>
  %3 = xor <8 x i32> %2, <i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1>
  ret <8 x i32> %3
}

define <8 x i32> @nandd_v4i32_ri64_0(<8 x i32> %0) {
; ALL-LABEL: nandd_v4i32_ri64_0:
; ALL:       # %bb.0:
; ALL-NEXT:    make $r4 = 0xfffffffd00000400
; ALL-NEXT:    make $r5 = 32
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    andd $r0 = $r0, $r4
; ALL-NEXT:    andd $r1 = $r1, $r5
; ALL-NEXT:    andd $r2 = $r2, $r4
; ALL-NEXT:    andd $r3 = $r3, $r5
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    make $r4 = -1
; ALL-NEXT:    make $r5 = -1
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    eord $r0 = $r0, $r4
; ALL-NEXT:    eord $r1 = $r1, $r5
; ALL-NEXT:    eord $r2 = $r2, $r4
; ALL-NEXT:    eord $r3 = $r3, $r5
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 3)
  %2 = and <8 x i32> %0, <i32 1024, i32 -3, i32 32, i32 0, i32 1024, i32 -3, i32 32, i32 0>
  %3 = xor <8 x i32> %2, <i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1>
  ret <8 x i32> %3
}

define <8 x i32> @nandd_v4i32_ri64_1(<8 x i32> %0) {
; ALL-LABEL: nandd_v4i32_ri64_1:
; ALL:       # %bb.0:
; ALL-NEXT:    make $r4 = 0xfffffffd00000400
; ALL-NEXT:    make $r5 = 0xffffffff
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    andd $r0 = $r0, $r4
; ALL-NEXT:    andd $r1 = $r1, $r5
; ALL-NEXT:    andd $r2 = $r2, $r4
; ALL-NEXT:    andd $r3 = $r3, $r5
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    make $r4 = -1
; ALL-NEXT:    make $r5 = -1
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    eord $r0 = $r0, $r4
; ALL-NEXT:    eord $r1 = $r1, $r5
; ALL-NEXT:    eord $r2 = $r2, $r4
; ALL-NEXT:    eord $r3 = $r3, $r5
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 3)
  %2 = and <8 x i32> %0, <i32 1024, i32 -3, i32 -1, i32 0, i32 1024, i32 -3, i32 -1, i32 0>
  %3 = xor <8 x i32> %2, <i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1>
  ret <8 x i32> %3
}

define <8 x i32> @nandd_v4i32_ri64_2(<8 x i32> %0) {
; ALL-LABEL: nandd_v4i32_ri64_2:
; ALL:       # %bb.0:
; ALL-NEXT:    make $r4 = 0xfffffffd00000400
; ALL-NEXT:    make $r5 = 0x10000001f
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    andd $r0 = $r0, $r4
; ALL-NEXT:    andd $r1 = $r1, $r5
; ALL-NEXT:    andd $r2 = $r2, $r4
; ALL-NEXT:    andd $r3 = $r3, $r5
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    make $r4 = -1
; ALL-NEXT:    make $r5 = -1
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    eord $r0 = $r0, $r4
; ALL-NEXT:    eord $r1 = $r1, $r5
; ALL-NEXT:    eord $r2 = $r2, $r4
; ALL-NEXT:    eord $r3 = $r3, $r5
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 3)
  %2 = and <8 x i32> %0, <i32 1024, i32 -3, i32 31, i32 1, i32 1024, i32 -3, i32 31, i32 1>
  %3 = xor <8 x i32> %2, <i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1>
  ret <8 x i32> %3
}

define <8 x i32> @splat(i32 %0) {
; ALL-LABEL: splat:
; ALL:       # %bb.0:
; ALL-NEXT:    insf $r0 = $r0, 63, 32
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    copyd $r1 = $r0
; ALL-NEXT:    copyd $r2 = $r0
; ALL-NEXT:    copyd $r3 = $r0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 1)
  %2 = insertelement <8 x i32> undef, i32 %0, i32 0
  %3 = shufflevector <8 x i32> %2, <8 x i32> undef, <8 x i32> zeroinitializer
  ret <8 x i32> %3
}

define <8 x i32> @splat_0(<8 x i32> %0) {
; ALL-LABEL: splat_0:
; ALL:       # %bb.0:
; ALL-NEXT:    insf $r0 = $r0, 63, 32
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    copyd $r1 = $r0
; ALL-NEXT:    copyd $r2 = $r0
; ALL-NEXT:    copyd $r3 = $r0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 1)
  %2 = shufflevector <8 x i32> %0, <8 x i32> undef, <8 x i32> zeroinitializer
  ret <8 x i32> %2
}

define <8 x i32> @splat_1(<8 x i32> %0) {
; ALL-LABEL: splat_1:
; ALL:       # %bb.0:
; ALL-NEXT:    sbmm8 $r0 = $r0, 0x80402010.@
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    copyd $r1 = $r0
; ALL-NEXT:    copyd $r2 = $r0
; ALL-NEXT:    copyd $r3 = $r0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 1)
  %2 = shufflevector <8 x i32> %0, <8 x i32> undef, <8 x i32> <i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1>
  ret <8 x i32> %2
}

define <8 x i32> @splat_1_32(i32 %0) {
; ALL-LABEL: splat_1_32:
; ALL:       # %bb.0:
; ALL-NEXT:    srlw $r0 = $r0, 16
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    insf $r0 = $r0, 63, 32
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    copyd $r1 = $r0
; ALL-NEXT:    copyd $r2 = $r0
; ALL-NEXT:    copyd $r3 = $r0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 2)
  %2 = lshr i32 %0, 16
  %3 = insertelement <8 x i32> undef, i32 %2, i32 0
  %4 = shufflevector <8 x i32> %3, <8 x i32> undef, <8 x i32> zeroinitializer
  ret <8 x i32> %4
}

define <8 x i32> @splat_1_64(i64 %0) {
; ALL-LABEL: splat_1_64:
; ALL:       # %bb.0:
; ALL-NEXT:    srld $r0 = $r0, 16
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    insf $r0 = $r0, 63, 32
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    copyd $r1 = $r0
; ALL-NEXT:    copyd $r2 = $r0
; ALL-NEXT:    copyd $r3 = $r0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 2)
  %2 = lshr i64 %0, 16
  %3 = trunc i64 %2 to i32
  %4 = insertelement <8 x i32> undef, i32 %3, i32 0
  %5 = shufflevector <8 x i32> %4, <8 x i32> undef, <8 x i32> zeroinitializer
  ret <8 x i32> %5
}

define <8 x i32> @splat_2(<8 x i32> %0) {
; ALL-LABEL: splat_2:
; ALL:       # %bb.0:
; ALL-NEXT:    insf $r1 = $r1, 63, 32
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    copyd $r0 = $r1
; ALL-NEXT:    copyd $r2 = $r1
; ALL-NEXT:    copyd $r3 = $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 1)
  %2 = shufflevector <8 x i32> %0, <8 x i32> undef, <8 x i32> <i32 2, i32 2, i32 2, i32 2, i32 2, i32 2, i32 2, i32 2>
  ret <8 x i32> %2
}

define <8 x i32> @splat_2_64(i64 %0) {
; ALL-LABEL: splat_2_64:
; ALL:       # %bb.0:
; ALL-NEXT:    srld $r1 = $r0, 32
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    insf $r0 = $r1, 31, 0
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    copyd $r1 = $r0
; ALL-NEXT:    copyd $r2 = $r0
; ALL-NEXT:    copyd $r3 = $r0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 2)
  %2 = lshr i64 %0, 32
  %3 = trunc i64 %2 to i32
  %4 = insertelement <8 x i32> undef, i32 %3, i32 0
  %5 = shufflevector <8 x i32> %4, <8 x i32> undef, <8 x i32> zeroinitializer
  ret <8 x i32> %5
}

define <8 x i32> @splat_3(<8 x i32> %0) {
; ALL-LABEL: splat_3:
; ALL:       # %bb.0:
; ALL-NEXT:    sbmm8 $r0 = $r1, 0x80402010.@
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    copyd $r1 = $r0
; ALL-NEXT:    copyd $r2 = $r0
; ALL-NEXT:    copyd $r3 = $r0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 1)
  %2 = shufflevector <8 x i32> %0, <8 x i32> undef, <8 x i32> <i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3>
  ret <8 x i32> %2
}

define <8 x i32> @splat_4(<8 x i32> %0) {
; ALL-LABEL: splat_4:
; ALL:       # %bb.0:
; ALL-NEXT:    insf $r2 = $r2, 63, 32
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    copyd $r0 = $r2
; ALL-NEXT:    copyd $r1 = $r2
; ALL-NEXT:    copyd $r3 = $r2
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 1)
  %2 = shufflevector <8 x i32> %0, <8 x i32> undef, <8 x i32> <i32 4, i32 4, i32 4, i32 4, i32 4, i32 4, i32 4, i32 4>
  ret <8 x i32> %2
}

define <8 x i32> @splat_5(<8 x i32> %0) {
; ALL-LABEL: splat_5:
; ALL:       # %bb.0:
; ALL-NEXT:    sbmm8 $r0 = $r2, 0x80402010.@
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    copyd $r1 = $r0
; ALL-NEXT:    copyd $r2 = $r0
; ALL-NEXT:    copyd $r3 = $r0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 1)
  %2 = shufflevector <8 x i32> %0, <8 x i32> undef, <8 x i32> <i32 5, i32 5, i32 5, i32 5, i32 5, i32 5, i32 5, i32 5>
  ret <8 x i32> %2
}

define <8 x i32> @splat_3_64(i64 %0) {
; ALL-LABEL: splat_3_64:
; ALL:       # %bb.0:
; ALL-NEXT:    srld $r0 = $r0, 48
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    insf $r0 = $r0, 63, 32
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    copyd $r1 = $r0
; ALL-NEXT:    copyd $r2 = $r0
; ALL-NEXT:    copyd $r3 = $r0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 2)
  %2 = lshr i64 %0, 48
  %3 = trunc i64 %2 to i32
  %4 = insertelement <8 x i32> undef, i32 %3, i32 0
  %5 = shufflevector <8 x i32> %4, <8 x i32> undef, <8 x i32> zeroinitializer
  ret <8 x i32> %5
}

define  <8 x i32> @v4_maxhq_rr_i32(<8 x i32> %a, <8 x i32> %b) {
; ALL-LABEL: v4_maxhq_rr_i32:
; ALL:       # %bb.0: # %entry
; ALL-NEXT:    maxwp $r0 = $r0, $r4
; ALL-NEXT:    maxwp $r1 = $r1, $r5
; ALL-NEXT:    maxwp $r2 = $r2, $r6
; ALL-NEXT:    maxwp $r3 = $r3, $r7
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
entry:
  %0 = call <8 x i32> @llvm.smax.v4i32(<8 x i32> %a, <8 x i32> %b)
  ret <8 x i32> %0
}

define  <8 x i32> @v4_minhq_rr_i32(<8 x i32> %a, <8 x i32> %b) {
; ALL-LABEL: v4_minhq_rr_i32:
; ALL:       # %bb.0: # %entry
; ALL-NEXT:    minwp $r0 = $r0, $r4
; ALL-NEXT:    minwp $r1 = $r1, $r5
; ALL-NEXT:    minwp $r2 = $r2, $r6
; ALL-NEXT:    minwp $r3 = $r3, $r7
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
entry:
  %0 = call <8 x i32> @llvm.smin.v4i32(<8 x i32> %a, <8 x i32> %b)
  ret <8 x i32> %0
}

define  <8 x i32> @v4_umaxhq_rr_i32(<8 x i32> %a, <8 x i32> %b) {
; ALL-LABEL: v4_umaxhq_rr_i32:
; ALL:       # %bb.0: # %entry
; ALL-NEXT:    maxuwp $r0 = $r0, $r4
; ALL-NEXT:    maxuwp $r1 = $r1, $r5
; ALL-NEXT:    maxuwp $r2 = $r2, $r6
; ALL-NEXT:    maxuwp $r3 = $r3, $r7
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
entry:
  %0 = call <8 x i32> @llvm.umax.v4i32(<8 x i32> %a, <8 x i32> %b)
  ret <8 x i32> %0
}

define  <8 x i32> @v4_uminhq_rr_i32(<8 x i32> %a, <8 x i32> %b) {
; ALL-LABEL: v4_uminhq_rr_i32:
; ALL:       # %bb.0: # %entry
; ALL-NEXT:    minuwp $r0 = $r0, $r4
; ALL-NEXT:    minuwp $r1 = $r1, $r5
; ALL-NEXT:    minuwp $r2 = $r2, $r6
; ALL-NEXT:    minuwp $r3 = $r3, $r7
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
entry:
  %0 = call <8 x i32> @llvm.umin.v4i32(<8 x i32> %a, <8 x i32> %b)
  ret <8 x i32> %0
}

declare <8 x i32> @llvm.smax.v4i32(<8 x i32> %a, <8 x i32> %b)
declare <8 x i32> @llvm.smin.v4i32(<8 x i32> %a, <8 x i32> %b)
declare <8 x i32> @llvm.umax.v4i32(<8 x i32> %a, <8 x i32> %b)
declare <8 x i32> @llvm.umin.v4i32(<8 x i32> %a, <8 x i32> %b)

define <8 x i32> @add_splat_const_op1(<8 x i32> %vx) #0 {
; ALL-LABEL: add_splat_const_op1:
; ALL:       # %bb.0:
; ALL-NEXT:    addw $r0 = $r0, 42
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    insf $r0 = $r0, 63, 32
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    copyd $r1 = $r0
; ALL-NEXT:    copyd $r2 = $r0
; ALL-NEXT:    copyd $r3 = $r0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 2)
  %splatx = shufflevector <8 x i32> %vx, <8 x i32> undef, <8 x i32> zeroinitializer
  %r = add <8 x i32> %splatx, <i32 42, i32 42, i32 42, i32 42, i32 42, i32 42, i32 42, i32 42>
  ret <8 x i32> %r
}
attributes #0 = { nounwind }

define <8 x i32> @test_div_4(<8 x i32> %a, <8 x i32> %b) #0 {
; V1-LABEL: test_div_4:
; V1:       # %bb.0:
; V1-NEXT:    srawps $r4 = $r0, 31
; V1-NEXT:    srawps $r5 = $r1, 31
; V1-NEXT:    ;; # (end cycle 0)
; V1-NEXT:    srlwps $r4 = $r4, 30
; V1-NEXT:    srlwps $r5 = $r5, 30
; V1-NEXT:    ;; # (end cycle 1)
; V1-NEXT:    addwp $r0 = $r0, $r4
; V1-NEXT:    addwp $r1 = $r1, $r5
; V1-NEXT:    srawps $r6 = $r2, 31
; V1-NEXT:    srawps $r7 = $r3, 31
; V1-NEXT:    ;; # (end cycle 2)
; V1-NEXT:    srlwps $r4 = $r6, 30
; V1-NEXT:    srlwps $r5 = $r7, 30
; V1-NEXT:    ;; # (end cycle 3)
; V1-NEXT:    srawps $r0 = $r0, 2
; V1-NEXT:    srawps $r1 = $r1, 2
; V1-NEXT:    addwp $r2 = $r2, $r4
; V1-NEXT:    addwp $r3 = $r3, $r5
; V1-NEXT:    ;; # (end cycle 4)
; V1-NEXT:    srawps $r2 = $r2, 2
; V1-NEXT:    srawps $r3 = $r3, 2
; V1-NEXT:    ret
; V1-NEXT:    ;; # (end cycle 5)
;
; V2-LABEL: test_div_4:
; V2:       # %bb.0:
; V2-NEXT:    srawps $r4 = $r0, 31
; V2-NEXT:    srawps $r5 = $r1, 31
; V2-NEXT:    srawps $r6 = $r2, 31
; V2-NEXT:    srawps $r7 = $r3, 31
; V2-NEXT:    ;; # (end cycle 0)
; V2-NEXT:    srlwps $r4 = $r4, 30
; V2-NEXT:    srlwps $r5 = $r5, 30
; V2-NEXT:    srlwps $r6 = $r6, 30
; V2-NEXT:    srlwps $r7 = $r7, 30
; V2-NEXT:    ;; # (end cycle 1)
; V2-NEXT:    addwp $r0 = $r0, $r4
; V2-NEXT:    addwp $r1 = $r1, $r5
; V2-NEXT:    addwp $r2 = $r2, $r6
; V2-NEXT:    addwp $r3 = $r3, $r7
; V2-NEXT:    ;; # (end cycle 2)
; V2-NEXT:    srawps $r0 = $r0, 2
; V2-NEXT:    srawps $r1 = $r1, 2
; V2-NEXT:    srawps $r2 = $r2, 2
; V2-NEXT:    srawps $r3 = $r3, 2
; V2-NEXT:    ret
; V2-NEXT:    ;; # (end cycle 3)
  %r = sdiv <8 x i32> %a, <i32 4, i32 4, i32 4, i32 4, i32 4, i32 4, i32 4, i32 4>
  ret <8 x i32> %r
}

define <8 x i32> @test_div_32(<8 x i32> %a, <8 x i32> %b) #0 {
; V1-LABEL: test_div_32:
; V1:       # %bb.0:
; V1-NEXT:    srawps $r4 = $r0, 31
; V1-NEXT:    srawps $r5 = $r1, 31
; V1-NEXT:    ;; # (end cycle 0)
; V1-NEXT:    srlwps $r4 = $r4, 27
; V1-NEXT:    srlwps $r5 = $r5, 27
; V1-NEXT:    ;; # (end cycle 1)
; V1-NEXT:    addwp $r0 = $r0, $r4
; V1-NEXT:    addwp $r1 = $r1, $r5
; V1-NEXT:    srawps $r6 = $r2, 31
; V1-NEXT:    srawps $r7 = $r3, 31
; V1-NEXT:    ;; # (end cycle 2)
; V1-NEXT:    srlwps $r4 = $r6, 27
; V1-NEXT:    srlwps $r5 = $r7, 27
; V1-NEXT:    ;; # (end cycle 3)
; V1-NEXT:    srawps $r0 = $r0, 5
; V1-NEXT:    srawps $r1 = $r1, 5
; V1-NEXT:    addwp $r2 = $r2, $r4
; V1-NEXT:    addwp $r3 = $r3, $r5
; V1-NEXT:    ;; # (end cycle 4)
; V1-NEXT:    srawps $r2 = $r2, 5
; V1-NEXT:    srawps $r3 = $r3, 5
; V1-NEXT:    ret
; V1-NEXT:    ;; # (end cycle 5)
;
; V2-LABEL: test_div_32:
; V2:       # %bb.0:
; V2-NEXT:    srawps $r4 = $r0, 31
; V2-NEXT:    srawps $r5 = $r1, 31
; V2-NEXT:    srawps $r6 = $r2, 31
; V2-NEXT:    srawps $r7 = $r3, 31
; V2-NEXT:    ;; # (end cycle 0)
; V2-NEXT:    srlwps $r4 = $r4, 27
; V2-NEXT:    srlwps $r5 = $r5, 27
; V2-NEXT:    srlwps $r6 = $r6, 27
; V2-NEXT:    srlwps $r7 = $r7, 27
; V2-NEXT:    ;; # (end cycle 1)
; V2-NEXT:    addwp $r0 = $r0, $r4
; V2-NEXT:    addwp $r1 = $r1, $r5
; V2-NEXT:    addwp $r2 = $r2, $r6
; V2-NEXT:    addwp $r3 = $r3, $r7
; V2-NEXT:    ;; # (end cycle 2)
; V2-NEXT:    srawps $r0 = $r0, 5
; V2-NEXT:    srawps $r1 = $r1, 5
; V2-NEXT:    srawps $r2 = $r2, 5
; V2-NEXT:    srawps $r3 = $r3, 5
; V2-NEXT:    ret
; V2-NEXT:    ;; # (end cycle 3)
  %r = sdiv <8 x i32> %a, <i32 32, i32 32, i32 32, i32 32, i32 32, i32 32, i32 32, i32 32>
  ret <8 x i32> %r
}

define <8 x i32> @test_select_cmp(<8 x i32> %a, <8 x i32> %b, <8 x i32> %c, <8 x i32> %d) #0 {
; V1-LABEL: test_select_cmp:
; V1:       # %bb.0:
; V1-NEXT:    ld $r16 = 0[$r12]
; V1-NEXT:    ;; # (end cycle 0)
; V1-NEXT:    ld $r17 = 8[$r12]
; V1-NEXT:    ;; # (end cycle 1)
; V1-NEXT:    ld $r32 = 16[$r12]
; V1-NEXT:    ;; # (end cycle 2)
; V1-NEXT:    compnwp.ne $r8 = $r8, $r16
; V1-NEXT:    ld $r33 = 24[$r12]
; V1-NEXT:    ;; # (end cycle 3)
; V1-NEXT:    compnwp.ne $r9 = $r9, $r17
; V1-NEXT:    ;; # (end cycle 4)
; V1-NEXT:    andd $r8 = $r8, $r9
; V1-NEXT:    compnwp.ne $r10 = $r10, $r32
; V1-NEXT:    ;; # (end cycle 5)
; V1-NEXT:    andd $r8 = $r8, $r10
; V1-NEXT:    compnwp.ne $r9 = $r11, $r33
; V1-NEXT:    ;; # (end cycle 6)
; V1-NEXT:    andd $r8 = $r8, $r9
; V1-NEXT:    ;; # (end cycle 7)
; V1-NEXT:    compd.eq $r8 = $r8, -1
; V1-NEXT:    ;; # (end cycle 8)
; V1-NEXT:    cmoved.even $r8 ? $r2 = $r6
; V1-NEXT:    cmoved.even $r8 ? $r3 = $r7
; V1-NEXT:    ;; # (end cycle 9)
; V1-NEXT:    cmoved.even $r8 ? $r0 = $r4
; V1-NEXT:    cmoved.even $r8 ? $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;; # (end cycle 10)
;
; V2-LABEL: test_select_cmp:
; V2:       # %bb.0:
; V2-NEXT:    ld $r16 = 0[$r12]
; V2-NEXT:    ;; # (end cycle 0)
; V2-NEXT:    ld $r17 = 8[$r12]
; V2-NEXT:    ;; # (end cycle 1)
; V2-NEXT:    ld $r32 = 16[$r12]
; V2-NEXT:    ;; # (end cycle 2)
; V2-NEXT:    compnwp.ne $r8 = $r8, $r16
; V2-NEXT:    ld $r33 = 24[$r12]
; V2-NEXT:    ;; # (end cycle 3)
; V2-NEXT:    compnwp.ne $r9 = $r9, $r17
; V2-NEXT:    ;; # (end cycle 4)
; V2-NEXT:    andd $r8 = $r8, $r9
; V2-NEXT:    compnwp.ne $r10 = $r10, $r32
; V2-NEXT:    ;; # (end cycle 5)
; V2-NEXT:    andd $r8 = $r8, $r10
; V2-NEXT:    compnwp.ne $r9 = $r11, $r33
; V2-NEXT:    ;; # (end cycle 6)
; V2-NEXT:    andd $r8 = $r8, $r9
; V2-NEXT:    ;; # (end cycle 7)
; V2-NEXT:    compd.eq $r8 = $r8, -1
; V2-NEXT:    ;; # (end cycle 8)
; V2-NEXT:    cmoved.even $r8 ? $r0 = $r4
; V2-NEXT:    cmoved.even $r8 ? $r1 = $r5
; V2-NEXT:    cmoved.even $r8 ? $r2 = $r6
; V2-NEXT:    cmoved.even $r8 ? $r3 = $r7
; V2-NEXT:    ret
; V2-NEXT:    ;; # (end cycle 9)
  %cc = icmp ne <8 x i32> %c, %d
  %bc = bitcast <8 x i1> %cc to i8
  %cmp = icmp eq i8 %bc, -1
  %r = select i1 %cmp, <8 x i32> %a, <8 x i32> %b
  ret <8 x i32> %r
}

define <8 x i32> @fshl_rr(<8 x i32> %a, <8 x i32> %b, i32 %c) {
; V1-LABEL: fshl_rr:
; V1:       # %bb.0:
; V1-NEXT:    srlw $r7 = $r7, 1
; V1-NEXT:    andnw $r8 = $r8, 31
; V1-NEXT:    srad $r9 = $r7, 32
; V1-NEXT:    andw $r10 = $r8, 31
; V1-NEXT:    ;; # (end cycle 0)
; V1-NEXT:    sllw $r3 = $r3, $r10
; V1-NEXT:    srlw $r7 = $r7, $r8
; V1-NEXT:    srlw $r9 = $r9, 1
; V1-NEXT:    srad $r11 = $r3, 32
; V1-NEXT:    ;; # (end cycle 1)
; V1-NEXT:    srlw $r9 = $r9, $r8
; V1-NEXT:    sllw $r11 = $r11, $r10
; V1-NEXT:    srad $r15 = $r2, 32
; V1-NEXT:    srad $r16 = $r6, 32
; V1-NEXT:    ;; # (end cycle 2)
; V1-NEXT:    iorw $r3 = $r3, $r7
; V1-NEXT:    sllw $r7 = $r15, $r10
; V1-NEXT:    srad $r15 = $r1, 32
; V1-NEXT:    srad $r17 = $r0, 32
; V1-NEXT:    ;; # (end cycle 3)
; V1-NEXT:    sllw $r2 = $r2, $r10
; V1-NEXT:    iorw $r9 = $r11, $r9
; V1-NEXT:    srlw $r11 = $r16, 1
; V1-NEXT:    sllw $r15 = $r15, $r10
; V1-NEXT:    ;; # (end cycle 4)
; V1-NEXT:    sllw $r0 = $r0, $r10
; V1-NEXT:    sllw $r1 = $r1, $r10
; V1-NEXT:    srad $r16 = $r5, 32
; V1-NEXT:    sllw $r17 = $r17, $r10
; V1-NEXT:    ;; # (end cycle 5)
; V1-NEXT:    srlw $r5 = $r5, 1
; V1-NEXT:    srlw $r6 = $r6, 1
; V1-NEXT:    srad $r10 = $r4, 32
; V1-NEXT:    srlw $r16 = $r16, 1
; V1-NEXT:    ;; # (end cycle 6)
; V1-NEXT:    srlw $r4 = $r4, 1
; V1-NEXT:    srlw $r6 = $r6, $r8
; V1-NEXT:    srlw $r10 = $r10, 1
; V1-NEXT:    srlw $r11 = $r11, $r8
; V1-NEXT:    ;; # (end cycle 7)
; V1-NEXT:    srlw $r4 = $r4, $r8
; V1-NEXT:    srlw $r5 = $r5, $r8
; V1-NEXT:    srlw $r10 = $r10, $r8
; V1-NEXT:    srlw $r16 = $r16, $r8
; V1-NEXT:    ;; # (end cycle 8)
; V1-NEXT:    iorw $r0 = $r0, $r4
; V1-NEXT:    iorw $r1 = $r1, $r5
; V1-NEXT:    iorw $r5 = $r17, $r10
; V1-NEXT:    iorw $r8 = $r15, $r16
; V1-NEXT:    ;; # (end cycle 9)
; V1-NEXT:    insf $r0 = $r5, 63, 32
; V1-NEXT:    insf $r1 = $r8, 63, 32
; V1-NEXT:    iorw $r2 = $r2, $r6
; V1-NEXT:    iorw $r4 = $r7, $r11
; V1-NEXT:    ;; # (end cycle 10)
; V1-NEXT:    insf $r2 = $r4, 63, 32
; V1-NEXT:    insf $r3 = $r9, 63, 32
; V1-NEXT:    ret
; V1-NEXT:    ;; # (end cycle 11)
;
; V2-LABEL: fshl_rr:
; V2:       # %bb.0:
; V2-NEXT:    andnw $r8 = $r8, 31
; V2-NEXT:    srad $r9 = $r7, 32
; V2-NEXT:    andw $r10 = $r8, 31
; V2-NEXT:    srad $r11 = $r3, 32
; V2-NEXT:    ;; # (end cycle 0)
; V2-NEXT:    sllw $r3 = $r3, $r10
; V2-NEXT:    srlw $r7 = $r7, 1
; V2-NEXT:    srlw $r9 = $r9, 1
; V2-NEXT:    sllw $r11 = $r11, $r10
; V2-NEXT:    ;; # (end cycle 1)
; V2-NEXT:    sllw $r2 = $r2, $r10
; V2-NEXT:    srlw $r7 = $r7, $r8
; V2-NEXT:    srlw $r9 = $r9, $r8
; V2-NEXT:    srad $r15 = $r2, 32
; V2-NEXT:    ;; # (end cycle 2)
; V2-NEXT:    iorw $r3 = $r3, $r7
; V2-NEXT:    sllw $r7 = $r15, $r10
; V2-NEXT:    iorw $r9 = $r11, $r9
; V2-NEXT:    srad $r11 = $r6, 32
; V2-NEXT:    ;; # (end cycle 3)
; V2-NEXT:    sllw $r0 = $r0, $r10
; V2-NEXT:    srlw $r6 = $r6, 1
; V2-NEXT:    srlw $r11 = $r11, 1
; V2-NEXT:    srad $r15 = $r0, 32
; V2-NEXT:    ;; # (end cycle 4)
; V2-NEXT:    insf $r3 = $r9, 63, 32
; V2-NEXT:    srlw $r6 = $r6, $r8
; V2-NEXT:    srlw $r11 = $r11, $r8
; V2-NEXT:    sllw $r15 = $r15, $r10
; V2-NEXT:    ;; # (end cycle 5)
; V2-NEXT:    sllw $r1 = $r1, $r10
; V2-NEXT:    iorw $r2 = $r2, $r6
; V2-NEXT:    iorw $r7 = $r7, $r11
; V2-NEXT:    srad $r11 = $r1, 32
; V2-NEXT:    ;; # (end cycle 6)
; V2-NEXT:    srlw $r5 = $r5, 1
; V2-NEXT:    sllw $r6 = $r11, $r10
; V2-NEXT:    srad $r10 = $r4, 32
; V2-NEXT:    srad $r11 = $r5, 32
; V2-NEXT:    ;; # (end cycle 7)
; V2-NEXT:    srlw $r4 = $r4, 1
; V2-NEXT:    srlw $r5 = $r5, $r8
; V2-NEXT:    srlw $r10 = $r10, 1
; V2-NEXT:    srlw $r11 = $r11, 1
; V2-NEXT:    ;; # (end cycle 8)
; V2-NEXT:    iorw $r1 = $r1, $r5
; V2-NEXT:    srlw $r4 = $r4, $r8
; V2-NEXT:    srlw $r10 = $r10, $r8
; V2-NEXT:    srlw $r11 = $r11, $r8
; V2-NEXT:    ;; # (end cycle 9)
; V2-NEXT:    iorw $r0 = $r0, $r4
; V2-NEXT:    insf $r2 = $r7, 63, 32
; V2-NEXT:    iorw $r5 = $r15, $r10
; V2-NEXT:    iorw $r6 = $r6, $r11
; V2-NEXT:    ;; # (end cycle 10)
; V2-NEXT:    insf $r0 = $r5, 63, 32
; V2-NEXT:    insf $r1 = $r6, 63, 32
; V2-NEXT:    ret
; V2-NEXT:    ;; # (end cycle 11)
  %i = insertelement <8 x i32> undef, i32 %c, i32 0
  %s = shufflevector <8 x i32> %i, <8 x i32> undef, <8 x i32> zeroinitializer
  %r = call <8 x i32> @llvm.fshl.v4i32(<8 x i32> %a, <8 x i32> %b, <8 x i32> %s)
  ret <8 x i32> %r
}

define <8 x i32> @fshl_ri(<8 x i32> %a, <8 x i32> %b) {
; V1-LABEL: fshl_ri:
; V1:       # %bb.0:
; V1-NEXT:    sllw $r3 = $r3, 3
; V1-NEXT:    srlw $r7 = $r7, 29
; V1-NEXT:    srad $r8 = $r7, 32
; V1-NEXT:    srad $r9 = $r3, 32
; V1-NEXT:    ;; # (end cycle 0)
; V1-NEXT:    iorw $r3 = $r3, $r7
; V1-NEXT:    srad $r7 = $r5, 32
; V1-NEXT:    srlw $r8 = $r8, 29
; V1-NEXT:    sllw $r9 = $r9, 3
; V1-NEXT:    ;; # (end cycle 1)
; V1-NEXT:    srlw $r5 = $r5, 29
; V1-NEXT:    srlw $r7 = $r7, 29
; V1-NEXT:    iorw $r8 = $r9, $r8
; V1-NEXT:    srad $r9 = $r1, 32
; V1-NEXT:    ;; # (end cycle 2)
; V1-NEXT:    sllw $r1 = $r1, 3
; V1-NEXT:    insf $r3 = $r8, 63, 32
; V1-NEXT:    sllw $r9 = $r9, 3
; V1-NEXT:    ;; # (end cycle 3)
; V1-NEXT:    iorw $r1 = $r1, $r5
; V1-NEXT:    srad $r5 = $r4, 32
; V1-NEXT:    iorw $r7 = $r9, $r7
; V1-NEXT:    srad $r9 = $r0, 32
; V1-NEXT:    ;; # (end cycle 4)
; V1-NEXT:    sllw $r0 = $r0, 3
; V1-NEXT:    srlw $r4 = $r4, 29
; V1-NEXT:    srlw $r5 = $r5, 29
; V1-NEXT:    sllw $r9 = $r9, 3
; V1-NEXT:    ;; # (end cycle 5)
; V1-NEXT:    iorw $r0 = $r0, $r4
; V1-NEXT:    srad $r4 = $r2, 32
; V1-NEXT:    iorw $r5 = $r9, $r5
; V1-NEXT:    srad $r9 = $r6, 32
; V1-NEXT:    ;; # (end cycle 6)
; V1-NEXT:    sllw $r2 = $r2, 3
; V1-NEXT:    sllw $r4 = $r4, 3
; V1-NEXT:    srlw $r6 = $r6, 29
; V1-NEXT:    srlw $r9 = $r9, 29
; V1-NEXT:    ;; # (end cycle 7)
; V1-NEXT:    insf $r0 = $r5, 63, 32
; V1-NEXT:    insf $r1 = $r7, 63, 32
; V1-NEXT:    iorw $r2 = $r2, $r6
; V1-NEXT:    iorw $r4 = $r4, $r9
; V1-NEXT:    ;; # (end cycle 8)
; V1-NEXT:    insf $r2 = $r4, 63, 32
; V1-NEXT:    ret
; V1-NEXT:    ;; # (end cycle 9)
;
; V2-LABEL: fshl_ri:
; V2:       # %bb.0:
; V2-NEXT:    sllw $r3 = $r3, 3
; V2-NEXT:    srlw $r7 = $r7, 29
; V2-NEXT:    srad $r8 = $r7, 32
; V2-NEXT:    srad $r9 = $r3, 32
; V2-NEXT:    ;; # (end cycle 0)
; V2-NEXT:    srlw $r8 = $r8, 29
; V2-NEXT:    sllw $r9 = $r9, 3
; V2-NEXT:    srad $r10 = $r6, 32
; V2-NEXT:    srad $r11 = $r2, 32
; V2-NEXT:    ;; # (end cycle 1)
; V2-NEXT:    iorw $r3 = $r3, $r7
; V2-NEXT:    srlw $r7 = $r10, 29
; V2-NEXT:    iorw $r8 = $r9, $r8
; V2-NEXT:    sllw $r9 = $r11, 3
; V2-NEXT:    ;; # (end cycle 2)
; V2-NEXT:    sllw $r2 = $r2, 3
; V2-NEXT:    srlw $r6 = $r6, 29
; V2-NEXT:    srad $r10 = $r5, 32
; V2-NEXT:    srad $r11 = $r1, 32
; V2-NEXT:    ;; # (end cycle 3)
; V2-NEXT:    iorw $r2 = $r2, $r6
; V2-NEXT:    srlw $r6 = $r10, 29
; V2-NEXT:    iorw $r7 = $r9, $r7
; V2-NEXT:    sllw $r9 = $r11, 3
; V2-NEXT:    ;; # (end cycle 4)
; V2-NEXT:    sllw $r1 = $r1, 3
; V2-NEXT:    srlw $r5 = $r5, 29
; V2-NEXT:    srad $r10 = $r4, 32
; V2-NEXT:    srad $r11 = $r0, 32
; V2-NEXT:    ;; # (end cycle 5)
; V2-NEXT:    sllw $r0 = $r0, 3
; V2-NEXT:    srlw $r4 = $r4, 29
; V2-NEXT:    srlw $r10 = $r10, 29
; V2-NEXT:    sllw $r11 = $r11, 3
; V2-NEXT:    ;; # (end cycle 6)
; V2-NEXT:    iorw $r0 = $r0, $r4
; V2-NEXT:    iorw $r1 = $r1, $r5
; V2-NEXT:    iorw $r5 = $r11, $r10
; V2-NEXT:    iorw $r6 = $r9, $r6
; V2-NEXT:    ;; # (end cycle 7)
; V2-NEXT:    insf $r0 = $r5, 63, 32
; V2-NEXT:    insf $r1 = $r6, 63, 32
; V2-NEXT:    insf $r2 = $r7, 63, 32
; V2-NEXT:    insf $r3 = $r8, 63, 32
; V2-NEXT:    ret
; V2-NEXT:    ;; # (end cycle 8)
  %r = call <8 x i32> @llvm.fshl.v4i32(<8 x i32> %a, <8 x i32> %b, <8 x i32> <i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3>)
  ret <8 x i32> %r
}

define <8 x i32> @fshl_vec(<8 x i32> %a, <8 x i32> %b, <8 x i32> %c) {
; V1-LABEL: fshl_vec:
; V1:       # %bb.0:
; V1-NEXT:    srlw $r7 = $r7, 1
; V1-NEXT:    srad $r15 = $r11, 32
; V1-NEXT:    srad $r16 = $r7, 32
; V1-NEXT:    srad $r32 = $r3, 32
; V1-NEXT:    ;; # (end cycle 0)
; V1-NEXT:    andw $r15 = $r15, 31
; V1-NEXT:    srlw $r16 = $r16, 1
; V1-NEXT:    andnw $r17 = $r15, 31
; V1-NEXT:    srad $r33 = $r0, 32
; V1-NEXT:    ;; # (end cycle 1)
; V1-NEXT:    sllw $r15 = $r32, $r15
; V1-NEXT:    srlw $r16 = $r16, $r17
; V1-NEXT:    srad $r17 = $r10, 32
; V1-NEXT:    srad $r32 = $r6, 32
; V1-NEXT:    ;; # (end cycle 2)
; V1-NEXT:    srlw $r6 = $r6, 1
; V1-NEXT:    andw $r11 = $r11, 31
; V1-NEXT:    iorw $r15 = $r15, $r16
; V1-NEXT:    andnw $r16 = $r11, 31
; V1-NEXT:    ;; # (end cycle 3)
; V1-NEXT:    sllw $r3 = $r3, $r11
; V1-NEXT:    srlw $r7 = $r7, $r16
; V1-NEXT:    andnw $r11 = $r17, 31
; V1-NEXT:    srlw $r16 = $r32, 1
; V1-NEXT:    ;; # (end cycle 4)
; V1-NEXT:    iorw $r3 = $r3, $r7
; V1-NEXT:    srlw $r7 = $r16, $r11
; V1-NEXT:    srad $r11 = $r2, 32
; V1-NEXT:    andw $r16 = $r17, 31
; V1-NEXT:    ;; # (end cycle 5)
; V1-NEXT:    andw $r10 = $r10, 31
; V1-NEXT:    sllw $r11 = $r11, $r16
; V1-NEXT:    andnw $r16 = $r10, 31
; V1-NEXT:    srad $r32 = $r1, 32
; V1-NEXT:    ;; # (end cycle 6)
; V1-NEXT:    sllw $r2 = $r2, $r10
; V1-NEXT:    srlw $r6 = $r6, $r16
; V1-NEXT:    srad $r10 = $r9, 32
; V1-NEXT:    srad $r16 = $r5, 32
; V1-NEXT:    ;; # (end cycle 7)
; V1-NEXT:    srlw $r5 = $r5, 1
; V1-NEXT:    andw $r10 = $r10, 31
; V1-NEXT:    srlw $r16 = $r16, 1
; V1-NEXT:    andnw $r17 = $r10, 31
; V1-NEXT:    ;; # (end cycle 8)
; V1-NEXT:    andw $r9 = $r9, 31
; V1-NEXT:    sllw $r10 = $r32, $r10
; V1-NEXT:    srlw $r16 = $r16, $r17
; V1-NEXT:    andnw $r17 = $r9, 31
; V1-NEXT:    ;; # (end cycle 9)
; V1-NEXT:    sllw $r1 = $r1, $r9
; V1-NEXT:    srlw $r5 = $r5, $r17
; V1-NEXT:    srad $r17 = $r8, 32
; V1-NEXT:    srad $r32 = $r4, 32
; V1-NEXT:    ;; # (end cycle 10)
; V1-NEXT:    andnw $r9 = $r17, 31
; V1-NEXT:    andw $r17 = $r17, 31
; V1-NEXT:    srlw $r32 = $r32, 1
; V1-NEXT:    andnw $r34 = $r8, 31
; V1-NEXT:    ;; # (end cycle 11)
; V1-NEXT:    srlw $r4 = $r4, 1
; V1-NEXT:    andw $r8 = $r8, 31
; V1-NEXT:    srlw $r9 = $r32, $r9
; V1-NEXT:    sllw $r17 = $r33, $r17
; V1-NEXT:    ;; # (end cycle 12)
; V1-NEXT:    sllw $r0 = $r0, $r8
; V1-NEXT:    iorw $r1 = $r1, $r5
; V1-NEXT:    srlw $r4 = $r4, $r34
; V1-NEXT:    iorw $r8 = $r10, $r16
; V1-NEXT:    ;; # (end cycle 13)
; V1-NEXT:    iorw $r0 = $r0, $r4
; V1-NEXT:    iorw $r2 = $r2, $r6
; V1-NEXT:    iorw $r4 = $r11, $r7
; V1-NEXT:    iorw $r5 = $r17, $r9
; V1-NEXT:    ;; # (end cycle 14)
; V1-NEXT:    insf $r0 = $r5, 63, 32
; V1-NEXT:    insf $r1 = $r8, 63, 32
; V1-NEXT:    ;; # (end cycle 15)
; V1-NEXT:    insf $r2 = $r4, 63, 32
; V1-NEXT:    insf $r3 = $r15, 63, 32
; V1-NEXT:    ret
; V1-NEXT:    ;; # (end cycle 16)
;
; V2-LABEL: fshl_vec:
; V2:       # %bb.0:
; V2-NEXT:    srlw $r7 = $r7, 1
; V2-NEXT:    srad $r15 = $r11, 32
; V2-NEXT:    srad $r16 = $r7, 32
; V2-NEXT:    srad $r32 = $r3, 32
; V2-NEXT:    ;; # (end cycle 0)
; V2-NEXT:    andw $r15 = $r15, 31
; V2-NEXT:    srlw $r16 = $r16, 1
; V2-NEXT:    andnw $r17 = $r15, 31
; V2-NEXT:    ;; # (end cycle 1)
; V2-NEXT:    sllw $r15 = $r32, $r15
; V2-NEXT:    srlw $r16 = $r16, $r17
; V2-NEXT:    srad $r17 = $r2, 32
; V2-NEXT:    andnw $r32 = $r10, 31
; V2-NEXT:    ;; # (end cycle 2)
; V2-NEXT:    andw $r11 = $r11, 31
; V2-NEXT:    iorw $r15 = $r15, $r16
; V2-NEXT:    andnw $r16 = $r11, 31
; V2-NEXT:    ;; # (end cycle 3)
; V2-NEXT:    sllw $r3 = $r3, $r11
; V2-NEXT:    srlw $r7 = $r7, $r16
; V2-NEXT:    srad $r11 = $r10, 32
; V2-NEXT:    srad $r16 = $r6, 32
; V2-NEXT:    ;; # (end cycle 4)
; V2-NEXT:    iorw $r3 = $r3, $r7
; V2-NEXT:    srlw $r6 = $r6, 1
; V2-NEXT:    andnw $r7 = $r11, 31
; V2-NEXT:    andw $r10 = $r10, 31
; V2-NEXT:    ;; # (end cycle 5)
; V2-NEXT:    sllw $r2 = $r2, $r10
; V2-NEXT:    srlw $r6 = $r6, $r32
; V2-NEXT:    andw $r11 = $r11, 31
; V2-NEXT:    srlw $r16 = $r16, 1
; V2-NEXT:    ;; # (end cycle 6)
; V2-NEXT:    iorw $r2 = $r2, $r6
; V2-NEXT:    srad $r6 = $r9, 32
; V2-NEXT:    srlw $r7 = $r16, $r7
; V2-NEXT:    sllw $r11 = $r17, $r11
; V2-NEXT:    ;; # (end cycle 7)
; V2-NEXT:    iorw $r7 = $r11, $r7
; V2-NEXT:    srad $r10 = $r5, 32
; V2-NEXT:    andnw $r11 = $r6, 31
; V2-NEXT:    srad $r16 = $r1, 32
; V2-NEXT:    ;; # (end cycle 8)
; V2-NEXT:    srlw $r5 = $r5, 1
; V2-NEXT:    andw $r6 = $r6, 31
; V2-NEXT:    srlw $r10 = $r10, 1
; V2-NEXT:    srad $r17 = $r0, 32
; V2-NEXT:    ;; # (end cycle 9)
; V2-NEXT:    sllw $r6 = $r16, $r6
; V2-NEXT:    andw $r9 = $r9, 31
; V2-NEXT:    srlw $r10 = $r10, $r11
; V2-NEXT:    andnw $r11 = $r9, 31
; V2-NEXT:    ;; # (end cycle 10)
; V2-NEXT:    sllw $r1 = $r1, $r9
; V2-NEXT:    srlw $r5 = $r5, $r11
; V2-NEXT:    srad $r11 = $r8, 32
; V2-NEXT:    srad $r16 = $r4, 32
; V2-NEXT:    ;; # (end cycle 11)
; V2-NEXT:    andnw $r9 = $r11, 31
; V2-NEXT:    andw $r11 = $r11, 31
; V2-NEXT:    srlw $r16 = $r16, 1
; V2-NEXT:    andnw $r32 = $r8, 31
; V2-NEXT:    ;; # (end cycle 12)
; V2-NEXT:    srlw $r4 = $r4, 1
; V2-NEXT:    andw $r8 = $r8, 31
; V2-NEXT:    srlw $r9 = $r16, $r9
; V2-NEXT:    sllw $r11 = $r17, $r11
; V2-NEXT:    ;; # (end cycle 13)
; V2-NEXT:    sllw $r0 = $r0, $r8
; V2-NEXT:    iorw $r1 = $r1, $r5
; V2-NEXT:    srlw $r4 = $r4, $r32
; V2-NEXT:    iorw $r6 = $r6, $r10
; V2-NEXT:    ;; # (end cycle 14)
; V2-NEXT:    iorw $r0 = $r0, $r4
; V2-NEXT:    insf $r1 = $r6, 63, 32
; V2-NEXT:    insf $r2 = $r7, 63, 32
; V2-NEXT:    iorw $r5 = $r11, $r9
; V2-NEXT:    ;; # (end cycle 15)
; V2-NEXT:    insf $r0 = $r5, 63, 32
; V2-NEXT:    insf $r3 = $r15, 63, 32
; V2-NEXT:    ret
; V2-NEXT:    ;; # (end cycle 16)
  %r = call <8 x i32> @llvm.fshl.v4i32(<8 x i32> %a, <8 x i32> %b, <8 x i32> %c)
  ret <8 x i32> %r
}
define <8 x i32> @fshr_rr(<8 x i32> %a, <8 x i32> %b, i32 %c) {
; V1-LABEL: fshr_rr:
; V1:       # %bb.0:
; V1-NEXT:    sllw $r3 = $r3, 1
; V1-NEXT:    andnw $r8 = $r8, 31
; V1-NEXT:    srad $r9 = $r3, 32
; V1-NEXT:    andw $r10 = $r8, 31
; V1-NEXT:    ;; # (end cycle 0)
; V1-NEXT:    sllw $r3 = $r3, $r8
; V1-NEXT:    srlw $r7 = $r7, $r10
; V1-NEXT:    sllw $r9 = $r9, 1
; V1-NEXT:    srad $r11 = $r7, 32
; V1-NEXT:    ;; # (end cycle 1)
; V1-NEXT:    sllw $r9 = $r9, $r8
; V1-NEXT:    srlw $r11 = $r11, $r10
; V1-NEXT:    srad $r15 = $r6, 32
; V1-NEXT:    srad $r16 = $r2, 32
; V1-NEXT:    ;; # (end cycle 2)
; V1-NEXT:    iorw $r3 = $r3, $r7
; V1-NEXT:    srlw $r7 = $r15, $r10
; V1-NEXT:    srad $r15 = $r5, 32
; V1-NEXT:    srad $r17 = $r4, 32
; V1-NEXT:    ;; # (end cycle 3)
; V1-NEXT:    srlw $r6 = $r6, $r10
; V1-NEXT:    iorw $r9 = $r9, $r11
; V1-NEXT:    sllw $r11 = $r16, 1
; V1-NEXT:    srlw $r15 = $r15, $r10
; V1-NEXT:    ;; # (end cycle 4)
; V1-NEXT:    srlw $r4 = $r4, $r10
; V1-NEXT:    srlw $r5 = $r5, $r10
; V1-NEXT:    srad $r16 = $r1, 32
; V1-NEXT:    srlw $r17 = $r17, $r10
; V1-NEXT:    ;; # (end cycle 5)
; V1-NEXT:    sllw $r1 = $r1, 1
; V1-NEXT:    sllw $r2 = $r2, 1
; V1-NEXT:    srad $r10 = $r0, 32
; V1-NEXT:    sllw $r16 = $r16, 1
; V1-NEXT:    ;; # (end cycle 6)
; V1-NEXT:    sllw $r0 = $r0, 1
; V1-NEXT:    sllw $r2 = $r2, $r8
; V1-NEXT:    sllw $r10 = $r10, 1
; V1-NEXT:    sllw $r11 = $r11, $r8
; V1-NEXT:    ;; # (end cycle 7)
; V1-NEXT:    sllw $r0 = $r0, $r8
; V1-NEXT:    sllw $r1 = $r1, $r8
; V1-NEXT:    sllw $r10 = $r10, $r8
; V1-NEXT:    sllw $r16 = $r16, $r8
; V1-NEXT:    ;; # (end cycle 8)
; V1-NEXT:    iorw $r0 = $r0, $r4
; V1-NEXT:    iorw $r1 = $r1, $r5
; V1-NEXT:    iorw $r5 = $r10, $r17
; V1-NEXT:    iorw $r8 = $r16, $r15
; V1-NEXT:    ;; # (end cycle 9)
; V1-NEXT:    insf $r0 = $r5, 63, 32
; V1-NEXT:    insf $r1 = $r8, 63, 32
; V1-NEXT:    iorw $r2 = $r2, $r6
; V1-NEXT:    iorw $r4 = $r11, $r7
; V1-NEXT:    ;; # (end cycle 10)
; V1-NEXT:    insf $r2 = $r4, 63, 32
; V1-NEXT:    insf $r3 = $r9, 63, 32
; V1-NEXT:    ret
; V1-NEXT:    ;; # (end cycle 11)
;
; V2-LABEL: fshr_rr:
; V2:       # %bb.0:
; V2-NEXT:    andnw $r8 = $r8, 31
; V2-NEXT:    srad $r9 = $r3, 32
; V2-NEXT:    andw $r10 = $r8, 31
; V2-NEXT:    srad $r11 = $r7, 32
; V2-NEXT:    ;; # (end cycle 0)
; V2-NEXT:    sllw $r3 = $r3, 1
; V2-NEXT:    srlw $r7 = $r7, $r10
; V2-NEXT:    sllw $r9 = $r9, 1
; V2-NEXT:    srlw $r11 = $r11, $r10
; V2-NEXT:    ;; # (end cycle 1)
; V2-NEXT:    sllw $r3 = $r3, $r8
; V2-NEXT:    srlw $r6 = $r6, $r10
; V2-NEXT:    sllw $r9 = $r9, $r8
; V2-NEXT:    srad $r15 = $r6, 32
; V2-NEXT:    ;; # (end cycle 2)
; V2-NEXT:    iorw $r3 = $r3, $r7
; V2-NEXT:    srlw $r7 = $r15, $r10
; V2-NEXT:    iorw $r9 = $r9, $r11
; V2-NEXT:    srad $r11 = $r2, 32
; V2-NEXT:    ;; # (end cycle 3)
; V2-NEXT:    sllw $r2 = $r2, 1
; V2-NEXT:    srlw $r4 = $r4, $r10
; V2-NEXT:    sllw $r11 = $r11, 1
; V2-NEXT:    srad $r15 = $r4, 32
; V2-NEXT:    ;; # (end cycle 4)
; V2-NEXT:    sllw $r2 = $r2, $r8
; V2-NEXT:    insf $r3 = $r9, 63, 32
; V2-NEXT:    sllw $r11 = $r11, $r8
; V2-NEXT:    srlw $r15 = $r15, $r10
; V2-NEXT:    ;; # (end cycle 5)
; V2-NEXT:    iorw $r2 = $r2, $r6
; V2-NEXT:    srlw $r5 = $r5, $r10
; V2-NEXT:    iorw $r7 = $r11, $r7
; V2-NEXT:    srad $r11 = $r5, 32
; V2-NEXT:    ;; # (end cycle 6)
; V2-NEXT:    sllw $r1 = $r1, 1
; V2-NEXT:    srlw $r6 = $r11, $r10
; V2-NEXT:    srad $r10 = $r0, 32
; V2-NEXT:    srad $r11 = $r1, 32
; V2-NEXT:    ;; # (end cycle 7)
; V2-NEXT:    sllw $r0 = $r0, 1
; V2-NEXT:    sllw $r1 = $r1, $r8
; V2-NEXT:    sllw $r10 = $r10, 1
; V2-NEXT:    sllw $r11 = $r11, 1
; V2-NEXT:    ;; # (end cycle 8)
; V2-NEXT:    sllw $r0 = $r0, $r8
; V2-NEXT:    iorw $r1 = $r1, $r5
; V2-NEXT:    sllw $r10 = $r10, $r8
; V2-NEXT:    sllw $r11 = $r11, $r8
; V2-NEXT:    ;; # (end cycle 9)
; V2-NEXT:    iorw $r0 = $r0, $r4
; V2-NEXT:    insf $r2 = $r7, 63, 32
; V2-NEXT:    iorw $r5 = $r10, $r15
; V2-NEXT:    iorw $r6 = $r11, $r6
; V2-NEXT:    ;; # (end cycle 10)
; V2-NEXT:    insf $r0 = $r5, 63, 32
; V2-NEXT:    insf $r1 = $r6, 63, 32
; V2-NEXT:    ret
; V2-NEXT:    ;; # (end cycle 11)
  %i = insertelement <8 x i32> undef, i32 %c, i32 0
  %s = shufflevector <8 x i32> %i, <8 x i32> undef, <8 x i32> zeroinitializer
  %r = call <8 x i32> @llvm.fshr.v4i32(<8 x i32> %a, <8 x i32> %b, <8 x i32> %s)
  ret <8 x i32> %r
}

define <8 x i32> @fshr_ri(<8 x i32> %a, <8 x i32> %b, i32 %c) {
; V1-LABEL: fshr_ri:
; V1:       # %bb.0:
; V1-NEXT:    sllw $r3 = $r3, 29
; V1-NEXT:    srlw $r7 = $r7, 3
; V1-NEXT:    srad $r8 = $r7, 32
; V1-NEXT:    srad $r9 = $r3, 32
; V1-NEXT:    ;; # (end cycle 0)
; V1-NEXT:    iorw $r3 = $r3, $r7
; V1-NEXT:    srad $r7 = $r5, 32
; V1-NEXT:    srlw $r8 = $r8, 3
; V1-NEXT:    sllw $r9 = $r9, 29
; V1-NEXT:    ;; # (end cycle 1)
; V1-NEXT:    srlw $r5 = $r5, 3
; V1-NEXT:    srlw $r7 = $r7, 3
; V1-NEXT:    iorw $r8 = $r9, $r8
; V1-NEXT:    srad $r9 = $r1, 32
; V1-NEXT:    ;; # (end cycle 2)
; V1-NEXT:    sllw $r1 = $r1, 29
; V1-NEXT:    insf $r3 = $r8, 63, 32
; V1-NEXT:    sllw $r9 = $r9, 29
; V1-NEXT:    ;; # (end cycle 3)
; V1-NEXT:    iorw $r1 = $r1, $r5
; V1-NEXT:    srad $r5 = $r4, 32
; V1-NEXT:    iorw $r7 = $r9, $r7
; V1-NEXT:    srad $r9 = $r0, 32
; V1-NEXT:    ;; # (end cycle 4)
; V1-NEXT:    sllw $r0 = $r0, 29
; V1-NEXT:    srlw $r4 = $r4, 3
; V1-NEXT:    srlw $r5 = $r5, 3
; V1-NEXT:    sllw $r9 = $r9, 29
; V1-NEXT:    ;; # (end cycle 5)
; V1-NEXT:    iorw $r0 = $r0, $r4
; V1-NEXT:    srad $r4 = $r2, 32
; V1-NEXT:    iorw $r5 = $r9, $r5
; V1-NEXT:    srad $r9 = $r6, 32
; V1-NEXT:    ;; # (end cycle 6)
; V1-NEXT:    sllw $r2 = $r2, 29
; V1-NEXT:    sllw $r4 = $r4, 29
; V1-NEXT:    srlw $r6 = $r6, 3
; V1-NEXT:    srlw $r9 = $r9, 3
; V1-NEXT:    ;; # (end cycle 7)
; V1-NEXT:    insf $r0 = $r5, 63, 32
; V1-NEXT:    insf $r1 = $r7, 63, 32
; V1-NEXT:    iorw $r2 = $r2, $r6
; V1-NEXT:    iorw $r4 = $r4, $r9
; V1-NEXT:    ;; # (end cycle 8)
; V1-NEXT:    insf $r2 = $r4, 63, 32
; V1-NEXT:    ret
; V1-NEXT:    ;; # (end cycle 9)
;
; V2-LABEL: fshr_ri:
; V2:       # %bb.0:
; V2-NEXT:    sllw $r3 = $r3, 29
; V2-NEXT:    srlw $r7 = $r7, 3
; V2-NEXT:    srad $r8 = $r7, 32
; V2-NEXT:    srad $r9 = $r3, 32
; V2-NEXT:    ;; # (end cycle 0)
; V2-NEXT:    srlw $r8 = $r8, 3
; V2-NEXT:    sllw $r9 = $r9, 29
; V2-NEXT:    srad $r10 = $r6, 32
; V2-NEXT:    srad $r11 = $r2, 32
; V2-NEXT:    ;; # (end cycle 1)
; V2-NEXT:    iorw $r3 = $r3, $r7
; V2-NEXT:    srlw $r7 = $r10, 3
; V2-NEXT:    iorw $r8 = $r9, $r8
; V2-NEXT:    sllw $r9 = $r11, 29
; V2-NEXT:    ;; # (end cycle 2)
; V2-NEXT:    sllw $r2 = $r2, 29
; V2-NEXT:    srlw $r6 = $r6, 3
; V2-NEXT:    srad $r10 = $r5, 32
; V2-NEXT:    srad $r11 = $r1, 32
; V2-NEXT:    ;; # (end cycle 3)
; V2-NEXT:    iorw $r2 = $r2, $r6
; V2-NEXT:    srlw $r6 = $r10, 3
; V2-NEXT:    iorw $r7 = $r9, $r7
; V2-NEXT:    sllw $r9 = $r11, 29
; V2-NEXT:    ;; # (end cycle 4)
; V2-NEXT:    sllw $r1 = $r1, 29
; V2-NEXT:    srlw $r5 = $r5, 3
; V2-NEXT:    srad $r10 = $r4, 32
; V2-NEXT:    srad $r11 = $r0, 32
; V2-NEXT:    ;; # (end cycle 5)
; V2-NEXT:    sllw $r0 = $r0, 29
; V2-NEXT:    srlw $r4 = $r4, 3
; V2-NEXT:    srlw $r10 = $r10, 3
; V2-NEXT:    sllw $r11 = $r11, 29
; V2-NEXT:    ;; # (end cycle 6)
; V2-NEXT:    iorw $r0 = $r0, $r4
; V2-NEXT:    iorw $r1 = $r1, $r5
; V2-NEXT:    iorw $r5 = $r11, $r10
; V2-NEXT:    iorw $r6 = $r9, $r6
; V2-NEXT:    ;; # (end cycle 7)
; V2-NEXT:    insf $r0 = $r5, 63, 32
; V2-NEXT:    insf $r1 = $r6, 63, 32
; V2-NEXT:    insf $r2 = $r7, 63, 32
; V2-NEXT:    insf $r3 = $r8, 63, 32
; V2-NEXT:    ret
; V2-NEXT:    ;; # (end cycle 8)
  %r = call <8 x i32> @llvm.fshr.v4i32(<8 x i32> %a, <8 x i32> %b, <8 x i32> <i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3>)
  ret <8 x i32> %r
}

define <8 x i32> @fshr_vec(<8 x i32> %a, <8 x i32> %b, <8 x i32> %c) {
; V1-LABEL: fshr_vec:
; V1:       # %bb.0:
; V1-NEXT:    sllw $r3 = $r3, 1
; V1-NEXT:    srad $r15 = $r11, 32
; V1-NEXT:    srad $r16 = $r7, 32
; V1-NEXT:    srad $r32 = $r3, 32
; V1-NEXT:    ;; # (end cycle 0)
; V1-NEXT:    andnw $r15 = $r15, 31
; V1-NEXT:    andw $r17 = $r15, 31
; V1-NEXT:    srad $r33 = $r0, 32
; V1-NEXT:    andw $r34 = $r8, 31
; V1-NEXT:    ;; # (end cycle 1)
; V1-NEXT:    srlw $r16 = $r16, $r17
; V1-NEXT:    sllw $r17 = $r32, 1
; V1-NEXT:    andw $r32 = $r10, 31
; V1-NEXT:    sllw $r33 = $r33, 1
; V1-NEXT:    ;; # (end cycle 2)
; V1-NEXT:    sllw $r0 = $r0, 1
; V1-NEXT:    andnw $r11 = $r11, 31
; V1-NEXT:    sllw $r15 = $r17, $r15
; V1-NEXT:    andw $r17 = $r11, 31
; V1-NEXT:    ;; # (end cycle 3)
; V1-NEXT:    sllw $r3 = $r3, $r11
; V1-NEXT:    srad $r11 = $r10, 32
; V1-NEXT:    iorw $r15 = $r15, $r16
; V1-NEXT:    srad $r16 = $r2, 32
; V1-NEXT:    ;; # (end cycle 4)
; V1-NEXT:    srlw $r7 = $r7, $r17
; V1-NEXT:    andnw $r11 = $r11, 31
; V1-NEXT:    sllw $r16 = $r16, 1
; V1-NEXT:    andw $r17 = $r11, 31
; V1-NEXT:    ;; # (end cycle 5)
; V1-NEXT:    iorw $r3 = $r3, $r7
; V1-NEXT:    srad $r7 = $r6, 32
; V1-NEXT:    sllw $r11 = $r16, $r11
; V1-NEXT:    srad $r16 = $r9, 32
; V1-NEXT:    ;; # (end cycle 6)
; V1-NEXT:    sllw $r2 = $r2, 1
; V1-NEXT:    srlw $r6 = $r6, $r32
; V1-NEXT:    srlw $r7 = $r7, $r17
; V1-NEXT:    andnw $r10 = $r10, 31
; V1-NEXT:    ;; # (end cycle 7)
; V1-NEXT:    sllw $r2 = $r2, $r10
; V1-NEXT:    andnw $r16 = $r16, 31
; V1-NEXT:    srad $r17 = $r5, 32
; V1-NEXT:    andw $r32 = $r16, 31
; V1-NEXT:    ;; # (end cycle 8)
; V1-NEXT:    andnw $r9 = $r9, 31
; V1-NEXT:    srlw $r10 = $r17, $r32
; V1-NEXT:    srad $r17 = $r1, 32
; V1-NEXT:    andw $r32 = $r9, 31
; V1-NEXT:    ;; # (end cycle 9)
; V1-NEXT:    sllw $r1 = $r1, 1
; V1-NEXT:    iorw $r2 = $r2, $r6
; V1-NEXT:    srlw $r5 = $r5, $r32
; V1-NEXT:    sllw $r17 = $r17, 1
; V1-NEXT:    ;; # (end cycle 10)
; V1-NEXT:    sllw $r1 = $r1, $r9
; V1-NEXT:    srad $r9 = $r8, 32
; V1-NEXT:    sllw $r16 = $r17, $r16
; V1-NEXT:    srad $r17 = $r4, 32
; V1-NEXT:    ;; # (end cycle 11)
; V1-NEXT:    srlw $r4 = $r4, $r34
; V1-NEXT:    andnw $r8 = $r8, 31
; V1-NEXT:    andnw $r9 = $r9, 31
; V1-NEXT:    andw $r32 = $r9, 31
; V1-NEXT:    ;; # (end cycle 12)
; V1-NEXT:    sllw $r0 = $r0, $r8
; V1-NEXT:    iorw $r8 = $r16, $r10
; V1-NEXT:    sllw $r9 = $r33, $r9
; V1-NEXT:    srlw $r17 = $r17, $r32
; V1-NEXT:    ;; # (end cycle 13)
; V1-NEXT:    iorw $r0 = $r0, $r4
; V1-NEXT:    iorw $r1 = $r1, $r5
; V1-NEXT:    iorw $r4 = $r11, $r7
; V1-NEXT:    iorw $r5 = $r9, $r17
; V1-NEXT:    ;; # (end cycle 14)
; V1-NEXT:    insf $r0 = $r5, 63, 32
; V1-NEXT:    insf $r1 = $r8, 63, 32
; V1-NEXT:    ;; # (end cycle 15)
; V1-NEXT:    insf $r2 = $r4, 63, 32
; V1-NEXT:    insf $r3 = $r15, 63, 32
; V1-NEXT:    ret
; V1-NEXT:    ;; # (end cycle 16)
;
; V2-LABEL: fshr_vec:
; V2:       # %bb.0:
; V2-NEXT:    srad $r15 = $r11, 32
; V2-NEXT:    srad $r16 = $r3, 32
; V2-NEXT:    srad $r17 = $r7, 32
; V2-NEXT:    srad $r33 = $r2, 32
; V2-NEXT:    ;; # (end cycle 0)
; V2-NEXT:    sllw $r3 = $r3, 1
; V2-NEXT:    andnw $r15 = $r15, 31
; V2-NEXT:    sllw $r16 = $r16, 1
; V2-NEXT:    andw $r32 = $r15, 31
; V2-NEXT:    ;; # (end cycle 1)
; V2-NEXT:    sllw $r15 = $r16, $r15
; V2-NEXT:    andw $r16 = $r11, 31
; V2-NEXT:    srlw $r17 = $r17, $r32
; V2-NEXT:    srad $r32 = $r10, 32
; V2-NEXT:    ;; # (end cycle 2)
; V2-NEXT:    andnw $r11 = $r11, 31
; V2-NEXT:    andnw $r32 = $r32, 31
; V2-NEXT:    srad $r34 = $r6, 32
; V2-NEXT:    andw $r35 = $r32, 31
; V2-NEXT:    ;; # (end cycle 3)
; V2-NEXT:    sllw $r3 = $r3, $r11
; V2-NEXT:    srlw $r7 = $r7, $r16
; V2-NEXT:    srlw $r11 = $r34, $r35
; V2-NEXT:    sllw $r33 = $r33, 1
; V2-NEXT:    ;; # (end cycle 4)
; V2-NEXT:    sllw $r2 = $r2, 1
; V2-NEXT:    iorw $r3 = $r3, $r7
; V2-NEXT:    iorw $r15 = $r15, $r17
; V2-NEXT:    sllw $r16 = $r33, $r32
; V2-NEXT:    ;; # (end cycle 5)
; V2-NEXT:    iorw $r7 = $r16, $r11
; V2-NEXT:    andnw $r10 = $r10, 31
; V2-NEXT:    andw $r11 = $r10, 31
; V2-NEXT:    srad $r16 = $r9, 32
; V2-NEXT:    ;; # (end cycle 6)
; V2-NEXT:    sllw $r2 = $r2, $r10
; V2-NEXT:    srlw $r6 = $r6, $r11
; V2-NEXT:    srad $r10 = $r5, 32
; V2-NEXT:    andw $r11 = $r16, 31
; V2-NEXT:    ;; # (end cycle 7)
; V2-NEXT:    iorw $r2 = $r2, $r6
; V2-NEXT:    srlw $r6 = $r10, $r11
; V2-NEXT:    andnw $r10 = $r16, 31
; V2-NEXT:    srad $r11 = $r1, 32
; V2-NEXT:    ;; # (end cycle 8)
; V2-NEXT:    sllw $r1 = $r1, 1
; V2-NEXT:    andnw $r9 = $r9, 31
; V2-NEXT:    sllw $r11 = $r11, 1
; V2-NEXT:    andw $r16 = $r9, 31
; V2-NEXT:    ;; # (end cycle 9)
; V2-NEXT:    sllw $r1 = $r1, $r9
; V2-NEXT:    srad $r9 = $r8, 32
; V2-NEXT:    sllw $r10 = $r11, $r10
; V2-NEXT:    srad $r17 = $r0, 32
; V2-NEXT:    ;; # (end cycle 10)
; V2-NEXT:    srlw $r5 = $r5, $r16
; V2-NEXT:    andnw $r9 = $r9, 31
; V2-NEXT:    srad $r11 = $r4, 32
; V2-NEXT:    andw $r16 = $r9, 31
; V2-NEXT:    ;; # (end cycle 11)
; V2-NEXT:    sllw $r0 = $r0, 1
; V2-NEXT:    andnw $r8 = $r8, 31
; V2-NEXT:    sllw $r17 = $r17, 1
; V2-NEXT:    andw $r32 = $r8, 31
; V2-NEXT:    ;; # (end cycle 12)
; V2-NEXT:    sllw $r0 = $r0, $r8
; V2-NEXT:    srlw $r4 = $r4, $r32
; V2-NEXT:    sllw $r9 = $r17, $r9
; V2-NEXT:    srlw $r11 = $r11, $r16
; V2-NEXT:    ;; # (end cycle 13)
; V2-NEXT:    iorw $r0 = $r0, $r4
; V2-NEXT:    iorw $r1 = $r1, $r5
; V2-NEXT:    iorw $r5 = $r9, $r11
; V2-NEXT:    iorw $r6 = $r10, $r6
; V2-NEXT:    ;; # (end cycle 14)
; V2-NEXT:    insf $r0 = $r5, 63, 32
; V2-NEXT:    insf $r1 = $r6, 63, 32
; V2-NEXT:    insf $r2 = $r7, 63, 32
; V2-NEXT:    insf $r3 = $r15, 63, 32
; V2-NEXT:    ret
; V2-NEXT:    ;; # (end cycle 15)
  %r = call <8 x i32> @llvm.fshr.v4i32(<8 x i32> %a, <8 x i32> %b, <8 x i32> %c)
  ret <8 x i32> %r
}

declare <8 x i32> @llvm.fshr.v4i32(<8 x i32>, <8 x i32>, <8 x i32>)
declare <8 x i32> @llvm.fshl.v4i32(<8 x i32>, <8 x i32>, <8 x i32>)

define <8 x i32> @sdiv(<8 x i32> %a, <8 x i32> %b) {
; ALL-LABEL: sdiv:
; ALL:       # %bb.0:
; ALL-NEXT:    addd $r12 = $r12, -32
; ALL-NEXT:    get $r16 = $ra
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sd 24[$r12] = $r16
; ALL-NEXT:    call __divv8si3
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    ld $r16 = 24[$r12]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    set $ra = $r16
; ALL-NEXT:    addd $r12 = $r12, 32
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %r = sdiv <8 x i32> %a, %b
  ret <8 x i32> %r
}

define <8 x i32> @sdiv_vs(<8 x i32> %a, i32 %b) {
; ALL-LABEL: sdiv_vs:
; ALL:       # %bb.0:
; ALL-NEXT:    insf $r4 = $r4, 63, 32
; ALL-NEXT:    addd $r12 = $r12, -32
; ALL-NEXT:    get $r16 = $ra
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sd 24[$r12] = $r16
; ALL-NEXT:    copyd $r5 = $r4
; ALL-NEXT:    copyd $r6 = $r4
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    copyd $r7 = $r4
; ALL-NEXT:    call __divv8si3
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    ld $r16 = 24[$r12]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    set $ra = $r16
; ALL-NEXT:    addd $r12 = $r12, 32
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %v0 = insertelement <8 x i32> undef, i32 %b, i32 0
  %splat = shufflevector <8 x i32> %v0, <8 x i32> undef, <8 x i32> zeroinitializer
  %div = sdiv <8 x i32> %a, %splat
  ret <8 x i32> %div
}

define <8 x i32> @sdiv_sv(<8 x i32> %a, i32 %b) {
; ALL-LABEL: sdiv_sv:
; ALL:       # %bb.0:
; ALL-NEXT:    insf $r4 = $r4, 63, 32
; ALL-NEXT:    addd $r12 = $r12, -32
; ALL-NEXT:    get $r16 = $ra
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sd 24[$r12] = $r16
; ALL-NEXT:    copyd $r6 = $r2
; ALL-NEXT:    copyd $r7 = $r3
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    copyd $r0 = $r4
; ALL-NEXT:    copyd $r1 = $r4
; ALL-NEXT:    copyd $r5 = $r1
; ALL-NEXT:    copyd $r8 = $r0
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    copyd $r2 = $r4
; ALL-NEXT:    copyd $r3 = $r4
; ALL-NEXT:    copyd $r4 = $r8
; ALL-NEXT:    call __divv8si3
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    ld $r16 = 24[$r12]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    set $ra = $r16
; ALL-NEXT:    addd $r12 = $r12, 32
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %v0 = insertelement <8 x i32> undef, i32 %b, i32 0
  %splat = shufflevector <8 x i32> %v0, <8 x i32> undef, <8 x i32> zeroinitializer
  %div = sdiv <8 x i32> %splat, %a
  ret <8 x i32> %div
}

define <8 x i32> @sdiv_ss(i32 %a, i32 %b) {
; ALL-LABEL: sdiv_ss:
; ALL:       # %bb.0:
; ALL-NEXT:    insf $r1 = $r1, 63, 32
; ALL-NEXT:    addd $r12 = $r12, -32
; ALL-NEXT:    get $r16 = $ra
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sd 24[$r12] = $r16
; ALL-NEXT:    copyd $r0 = $r1
; ALL-NEXT:    copyd $r4 = $r0
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    copyd $r2 = $r1
; ALL-NEXT:    copyd $r3 = $r1
; ALL-NEXT:    insf $r4 = $r4, 63, 32
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    copyd $r5 = $r4
; ALL-NEXT:    copyd $r6 = $r4
; ALL-NEXT:    copyd $r7 = $r4
; ALL-NEXT:    call __divv8si3
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    ld $r16 = 24[$r12]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    set $ra = $r16
; ALL-NEXT:    addd $r12 = $r12, 32
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %va = insertelement <8 x i32> undef, i32 %b, i32 0
  %vb = insertelement <8 x i32> undef, i32 %a, i32 0
  %splata = shufflevector <8 x i32> %va, <8 x i32> undef, <8 x i32> zeroinitializer
  %splatb = shufflevector <8 x i32> %vb, <8 x i32> undef, <8 x i32> zeroinitializer
  %div = sdiv <8 x i32> %splata, %splatb
  ret <8 x i32> %div
}

define <8 x i32> @srem(<8 x i32> %a, <8 x i32> %b) {
; ALL-LABEL: srem:
; ALL:       # %bb.0:
; ALL-NEXT:    addd $r12 = $r12, -32
; ALL-NEXT:    get $r16 = $ra
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sd 24[$r12] = $r16
; ALL-NEXT:    call __modv8si3
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    ld $r16 = 24[$r12]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    set $ra = $r16
; ALL-NEXT:    addd $r12 = $r12, 32
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %r = srem <8 x i32> %a, %b
  ret <8 x i32> %r
}

define <8 x i32> @srem_vs(<8 x i32> %a, i32 %b) {
; ALL-LABEL: srem_vs:
; ALL:       # %bb.0:
; ALL-NEXT:    insf $r4 = $r4, 63, 32
; ALL-NEXT:    addd $r12 = $r12, -32
; ALL-NEXT:    get $r16 = $ra
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sd 24[$r12] = $r16
; ALL-NEXT:    copyd $r5 = $r4
; ALL-NEXT:    copyd $r6 = $r4
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    copyd $r7 = $r4
; ALL-NEXT:    call __modv8si3
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    ld $r16 = 24[$r12]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    set $ra = $r16
; ALL-NEXT:    addd $r12 = $r12, 32
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %v0 = insertelement <8 x i32> undef, i32 %b, i32 0
  %splat = shufflevector <8 x i32> %v0, <8 x i32> undef, <8 x i32> zeroinitializer
  %div = srem <8 x i32> %a, %splat
  ret <8 x i32> %div
}

define <8 x i32> @srem_sv(<8 x i32> %a, i32 %b) {
; ALL-LABEL: srem_sv:
; ALL:       # %bb.0:
; ALL-NEXT:    insf $r4 = $r4, 63, 32
; ALL-NEXT:    addd $r12 = $r12, -32
; ALL-NEXT:    get $r16 = $ra
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sd 24[$r12] = $r16
; ALL-NEXT:    copyd $r6 = $r2
; ALL-NEXT:    copyd $r7 = $r3
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    copyd $r0 = $r4
; ALL-NEXT:    copyd $r1 = $r4
; ALL-NEXT:    copyd $r5 = $r1
; ALL-NEXT:    copyd $r8 = $r0
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    copyd $r2 = $r4
; ALL-NEXT:    copyd $r3 = $r4
; ALL-NEXT:    copyd $r4 = $r8
; ALL-NEXT:    call __modv8si3
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    ld $r16 = 24[$r12]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    set $ra = $r16
; ALL-NEXT:    addd $r12 = $r12, 32
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %v0 = insertelement <8 x i32> undef, i32 %b, i32 0
  %splat = shufflevector <8 x i32> %v0, <8 x i32> undef, <8 x i32> zeroinitializer
  %div = srem <8 x i32> %splat, %a
  ret <8 x i32> %div
}

define <8 x i32> @srem_ss(i32 %a, i32 %b) {
; ALL-LABEL: srem_ss:
; ALL:       # %bb.0:
; ALL-NEXT:    insf $r1 = $r1, 63, 32
; ALL-NEXT:    addd $r12 = $r12, -32
; ALL-NEXT:    get $r16 = $ra
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sd 24[$r12] = $r16
; ALL-NEXT:    copyd $r0 = $r1
; ALL-NEXT:    copyd $r4 = $r0
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    copyd $r2 = $r1
; ALL-NEXT:    copyd $r3 = $r1
; ALL-NEXT:    insf $r4 = $r4, 63, 32
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    copyd $r5 = $r4
; ALL-NEXT:    copyd $r6 = $r4
; ALL-NEXT:    copyd $r7 = $r4
; ALL-NEXT:    call __modv8si3
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    ld $r16 = 24[$r12]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    set $ra = $r16
; ALL-NEXT:    addd $r12 = $r12, 32
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %va = insertelement <8 x i32> undef, i32 %b, i32 0
  %vb = insertelement <8 x i32> undef, i32 %a, i32 0
  %splata = shufflevector <8 x i32> %va, <8 x i32> undef, <8 x i32> zeroinitializer
  %splatb = shufflevector <8 x i32> %vb, <8 x i32> undef, <8 x i32> zeroinitializer
  %div = srem <8 x i32> %splata, %splatb
  ret <8 x i32> %div
}

define <8 x i32> @udiv(<8 x i32> %a, <8 x i32> %b) {
; ALL-LABEL: udiv:
; ALL:       # %bb.0:
; ALL-NEXT:    addd $r12 = $r12, -32
; ALL-NEXT:    get $r16 = $ra
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sd 24[$r12] = $r16
; ALL-NEXT:    call __udivv8si3
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    ld $r16 = 24[$r12]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    set $ra = $r16
; ALL-NEXT:    addd $r12 = $r12, 32
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %r = udiv <8 x i32> %a, %b
  ret <8 x i32> %r
}

define <8 x i32> @udiv_vs(<8 x i32> %a, i32 %b) {
; ALL-LABEL: udiv_vs:
; ALL:       # %bb.0:
; ALL-NEXT:    insf $r4 = $r4, 63, 32
; ALL-NEXT:    addd $r12 = $r12, -32
; ALL-NEXT:    get $r16 = $ra
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sd 24[$r12] = $r16
; ALL-NEXT:    copyd $r5 = $r4
; ALL-NEXT:    copyd $r6 = $r4
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    copyd $r7 = $r4
; ALL-NEXT:    call __udivv8si3
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    ld $r16 = 24[$r12]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    set $ra = $r16
; ALL-NEXT:    addd $r12 = $r12, 32
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %v0 = insertelement <8 x i32> undef, i32 %b, i32 0
  %splat = shufflevector <8 x i32> %v0, <8 x i32> undef, <8 x i32> zeroinitializer
  %div = udiv <8 x i32> %a, %splat
  ret <8 x i32> %div
}

define <8 x i32> @udiv_sv(<8 x i32> %a, i32 %b) {
; ALL-LABEL: udiv_sv:
; ALL:       # %bb.0:
; ALL-NEXT:    insf $r4 = $r4, 63, 32
; ALL-NEXT:    addd $r12 = $r12, -32
; ALL-NEXT:    get $r16 = $ra
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sd 24[$r12] = $r16
; ALL-NEXT:    copyd $r6 = $r2
; ALL-NEXT:    copyd $r7 = $r3
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    copyd $r0 = $r4
; ALL-NEXT:    copyd $r1 = $r4
; ALL-NEXT:    copyd $r5 = $r1
; ALL-NEXT:    copyd $r8 = $r0
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    copyd $r2 = $r4
; ALL-NEXT:    copyd $r3 = $r4
; ALL-NEXT:    copyd $r4 = $r8
; ALL-NEXT:    call __udivv8si3
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    ld $r16 = 24[$r12]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    set $ra = $r16
; ALL-NEXT:    addd $r12 = $r12, 32
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %v0 = insertelement <8 x i32> undef, i32 %b, i32 0
  %splat = shufflevector <8 x i32> %v0, <8 x i32> undef, <8 x i32> zeroinitializer
  %div = udiv <8 x i32> %splat, %a
  ret <8 x i32> %div
}

define <8 x i32> @udiv_ss(i32 %a, i32 %b) {
; ALL-LABEL: udiv_ss:
; ALL:       # %bb.0:
; ALL-NEXT:    insf $r1 = $r1, 63, 32
; ALL-NEXT:    addd $r12 = $r12, -32
; ALL-NEXT:    get $r16 = $ra
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sd 24[$r12] = $r16
; ALL-NEXT:    copyd $r0 = $r1
; ALL-NEXT:    copyd $r4 = $r0
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    copyd $r2 = $r1
; ALL-NEXT:    copyd $r3 = $r1
; ALL-NEXT:    insf $r4 = $r4, 63, 32
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    copyd $r5 = $r4
; ALL-NEXT:    copyd $r6 = $r4
; ALL-NEXT:    copyd $r7 = $r4
; ALL-NEXT:    call __udivv8si3
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    ld $r16 = 24[$r12]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    set $ra = $r16
; ALL-NEXT:    addd $r12 = $r12, 32
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %va = insertelement <8 x i32> undef, i32 %b, i32 0
  %vb = insertelement <8 x i32> undef, i32 %a, i32 0
  %splata = shufflevector <8 x i32> %va, <8 x i32> undef, <8 x i32> zeroinitializer
  %splatb = shufflevector <8 x i32> %vb, <8 x i32> undef, <8 x i32> zeroinitializer
  %div = udiv <8 x i32> %splata, %splatb
  ret <8 x i32> %div
}

define <8 x i32> @urem(<8 x i32> %a, <8 x i32> %b) {
; ALL-LABEL: urem:
; ALL:       # %bb.0:
; ALL-NEXT:    addd $r12 = $r12, -32
; ALL-NEXT:    get $r16 = $ra
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sd 24[$r12] = $r16
; ALL-NEXT:    call __umodv8si3
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    ld $r16 = 24[$r12]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    set $ra = $r16
; ALL-NEXT:    addd $r12 = $r12, 32
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %r = urem <8 x i32> %a, %b
  ret <8 x i32> %r
}

define <8 x i32> @urem_vs(<8 x i32> %a, i32 %b) {
; ALL-LABEL: urem_vs:
; ALL:       # %bb.0:
; ALL-NEXT:    insf $r4 = $r4, 63, 32
; ALL-NEXT:    addd $r12 = $r12, -32
; ALL-NEXT:    get $r16 = $ra
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sd 24[$r12] = $r16
; ALL-NEXT:    copyd $r5 = $r4
; ALL-NEXT:    copyd $r6 = $r4
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    copyd $r7 = $r4
; ALL-NEXT:    call __umodv8si3
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    ld $r16 = 24[$r12]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    set $ra = $r16
; ALL-NEXT:    addd $r12 = $r12, 32
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %v0 = insertelement <8 x i32> undef, i32 %b, i32 0
  %splat = shufflevector <8 x i32> %v0, <8 x i32> undef, <8 x i32> zeroinitializer
  %div = urem <8 x i32> %a, %splat
  ret <8 x i32> %div
}

define <8 x i32> @urem_sv(<8 x i32> %a, i32 %b) {
; ALL-LABEL: urem_sv:
; ALL:       # %bb.0:
; ALL-NEXT:    insf $r4 = $r4, 63, 32
; ALL-NEXT:    addd $r12 = $r12, -32
; ALL-NEXT:    get $r16 = $ra
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sd 24[$r12] = $r16
; ALL-NEXT:    copyd $r6 = $r2
; ALL-NEXT:    copyd $r7 = $r3
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    copyd $r0 = $r4
; ALL-NEXT:    copyd $r1 = $r4
; ALL-NEXT:    copyd $r5 = $r1
; ALL-NEXT:    copyd $r8 = $r0
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    copyd $r2 = $r4
; ALL-NEXT:    copyd $r3 = $r4
; ALL-NEXT:    copyd $r4 = $r8
; ALL-NEXT:    call __umodv8si3
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    ld $r16 = 24[$r12]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    set $ra = $r16
; ALL-NEXT:    addd $r12 = $r12, 32
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %v0 = insertelement <8 x i32> undef, i32 %b, i32 0
  %splat = shufflevector <8 x i32> %v0, <8 x i32> undef, <8 x i32> zeroinitializer
  %div = urem <8 x i32> %splat, %a
  ret <8 x i32> %div
}

define <8 x i32> @urem_ss(i32 %a, i32 %b) {
; ALL-LABEL: urem_ss:
; ALL:       # %bb.0:
; ALL-NEXT:    insf $r1 = $r1, 63, 32
; ALL-NEXT:    addd $r12 = $r12, -32
; ALL-NEXT:    get $r16 = $ra
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sd 24[$r12] = $r16
; ALL-NEXT:    copyd $r0 = $r1
; ALL-NEXT:    copyd $r4 = $r0
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    copyd $r2 = $r1
; ALL-NEXT:    copyd $r3 = $r1
; ALL-NEXT:    insf $r4 = $r4, 63, 32
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    copyd $r5 = $r4
; ALL-NEXT:    copyd $r6 = $r4
; ALL-NEXT:    copyd $r7 = $r4
; ALL-NEXT:    call __umodv8si3
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    ld $r16 = 24[$r12]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    set $ra = $r16
; ALL-NEXT:    addd $r12 = $r12, 32
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %va = insertelement <8 x i32> undef, i32 %b, i32 0
  %vb = insertelement <8 x i32> undef, i32 %a, i32 0
  %splata = shufflevector <8 x i32> %va, <8 x i32> undef, <8 x i32> zeroinitializer
  %splatb = shufflevector <8 x i32> %vb, <8 x i32> undef, <8 x i32> zeroinitializer
  %div = urem <8 x i32> %splata, %splatb
  ret <8 x i32> %div
}

define <8 x i32> @sdivrem(<8 x i32> %a, <8 x i32> %b) {
; V1-LABEL: sdivrem:
; V1:       # %bb.0:
; V1-NEXT:    addd $r12 = $r12, -96
; V1-NEXT:    get $r16 = $ra
; V1-NEXT:    ;; # (end cycle 0)
; V1-NEXT:    sd 88[$r12] = $r16
; V1-NEXT:    ;; # (end cycle 1)
; V1-NEXT:    sq 72[$r12] = $r24r25
; V1-NEXT:    ;; # (end cycle 2)
; V1-NEXT:    so 40[$r12] = $r20r21r22r23
; V1-NEXT:    ;; # (end cycle 3)
; V1-NEXT:    sq 24[$r12] = $r18r19
; V1-NEXT:    copyd $r18 = $r6
; V1-NEXT:    copyd $r19 = $r7
; V1-NEXT:    copyd $r21 = $r5
; V1-NEXT:    ;; # (end cycle 4)
; V1-NEXT:    copyd $r20 = $r4
; V1-NEXT:    copyd $r22 = $r2
; V1-NEXT:    copyd $r23 = $r3
; V1-NEXT:    copyd $r25 = $r1
; V1-NEXT:    ;; # (end cycle 5)
; V1-NEXT:    copyd $r24 = $r0
; V1-NEXT:    call __divv8si3
; V1-NEXT:    ;; # (end cycle 6)
; V1-NEXT:    msbfwp $r24 = $r0, $r20
; V1-NEXT:    ;; # (end cycle 0)
; V1-NEXT:    msbfwp $r25 = $r1, $r21
; V1-NEXT:    ;; # (end cycle 1)
; V1-NEXT:    addwp $r0 = $r0, $r24
; V1-NEXT:    msbfwp $r22 = $r2, $r18
; V1-NEXT:    ;; # (end cycle 2)
; V1-NEXT:    lq $r18r19 = 24[$r12]
; V1-NEXT:    addwp $r1 = $r1, $r25
; V1-NEXT:    msbfwp $r23 = $r3, $r19
; V1-NEXT:    ;; # (end cycle 3)
; V1-NEXT:    addwp $r2 = $r2, $r22
; V1-NEXT:    ;; # (end cycle 4)
; V1-NEXT:    lo $r20r21r22r23 = 40[$r12]
; V1-NEXT:    addwp $r3 = $r3, $r23
; V1-NEXT:    ;; # (end cycle 5)
; V1-NEXT:    lq $r24r25 = 72[$r12]
; V1-NEXT:    ;; # (end cycle 6)
; V1-NEXT:    ld $r16 = 88[$r12]
; V1-NEXT:    ;; # (end cycle 7)
; V1-NEXT:    set $ra = $r16
; V1-NEXT:    addd $r12 = $r12, 96
; V1-NEXT:    ;; # (end cycle 12)
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: sdivrem:
; V2:       # %bb.0:
; V2-NEXT:    addd $r12 = $r12, -96
; V2-NEXT:    get $r16 = $ra
; V2-NEXT:    ;; # (end cycle 0)
; V2-NEXT:    sd 88[$r12] = $r16
; V2-NEXT:    ;; # (end cycle 1)
; V2-NEXT:    sq 72[$r12] = $r24r25
; V2-NEXT:    ;; # (end cycle 2)
; V2-NEXT:    so 40[$r12] = $r20r21r22r23
; V2-NEXT:    ;; # (end cycle 3)
; V2-NEXT:    sq 24[$r12] = $r18r19
; V2-NEXT:    copyd $r19 = $r7
; V2-NEXT:    copyd $r21 = $r5
; V2-NEXT:    copyd $r23 = $r3
; V2-NEXT:    ;; # (end cycle 4)
; V2-NEXT:    copyd $r18 = $r6
; V2-NEXT:    copyd $r20 = $r4
; V2-NEXT:    copyd $r22 = $r2
; V2-NEXT:    copyd $r25 = $r1
; V2-NEXT:    ;; # (end cycle 5)
; V2-NEXT:    copyd $r24 = $r0
; V2-NEXT:    call __divv8si3
; V2-NEXT:    ;; # (end cycle 6)
; V2-NEXT:    msbfwq $r24r25 = $r0r1, $r20r21
; V2-NEXT:    ;; # (end cycle 0)
; V2-NEXT:    lq $r18r19 = 24[$r12]
; V2-NEXT:    msbfwq $r22r23 = $r2r3, $r18r19
; V2-NEXT:    ;; # (end cycle 1)
; V2-NEXT:    addwp $r0 = $r0, $r24
; V2-NEXT:    addwp $r1 = $r1, $r25
; V2-NEXT:    ;; # (end cycle 2)
; V2-NEXT:    lo $r20r21r22r23 = 40[$r12]
; V2-NEXT:    addwp $r2 = $r2, $r22
; V2-NEXT:    addwp $r3 = $r3, $r23
; V2-NEXT:    ;; # (end cycle 3)
; V2-NEXT:    lq $r24r25 = 72[$r12]
; V2-NEXT:    ;; # (end cycle 4)
; V2-NEXT:    ld $r16 = 88[$r12]
; V2-NEXT:    ;; # (end cycle 5)
; V2-NEXT:    set $ra = $r16
; V2-NEXT:    addd $r12 = $r12, 96
; V2-NEXT:    ;; # (end cycle 10)
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %q = sdiv <8 x i32> %a, %b
  %r = srem <8 x i32> %a, %b
  %res = add <8 x i32> %q, %r
  ret <8 x i32> %res
}

define <8 x i32> @udivrem(<8 x i32> %a, <8 x i32> %b) {
; V1-LABEL: udivrem:
; V1:       # %bb.0:
; V1-NEXT:    addd $r12 = $r12, -96
; V1-NEXT:    get $r16 = $ra
; V1-NEXT:    ;; # (end cycle 0)
; V1-NEXT:    sd 88[$r12] = $r16
; V1-NEXT:    ;; # (end cycle 1)
; V1-NEXT:    sq 72[$r12] = $r24r25
; V1-NEXT:    ;; # (end cycle 2)
; V1-NEXT:    so 40[$r12] = $r20r21r22r23
; V1-NEXT:    ;; # (end cycle 3)
; V1-NEXT:    sq 24[$r12] = $r18r19
; V1-NEXT:    copyd $r18 = $r6
; V1-NEXT:    copyd $r19 = $r7
; V1-NEXT:    copyd $r21 = $r5
; V1-NEXT:    ;; # (end cycle 4)
; V1-NEXT:    copyd $r20 = $r4
; V1-NEXT:    copyd $r22 = $r2
; V1-NEXT:    copyd $r23 = $r3
; V1-NEXT:    copyd $r25 = $r1
; V1-NEXT:    ;; # (end cycle 5)
; V1-NEXT:    copyd $r24 = $r0
; V1-NEXT:    call __udivv8si3
; V1-NEXT:    ;; # (end cycle 6)
; V1-NEXT:    msbfwp $r24 = $r0, $r20
; V1-NEXT:    ;; # (end cycle 0)
; V1-NEXT:    msbfwp $r25 = $r1, $r21
; V1-NEXT:    ;; # (end cycle 1)
; V1-NEXT:    addwp $r0 = $r0, $r24
; V1-NEXT:    msbfwp $r22 = $r2, $r18
; V1-NEXT:    ;; # (end cycle 2)
; V1-NEXT:    lq $r18r19 = 24[$r12]
; V1-NEXT:    addwp $r1 = $r1, $r25
; V1-NEXT:    msbfwp $r23 = $r3, $r19
; V1-NEXT:    ;; # (end cycle 3)
; V1-NEXT:    addwp $r2 = $r2, $r22
; V1-NEXT:    ;; # (end cycle 4)
; V1-NEXT:    lo $r20r21r22r23 = 40[$r12]
; V1-NEXT:    addwp $r3 = $r3, $r23
; V1-NEXT:    ;; # (end cycle 5)
; V1-NEXT:    lq $r24r25 = 72[$r12]
; V1-NEXT:    ;; # (end cycle 6)
; V1-NEXT:    ld $r16 = 88[$r12]
; V1-NEXT:    ;; # (end cycle 7)
; V1-NEXT:    set $ra = $r16
; V1-NEXT:    addd $r12 = $r12, 96
; V1-NEXT:    ;; # (end cycle 12)
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: udivrem:
; V2:       # %bb.0:
; V2-NEXT:    addd $r12 = $r12, -96
; V2-NEXT:    get $r16 = $ra
; V2-NEXT:    ;; # (end cycle 0)
; V2-NEXT:    sd 88[$r12] = $r16
; V2-NEXT:    ;; # (end cycle 1)
; V2-NEXT:    sq 72[$r12] = $r24r25
; V2-NEXT:    ;; # (end cycle 2)
; V2-NEXT:    so 40[$r12] = $r20r21r22r23
; V2-NEXT:    ;; # (end cycle 3)
; V2-NEXT:    sq 24[$r12] = $r18r19
; V2-NEXT:    copyd $r19 = $r7
; V2-NEXT:    copyd $r21 = $r5
; V2-NEXT:    copyd $r23 = $r3
; V2-NEXT:    ;; # (end cycle 4)
; V2-NEXT:    copyd $r18 = $r6
; V2-NEXT:    copyd $r20 = $r4
; V2-NEXT:    copyd $r22 = $r2
; V2-NEXT:    copyd $r25 = $r1
; V2-NEXT:    ;; # (end cycle 5)
; V2-NEXT:    copyd $r24 = $r0
; V2-NEXT:    call __udivv8si3
; V2-NEXT:    ;; # (end cycle 6)
; V2-NEXT:    msbfwq $r24r25 = $r0r1, $r20r21
; V2-NEXT:    ;; # (end cycle 0)
; V2-NEXT:    lq $r18r19 = 24[$r12]
; V2-NEXT:    msbfwq $r22r23 = $r2r3, $r18r19
; V2-NEXT:    ;; # (end cycle 1)
; V2-NEXT:    addwp $r0 = $r0, $r24
; V2-NEXT:    addwp $r1 = $r1, $r25
; V2-NEXT:    ;; # (end cycle 2)
; V2-NEXT:    lo $r20r21r22r23 = 40[$r12]
; V2-NEXT:    addwp $r2 = $r2, $r22
; V2-NEXT:    addwp $r3 = $r3, $r23
; V2-NEXT:    ;; # (end cycle 3)
; V2-NEXT:    lq $r24r25 = 72[$r12]
; V2-NEXT:    ;; # (end cycle 4)
; V2-NEXT:    ld $r16 = 88[$r12]
; V2-NEXT:    ;; # (end cycle 5)
; V2-NEXT:    set $ra = $r16
; V2-NEXT:    addd $r12 = $r12, 96
; V2-NEXT:    ;; # (end cycle 10)
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %q = udiv <8 x i32> %a, %b
  %r = urem <8 x i32> %a, %b
  %res = add <8 x i32> %q, %r
  ret <8 x i32> %res
}
