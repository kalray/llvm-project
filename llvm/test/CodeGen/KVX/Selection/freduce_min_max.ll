; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mcpu=kv3-1 -O2 -o - %s | FileCheck %s --check-prefix=CV1
; RUN: llc -mcpu=kv3-2 -O2 -o - %s | FileCheck %s --check-prefix=CV2
; RUN: clang -O2 -march=kv3-1 -c -o /dev/null %s
; RUN: clang -O2 -march=kv3-2 -c -o /dev/null %s

target triple = "kvx-kalray-cos"

define float @red_max_float2(<2 x float> %0) {
; CV1-LABEL: red_max_float2:
; CV1:       # %bb.0:
; CV1-NEXT:    srad $r1 = $r0, 32
; CV1-NEXT:    sllw $r2 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    compw.gtu $r2 = $r2, 0xff000000
; CV1-NEXT:    fcompw.olt $r3 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    orw $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    cmoved.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: red_max_float2:
; CV2:       # %bb.0:
; CV2-NEXT:    srad $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    fmaxw $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 1)
  %2 = tail call float @llvm.vector.reduce.fmax.v2f32(<2 x float> %0)
  ret float %2
}

declare float @llvm.vector.reduce.fmax.v2f32(<2 x float>)

define float @red_max_float4(<4 x float> %0) {
; CV1-LABEL: red_max_float4:
; CV1:       # %bb.0:
; CV1-NEXT:    fcompnwp.olt $r2 = $r0, $r1
; CV1-NEXT:    sllwps $r3 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    compnwp.gtu $r3 = $r3, 0xff000000.@
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    ord $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    cmovewp.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srad $r1 = $r0, 32
; CV1-NEXT:    sllw $r2 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    compw.gtu $r2 = $r2, 0xff000000
; CV1-NEXT:    fcompw.olt $r3 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    orw $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    cmoved.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: red_max_float4:
; CV2:       # %bb.0:
; CV2-NEXT:    fmaxwp $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srad $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    fmaxw $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 2)
  %2 = tail call float @llvm.vector.reduce.fmax.v4f32(<4 x float> %0)
  ret float %2
}

declare float @llvm.vector.reduce.fmax.v4f32(<4 x float>)

define float @red_max_float8(<8 x float> %0) {
; CV1-LABEL: red_max_float8:
; CV1:       # %bb.0:
; CV1-NEXT:    srad $r4 = $r0, 32
; CV1-NEXT:    srad $r5 = $r1, 32
; CV1-NEXT:    srad $r6 = $r2, 32
; CV1-NEXT:    srad $r9 = $r3, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sllw $r7 = $r0, 1
; CV1-NEXT:    sllw $r8 = $r4, 1
; CV1-NEXT:    sllw $r10 = $r5, 1
; CV1-NEXT:    fcompw.olt $r17 = $r5, $r9
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    compw.gtu $r7 = $r7, 0xff000000
; CV1-NEXT:    compw.gtu $r8 = $r8, 0xff000000
; CV1-NEXT:    compw.gtu $r10 = $r10, 0xff000000
; CV1-NEXT:    sllw $r11 = $r1, 1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    orw $r10 = $r17, $r10
; CV1-NEXT:    fcompw.olt $r15 = $r4, $r6
; CV1-NEXT:    fcompw.olt $r16 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    cmoved.odd $r10 ? $r5 = $r9
; CV1-NEXT:    orw $r7 = $r16, $r7
; CV1-NEXT:    orw $r8 = $r15, $r8
; CV1-NEXT:    fcompw.olt $r9 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    cmoved.odd $r7 ? $r0 = $r2
; CV1-NEXT:    cmoved.odd $r8 ? $r4 = $r6
; CV1-NEXT:    compw.gtu $r10 = $r11, 0xff000000
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    insf $r0 = $r4, 63, 32
; CV1-NEXT:    orw $r6 = $r9, $r10
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    cmoved.odd $r6 ? $r1 = $r3
; CV1-NEXT:    sllwps $r2 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r5, 63, 32
; CV1-NEXT:    compnwp.gtu $r2 = $r2, 0xff000000.@
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    fcompnwp.olt $r3 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    ord $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    cmovewp.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    srad $r1 = $r0, 32
; CV1-NEXT:    sllw $r2 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    compw.gtu $r2 = $r2, 0xff000000
; CV1-NEXT:    fcompw.olt $r3 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    orw $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    cmoved.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 15)
;
; CV2-LABEL: red_max_float8:
; CV2:       # %bb.0:
; CV2-NEXT:    fmaxw $r1 = $r1, $r3
; CV2-NEXT:    srad $r3 = $r2, 32
; CV2-NEXT:    srad $r4 = $r3, 32
; CV2-NEXT:    srad $r5 = $r1, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    fmaxw $r0 = $r0, $r2
; CV2-NEXT:    fmaxw $r4 = $r5, $r4
; CV2-NEXT:    srad $r6 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r1 = $r4, 63, 32
; CV2-NEXT:    fmaxw $r3 = $r6, $r3
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r0 = $r3, 63, 32
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    fmaxwp $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    srad $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    fmaxw $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 6)
  %2 = tail call float @llvm.vector.reduce.fmax.v8f32(<8 x float> %0)
  ret float %2
}

declare float @llvm.vector.reduce.fmax.v8f32(<8 x float>)

define float @red_max_float16(ptr %0) {
; CV1-LABEL: red_max_float16:
; CV1:       # %bb.0:
; CV1-NEXT:    lo $r8r9r10r11 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sllw $r0 = $r11, 1
; CV1-NEXT:    sllw $r1 = $r9, 1
; CV1-NEXT:    srld $r2 = $r11, 32
; CV1-NEXT:    srld $r3 = $r9, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    compw.gtu $r0 = $r0, 0xff000000
; CV1-NEXT:    compw.gtu $r1 = $r1, 0xff000000
; CV1-NEXT:    fcompw.olt $r15 = $r11, $r7
; CV1-NEXT:    fcompw.olt $r16 = $r9, $r5
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    orw $r0 = $r16, $r1
; CV1-NEXT:    srld $r1 = $r8, 32
; CV1-NEXT:    orw $r15 = $r15, $r0
; CV1-NEXT:    sllw $r16 = $r8, 1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sllw $r0 = $r1, 1
; CV1-NEXT:    cmoved.odd $r0 ? $r9 = $r5
; CV1-NEXT:    compw.gtu $r16 = $r16, 0xff000000
; CV1-NEXT:    fcompw.olt $r35 = $r8, $r4
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    sllw $r0 = $r10, 1
; CV1-NEXT:    compw.gtu $r34 = $r0, 0xff000000
; CV1-NEXT:    srld $r38 = $r7, 32
; CV1-NEXT:    sllw $r39 = $r2, 1
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    sllw $r7 = $r9, 1
; CV1-NEXT:    cmoved.odd $r15 ? $r11 = $r7
; CV1-NEXT:    compw.gtu $r36 = $r0, 0xff000000
; CV1-NEXT:    fcompw.olt $r40 = $r10, $r6
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    copyd $r0 = $r8
; CV1-NEXT:    compw.gtu $r8 = $r39, 0xff000000
; CV1-NEXT:    orw $r16 = $r35, $r16
; CV1-NEXT:    orw $r36 = $r40, $r36
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    compw.gtu $r7 = $r7, 0xff000000
; CV1-NEXT:    fcompw.olt $r15 = $r9, $r11
; CV1-NEXT:    srld $r17 = $r10, 32
; CV1-NEXT:    fcompw.olt $r39 = $r2, $r38
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    srld $r5 = $r5, 32
; CV1-NEXT:    srld $r32 = $r4, 32
; CV1-NEXT:    sllw $r33 = $r3, 1
; CV1-NEXT:    srld $r35 = $r6, 32
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    cmoved.odd $r16 ? $r0 = $r4
; CV1-NEXT:    orw $r4 = $r15, $r7
; CV1-NEXT:    orw $r6 = $r39, $r8
; CV1-NEXT:    cmoved.odd $r36 ? $r10 = $r6
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    cmoved.odd $r6 ? $r2 = $r38
; CV1-NEXT:    cmoved.odd $r4 ? $r9 = $r11
; CV1-NEXT:    compw.gtu $r33 = $r33, 0xff000000
; CV1-NEXT:    sllw $r37 = $r17, 1
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    fcompw.olt $r4 = $r3, $r5
; CV1-NEXT:    fcompw.olt $r6 = $r1, $r32
; CV1-NEXT:    compw.gtu $r8 = $r37, 0xff000000
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    orw $r4 = $r4, $r33
; CV1-NEXT:    orw $r6 = $r6, $r34
; CV1-NEXT:    fcompw.olt $r7 = $r17, $r35
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    cmoved.odd $r6 ? $r1 = $r32
; CV1-NEXT:    cmoved.odd $r4 ? $r3 = $r5
; CV1-NEXT:    orw $r4 = $r7, $r8
; CV1-NEXT:    sllw $r6 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    sllw $r4 = $r1, 1
; CV1-NEXT:    sllw $r5 = $r3, 1
; CV1-NEXT:    fcompw.olt $r7 = $r0, $r10
; CV1-NEXT:    cmoved.odd $r4 ? $r17 = $r35
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    compw.gtu $r4 = $r4, 0xff000000
; CV1-NEXT:    compw.gtu $r6 = $r6, 0xff000000
; CV1-NEXT:    fcompw.olt $r8 = $r1, $r17
; CV1-NEXT:    fcompw.olt $r11 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    orw $r4 = $r8, $r4
; CV1-NEXT:    compw.gtu $r5 = $r5, 0xff000000
; CV1-NEXT:    orw $r6 = $r7, $r6
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    cmoved.odd $r6 ? $r0 = $r10
; CV1-NEXT:    cmoved.odd $r4 ? $r1 = $r17
; CV1-NEXT:    orw $r5 = $r11, $r5
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    insf $r0 = $r1, 63, 32
; CV1-NEXT:    cmoved.odd $r5 ? $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    sllwps $r1 = $r0, 1
; CV1-NEXT:    insf $r9 = $r3, 63, 32
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    compnwp.gtu $r1 = $r1, 0xff000000.@
; CV1-NEXT:    fcompnwp.olt $r2 = $r0, $r9
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    ord $r1 = $r2, $r1
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    cmovewp.odd $r1 ? $r0 = $r9
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    srad $r1 = $r0, 32
; CV1-NEXT:    sllw $r2 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    compw.gtu $r2 = $r2, 0xff000000
; CV1-NEXT:    fcompw.olt $r3 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 27)
; CV1-NEXT:    orw $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 28)
; CV1-NEXT:    cmoved.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 29)
;
; CV2-LABEL: red_max_float16:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r0r1r2r3 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    srld $r8 = $r7, 32
; CV2-NEXT:    srld $r9 = $r5, 32
; CV2-NEXT:    srld $r10 = $r6, 32
; CV2-NEXT:    srld $r11 = $r4, 32
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srld $r15 = $r3, 32
; CV2-NEXT:    srld $r16 = $r1, 32
; CV2-NEXT:    srld $r17 = $r2, 32
; CV2-NEXT:    srld $r32 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    fmaxw $r0 = $r4, $r0
; CV2-NEXT:    fmaxw $r1 = $r5, $r1
; CV2-NEXT:    fmaxw $r2 = $r6, $r2
; CV2-NEXT:    fmaxw $r3 = $r7, $r3
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    fmaxw $r4 = $r8, $r15
; CV2-NEXT:    fmaxw $r5 = $r9, $r16
; CV2-NEXT:    fmaxw $r6 = $r10, $r17
; CV2-NEXT:    fmaxw $r7 = $r11, $r32
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    fmaxw $r0 = $r0, $r2
; CV2-NEXT:    fmaxw $r1 = $r1, $r3
; CV2-NEXT:    fmaxw $r2 = $r7, $r6
; CV2-NEXT:    fmaxw $r3 = $r5, $r4
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r0 = $r2, 63, 32
; CV2-NEXT:    insf $r1 = $r3, 63, 32
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    fmaxwp $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    srad $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    fmaxw $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 11)
  %2 = load <16 x float>, ptr %0
  %3 = tail call float @llvm.vector.reduce.fmax.v16f32(<16 x float> %2)
  ret float %3
}

declare float @llvm.vector.reduce.fmax.v16f32(<16 x float>)

define double @red_max_double2(<2 x double> %0) {
; CV1-LABEL: red_max_double2:
; CV1:       # %bb.0:
; CV1-NEXT:    slld $r2 = $r0, 1
; CV1-NEXT:    fcompd.olt $r3 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    compd.gtu $r2 = $r2, 0xffe0000000000000
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    ord $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    cmoved.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: red_max_double2:
; CV2:       # %bb.0:
; CV2-NEXT:    fmaxd $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %2 = tail call double @llvm.vector.reduce.fmax.v2f64(<2 x double> %0)
  ret double %2
}

declare double @llvm.vector.reduce.fmax.v2f64(<2 x double>)

define double @red_max_double4(<4 x double> %0) {
; CV1-LABEL: red_max_double4:
; CV1:       # %bb.0:
; CV1-NEXT:    slld $r4 = $r0, 1
; CV1-NEXT:    fcompd.olt $r5 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    compd.gtu $r4 = $r4, 0xffe0000000000000
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    ord $r4 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    cmoved.odd $r4 ? $r0 = $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    slld $r1 = $r0, 1
; CV1-NEXT:    fcompd.olt $r4 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    compd.gtu $r1 = $r1, 0xffe0000000000000
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    ord $r1 = $r4, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    cmoved.odd $r1 ? $r0 = $r2
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    slld $r1 = $r0, 1
; CV1-NEXT:    fcompd.olt $r2 = $r0, $r3
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    compd.gtu $r1 = $r1, 0xffe0000000000000
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    ord $r1 = $r2, $r1
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    cmoved.odd $r1 ? $r0 = $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 11)
;
; CV2-LABEL: red_max_double4:
; CV2:       # %bb.0:
; CV2-NEXT:    fmaxd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    fmaxd $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    fmaxd $r0 = $r0, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 2)
  %2 = tail call double @llvm.vector.reduce.fmax.v4f64(<4 x double> %0)
  ret double %2
}

declare double @llvm.vector.reduce.fmax.v4f64(<4 x double>)

define double @red_max_double8(ptr %0) {
; CV1-LABEL: red_max_double8:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r4 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r0r1r2r3 = 0[$r4]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    lo $r4r5r6r7 = 32[$r4]
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    slld $r8 = $r1, 1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    compd.gtu $r8 = $r8, 0xffe0000000000000
; CV1-NEXT:    fcompd.olt $r9 = $r1, $r5
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    ord $r8 = $r9, $r8
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    cmoved.odd $r8 ? $r1 = $r5
; CV1-NEXT:    fcompd.olt $r5 = $r0, $r4
; CV1-NEXT:    slld $r8 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    compd.gtu $r8 = $r8, 0xffe0000000000000
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    ord $r5 = $r5, $r8
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    cmoved.odd $r5 ? $r0 = $r4
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    fcompd.olt $r4 = $r0, $r1
; CV1-NEXT:    slld $r5 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    compd.gtu $r5 = $r5, 0xffe0000000000000
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    ord $r4 = $r4, $r5
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    cmoved.odd $r4 ? $r0 = $r1
; CV1-NEXT:    slld $r1 = $r2, 1
; CV1-NEXT:    fcompd.olt $r4 = $r2, $r6
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    compd.gtu $r1 = $r1, 0xffe0000000000000
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    ord $r1 = $r4, $r1
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    slld $r1 = $r0, 1
; CV1-NEXT:    cmoved.odd $r1 ? $r2 = $r6
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    compd.gtu $r1 = $r1, 0xffe0000000000000
; CV1-NEXT:    fcompd.olt $r4 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    ord $r1 = $r4, $r1
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    cmoved.odd $r1 ? $r0 = $r2
; CV1-NEXT:    slld $r1 = $r3, 1
; CV1-NEXT:    fcompd.olt $r2 = $r3, $r7
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    compd.gtu $r1 = $r1, 0xffe0000000000000
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    ord $r1 = $r2, $r1
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    slld $r1 = $r0, 1
; CV1-NEXT:    cmoved.odd $r1 ? $r3 = $r7
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    compd.gtu $r1 = $r1, 0xffe0000000000000
; CV1-NEXT:    fcompd.olt $r2 = $r0, $r3
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    ord $r1 = $r2, $r1
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    cmoved.odd $r1 ? $r0 = $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 26)
;
; CV2-LABEL: red_max_double8:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r0r1r2r3 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    fmaxd $r0 = $r4, $r0
; CV2-NEXT:    fmaxd $r1 = $r5, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    fmaxd $r0 = $r0, $r1
; CV2-NEXT:    fmaxd $r1 = $r6, $r2
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    fmaxd $r0 = $r0, $r1
; CV2-NEXT:    fmaxd $r1 = $r7, $r3
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    fmaxd $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = load <8 x double>, ptr %0
  %3 = tail call double @llvm.vector.reduce.fmax.v8f64(<8 x double> %2)
  ret double %3
}

declare double @llvm.vector.reduce.fmax.v8f64(<8 x double>)

define double @red_max_double16(ptr %0) {
; CV1-LABEL: red_max_double16:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r4 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r0r1r2r3 = 0[$r4]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    lo $r32r33r34r35 = 64[$r4]
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    lo $r8r9r10r11 = 32[$r4]
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    lo $r4r5r6r7 = 96[$r4]
; CV1-NEXT:    slld $r15 = $r1, 1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    compd.gtu $r15 = $r15, 0xffe0000000000000
; CV1-NEXT:    fcompd.olt $r16 = $r1, $r33
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    ord $r15 = $r16, $r15
; CV1-NEXT:    slld $r16 = $r9, 1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    cmoved.odd $r15 ? $r1 = $r33
; CV1-NEXT:    fcompd.olt $r15 = $r9, $r5
; CV1-NEXT:    compd.gtu $r16 = $r16, 0xffe0000000000000
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    ord $r15 = $r15, $r16
; CV1-NEXT:    slld $r16 = $r1, 1
; CV1-NEXT:    fcompd.olt $r17 = $r8, $r4
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    compd.gtu $r5 = $r16, 0xffe0000000000000
; CV1-NEXT:    cmoved.odd $r15 ? $r9 = $r5
; CV1-NEXT:    slld $r16 = $r8, 1
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    fcompd.olt $r15 = $r1, $r9
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    ord $r5 = $r15, $r5
; CV1-NEXT:    compd.gtu $r15 = $r16, 0xffe0000000000000
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    cmoved.odd $r5 ? $r1 = $r9
; CV1-NEXT:    ord $r5 = $r17, $r15
; CV1-NEXT:    slld $r9 = $r0, 1
; CV1-NEXT:    fcompd.olt $r15 = $r0, $r32
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    compd.gtu $r4 = $r9, 0xffe0000000000000
; CV1-NEXT:    cmoved.odd $r5 ? $r8 = $r4
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    ord $r4 = $r15, $r4
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    cmoved.odd $r4 ? $r0 = $r32
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    slld $r4 = $r0, 1
; CV1-NEXT:    fcompd.olt $r5 = $r0, $r8
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    compd.gtu $r4 = $r4, 0xffe0000000000000
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    ord $r4 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    cmoved.odd $r4 ? $r0 = $r8
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    slld $r4 = $r0, 1
; CV1-NEXT:    fcompd.olt $r5 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    compd.gtu $r4 = $r4, 0xffe0000000000000
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    ord $r4 = $r5, $r4
; CV1-NEXT:    fcompd.olt $r5 = $r2, $r34
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    cmoved.odd $r4 ? $r0 = $r1
; CV1-NEXT:    slld $r1 = $r2, 1
; CV1-NEXT:    slld $r4 = $r10, 1
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    compd.gtu $r1 = $r1, 0xffe0000000000000
; CV1-NEXT:    compd.gtu $r4 = $r4, 0xffe0000000000000
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    ord $r1 = $r5, $r1
; CV1-NEXT:    fcompd.olt $r5 = $r10, $r6
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    cmoved.odd $r1 ? $r2 = $r34
; CV1-NEXT:    ord $r4 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    slld $r1 = $r2, 1
; CV1-NEXT:    cmoved.odd $r4 ? $r10 = $r6
; CV1-NEXT:    ;; # (end cycle 27)
; CV1-NEXT:    compd.gtu $r1 = $r1, 0xffe0000000000000
; CV1-NEXT:    fcompd.olt $r4 = $r2, $r10
; CV1-NEXT:    ;; # (end cycle 28)
; CV1-NEXT:    ord $r1 = $r4, $r1
; CV1-NEXT:    ;; # (end cycle 29)
; CV1-NEXT:    slld $r1 = $r0, 1
; CV1-NEXT:    cmoved.odd $r1 ? $r2 = $r10
; CV1-NEXT:    ;; # (end cycle 30)
; CV1-NEXT:    compd.gtu $r1 = $r1, 0xffe0000000000000
; CV1-NEXT:    fcompd.olt $r4 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 31)
; CV1-NEXT:    ord $r1 = $r4, $r1
; CV1-NEXT:    fcompd.olt $r4 = $r3, $r35
; CV1-NEXT:    ;; # (end cycle 32)
; CV1-NEXT:    cmoved.odd $r1 ? $r0 = $r2
; CV1-NEXT:    slld $r1 = $r3, 1
; CV1-NEXT:    slld $r2 = $r11, 1
; CV1-NEXT:    ;; # (end cycle 33)
; CV1-NEXT:    compd.gtu $r1 = $r1, 0xffe0000000000000
; CV1-NEXT:    compd.gtu $r2 = $r2, 0xffe0000000000000
; CV1-NEXT:    ;; # (end cycle 34)
; CV1-NEXT:    ord $r1 = $r4, $r1
; CV1-NEXT:    fcompd.olt $r4 = $r11, $r7
; CV1-NEXT:    ;; # (end cycle 35)
; CV1-NEXT:    ord $r2 = $r4, $r2
; CV1-NEXT:    cmoved.odd $r1 ? $r3 = $r35
; CV1-NEXT:    ;; # (end cycle 36)
; CV1-NEXT:    slld $r1 = $r3, 1
; CV1-NEXT:    cmoved.odd $r2 ? $r11 = $r7
; CV1-NEXT:    ;; # (end cycle 37)
; CV1-NEXT:    compd.gtu $r1 = $r1, 0xffe0000000000000
; CV1-NEXT:    fcompd.olt $r2 = $r3, $r11
; CV1-NEXT:    ;; # (end cycle 38)
; CV1-NEXT:    ord $r1 = $r2, $r1
; CV1-NEXT:    ;; # (end cycle 39)
; CV1-NEXT:    slld $r1 = $r0, 1
; CV1-NEXT:    cmoved.odd $r1 ? $r3 = $r11
; CV1-NEXT:    ;; # (end cycle 40)
; CV1-NEXT:    compd.gtu $r1 = $r1, 0xffe0000000000000
; CV1-NEXT:    fcompd.olt $r2 = $r0, $r3
; CV1-NEXT:    ;; # (end cycle 41)
; CV1-NEXT:    ord $r1 = $r2, $r1
; CV1-NEXT:    ;; # (end cycle 42)
; CV1-NEXT:    cmoved.odd $r1 ? $r0 = $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 43)
;
; CV2-LABEL: red_max_double16:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r8r9r10r11 = 64[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    lo $r32r33r34r35 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    lo $r0r1r2r3 = 96[$r0]
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    fmaxd $r4 = $r4, $r8
; CV2-NEXT:    fmaxd $r5 = $r5, $r9
; CV2-NEXT:    fmaxd $r6 = $r6, $r10
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    fmaxd $r0 = $r32, $r0
; CV2-NEXT:    fmaxd $r1 = $r33, $r1
; CV2-NEXT:    fmaxd $r2 = $r34, $r2
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    fmaxd $r0 = $r4, $r0
; CV2-NEXT:    fmaxd $r1 = $r5, $r1
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    fmaxd $r0 = $r0, $r1
; CV2-NEXT:    fmaxd $r1 = $r6, $r2
; CV2-NEXT:    fmaxd $r2 = $r35, $r3
; CV2-NEXT:    fmaxd $r3 = $r7, $r11
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    fmaxd $r0 = $r0, $r1
; CV2-NEXT:    fmaxd $r1 = $r3, $r2
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    fmaxd $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 10)
  %2 = load <16 x double>, ptr %0
  %3 = tail call double @llvm.vector.reduce.fmax.v16f64(<16 x double> %2)
  ret double %3
}

declare double @llvm.vector.reduce.fmax.v16f64(<16 x double>)

define half @red_max_half2(<2 x half> %0) {
; CV1-LABEL: red_max_half2:
; CV1:       # %bb.0:
; CV1-NEXT:    srlw $r1 = $r0, 16
; CV1-NEXT:    sllhqs $r2 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    compnhq.gtu $r2 = $r2, 0xf800
; CV1-NEXT:    fcompnhq.olt $r3 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    orw $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    cmovehq.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: red_max_half2:
; CV2:       # %bb.0:
; CV2-NEXT:    srlw $r1 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    fmaxhq $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 1)
  %2 = tail call half @llvm.vector.reduce.fmax.v2f16(<2 x half> %0)
  ret half %2
}

declare half @llvm.vector.reduce.fmax.v2f16(<2 x half>)

define half @red_max_half4(<4 x half> %0) {
; CV1-LABEL: red_max_half4:
; CV1:       # %bb.0:
; CV1-NEXT:    srld $r1 = $r0, 32
; CV1-NEXT:    sllhqs $r2 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    compnhq.gtu $r2 = $r2, 0xf800f800
; CV1-NEXT:    fcompnhq.olt $r3 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    orw $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    cmovehq.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srlw $r1 = $r0, 16
; CV1-NEXT:    sllhqs $r2 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    compnhq.gtu $r2 = $r2, 0xf800
; CV1-NEXT:    fcompnhq.olt $r3 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    orw $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    cmovehq.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: red_max_half4:
; CV2:       # %bb.0:
; CV2-NEXT:    srld $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    fmaxhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    srlw $r1 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    fmaxhq $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 3)
  %2 = tail call half @llvm.vector.reduce.fmax.v4f16(<4 x half> %0)
  ret half %2
}

declare half @llvm.vector.reduce.fmax.v4f16(<4 x half>)

define half @red_max_half8(<8 x half> %0) {
; CV1-LABEL: red_max_half8:
; CV1:       # %bb.0:
; CV1-NEXT:    fcompnhq.olt $r2 = $r0, $r1
; CV1-NEXT:    sllhqs $r3 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    compnhq.gtu $r3 = $r3, 0xf800f800.@
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    ord $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    cmovehq.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srld $r1 = $r0, 32
; CV1-NEXT:    sllhqs $r2 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    compnhq.gtu $r2 = $r2, 0xf800f800
; CV1-NEXT:    fcompnhq.olt $r3 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    orw $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    cmovehq.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    srlw $r1 = $r0, 16
; CV1-NEXT:    sllhqs $r2 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    compnhq.gtu $r2 = $r2, 0xf800
; CV1-NEXT:    fcompnhq.olt $r3 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    orw $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    cmovehq.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 11)
;
; CV2-LABEL: red_max_half8:
; CV2:       # %bb.0:
; CV2-NEXT:    fmaxhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srld $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    fmaxhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    srlw $r1 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    fmaxhq $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 4)
  %2 = tail call half @llvm.vector.reduce.fmax.v8f16(<8 x half> %0)
  ret half %2
}

declare half @llvm.vector.reduce.fmax.v8f16(<8 x half>)

define half @red_max_half16(<16 x half> %0) {
; CV1-LABEL: red_max_half16:
; CV1:       # %bb.0:
; CV1-NEXT:    srld $r7 = $r1, 48
; CV1-NEXT:    srld $r9 = $r1, 32
; CV1-NEXT:    srlw $r10 = $r1, 16
; CV1-NEXT:    srld $r35 = $r3, 48
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sllhqs $r8 = $r7, 1
; CV1-NEXT:    srld $r36 = $r3, 32
; CV1-NEXT:    sllhqs $r37 = $r9, 1
; CV1-NEXT:    srlw $r38 = $r3, 16
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srld $r4 = $r0, 48
; CV1-NEXT:    compnhq.gtu $r8 = $r8, 0xf800
; CV1-NEXT:    sllhqs $r39 = $r10, 1
; CV1-NEXT:    fcompnhq.olt $r42 = $r7, $r35
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    compnhq.gtu $r37 = $r37, 0xf800
; CV1-NEXT:    compnhq.gtu $r39 = $r39, 0xf800
; CV1-NEXT:    sllhqs $r40 = $r1, 1
; CV1-NEXT:    fcompnhq.olt $r43 = $r9, $r36
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srld $r5 = $r0, 32
; CV1-NEXT:    orw $r8 = $r42, $r8
; CV1-NEXT:    srld $r11 = $r2, 48
; CV1-NEXT:    fcompnhq.olt $r42 = $r10, $r38
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    cmovehq.odd $r8 ? $r7 = $r35
; CV1-NEXT:    orw $r8 = $r43, $r37
; CV1-NEXT:    sllhqs $r15 = $r4, 1
; CV1-NEXT:    orw $r35 = $r42, $r39
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r6 = $r0, 16
; CV1-NEXT:    srld $r16 = $r2, 32
; CV1-NEXT:    fcompnhq.olt $r37 = $r1, $r3
; CV1-NEXT:    compnhq.gtu $r39 = $r40, 0xf800
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    compnhq.gtu $r15 = $r15, 0xf800
; CV1-NEXT:    sllhqs $r17 = $r5, 1
; CV1-NEXT:    srlw $r32 = $r2, 16
; CV1-NEXT:    fcompnhq.olt $r41 = $r4, $r11
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    orw $r8 = $r37, $r39
; CV1-NEXT:    cmovehq.odd $r8 ? $r9 = $r36
; CV1-NEXT:    compnhq.gtu $r17 = $r17, 0xf800
; CV1-NEXT:    sllhqs $r33 = $r6, 1
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    cmovehq.odd $r8 ? $r1 = $r3
; CV1-NEXT:    orw $r15 = $r41, $r15
; CV1-NEXT:    compnhq.gtu $r33 = $r33, 0xf800
; CV1-NEXT:    sllhqs $r34 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    fcompnhq.olt $r3 = $r5, $r16
; CV1-NEXT:    cmovehq.odd $r15 ? $r4 = $r11
; CV1-NEXT:    compnhq.gtu $r15 = $r34, 0xf800
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    orw $r3 = $r3, $r17
; CV1-NEXT:    fcompnhq.olt $r8 = $r6, $r32
; CV1-NEXT:    fcompnhq.olt $r11 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    orw $r3 = $r11, $r15
; CV1-NEXT:    cmovehq.odd $r3 ? $r5 = $r16
; CV1-NEXT:    orw $r8 = $r8, $r33
; CV1-NEXT:    cmovehq.odd $r35 ? $r10 = $r38
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    cmovehq.odd $r3 ? $r0 = $r2
; CV1-NEXT:    cmovehq.odd $r8 ? $r6 = $r32
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    insf $r0 = $r6, 31, 16
; CV1-NEXT:    insf $r5 = $r4, 31, 16
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    insf $r1 = $r10, 31, 16
; CV1-NEXT:    insf $r9 = $r7, 31, 16
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    insf $r0 = $r5, 63, 32
; CV1-NEXT:    insf $r1 = $r9, 63, 32
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    sllhqs $r2 = $r0, 1
; CV1-NEXT:    fcompnhq.olt $r3 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    compnhq.gtu $r2 = $r2, 0xf800f800.@
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    ord $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    cmovehq.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    srld $r1 = $r0, 32
; CV1-NEXT:    sllhqs $r2 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    compnhq.gtu $r2 = $r2, 0xf800f800
; CV1-NEXT:    fcompnhq.olt $r3 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    orw $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    cmovehq.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    srlw $r1 = $r0, 16
; CV1-NEXT:    sllhqs $r2 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    compnhq.gtu $r2 = $r2, 0xf800
; CV1-NEXT:    fcompnhq.olt $r3 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    orw $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 27)
; CV1-NEXT:    cmovehq.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 28)
;
; CV2-LABEL: red_max_half16:
; CV2:       # %bb.0:
; CV2-NEXT:    srld $r4 = $r3, 48
; CV2-NEXT:    srld $r5 = $r1, 48
; CV2-NEXT:    srld $r6 = $r3, 32
; CV2-NEXT:    srld $r7 = $r1, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    fmaxhq $r4 = $r5, $r4
; CV2-NEXT:    srlw $r5 = $r3, 16
; CV2-NEXT:    fmaxhq $r6 = $r7, $r6
; CV2-NEXT:    srlw $r7 = $r1, 16
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    fmaxhq $r1 = $r1, $r3
; CV2-NEXT:    srld $r3 = $r0, 48
; CV2-NEXT:    fmaxhq $r5 = $r7, $r5
; CV2-NEXT:    srld $r7 = $r2, 48
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    srld $r8 = $r2, 32
; CV2-NEXT:    srld $r9 = $r0, 32
; CV2-NEXT:    srlw $r10 = $r2, 16
; CV2-NEXT:    srlw $r11 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    fmaxhq $r0 = $r0, $r2
; CV2-NEXT:    fmaxhq $r3 = $r3, $r7
; CV2-NEXT:    fmaxhq $r7 = $r9, $r8
; CV2-NEXT:    fmaxhq $r8 = $r11, $r10
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r0 = $r8, 31, 16
; CV2-NEXT:    insf $r1 = $r5, 31, 16
; CV2-NEXT:    insf $r6 = $r4, 31, 16
; CV2-NEXT:    insf $r7 = $r3, 31, 16
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r0 = $r7, 63, 32
; CV2-NEXT:    insf $r1 = $r6, 63, 32
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    fmaxhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    srld $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    fmaxhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    srlw $r1 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    fmaxhq $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 11)
  %2 = tail call half @llvm.vector.reduce.fmax.v16f16(<16 x half> %0)
  ret half %2
}

declare half @llvm.vector.reduce.fmax.v16f16(<16 x half>)

define float @red_min_float2(<2 x float> %0) {
; CV1-LABEL: red_min_float2:
; CV1:       # %bb.0:
; CV1-NEXT:    srad $r1 = $r0, 32
; CV1-NEXT:    sllw $r2 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    compw.gtu $r2 = $r2, 0xff000000
; CV1-NEXT:    fcompw.olt $r3 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    orw $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    cmoved.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: red_min_float2:
; CV2:       # %bb.0:
; CV2-NEXT:    srad $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    fminw $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 1)
  %2 = tail call float @llvm.vector.reduce.fmin.v2f32(<2 x float> %0)
  ret float %2
}

declare float @llvm.vector.reduce.fmin.v2f32(<2 x float>)

define float @red_min_float4(<4 x float> %0) {
; CV1-LABEL: red_min_float4:
; CV1:       # %bb.0:
; CV1-NEXT:    fcompnwp.olt $r2 = $r1, $r0
; CV1-NEXT:    sllwps $r3 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    compnwp.gtu $r3 = $r3, 0xff000000.@
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    ord $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    cmovewp.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srad $r1 = $r0, 32
; CV1-NEXT:    sllw $r2 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    compw.gtu $r2 = $r2, 0xff000000
; CV1-NEXT:    fcompw.olt $r3 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    orw $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    cmoved.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: red_min_float4:
; CV2:       # %bb.0:
; CV2-NEXT:    fminwp $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srad $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    fminw $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 2)
  %2 = tail call float @llvm.vector.reduce.fmin.v4f32(<4 x float> %0)
  ret float %2
}

declare float @llvm.vector.reduce.fmin.v4f32(<4 x float>)

define float @red_min_float8(<8 x float> %0) {
; CV1-LABEL: red_min_float8:
; CV1:       # %bb.0:
; CV1-NEXT:    srad $r4 = $r0, 32
; CV1-NEXT:    srad $r5 = $r2, 32
; CV1-NEXT:    sllw $r6 = $r0, 1
; CV1-NEXT:    fcompw.olt $r10 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    compw.gtu $r6 = $r6, 0xff000000
; CV1-NEXT:    srad $r7 = $r1, 32
; CV1-NEXT:    sllw $r8 = $r4, 1
; CV1-NEXT:    fcompw.olt $r9 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    orw $r6 = $r10, $r6
; CV1-NEXT:    compw.gtu $r8 = $r8, 0xff000000
; CV1-NEXT:    srad $r10 = $r3, 32
; CV1-NEXT:    sllw $r11 = $r7, 1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    cmoved.odd $r6 ? $r0 = $r2
; CV1-NEXT:    sllw $r2 = $r1, 1
; CV1-NEXT:    compw.gtu $r6 = $r11, 0xff000000
; CV1-NEXT:    orw $r8 = $r9, $r8
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    compw.gtu $r2 = $r2, 0xff000000
; CV1-NEXT:    cmoved.odd $r8 ? $r4 = $r5
; CV1-NEXT:    fcompw.olt $r5 = $r10, $r7
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    insf $r0 = $r4, 63, 32
; CV1-NEXT:    orw $r4 = $r5, $r6
; CV1-NEXT:    fcompw.olt $r8 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    orw $r2 = $r8, $r2
; CV1-NEXT:    cmoved.odd $r4 ? $r7 = $r10
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    cmoved.odd $r2 ? $r1 = $r3
; CV1-NEXT:    sllwps $r2 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r7, 63, 32
; CV1-NEXT:    compnwp.gtu $r2 = $r2, 0xff000000.@
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    fcompnwp.olt $r3 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    ord $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    cmovewp.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    srad $r1 = $r0, 32
; CV1-NEXT:    sllw $r2 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    compw.gtu $r2 = $r2, 0xff000000
; CV1-NEXT:    fcompw.olt $r3 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    orw $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    cmoved.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 15)
;
; CV2-LABEL: red_min_float8:
; CV2:       # %bb.0:
; CV2-NEXT:    fminw $r1 = $r1, $r3
; CV2-NEXT:    srad $r3 = $r2, 32
; CV2-NEXT:    srad $r4 = $r3, 32
; CV2-NEXT:    srad $r5 = $r1, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    fminw $r0 = $r0, $r2
; CV2-NEXT:    fminw $r4 = $r5, $r4
; CV2-NEXT:    srad $r6 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r1 = $r4, 63, 32
; CV2-NEXT:    fminw $r3 = $r6, $r3
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r0 = $r3, 63, 32
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    fminwp $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    srad $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    fminw $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 6)
  %2 = tail call float @llvm.vector.reduce.fmin.v8f32(<8 x float> %0)
  ret float %2
}

declare float @llvm.vector.reduce.fmin.v8f32(<8 x float>)

define float @red_min_float16(ptr %0) {
; CV1-LABEL: red_min_float16:
; CV1:       # %bb.0:
; CV1-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r0r1r2r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srld $r8 = $r4, 32
; CV1-NEXT:    srld $r9 = $r6, 32
; CV1-NEXT:    srld $r10 = $r5, 32
; CV1-NEXT:    srld $r11 = $r7, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    fcompw.olt $r15 = $r4, $r0
; CV1-NEXT:    sllw $r16 = $r0, 1
; CV1-NEXT:    fcompw.olt $r17 = $r6, $r2
; CV1-NEXT:    sllw $r32 = $r2, 1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    compw.gtu $r16 = $r16, 0xff000000
; CV1-NEXT:    compw.gtu $r32 = $r32, 0xff000000
; CV1-NEXT:    srld $r33 = $r0, 32
; CV1-NEXT:    srld $r34 = $r2, 32
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    orw $r15 = $r15, $r16
; CV1-NEXT:    orw $r16 = $r17, $r32
; CV1-NEXT:    sllw $r17 = $r33, 1
; CV1-NEXT:    fcompw.olt $r35 = $r8, $r33
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    cmoved.odd $r15 ? $r0 = $r4
; CV1-NEXT:    cmoved.odd $r16 ? $r2 = $r6
; CV1-NEXT:    compw.gtu $r17 = $r17, 0xff000000
; CV1-NEXT:    sllw $r36 = $r34, 1
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    sllw $r4 = $r0, 1
; CV1-NEXT:    orw $r17 = $r35, $r17
; CV1-NEXT:    srld $r32 = $r1, 32
; CV1-NEXT:    compw.gtu $r36 = $r36, 0xff000000
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    compw.gtu $r4 = $r4, 0xff000000
; CV1-NEXT:    fcompw.olt $r17 = $r9, $r34
; CV1-NEXT:    cmoved.odd $r17 ? $r33 = $r8
; CV1-NEXT:    sllw $r35 = $r1, 1
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    sllw $r6 = $r32, 1
; CV1-NEXT:    srld $r8 = $r3, 32
; CV1-NEXT:    orw $r17 = $r17, $r36
; CV1-NEXT:    sllw $r37 = $r33, 1
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    fcompw.olt $r9 = $r2, $r0
; CV1-NEXT:    compw.gtu $r16 = $r35, 0xff000000
; CV1-NEXT:    cmoved.odd $r17 ? $r34 = $r9
; CV1-NEXT:    compw.gtu $r36 = $r37, 0xff000000
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    orw $r4 = $r9, $r4
; CV1-NEXT:    compw.gtu $r6 = $r6, 0xff000000
; CV1-NEXT:    fcompw.olt $r15 = $r34, $r33
; CV1-NEXT:    fcompw.olt $r35 = $r5, $r1
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    cmoved.odd $r4 ? $r0 = $r2
; CV1-NEXT:    fcompw.olt $r4 = $r10, $r32
; CV1-NEXT:    orw $r9 = $r15, $r36
; CV1-NEXT:    sllw $r15 = $r3, 1
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    orw $r2 = $r35, $r16
; CV1-NEXT:    sllw $r9 = $r8, 1
; CV1-NEXT:    fcompw.olt $r17 = $r7, $r3
; CV1-NEXT:    cmoved.odd $r9 ? $r33 = $r34
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    orw $r4 = $r4, $r6
; CV1-NEXT:    fcompw.olt $r6 = $r11, $r8
; CV1-NEXT:    compw.gtu $r9 = $r9, 0xff000000
; CV1-NEXT:    compw.gtu $r15 = $r15, 0xff000000
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    cmoved.odd $r2 ? $r1 = $r5
; CV1-NEXT:    orw $r2 = $r17, $r15
; CV1-NEXT:    orw $r4 = $r6, $r9
; CV1-NEXT:    cmoved.odd $r4 ? $r32 = $r10
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    sllw $r2 = $r1, 1
; CV1-NEXT:    cmoved.odd $r2 ? $r3 = $r7
; CV1-NEXT:    sllw $r4 = $r32, 1
; CV1-NEXT:    cmoved.odd $r4 ? $r8 = $r11
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    compw.gtu $r2 = $r2, 0xff000000
; CV1-NEXT:    compw.gtu $r4 = $r4, 0xff000000
; CV1-NEXT:    fcompw.olt $r5 = $r3, $r1
; CV1-NEXT:    fcompw.olt $r6 = $r8, $r32
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    insf $r0 = $r33, 63, 32
; CV1-NEXT:    orw $r2 = $r5, $r2
; CV1-NEXT:    orw $r4 = $r6, $r4
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    cmoved.odd $r2 ? $r1 = $r3
; CV1-NEXT:    cmoved.odd $r4 ? $r32 = $r8
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    insf $r1 = $r32, 63, 32
; CV1-NEXT:    sllwps $r2 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    compnwp.gtu $r2 = $r2, 0xff000000.@
; CV1-NEXT:    fcompnwp.olt $r3 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    ord $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    cmovewp.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    srad $r1 = $r0, 32
; CV1-NEXT:    sllw $r2 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    compw.gtu $r2 = $r2, 0xff000000
; CV1-NEXT:    fcompw.olt $r3 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    orw $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 27)
; CV1-NEXT:    cmoved.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 28)
;
; CV2-LABEL: red_min_float16:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r0r1r2r3 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    srld $r8 = $r7, 32
; CV2-NEXT:    srld $r9 = $r5, 32
; CV2-NEXT:    srld $r10 = $r6, 32
; CV2-NEXT:    srld $r11 = $r4, 32
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srld $r15 = $r3, 32
; CV2-NEXT:    srld $r16 = $r1, 32
; CV2-NEXT:    srld $r17 = $r2, 32
; CV2-NEXT:    srld $r32 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    fminw $r0 = $r4, $r0
; CV2-NEXT:    fminw $r1 = $r5, $r1
; CV2-NEXT:    fminw $r2 = $r6, $r2
; CV2-NEXT:    fminw $r3 = $r7, $r3
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    fminw $r4 = $r8, $r15
; CV2-NEXT:    fminw $r5 = $r9, $r16
; CV2-NEXT:    fminw $r6 = $r10, $r17
; CV2-NEXT:    fminw $r7 = $r11, $r32
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    fminw $r0 = $r0, $r2
; CV2-NEXT:    fminw $r1 = $r1, $r3
; CV2-NEXT:    fminw $r2 = $r7, $r6
; CV2-NEXT:    fminw $r3 = $r5, $r4
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r0 = $r2, 63, 32
; CV2-NEXT:    insf $r1 = $r3, 63, 32
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    fminwp $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    srad $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    fminw $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 11)
  %2 = load <16 x float>, ptr %0
  %3 = tail call float @llvm.vector.reduce.fmin.v16f32(<16 x float> %2)
  ret float %3
}

declare float @llvm.vector.reduce.fmin.v16f32(<16 x float>)

define double @red_min_double2(<2 x double> %0) {
; CV1-LABEL: red_min_double2:
; CV1:       # %bb.0:
; CV1-NEXT:    slld $r2 = $r0, 1
; CV1-NEXT:    fcompd.olt $r3 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    compd.gtu $r2 = $r2, 0xffe0000000000000
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    ord $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    cmoved.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: red_min_double2:
; CV2:       # %bb.0:
; CV2-NEXT:    fmind $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %2 = tail call double @llvm.vector.reduce.fmin.v2f64(<2 x double> %0)
  ret double %2
}

declare double @llvm.vector.reduce.fmin.v2f64(<2 x double>)

define double @red_min_double4(<4 x double> %0) {
; CV1-LABEL: red_min_double4:
; CV1:       # %bb.0:
; CV1-NEXT:    slld $r4 = $r0, 1
; CV1-NEXT:    fcompd.olt $r5 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    compd.gtu $r4 = $r4, 0xffe0000000000000
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    ord $r4 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    cmoved.odd $r4 ? $r0 = $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    slld $r1 = $r0, 1
; CV1-NEXT:    fcompd.olt $r4 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    compd.gtu $r1 = $r1, 0xffe0000000000000
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    ord $r1 = $r4, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    cmoved.odd $r1 ? $r0 = $r2
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    slld $r1 = $r0, 1
; CV1-NEXT:    fcompd.olt $r2 = $r3, $r0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    compd.gtu $r1 = $r1, 0xffe0000000000000
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    ord $r1 = $r2, $r1
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    cmoved.odd $r1 ? $r0 = $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 11)
;
; CV2-LABEL: red_min_double4:
; CV2:       # %bb.0:
; CV2-NEXT:    fmind $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    fmind $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    fmind $r0 = $r0, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 2)
  %2 = tail call double @llvm.vector.reduce.fmin.v4f64(<4 x double> %0)
  ret double %2
}

declare double @llvm.vector.reduce.fmin.v4f64(<4 x double>)

define double @red_min_double8(ptr %0) {
; CV1-LABEL: red_min_double8:
; CV1:       # %bb.0:
; CV1-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r0r1r2r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    slld $r8 = $r0, 1
; CV1-NEXT:    fcompd.olt $r9 = $r4, $r0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    compd.gtu $r8 = $r8, 0xffe0000000000000
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    ord $r8 = $r9, $r8
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    cmoved.odd $r8 ? $r0 = $r4
; CV1-NEXT:    slld $r4 = $r1, 1
; CV1-NEXT:    fcompd.olt $r8 = $r5, $r1
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    compd.gtu $r4 = $r4, 0xffe0000000000000
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    ord $r4 = $r8, $r4
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    cmoved.odd $r4 ? $r1 = $r5
; CV1-NEXT:    slld $r4 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    compd.gtu $r4 = $r4, 0xffe0000000000000
; CV1-NEXT:    fcompd.olt $r5 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    ord $r4 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    cmoved.odd $r4 ? $r0 = $r1
; CV1-NEXT:    slld $r1 = $r2, 1
; CV1-NEXT:    fcompd.olt $r4 = $r6, $r2
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    compd.gtu $r1 = $r1, 0xffe0000000000000
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    ord $r1 = $r4, $r1
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    slld $r1 = $r0, 1
; CV1-NEXT:    cmoved.odd $r1 ? $r2 = $r6
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    compd.gtu $r1 = $r1, 0xffe0000000000000
; CV1-NEXT:    fcompd.olt $r4 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    ord $r1 = $r4, $r1
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    cmoved.odd $r1 ? $r0 = $r2
; CV1-NEXT:    slld $r1 = $r3, 1
; CV1-NEXT:    fcompd.olt $r2 = $r7, $r3
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    compd.gtu $r1 = $r1, 0xffe0000000000000
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    ord $r1 = $r2, $r1
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    slld $r1 = $r0, 1
; CV1-NEXT:    cmoved.odd $r1 ? $r3 = $r7
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    compd.gtu $r1 = $r1, 0xffe0000000000000
; CV1-NEXT:    fcompd.olt $r2 = $r3, $r0
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    ord $r1 = $r2, $r1
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    cmoved.odd $r1 ? $r0 = $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 25)
;
; CV2-LABEL: red_min_double8:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r0r1r2r3 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    fmind $r0 = $r4, $r0
; CV2-NEXT:    fmind $r1 = $r5, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    fmind $r0 = $r0, $r1
; CV2-NEXT:    fmind $r1 = $r6, $r2
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    fmind $r0 = $r0, $r1
; CV2-NEXT:    fmind $r1 = $r7, $r3
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    fmind $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = load <8 x double>, ptr %0
  %3 = tail call double @llvm.vector.reduce.fmin.v8f64(<8 x double> %2)
  ret double %3
}

declare double @llvm.vector.reduce.fmin.v8f64(<8 x double>)

define double @red_min_double16(ptr %0) {
; CV1-LABEL: red_min_double16:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r4 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r0r1r2r3 = 0[$r4]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    lo $r32r33r34r35 = 96[$r4]
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    lo $r8r9r10r11 = 32[$r4]
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    lo $r4r5r6r7 = 64[$r4]
; CV1-NEXT:    slld $r15 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    compd.gtu $r15 = $r15, 0xffe0000000000000
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    slld $r16 = $r8, 1
; CV1-NEXT:    fcompd.olt $r17 = $r32, $r8
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    compd.gtu $r16 = $r16, 0xffe0000000000000
; CV1-NEXT:    fcompd.olt $r36 = $r4, $r0
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    ord $r15 = $r36, $r15
; CV1-NEXT:    ord $r16 = $r17, $r16
; CV1-NEXT:    fcompd.olt $r17 = $r33, $r9
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    cmoved.odd $r15 ? $r0 = $r4
; CV1-NEXT:    cmoved.odd $r16 ? $r8 = $r32
; CV1-NEXT:    slld $r16 = $r9, 1
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    slld $r4 = $r0, 1
; CV1-NEXT:    fcompd.olt $r15 = $r8, $r0
; CV1-NEXT:    compd.gtu $r16 = $r16, 0xffe0000000000000
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    compd.gtu $r4 = $r4, 0xffe0000000000000
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    ord $r4 = $r15, $r4
; CV1-NEXT:    ord $r15 = $r17, $r16
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    cmoved.odd $r4 ? $r0 = $r8
; CV1-NEXT:    slld $r4 = $r1, 1
; CV1-NEXT:    fcompd.olt $r8 = $r5, $r1
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    compd.gtu $r4 = $r4, 0xffe0000000000000
; CV1-NEXT:    cmoved.odd $r15 ? $r9 = $r33
; CV1-NEXT:    slld $r15 = $r10, 1
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    ord $r4 = $r8, $r4
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    cmoved.odd $r4 ? $r1 = $r5
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    slld $r4 = $r1, 1
; CV1-NEXT:    fcompd.olt $r5 = $r9, $r1
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    compd.gtu $r4 = $r4, 0xffe0000000000000
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    ord $r4 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    cmoved.odd $r4 ? $r1 = $r9
; CV1-NEXT:    slld $r4 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    compd.gtu $r4 = $r4, 0xffe0000000000000
; CV1-NEXT:    fcompd.olt $r5 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    ord $r4 = $r5, $r4
; CV1-NEXT:    compd.gtu $r5 = $r15, 0xffe0000000000000
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    cmoved.odd $r4 ? $r0 = $r1
; CV1-NEXT:    slld $r1 = $r2, 1
; CV1-NEXT:    fcompd.olt $r4 = $r6, $r2
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    compd.gtu $r1 = $r1, 0xffe0000000000000
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    ord $r1 = $r4, $r1
; CV1-NEXT:    fcompd.olt $r4 = $r34, $r10
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    ord $r1 = $r4, $r5
; CV1-NEXT:    cmoved.odd $r1 ? $r2 = $r6
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    slld $r1 = $r2, 1
; CV1-NEXT:    cmoved.odd $r1 ? $r10 = $r34
; CV1-NEXT:    ;; # (end cycle 27)
; CV1-NEXT:    compd.gtu $r1 = $r1, 0xffe0000000000000
; CV1-NEXT:    fcompd.olt $r4 = $r10, $r2
; CV1-NEXT:    ;; # (end cycle 28)
; CV1-NEXT:    ord $r1 = $r4, $r1
; CV1-NEXT:    ;; # (end cycle 29)
; CV1-NEXT:    slld $r1 = $r0, 1
; CV1-NEXT:    cmoved.odd $r1 ? $r2 = $r10
; CV1-NEXT:    ;; # (end cycle 30)
; CV1-NEXT:    compd.gtu $r1 = $r1, 0xffe0000000000000
; CV1-NEXT:    fcompd.olt $r4 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 31)
; CV1-NEXT:    ord $r1 = $r4, $r1
; CV1-NEXT:    slld $r4 = $r11, 1
; CV1-NEXT:    ;; # (end cycle 32)
; CV1-NEXT:    cmoved.odd $r1 ? $r0 = $r2
; CV1-NEXT:    slld $r1 = $r3, 1
; CV1-NEXT:    fcompd.olt $r2 = $r7, $r3
; CV1-NEXT:    compd.gtu $r4 = $r4, 0xffe0000000000000
; CV1-NEXT:    ;; # (end cycle 33)
; CV1-NEXT:    compd.gtu $r1 = $r1, 0xffe0000000000000
; CV1-NEXT:    ;; # (end cycle 34)
; CV1-NEXT:    ord $r1 = $r2, $r1
; CV1-NEXT:    fcompd.olt $r2 = $r35, $r11
; CV1-NEXT:    ;; # (end cycle 35)
; CV1-NEXT:    ord $r1 = $r2, $r4
; CV1-NEXT:    cmoved.odd $r1 ? $r3 = $r7
; CV1-NEXT:    ;; # (end cycle 36)
; CV1-NEXT:    slld $r1 = $r3, 1
; CV1-NEXT:    cmoved.odd $r1 ? $r11 = $r35
; CV1-NEXT:    ;; # (end cycle 37)
; CV1-NEXT:    compd.gtu $r1 = $r1, 0xffe0000000000000
; CV1-NEXT:    fcompd.olt $r2 = $r11, $r3
; CV1-NEXT:    ;; # (end cycle 38)
; CV1-NEXT:    ord $r1 = $r2, $r1
; CV1-NEXT:    ;; # (end cycle 39)
; CV1-NEXT:    slld $r1 = $r0, 1
; CV1-NEXT:    cmoved.odd $r1 ? $r3 = $r11
; CV1-NEXT:    ;; # (end cycle 40)
; CV1-NEXT:    compd.gtu $r1 = $r1, 0xffe0000000000000
; CV1-NEXT:    fcompd.olt $r2 = $r3, $r0
; CV1-NEXT:    ;; # (end cycle 41)
; CV1-NEXT:    ord $r1 = $r2, $r1
; CV1-NEXT:    ;; # (end cycle 42)
; CV1-NEXT:    cmoved.odd $r1 ? $r0 = $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 43)
;
; CV2-LABEL: red_min_double16:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r8r9r10r11 = 64[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    lo $r32r33r34r35 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    lo $r0r1r2r3 = 96[$r0]
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    fmind $r4 = $r4, $r8
; CV2-NEXT:    fmind $r5 = $r5, $r9
; CV2-NEXT:    fmind $r6 = $r6, $r10
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    fmind $r0 = $r32, $r0
; CV2-NEXT:    fmind $r1 = $r33, $r1
; CV2-NEXT:    fmind $r2 = $r34, $r2
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    fmind $r0 = $r4, $r0
; CV2-NEXT:    fmind $r1 = $r5, $r1
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    fmind $r0 = $r0, $r1
; CV2-NEXT:    fmind $r1 = $r6, $r2
; CV2-NEXT:    fmind $r2 = $r35, $r3
; CV2-NEXT:    fmind $r3 = $r7, $r11
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    fmind $r0 = $r0, $r1
; CV2-NEXT:    fmind $r1 = $r3, $r2
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    fmind $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 10)
  %2 = load <16 x double>, ptr %0
  %3 = tail call double @llvm.vector.reduce.fmin.v16f64(<16 x double> %2)
  ret double %3
}

declare double @llvm.vector.reduce.fmin.v16f64(<16 x double>)

define half @red_min_half2(<2 x half> %0) {
; CV1-LABEL: red_min_half2:
; CV1:       # %bb.0:
; CV1-NEXT:    srlw $r1 = $r0, 16
; CV1-NEXT:    sllhqs $r2 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    compnhq.gtu $r2 = $r2, 0xf800
; CV1-NEXT:    fcompnhq.olt $r3 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    orw $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    cmovehq.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: red_min_half2:
; CV2:       # %bb.0:
; CV2-NEXT:    srlw $r1 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    fminhq $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 1)
  %2 = tail call half @llvm.vector.reduce.fmin.v2f16(<2 x half> %0)
  ret half %2
}

declare half @llvm.vector.reduce.fmin.v2f16(<2 x half>)

define half @red_min_half4(<4 x half> %0) {
; CV1-LABEL: red_min_half4:
; CV1:       # %bb.0:
; CV1-NEXT:    srld $r1 = $r0, 32
; CV1-NEXT:    sllhqs $r2 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    compnhq.gtu $r2 = $r2, 0xf800f800
; CV1-NEXT:    fcompnhq.olt $r3 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    orw $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    cmovehq.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srlw $r1 = $r0, 16
; CV1-NEXT:    sllhqs $r2 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    compnhq.gtu $r2 = $r2, 0xf800
; CV1-NEXT:    fcompnhq.olt $r3 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    orw $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    cmovehq.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: red_min_half4:
; CV2:       # %bb.0:
; CV2-NEXT:    srld $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    fminhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    srlw $r1 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    fminhq $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 3)
  %2 = tail call half @llvm.vector.reduce.fmin.v4f16(<4 x half> %0)
  ret half %2
}

declare half @llvm.vector.reduce.fmin.v4f16(<4 x half>)

define half @red_min_half8(<8 x half> %0) {
; CV1-LABEL: red_min_half8:
; CV1:       # %bb.0:
; CV1-NEXT:    fcompnhq.olt $r2 = $r1, $r0
; CV1-NEXT:    sllhqs $r3 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    compnhq.gtu $r3 = $r3, 0xf800f800.@
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    ord $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    cmovehq.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srld $r1 = $r0, 32
; CV1-NEXT:    sllhqs $r2 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    compnhq.gtu $r2 = $r2, 0xf800f800
; CV1-NEXT:    fcompnhq.olt $r3 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    orw $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    cmovehq.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    srlw $r1 = $r0, 16
; CV1-NEXT:    sllhqs $r2 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    compnhq.gtu $r2 = $r2, 0xf800
; CV1-NEXT:    fcompnhq.olt $r3 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    orw $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    cmovehq.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 11)
;
; CV2-LABEL: red_min_half8:
; CV2:       # %bb.0:
; CV2-NEXT:    fminhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srld $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    fminhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    srlw $r1 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    fminhq $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 4)
  %2 = tail call half @llvm.vector.reduce.fmin.v8f16(<8 x half> %0)
  ret half %2
}

declare half @llvm.vector.reduce.fmin.v8f16(<8 x half>)

define half @red_min_half16(<16 x half> %0) {
; CV1-LABEL: red_min_half16:
; CV1:       # %bb.0:
; CV1-NEXT:    srld $r4 = $r0, 48
; CV1-NEXT:    srld $r5 = $r0, 32
; CV1-NEXT:    srlw $r6 = $r0, 16
; CV1-NEXT:    srld $r7 = $r2, 48
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    fcompnhq.olt $r8 = $r7, $r4
; CV1-NEXT:    sllhqs $r9 = $r4, 1
; CV1-NEXT:    srld $r10 = $r2, 32
; CV1-NEXT:    srlw $r11 = $r2, 16
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    compnhq.gtu $r9 = $r9, 0xf800
; CV1-NEXT:    sllhqs $r15 = $r5, 1
; CV1-NEXT:    sllhqs $r16 = $r6, 1
; CV1-NEXT:    srld $r17 = $r1, 48
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    orw $r8 = $r8, $r9
; CV1-NEXT:    sllhqs $r9 = $r0, 1
; CV1-NEXT:    compnhq.gtu $r15 = $r15, 0xf800
; CV1-NEXT:    fcompnhq.olt $r32 = $r10, $r5
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    compnhq.gtu $r9 = $r9, 0xf800
; CV1-NEXT:    orw $r15 = $r32, $r15
; CV1-NEXT:    compnhq.gtu $r16 = $r16, 0xf800
; CV1-NEXT:    fcompnhq.olt $r33 = $r11, $r6
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    cmovehq.odd $r8 ? $r4 = $r7
; CV1-NEXT:    orw $r16 = $r33, $r16
; CV1-NEXT:    fcompnhq.olt $r32 = $r2, $r0
; CV1-NEXT:    srld $r34 = $r3, 48
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    cmovehq.odd $r15 ? $r5 = $r10
; CV1-NEXT:    srld $r7 = $r1, 32
; CV1-NEXT:    orw $r9 = $r32, $r9
; CV1-NEXT:    sllhqs $r33 = $r17, 1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    cmovehq.odd $r9 ? $r0 = $r2
; CV1-NEXT:    compnhq.gtu $r4 = $r33, 0xf800
; CV1-NEXT:    insf $r5 = $r4, 31, 16
; CV1-NEXT:    srld $r35 = $r3, 32
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    fcompnhq.olt $r2 = $r34, $r17
; CV1-NEXT:    cmovehq.odd $r16 ? $r6 = $r11
; CV1-NEXT:    srlw $r8 = $r1, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r0 = $r6, 31, 16
; CV1-NEXT:    orw $r2 = $r2, $r4
; CV1-NEXT:    sllhqs $r4 = $r7, 1
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    fcompnhq.olt $r2 = $r35, $r7
; CV1-NEXT:    compnhq.gtu $r4 = $r4, 0xf800
; CV1-NEXT:    cmovehq.odd $r2 ? $r17 = $r34
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    orw $r2 = $r2, $r4
; CV1-NEXT:    srlw $r4 = $r3, 16
; CV1-NEXT:    sllhqs $r6 = $r8, 1
; CV1-NEXT:    sllhqs $r9 = $r1, 1
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    compnhq.gtu $r6 = $r6, 0xf800
; CV1-NEXT:    compnhq.gtu $r9 = $r9, 0xf800
; CV1-NEXT:    fcompnhq.olt $r10 = $r4, $r8
; CV1-NEXT:    fcompnhq.olt $r11 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    insf $r0 = $r5, 63, 32
; CV1-NEXT:    orw $r2 = $r10, $r6
; CV1-NEXT:    orw $r5 = $r11, $r9
; CV1-NEXT:    cmovehq.odd $r2 ? $r7 = $r35
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    cmovehq.odd $r5 ? $r1 = $r3
; CV1-NEXT:    cmovehq.odd $r2 ? $r8 = $r4
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    insf $r1 = $r8, 31, 16
; CV1-NEXT:    insf $r7 = $r17, 31, 16
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    insf $r1 = $r7, 63, 32
; CV1-NEXT:    sllhqs $r2 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    compnhq.gtu $r2 = $r2, 0xf800f800.@
; CV1-NEXT:    fcompnhq.olt $r3 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    ord $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    cmovehq.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    srld $r1 = $r0, 32
; CV1-NEXT:    sllhqs $r2 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    compnhq.gtu $r2 = $r2, 0xf800f800
; CV1-NEXT:    fcompnhq.olt $r3 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    orw $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    cmovehq.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    srlw $r1 = $r0, 16
; CV1-NEXT:    sllhqs $r2 = $r0, 1
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    compnhq.gtu $r2 = $r2, 0xf800
; CV1-NEXT:    fcompnhq.olt $r3 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    orw $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    cmovehq.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 27)
;
; CV2-LABEL: red_min_half16:
; CV2:       # %bb.0:
; CV2-NEXT:    srld $r4 = $r3, 48
; CV2-NEXT:    srld $r5 = $r1, 48
; CV2-NEXT:    srld $r6 = $r3, 32
; CV2-NEXT:    srld $r7 = $r1, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    fminhq $r4 = $r5, $r4
; CV2-NEXT:    srlw $r5 = $r3, 16
; CV2-NEXT:    fminhq $r6 = $r7, $r6
; CV2-NEXT:    srlw $r7 = $r1, 16
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    fminhq $r1 = $r1, $r3
; CV2-NEXT:    srld $r3 = $r0, 48
; CV2-NEXT:    fminhq $r5 = $r7, $r5
; CV2-NEXT:    srld $r7 = $r2, 48
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    srld $r8 = $r2, 32
; CV2-NEXT:    srld $r9 = $r0, 32
; CV2-NEXT:    srlw $r10 = $r2, 16
; CV2-NEXT:    srlw $r11 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    fminhq $r0 = $r0, $r2
; CV2-NEXT:    fminhq $r3 = $r3, $r7
; CV2-NEXT:    fminhq $r7 = $r9, $r8
; CV2-NEXT:    fminhq $r8 = $r11, $r10
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r0 = $r8, 31, 16
; CV2-NEXT:    insf $r1 = $r5, 31, 16
; CV2-NEXT:    insf $r6 = $r4, 31, 16
; CV2-NEXT:    insf $r7 = $r3, 31, 16
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r0 = $r7, 63, 32
; CV2-NEXT:    insf $r1 = $r6, 63, 32
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    fminhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    srld $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    fminhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    srlw $r1 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    fminhq $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 11)
  %2 = tail call half @llvm.vector.reduce.fmin.v16f16(<16 x half> %0)
  ret half %2
}

declare half @llvm.vector.reduce.fmin.v16f16(<16 x half>)
