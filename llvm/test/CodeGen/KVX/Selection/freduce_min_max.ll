; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mcpu=kv3-1 -O2 -o - %s | FileCheck %s --check-prefix=CV1
; RUN: llc -mcpu=kv3-2 -O2 -o - %s | FileCheck %s --check-prefix=CV2
; RUN: clang -O2 -march=kv3-1 -c -o /dev/null %s
; RUN: clang -O2 -march=kv3-2 -c -o /dev/null %s

target triple = "kvx-kalray-cos"

define float @red_max_float2(<2 x float> %0) {
; CV1-LABEL: red_max_float2:
; CV1:       # %bb.0:
; CV1-NEXT:    srad $r1 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    fcompw.olt $r2 = $r0, $r1
; CV1-NEXT:    fcompw.une $r3 = $r1, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    iorw $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    cmoved.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: red_max_float2:
; CV2:       # %bb.0:
; CV2-NEXT:    srad $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    fmaxw $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 1)
  %2 = tail call float @llvm.vector.reduce.fmax.v2f32(<2 x float> %0)
  ret float %2
}

declare float @llvm.vector.reduce.fmax.v2f32(<2 x float>)

define float @red_max_float4(<4 x float> %0) {
; CV1-LABEL: red_max_float4:
; CV1:       # %bb.0:
; CV1-NEXT:    fcompnwp.olt $r2 = $r0, $r1
; CV1-NEXT:    fcompnwp.une $r3 = $r1, $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    iord $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    cmovewp.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    srad $r1 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    fcompw.olt $r2 = $r0, $r1
; CV1-NEXT:    fcompw.une $r3 = $r1, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    iorw $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    cmoved.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: red_max_float4:
; CV2:       # %bb.0:
; CV2-NEXT:    fmaxwp $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srad $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    fmaxw $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 2)
  %2 = tail call float @llvm.vector.reduce.fmax.v4f32(<4 x float> %0)
  ret float %2
}

declare float @llvm.vector.reduce.fmax.v4f32(<4 x float>)

define float @red_max_float8(<8 x float> %0) {
; CV1-LABEL: red_max_float8:
; CV1:       # %bb.0:
; CV1-NEXT:    srad $r4 = $r3, 32
; CV1-NEXT:    srad $r5 = $r1, 32
; CV1-NEXT:    fcompw.olt $r8 = $r1, $r3
; CV1-NEXT:    fcompw.une $r9 = $r3, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    fcompw.olt $r6 = $r5, $r4
; CV1-NEXT:    fcompw.une $r7 = $r4, $r4
; CV1-NEXT:    srad $r10 = $r2, 32
; CV1-NEXT:    srad $r11 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    iorw $r6 = $r6, $r7
; CV1-NEXT:    iorw $r7 = $r8, $r9
; CV1-NEXT:    fcompw.olt $r15 = $r11, $r10
; CV1-NEXT:    fcompw.une $r16 = $r10, $r10
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    fcompw.olt $r8 = $r0, $r2
; CV1-NEXT:    fcompw.une $r9 = $r2, $r2
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    cmoved.odd $r7 ? $r1 = $r3
; CV1-NEXT:    iorw $r3 = $r15, $r16
; CV1-NEXT:    iorw $r4 = $r8, $r9
; CV1-NEXT:    cmoved.odd $r6 ? $r5 = $r4
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    cmoved.odd $r4 ? $r0 = $r2
; CV1-NEXT:    cmoved.odd $r3 ? $r11 = $r10
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    insf $r0 = $r11, 63, 32
; CV1-NEXT:    insf $r1 = $r5, 63, 32
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    fcompnwp.olt $r2 = $r0, $r1
; CV1-NEXT:    fcompnwp.une $r3 = $r1, $r1
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    iord $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    cmovewp.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    srad $r1 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    fcompw.olt $r2 = $r0, $r1
; CV1-NEXT:    fcompw.une $r3 = $r1, $r1
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    iorw $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    cmoved.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 13)
;
; CV2-LABEL: red_max_float8:
; CV2:       # %bb.0:
; CV2-NEXT:    srad $r4 = $r3, 32
; CV2-NEXT:    srad $r5 = $r1, 32
; CV2-NEXT:    srad $r6 = $r2, 32
; CV2-NEXT:    srad $r7 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    fmaxw $r0 = $r0, $r2
; CV2-NEXT:    fmaxw $r1 = $r1, $r3
; CV2-NEXT:    fmaxw $r3 = $r7, $r6
; CV2-NEXT:    fmaxw $r4 = $r5, $r4
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r0 = $r3, 63, 32
; CV2-NEXT:    insf $r1 = $r4, 63, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    fmaxwp $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srad $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    fmaxw $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 5)
  %2 = tail call float @llvm.vector.reduce.fmax.v8f32(<8 x float> %0)
  ret float %2
}

declare float @llvm.vector.reduce.fmax.v8f32(<8 x float>)

define float @red_max_float16(ptr %0) {
; CV1-LABEL: red_max_float16:
; CV1:       # %bb.0:
; CV1-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r8r9r10r11 = 32[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srld $r15 = $r7, 32
; CV1-NEXT:    srld $r32 = $r5, 32
; CV1-NEXT:    srld $r34 = $r6, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    fcompw.olt $r0 = $r7, $r11
; CV1-NEXT:    fcompw.une $r1 = $r11, $r11
; CV1-NEXT:    srld $r3 = $r11, 32
; CV1-NEXT:    srld $r16 = $r9, 32
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    iorw $r0 = $r0, $r1
; CV1-NEXT:    fcompw.une $r1 = $r3, $r3
; CV1-NEXT:    fcompw.olt $r35 = $r15, $r3
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    iorw $r1 = $r35, $r1
; CV1-NEXT:    cmoved.odd $r0 ? $r7 = $r11
; CV1-NEXT:    srld $r35 = $r10, 32
; CV1-NEXT:    fcompw.olt $r36 = $r32, $r16
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    fcompw.une $r0 = $r16, $r16
; CV1-NEXT:    fcompw.olt $r11 = $r34, $r35
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    iorw $r0 = $r36, $r0
; CV1-NEXT:    copyd $r1 = $r5
; CV1-NEXT:    fcompw.une $r3 = $r35, $r35
; CV1-NEXT:    cmoved.odd $r1 ? $r15 = $r3
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    fcompw.olt $r2 = $r5, $r9
; CV1-NEXT:    iorw $r3 = $r11, $r3
; CV1-NEXT:    copyd $r5 = $r6
; CV1-NEXT:    cmoved.odd $r0 ? $r32 = $r16
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    fcompw.une $r0 = $r9, $r9
; CV1-NEXT:    fcompw.olt $r33 = $r4, $r8
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    copyd $r0 = $r4
; CV1-NEXT:    iorw $r2 = $r2, $r0
; CV1-NEXT:    fcompw.une $r3 = $r8, $r8
; CV1-NEXT:    cmoved.odd $r3 ? $r34 = $r35
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    fcompw.une $r6 = $r10, $r10
; CV1-NEXT:    fcompw.olt $r17 = $r6, $r10
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    cmoved.odd $r2 ? $r1 = $r9
; CV1-NEXT:    iorw $r2 = $r33, $r3
; CV1-NEXT:    srld $r3 = $r4, 32
; CV1-NEXT:    iorw $r6 = $r17, $r6
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    cmoved.odd $r2 ? $r0 = $r8
; CV1-NEXT:    srld $r2 = $r8, 32
; CV1-NEXT:    cmoved.odd $r6 ? $r5 = $r10
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    fcompw.olt $r6 = $r3, $r2
; CV1-NEXT:    fcompw.une $r8 = $r2, $r2
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    fcompw.olt $r4 = $r1, $r7
; CV1-NEXT:    iorw $r6 = $r6, $r8
; CV1-NEXT:    fcompw.une $r9 = $r7, $r7
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    iorw $r4 = $r4, $r9
; CV1-NEXT:    fcompw.une $r8 = $r15, $r15
; CV1-NEXT:    fcompw.olt $r10 = $r32, $r15
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    fcompw.olt $r2 = $r0, $r5
; CV1-NEXT:    cmoved.odd $r6 ? $r3 = $r2
; CV1-NEXT:    iorw $r8 = $r10, $r8
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    fcompw.une $r6 = $r5, $r5
; CV1-NEXT:    fcompw.olt $r9 = $r3, $r34
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    cmoved.odd $r4 ? $r1 = $r7
; CV1-NEXT:    iorw $r2 = $r2, $r6
; CV1-NEXT:    fcompw.une $r10 = $r34, $r34
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    cmoved.odd $r2 ? $r0 = $r5
; CV1-NEXT:    iorw $r4 = $r9, $r10
; CV1-NEXT:    cmoved.odd $r8 ? $r32 = $r15
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    insf $r1 = $r32, 63, 32
; CV1-NEXT:    cmoved.odd $r4 ? $r3 = $r34
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    insf $r0 = $r3, 63, 32
; CV1-NEXT:    fcompnwp.une $r3 = $r1, $r1
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    fcompnwp.olt $r2 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    iord $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    cmovewp.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    srad $r1 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 27)
; CV1-NEXT:    fcompw.olt $r2 = $r0, $r1
; CV1-NEXT:    fcompw.une $r3 = $r1, $r1
; CV1-NEXT:    ;; # (end cycle 28)
; CV1-NEXT:    iorw $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 29)
; CV1-NEXT:    cmoved.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 30)
;
; CV2-LABEL: red_max_float16:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r0r1r2r3 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    srld $r1 = $r1, 32
; CV2-NEXT:    srld $r3 = $r3, 32
; CV2-NEXT:    fmaxw $r8 = $r7, $r3
; CV2-NEXT:    fmaxw $r9 = $r5, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    srld $r0 = $r0, 32
; CV2-NEXT:    srld $r2 = $r2, 32
; CV2-NEXT:    fmaxw $r10 = $r6, $r2
; CV2-NEXT:    fmaxw $r11 = $r4, $r0
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    srld $r4 = $r4, 32
; CV2-NEXT:    srld $r5 = $r5, 32
; CV2-NEXT:    srld $r6 = $r6, 32
; CV2-NEXT:    srld $r7 = $r7, 32
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    fmaxw $r0 = $r4, $r0
; CV2-NEXT:    fmaxw $r1 = $r5, $r1
; CV2-NEXT:    fmaxw $r2 = $r6, $r2
; CV2-NEXT:    fmaxw $r3 = $r7, $r3
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    fmaxw $r0 = $r0, $r2
; CV2-NEXT:    fmaxw $r1 = $r1, $r3
; CV2-NEXT:    fmaxw $r3 = $r11, $r10
; CV2-NEXT:    fmaxw $r4 = $r9, $r8
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    insf $r3 = $r0, 63, 32
; CV2-NEXT:    insf $r4 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    fmaxwp $r0 = $r3, $r4
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    srad $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    fmaxw $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 12)
  %2 = load <16 x float>, ptr %0
  %3 = tail call float @llvm.vector.reduce.fmax.v16f32(<16 x float> %2)
  ret float %3
}

declare float @llvm.vector.reduce.fmax.v16f32(<16 x float>)

define double @red_max_double2(<2 x double> %0) {
; CV1-LABEL: red_max_double2:
; CV1:       # %bb.0:
; CV1-NEXT:    fcompd.olt $r2 = $r0, $r1
; CV1-NEXT:    fcompd.une $r3 = $r1, $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    iord $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    cmoved.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 2)
;
; CV2-LABEL: red_max_double2:
; CV2:       # %bb.0:
; CV2-NEXT:    fmaxd $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %2 = tail call double @llvm.vector.reduce.fmax.v2f64(<2 x double> %0)
  ret double %2
}

declare double @llvm.vector.reduce.fmax.v2f64(<2 x double>)

define double @red_max_double4(<4 x double> %0) {
; CV1-LABEL: red_max_double4:
; CV1:       # %bb.0:
; CV1-NEXT:    fcompd.olt $r4 = $r0, $r1
; CV1-NEXT:    fcompd.une $r5 = $r1, $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    iord $r4 = $r4, $r5
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    cmoved.odd $r4 ? $r0 = $r1
; CV1-NEXT:    fcompd.une $r4 = $r2, $r2
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    fcompd.olt $r1 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    iord $r1 = $r1, $r4
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    cmoved.odd $r1 ? $r0 = $r2
; CV1-NEXT:    fcompd.une $r2 = $r3, $r3
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    fcompd.olt $r1 = $r0, $r3
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    iord $r1 = $r1, $r2
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    cmoved.odd $r1 ? $r0 = $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 8)
;
; CV2-LABEL: red_max_double4:
; CV2:       # %bb.0:
; CV2-NEXT:    fmaxd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    fmaxd $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    fmaxd $r0 = $r0, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 2)
  %2 = tail call double @llvm.vector.reduce.fmax.v4f64(<4 x double> %0)
  ret double %2
}

declare double @llvm.vector.reduce.fmax.v4f64(<4 x double>)

define double @red_max_double8(ptr %0) {
; CV1-LABEL: red_max_double8:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r4 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r0r1r2r3 = 0[$r4]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    lo $r4r5r6r7 = 32[$r4]
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    fcompd.olt $r8 = $r1, $r5
; CV1-NEXT:    fcompd.une $r9 = $r5, $r5
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    iord $r8 = $r8, $r9
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    cmoved.odd $r8 ? $r1 = $r5
; CV1-NEXT:    fcompd.olt $r5 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    fcompd.une $r8 = $r4, $r4
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    iord $r5 = $r5, $r8
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    cmoved.odd $r5 ? $r0 = $r4
; CV1-NEXT:    fcompd.une $r5 = $r1, $r1
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    fcompd.olt $r4 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    iord $r4 = $r4, $r5
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    cmoved.odd $r4 ? $r0 = $r1
; CV1-NEXT:    fcompd.olt $r1 = $r2, $r6
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    fcompd.une $r4 = $r6, $r6
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    iord $r1 = $r1, $r4
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    cmoved.odd $r1 ? $r2 = $r6
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    fcompd.olt $r1 = $r0, $r2
; CV1-NEXT:    fcompd.une $r4 = $r2, $r2
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    iord $r1 = $r1, $r4
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    cmoved.odd $r1 ? $r0 = $r2
; CV1-NEXT:    fcompd.olt $r1 = $r3, $r7
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    fcompd.une $r2 = $r7, $r7
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    iord $r1 = $r1, $r2
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    cmoved.odd $r1 ? $r3 = $r7
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    fcompd.olt $r1 = $r0, $r3
; CV1-NEXT:    fcompd.une $r2 = $r3, $r3
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    iord $r1 = $r1, $r2
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    cmoved.odd $r1 ? $r0 = $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 25)
;
; CV2-LABEL: red_max_double8:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r0r1r2r3 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    fmaxd $r0 = $r4, $r0
; CV2-NEXT:    fmaxd $r1 = $r5, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    fmaxd $r0 = $r0, $r1
; CV2-NEXT:    fmaxd $r1 = $r6, $r2
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    fmaxd $r0 = $r0, $r1
; CV2-NEXT:    fmaxd $r1 = $r7, $r3
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    fmaxd $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = load <8 x double>, ptr %0
  %3 = tail call double @llvm.vector.reduce.fmax.v8f64(<8 x double> %2)
  ret double %3
}

declare double @llvm.vector.reduce.fmax.v8f64(<8 x double>)

define double @red_max_double16(ptr %0) {
; CV1-LABEL: red_max_double16:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r4 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r0r1r2r3 = 0[$r4]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    lo $r8r9r10r11 = 32[$r4]
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    lo $r32r33r34r35 = 96[$r4]
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    lo $r4r5r6r7 = 64[$r4]
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    fcompd.olt $r15 = $r9, $r33
; CV1-NEXT:    fcompd.une $r16 = $r33, $r33
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    iord $r15 = $r15, $r16
; CV1-NEXT:    fcompd.olt $r17 = $r1, $r5
; CV1-NEXT:    fcompd.une $r36 = $r5, $r5
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    cmoved.odd $r15 ? $r9 = $r33
; CV1-NEXT:    iord $r16 = $r17, $r36
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    cmoved.odd $r16 ? $r1 = $r5
; CV1-NEXT:    fcompd.une $r15 = $r9, $r9
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    fcompd.olt $r5 = $r1, $r9
; CV1-NEXT:    fcompd.une $r16 = $r32, $r32
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    iord $r5 = $r5, $r15
; CV1-NEXT:    fcompd.une $r15 = $r4, $r4
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    cmoved.odd $r5 ? $r1 = $r9
; CV1-NEXT:    fcompd.olt $r5 = $r8, $r32
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    iord $r5 = $r5, $r16
; CV1-NEXT:    fcompd.olt $r9 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    cmoved.odd $r5 ? $r8 = $r32
; CV1-NEXT:    iord $r9 = $r9, $r15
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    cmoved.odd $r9 ? $r0 = $r4
; CV1-NEXT:    fcompd.une $r4 = $r8, $r8
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    fcompd.olt $r5 = $r0, $r8
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    iord $r4 = $r5, $r4
; CV1-NEXT:    fcompd.une $r5 = $r1, $r1
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    cmoved.odd $r4 ? $r0 = $r8
; CV1-NEXT:    fcompd.une $r8 = $r34, $r34
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    fcompd.olt $r4 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    iord $r4 = $r4, $r5
; CV1-NEXT:    fcompd.une $r5 = $r6, $r6
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    cmoved.odd $r4 ? $r0 = $r1
; CV1-NEXT:    fcompd.olt $r1 = $r10, $r34
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    iord $r1 = $r1, $r8
; CV1-NEXT:    fcompd.olt $r4 = $r2, $r6
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    iord $r4 = $r4, $r5
; CV1-NEXT:    fcompd.une $r5 = $r7, $r7
; CV1-NEXT:    cmoved.odd $r1 ? $r10 = $r34
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    fcompd.une $r1 = $r10, $r10
; CV1-NEXT:    cmoved.odd $r4 ? $r2 = $r6
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    fcompd.olt $r4 = $r2, $r10
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    iord $r1 = $r4, $r1
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    cmoved.odd $r1 ? $r2 = $r10
; CV1-NEXT:    ;; # (end cycle 27)
; CV1-NEXT:    fcompd.olt $r1 = $r0, $r2
; CV1-NEXT:    fcompd.une $r4 = $r2, $r2
; CV1-NEXT:    ;; # (end cycle 28)
; CV1-NEXT:    iord $r1 = $r1, $r4
; CV1-NEXT:    fcompd.olt $r4 = $r3, $r7
; CV1-NEXT:    ;; # (end cycle 29)
; CV1-NEXT:    cmoved.odd $r1 ? $r0 = $r2
; CV1-NEXT:    fcompd.olt $r1 = $r11, $r35
; CV1-NEXT:    ;; # (end cycle 30)
; CV1-NEXT:    fcompd.une $r2 = $r35, $r35
; CV1-NEXT:    ;; # (end cycle 31)
; CV1-NEXT:    iord $r1 = $r1, $r2
; CV1-NEXT:    iord $r2 = $r4, $r5
; CV1-NEXT:    ;; # (end cycle 32)
; CV1-NEXT:    cmoved.odd $r2 ? $r3 = $r7
; CV1-NEXT:    cmoved.odd $r1 ? $r11 = $r35
; CV1-NEXT:    ;; # (end cycle 33)
; CV1-NEXT:    fcompd.olt $r1 = $r3, $r11
; CV1-NEXT:    fcompd.une $r2 = $r11, $r11
; CV1-NEXT:    ;; # (end cycle 34)
; CV1-NEXT:    iord $r1 = $r1, $r2
; CV1-NEXT:    ;; # (end cycle 35)
; CV1-NEXT:    cmoved.odd $r1 ? $r3 = $r11
; CV1-NEXT:    ;; # (end cycle 36)
; CV1-NEXT:    fcompd.olt $r1 = $r0, $r3
; CV1-NEXT:    fcompd.une $r2 = $r3, $r3
; CV1-NEXT:    ;; # (end cycle 37)
; CV1-NEXT:    iord $r1 = $r1, $r2
; CV1-NEXT:    ;; # (end cycle 38)
; CV1-NEXT:    cmoved.odd $r1 ? $r0 = $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 39)
;
; CV2-LABEL: red_max_double16:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r8r9r10r11 = 64[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    lo $r32r33r34r35 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    lo $r0r1r2r3 = 96[$r0]
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    fmaxd $r4 = $r4, $r8
; CV2-NEXT:    fmaxd $r5 = $r5, $r9
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    fmaxd $r0 = $r32, $r0
; CV2-NEXT:    fmaxd $r1 = $r33, $r1
; CV2-NEXT:    fmaxd $r2 = $r34, $r2
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    fmaxd $r0 = $r4, $r0
; CV2-NEXT:    fmaxd $r1 = $r5, $r1
; CV2-NEXT:    fmaxd $r4 = $r6, $r10
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    fmaxd $r0 = $r0, $r1
; CV2-NEXT:    fmaxd $r1 = $r4, $r2
; CV2-NEXT:    fmaxd $r2 = $r35, $r3
; CV2-NEXT:    fmaxd $r3 = $r7, $r11
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    fmaxd $r0 = $r0, $r1
; CV2-NEXT:    fmaxd $r1 = $r3, $r2
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    fmaxd $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 10)
  %2 = load <16 x double>, ptr %0
  %3 = tail call double @llvm.vector.reduce.fmax.v16f64(<16 x double> %2)
  ret double %3
}

declare double @llvm.vector.reduce.fmax.v16f64(<16 x double>)

define half @red_max_half2(<2 x half> %0) {
; CV1-LABEL: red_max_half2:
; CV1:       # %bb.0:
; CV1-NEXT:    srlw $r1 = $r0, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    fcompnhq.olt $r2 = $r0, $r1
; CV1-NEXT:    fcompnhq.une $r3 = $r1, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    iorw $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    cmovehq.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: red_max_half2:
; CV2:       # %bb.0:
; CV2-NEXT:    srlw $r1 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    fmaxhq $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 1)
  %2 = tail call half @llvm.vector.reduce.fmax.v2f16(<2 x half> %0)
  ret half %2
}

declare half @llvm.vector.reduce.fmax.v2f16(<2 x half>)

define half @red_max_half4(<4 x half> %0) {
; CV1-LABEL: red_max_half4:
; CV1:       # %bb.0:
; CV1-NEXT:    srld $r1 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    fcompnhq.olt $r2 = $r0, $r1
; CV1-NEXT:    fcompnhq.une $r3 = $r1, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    iorw $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    cmovehq.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srlw $r1 = $r0, 16
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    fcompnhq.olt $r2 = $r0, $r1
; CV1-NEXT:    fcompnhq.une $r3 = $r1, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    iorw $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    cmovehq.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: red_max_half4:
; CV2:       # %bb.0:
; CV2-NEXT:    srld $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    fmaxhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    srlw $r1 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    fmaxhq $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 3)
  %2 = tail call half @llvm.vector.reduce.fmax.v4f16(<4 x half> %0)
  ret half %2
}

declare half @llvm.vector.reduce.fmax.v4f16(<4 x half>)

define half @red_max_half8(<8 x half> %0) {
; CV1-LABEL: red_max_half8:
; CV1:       # %bb.0:
; CV1-NEXT:    fcompnhq.olt $r2 = $r0, $r1
; CV1-NEXT:    fcompnhq.une $r3 = $r1, $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    iord $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    cmovehq.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    srld $r1 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    fcompnhq.olt $r2 = $r0, $r1
; CV1-NEXT:    fcompnhq.une $r3 = $r1, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    iorw $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    cmovehq.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    srlw $r1 = $r0, 16
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    fcompnhq.olt $r2 = $r0, $r1
; CV1-NEXT:    fcompnhq.une $r3 = $r1, $r1
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    iorw $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    cmovehq.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 10)
;
; CV2-LABEL: red_max_half8:
; CV2:       # %bb.0:
; CV2-NEXT:    fmaxhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srld $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    fmaxhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    srlw $r1 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    fmaxhq $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 4)
  %2 = tail call half @llvm.vector.reduce.fmax.v8f16(<8 x half> %0)
  ret half %2
}

declare half @llvm.vector.reduce.fmax.v8f16(<8 x half>)

define half @red_max_half16(<16 x half> %0) {
; CV1-LABEL: red_max_half16:
; CV1:       # %bb.0:
; CV1-NEXT:    srld $r4 = $r3, 48
; CV1-NEXT:    srld $r5 = $r1, 48
; CV1-NEXT:    srld $r8 = $r3, 32
; CV1-NEXT:    srld $r9 = $r1, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    fcompnhq.olt $r6 = $r5, $r4
; CV1-NEXT:    fcompnhq.une $r7 = $r4, $r4
; CV1-NEXT:    srld $r15 = $r2, 48
; CV1-NEXT:    srld $r16 = $r0, 48
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    iorw $r6 = $r6, $r7
; CV1-NEXT:    fcompnhq.olt $r10 = $r9, $r8
; CV1-NEXT:    fcompnhq.une $r11 = $r8, $r8
; CV1-NEXT:    srld $r33 = $r2, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    srlw $r4 = $r3, 16
; CV1-NEXT:    cmovehq.odd $r6 ? $r5 = $r4
; CV1-NEXT:    srlw $r6 = $r1, 16
; CV1-NEXT:    iorw $r7 = $r10, $r11
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    fcompnhq.olt $r7 = $r6, $r4
; CV1-NEXT:    cmovehq.odd $r7 ? $r9 = $r8
; CV1-NEXT:    srld $r34 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    fcompnhq.une $r8 = $r4, $r4
; CV1-NEXT:    fcompnhq.olt $r10 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    iorw $r7 = $r7, $r8
; CV1-NEXT:    fcompnhq.une $r11 = $r3, $r3
; CV1-NEXT:    fcompnhq.olt $r17 = $r16, $r15
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    iorw $r8 = $r10, $r11
; CV1-NEXT:    fcompnhq.olt $r10 = $r34, $r33
; CV1-NEXT:    fcompnhq.une $r32 = $r15, $r15
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    cmovehq.odd $r7 ? $r6 = $r4
; CV1-NEXT:    fcompnhq.une $r11 = $r33, $r33
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    cmovehq.odd $r8 ? $r1 = $r3
; CV1-NEXT:    iorw $r3 = $r17, $r32
; CV1-NEXT:    iorw $r4 = $r10, $r11
; CV1-NEXT:    fcompnhq.une $r10 = $r2, $r2
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    srlw $r3 = $r2, 16
; CV1-NEXT:    srlw $r4 = $r0, 16
; CV1-NEXT:    cmovehq.odd $r3 ? $r16 = $r15
; CV1-NEXT:    cmovehq.odd $r4 ? $r34 = $r33
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    fcompnhq.olt $r7 = $r4, $r3
; CV1-NEXT:    fcompnhq.une $r8 = $r3, $r3
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    iorw $r7 = $r7, $r8
; CV1-NEXT:    fcompnhq.olt $r8 = $r0, $r2
; CV1-NEXT:    insf $r9 = $r5, 31, 16
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    insf $r1 = $r6, 31, 16
; CV1-NEXT:    iorw $r3 = $r8, $r10
; CV1-NEXT:    cmovehq.odd $r7 ? $r4 = $r3
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    cmovehq.odd $r3 ? $r0 = $r2
; CV1-NEXT:    insf $r34 = $r16, 31, 16
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    insf $r0 = $r4, 31, 16
; CV1-NEXT:    insf $r1 = $r9, 63, 32
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    insf $r0 = $r34, 63, 32
; CV1-NEXT:    fcompnhq.une $r3 = $r1, $r1
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    fcompnhq.olt $r2 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    iord $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    cmovehq.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    srld $r1 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    fcompnhq.olt $r2 = $r0, $r1
; CV1-NEXT:    fcompnhq.une $r3 = $r1, $r1
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    iorw $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    cmovehq.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    srlw $r1 = $r0, 16
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    fcompnhq.olt $r2 = $r0, $r1
; CV1-NEXT:    fcompnhq.une $r3 = $r1, $r1
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    iorw $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    cmovehq.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 27)
;
; CV2-LABEL: red_max_half16:
; CV2:       # %bb.0:
; CV2-NEXT:    srld $r4 = $r3, 48
; CV2-NEXT:    srld $r5 = $r1, 48
; CV2-NEXT:    srld $r6 = $r3, 32
; CV2-NEXT:    srld $r7 = $r1, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    fmaxhq $r4 = $r5, $r4
; CV2-NEXT:    fmaxhq $r5 = $r7, $r6
; CV2-NEXT:    srlw $r6 = $r3, 16
; CV2-NEXT:    srlw $r7 = $r1, 16
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    fmaxhq $r1 = $r1, $r3
; CV2-NEXT:    srld $r3 = $r2, 48
; CV2-NEXT:    fmaxhq $r6 = $r7, $r6
; CV2-NEXT:    srld $r7 = $r0, 48
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    srld $r8 = $r2, 32
; CV2-NEXT:    srld $r9 = $r0, 32
; CV2-NEXT:    srlw $r10 = $r2, 16
; CV2-NEXT:    srlw $r11 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    fmaxhq $r0 = $r0, $r2
; CV2-NEXT:    fmaxhq $r3 = $r7, $r3
; CV2-NEXT:    fmaxhq $r7 = $r9, $r8
; CV2-NEXT:    fmaxhq $r8 = $r11, $r10
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r0 = $r8, 31, 16
; CV2-NEXT:    insf $r1 = $r6, 31, 16
; CV2-NEXT:    insf $r5 = $r4, 31, 16
; CV2-NEXT:    insf $r7 = $r3, 31, 16
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r0 = $r7, 63, 32
; CV2-NEXT:    insf $r1 = $r5, 63, 32
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    fmaxhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    srld $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    fmaxhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    srlw $r1 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    fmaxhq $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 11)
  %2 = tail call half @llvm.vector.reduce.fmax.v16f16(<16 x half> %0)
  ret half %2
}

declare half @llvm.vector.reduce.fmax.v16f16(<16 x half>)

define float @red_min_float2(<2 x float> %0) {
; CV1-LABEL: red_min_float2:
; CV1:       # %bb.0:
; CV1-NEXT:    srad $r1 = $r0, 32
; CV1-NEXT:    fcompw.une $r3 = $r0, $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    fcompw.olt $r2 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    iorw $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    cmoved.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: red_min_float2:
; CV2:       # %bb.0:
; CV2-NEXT:    srad $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    fminw $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 1)
  %2 = tail call float @llvm.vector.reduce.fmin.v2f32(<2 x float> %0)
  ret float %2
}

declare float @llvm.vector.reduce.fmin.v2f32(<2 x float>)

define float @red_min_float4(<4 x float> %0) {
; CV1-LABEL: red_min_float4:
; CV1:       # %bb.0:
; CV1-NEXT:    fcompnwp.olt $r2 = $r1, $r0
; CV1-NEXT:    fcompnwp.une $r3 = $r0, $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    iord $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    cmovewp.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    srad $r1 = $r0, 32
; CV1-NEXT:    fcompw.une $r3 = $r0, $r0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    fcompw.olt $r2 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    iorw $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    cmoved.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: red_min_float4:
; CV2:       # %bb.0:
; CV2-NEXT:    fminwp $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srad $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    fminw $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 2)
  %2 = tail call float @llvm.vector.reduce.fmin.v4f32(<4 x float> %0)
  ret float %2
}

declare float @llvm.vector.reduce.fmin.v4f32(<4 x float>)

define float @red_min_float8(<8 x float> %0) {
; CV1-LABEL: red_min_float8:
; CV1:       # %bb.0:
; CV1-NEXT:    srad $r4 = $r0, 32
; CV1-NEXT:    srad $r5 = $r2, 32
; CV1-NEXT:    fcompw.olt $r8 = $r2, $r0
; CV1-NEXT:    fcompw.une $r9 = $r0, $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    fcompw.olt $r6 = $r5, $r4
; CV1-NEXT:    fcompw.une $r7 = $r4, $r4
; CV1-NEXT:    srad $r10 = $r1, 32
; CV1-NEXT:    srad $r11 = $r3, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    iorw $r6 = $r6, $r7
; CV1-NEXT:    iorw $r7 = $r8, $r9
; CV1-NEXT:    fcompw.olt $r15 = $r11, $r10
; CV1-NEXT:    fcompw.une $r16 = $r10, $r10
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    fcompw.olt $r8 = $r3, $r1
; CV1-NEXT:    fcompw.une $r9 = $r1, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    cmoved.odd $r7 ? $r0 = $r2
; CV1-NEXT:    iorw $r2 = $r15, $r16
; CV1-NEXT:    cmoved.odd $r6 ? $r4 = $r5
; CV1-NEXT:    iorw $r5 = $r8, $r9
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    cmoved.odd $r5 ? $r1 = $r3
; CV1-NEXT:    cmoved.odd $r2 ? $r10 = $r11
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    insf $r0 = $r4, 63, 32
; CV1-NEXT:    insf $r1 = $r10, 63, 32
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    fcompnwp.olt $r2 = $r1, $r0
; CV1-NEXT:    fcompnwp.une $r3 = $r0, $r0
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    iord $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    cmovewp.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    srad $r1 = $r0, 32
; CV1-NEXT:    fcompw.une $r3 = $r0, $r0
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    fcompw.olt $r2 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    iorw $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    cmoved.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 13)
;
; CV2-LABEL: red_min_float8:
; CV2:       # %bb.0:
; CV2-NEXT:    srad $r4 = $r3, 32
; CV2-NEXT:    srad $r5 = $r1, 32
; CV2-NEXT:    srad $r6 = $r2, 32
; CV2-NEXT:    srad $r7 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    fminw $r0 = $r0, $r2
; CV2-NEXT:    fminw $r1 = $r1, $r3
; CV2-NEXT:    fminw $r3 = $r7, $r6
; CV2-NEXT:    fminw $r4 = $r5, $r4
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r0 = $r3, 63, 32
; CV2-NEXT:    insf $r1 = $r4, 63, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    fminwp $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srad $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    fminw $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 5)
  %2 = tail call float @llvm.vector.reduce.fmin.v8f32(<8 x float> %0)
  ret float %2
}

declare float @llvm.vector.reduce.fmin.v8f32(<8 x float>)

define float @red_min_float16(ptr %0) {
; CV1-LABEL: red_min_float16:
; CV1:       # %bb.0:
; CV1-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r0r1r2r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srld $r11 = $r4, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    fcompw.olt $r8 = $r4, $r0
; CV1-NEXT:    fcompw.une $r9 = $r0, $r0
; CV1-NEXT:    srld $r10 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    iorw $r8 = $r8, $r9
; CV1-NEXT:    fcompw.olt $r9 = $r11, $r10
; CV1-NEXT:    fcompw.une $r15 = $r10, $r10
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    cmoved.odd $r8 ? $r0 = $r4
; CV1-NEXT:    iorw $r4 = $r9, $r15
; CV1-NEXT:    fcompw.olt $r8 = $r6, $r2
; CV1-NEXT:    srld $r9 = $r6, 32
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    srld $r4 = $r2, 32
; CV1-NEXT:    cmoved.odd $r4 ? $r10 = $r11
; CV1-NEXT:    fcompw.une $r15 = $r2, $r2
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    iorw $r8 = $r8, $r15
; CV1-NEXT:    fcompw.olt $r11 = $r9, $r4
; CV1-NEXT:    fcompw.une $r15 = $r4, $r4
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    cmoved.odd $r8 ? $r2 = $r6
; CV1-NEXT:    fcompw.une $r6 = $r10, $r10
; CV1-NEXT:    iorw $r8 = $r11, $r15
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    cmoved.odd $r8 ? $r4 = $r9
; CV1-NEXT:    fcompw.olt $r11 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    fcompw.une $r8 = $r0, $r0
; CV1-NEXT:    fcompw.olt $r9 = $r4, $r10
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    iorw $r6 = $r9, $r6
; CV1-NEXT:    iorw $r8 = $r11, $r8
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    cmoved.odd $r8 ? $r0 = $r2
; CV1-NEXT:    cmoved.odd $r6 ? $r10 = $r4
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    fcompw.olt $r2 = $r5, $r1
; CV1-NEXT:    fcompw.une $r4 = $r1, $r1
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    insf $r0 = $r10, 63, 32
; CV1-NEXT:    iorw $r2 = $r2, $r4
; CV1-NEXT:    srld $r4 = $r1, 32
; CV1-NEXT:    fcompw.une $r6 = $r3, $r3
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    cmoved.odd $r2 ? $r1 = $r5
; CV1-NEXT:    fcompw.olt $r2 = $r7, $r3
; CV1-NEXT:    srld $r5 = $r5, 32
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    iorw $r2 = $r2, $r6
; CV1-NEXT:    srld $r6 = $r3, 32
; CV1-NEXT:    fcompw.une $r8 = $r4, $r4
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    srld $r2 = $r7, 32
; CV1-NEXT:    cmoved.odd $r2 ? $r3 = $r7
; CV1-NEXT:    fcompw.olt $r7 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    iorw $r7 = $r7, $r8
; CV1-NEXT:    fcompw.olt $r9 = $r2, $r6
; CV1-NEXT:    fcompw.une $r10 = $r6, $r6
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    cmoved.odd $r7 ? $r4 = $r5
; CV1-NEXT:    iorw $r8 = $r9, $r10
; CV1-NEXT:    fcompw.olt $r9 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    cmoved.odd $r8 ? $r6 = $r2
; CV1-NEXT:    fcompw.une $r10 = $r1, $r1
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    fcompw.olt $r2 = $r6, $r4
; CV1-NEXT:    fcompw.une $r5 = $r4, $r4
; CV1-NEXT:    iorw $r7 = $r9, $r10
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    cmoved.odd $r7 ? $r1 = $r3
; CV1-NEXT:    iorw $r2 = $r2, $r5
; CV1-NEXT:    fcompnwp.une $r3 = $r0, $r0
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    cmoved.odd $r2 ? $r4 = $r6
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    insf $r1 = $r4, 63, 32
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    fcompnwp.olt $r2 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    iord $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 27)
; CV1-NEXT:    cmovewp.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ;; # (end cycle 28)
; CV1-NEXT:    srad $r1 = $r0, 32
; CV1-NEXT:    fcompw.une $r3 = $r0, $r0
; CV1-NEXT:    ;; # (end cycle 29)
; CV1-NEXT:    fcompw.olt $r2 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 30)
; CV1-NEXT:    iorw $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 31)
; CV1-NEXT:    cmoved.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 32)
;
; CV2-LABEL: red_min_float16:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r0r1r2r3 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    srld $r1 = $r1, 32
; CV2-NEXT:    srld $r3 = $r3, 32
; CV2-NEXT:    fminw $r8 = $r7, $r3
; CV2-NEXT:    fminw $r9 = $r5, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    srld $r0 = $r0, 32
; CV2-NEXT:    srld $r2 = $r2, 32
; CV2-NEXT:    fminw $r10 = $r6, $r2
; CV2-NEXT:    fminw $r11 = $r4, $r0
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    srld $r4 = $r4, 32
; CV2-NEXT:    srld $r5 = $r5, 32
; CV2-NEXT:    srld $r6 = $r6, 32
; CV2-NEXT:    srld $r7 = $r7, 32
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    fminw $r0 = $r4, $r0
; CV2-NEXT:    fminw $r1 = $r5, $r1
; CV2-NEXT:    fminw $r2 = $r6, $r2
; CV2-NEXT:    fminw $r3 = $r7, $r3
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    fminw $r0 = $r0, $r2
; CV2-NEXT:    fminw $r1 = $r1, $r3
; CV2-NEXT:    fminw $r3 = $r11, $r10
; CV2-NEXT:    fminw $r4 = $r9, $r8
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    insf $r3 = $r0, 63, 32
; CV2-NEXT:    insf $r4 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    fminwp $r0 = $r3, $r4
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    srad $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    fminw $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 12)
  %2 = load <16 x float>, ptr %0
  %3 = tail call float @llvm.vector.reduce.fmin.v16f32(<16 x float> %2)
  ret float %3
}

declare float @llvm.vector.reduce.fmin.v16f32(<16 x float>)

define double @red_min_double2(<2 x double> %0) {
; CV1-LABEL: red_min_double2:
; CV1:       # %bb.0:
; CV1-NEXT:    fcompd.olt $r2 = $r1, $r0
; CV1-NEXT:    fcompd.une $r3 = $r0, $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    iord $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    cmoved.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 2)
;
; CV2-LABEL: red_min_double2:
; CV2:       # %bb.0:
; CV2-NEXT:    fmind $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %2 = tail call double @llvm.vector.reduce.fmin.v2f64(<2 x double> %0)
  ret double %2
}

declare double @llvm.vector.reduce.fmin.v2f64(<2 x double>)

define double @red_min_double4(<4 x double> %0) {
; CV1-LABEL: red_min_double4:
; CV1:       # %bb.0:
; CV1-NEXT:    fcompd.olt $r4 = $r1, $r0
; CV1-NEXT:    fcompd.une $r5 = $r0, $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    iord $r4 = $r4, $r5
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    cmoved.odd $r4 ? $r0 = $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    fcompd.olt $r1 = $r2, $r0
; CV1-NEXT:    fcompd.une $r4 = $r0, $r0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    iord $r1 = $r1, $r4
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    cmoved.odd $r1 ? $r0 = $r2
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    fcompd.olt $r1 = $r3, $r0
; CV1-NEXT:    fcompd.une $r2 = $r0, $r0
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    iord $r1 = $r1, $r2
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    cmoved.odd $r1 ? $r0 = $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 8)
;
; CV2-LABEL: red_min_double4:
; CV2:       # %bb.0:
; CV2-NEXT:    fmind $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    fmind $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    fmind $r0 = $r0, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 2)
  %2 = tail call double @llvm.vector.reduce.fmin.v4f64(<4 x double> %0)
  ret double %2
}

declare double @llvm.vector.reduce.fmin.v4f64(<4 x double>)

define double @red_min_double8(ptr %0) {
; CV1-LABEL: red_min_double8:
; CV1:       # %bb.0:
; CV1-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r0r1r2r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    fcompd.olt $r8 = $r4, $r0
; CV1-NEXT:    fcompd.une $r9 = $r0, $r0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    iord $r8 = $r8, $r9
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    cmoved.odd $r8 ? $r0 = $r4
; CV1-NEXT:    fcompd.olt $r4 = $r5, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    fcompd.une $r8 = $r1, $r1
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    iord $r4 = $r4, $r8
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    cmoved.odd $r4 ? $r1 = $r5
; CV1-NEXT:    fcompd.une $r5 = $r0, $r0
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    fcompd.olt $r4 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    iord $r4 = $r4, $r5
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    cmoved.odd $r4 ? $r0 = $r1
; CV1-NEXT:    fcompd.olt $r1 = $r6, $r2
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    fcompd.une $r4 = $r2, $r2
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    iord $r1 = $r1, $r4
; CV1-NEXT:    fcompd.une $r4 = $r0, $r0
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    cmoved.odd $r1 ? $r2 = $r6
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    fcompd.olt $r1 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    iord $r1 = $r1, $r4
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    cmoved.odd $r1 ? $r0 = $r2
; CV1-NEXT:    fcompd.olt $r1 = $r7, $r3
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    fcompd.une $r2 = $r3, $r3
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    iord $r1 = $r1, $r2
; CV1-NEXT:    fcompd.une $r2 = $r0, $r0
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    cmoved.odd $r1 ? $r3 = $r7
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    fcompd.olt $r1 = $r3, $r0
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    iord $r1 = $r1, $r2
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    cmoved.odd $r1 ? $r0 = $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 24)
;
; CV2-LABEL: red_min_double8:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r0r1r2r3 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    fmind $r0 = $r4, $r0
; CV2-NEXT:    fmind $r1 = $r5, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    fmind $r0 = $r0, $r1
; CV2-NEXT:    fmind $r1 = $r6, $r2
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    fmind $r0 = $r0, $r1
; CV2-NEXT:    fmind $r1 = $r7, $r3
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    fmind $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = load <8 x double>, ptr %0
  %3 = tail call double @llvm.vector.reduce.fmin.v8f64(<8 x double> %2)
  ret double %3
}

declare double @llvm.vector.reduce.fmin.v8f64(<8 x double>)

define double @red_min_double16(ptr %0) {
; CV1-LABEL: red_min_double16:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r4 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r8r9r10r11 = 96[$r4]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    lo $r32r33r34r35 = 64[$r4]
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    lo $r0r1r2r3 = 0[$r4]
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    lo $r4r5r6r7 = 32[$r4]
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    fcompd.olt $r15 = $r32, $r0
; CV1-NEXT:    fcompd.une $r16 = $r0, $r0
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    iord $r15 = $r15, $r16
; CV1-NEXT:    fcompd.olt $r17 = $r8, $r4
; CV1-NEXT:    fcompd.une $r36 = $r4, $r4
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    cmoved.odd $r15 ? $r0 = $r32
; CV1-NEXT:    iord $r16 = $r17, $r36
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    cmoved.odd $r16 ? $r4 = $r8
; CV1-NEXT:    fcompd.une $r15 = $r0, $r0
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    fcompd.olt $r8 = $r4, $r0
; CV1-NEXT:    fcompd.olt $r16 = $r9, $r5
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    iord $r8 = $r8, $r15
; CV1-NEXT:    fcompd.une $r15 = $r5, $r5
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    cmoved.odd $r8 ? $r0 = $r4
; CV1-NEXT:    fcompd.olt $r4 = $r33, $r1
; CV1-NEXT:    iord $r15 = $r16, $r15
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    cmoved.odd $r15 ? $r5 = $r9
; CV1-NEXT:    fcompd.une $r8 = $r1, $r1
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    iord $r4 = $r4, $r8
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    cmoved.odd $r4 ? $r1 = $r33
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    fcompd.olt $r4 = $r5, $r1
; CV1-NEXT:    fcompd.une $r8 = $r1, $r1
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    iord $r4 = $r4, $r8
; CV1-NEXT:    fcompd.olt $r8 = $r10, $r6
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    cmoved.odd $r4 ? $r1 = $r5
; CV1-NEXT:    fcompd.une $r5 = $r0, $r0
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    fcompd.olt $r4 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    iord $r4 = $r4, $r5
; CV1-NEXT:    fcompd.une $r5 = $r6, $r6
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    cmoved.odd $r4 ? $r0 = $r1
; CV1-NEXT:    fcompd.olt $r1 = $r34, $r2
; CV1-NEXT:    iord $r5 = $r8, $r5
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    fcompd.une $r4 = $r2, $r2
; CV1-NEXT:    cmoved.odd $r5 ? $r6 = $r10
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    iord $r1 = $r1, $r4
; CV1-NEXT:    fcompd.une $r5 = $r7, $r7
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    cmoved.odd $r1 ? $r2 = $r34
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    fcompd.olt $r1 = $r6, $r2
; CV1-NEXT:    fcompd.une $r4 = $r2, $r2
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    iord $r1 = $r1, $r4
; CV1-NEXT:    fcompd.une $r4 = $r0, $r0
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    cmoved.odd $r1 ? $r2 = $r6
; CV1-NEXT:    ;; # (end cycle 27)
; CV1-NEXT:    fcompd.olt $r1 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 28)
; CV1-NEXT:    iord $r1 = $r1, $r4
; CV1-NEXT:    fcompd.olt $r4 = $r11, $r7
; CV1-NEXT:    ;; # (end cycle 29)
; CV1-NEXT:    cmoved.odd $r1 ? $r0 = $r2
; CV1-NEXT:    fcompd.olt $r1 = $r35, $r3
; CV1-NEXT:    ;; # (end cycle 30)
; CV1-NEXT:    fcompd.une $r2 = $r3, $r3
; CV1-NEXT:    ;; # (end cycle 31)
; CV1-NEXT:    iord $r1 = $r1, $r2
; CV1-NEXT:    iord $r2 = $r4, $r5
; CV1-NEXT:    ;; # (end cycle 32)
; CV1-NEXT:    cmoved.odd $r1 ? $r3 = $r35
; CV1-NEXT:    cmoved.odd $r2 ? $r7 = $r11
; CV1-NEXT:    ;; # (end cycle 33)
; CV1-NEXT:    fcompd.olt $r1 = $r7, $r3
; CV1-NEXT:    fcompd.une $r2 = $r3, $r3
; CV1-NEXT:    ;; # (end cycle 34)
; CV1-NEXT:    iord $r1 = $r1, $r2
; CV1-NEXT:    fcompd.une $r2 = $r0, $r0
; CV1-NEXT:    ;; # (end cycle 35)
; CV1-NEXT:    cmoved.odd $r1 ? $r3 = $r7
; CV1-NEXT:    ;; # (end cycle 36)
; CV1-NEXT:    fcompd.olt $r1 = $r3, $r0
; CV1-NEXT:    ;; # (end cycle 37)
; CV1-NEXT:    iord $r1 = $r1, $r2
; CV1-NEXT:    ;; # (end cycle 38)
; CV1-NEXT:    cmoved.odd $r1 ? $r0 = $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 39)
;
; CV2-LABEL: red_min_double16:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r8r9r10r11 = 64[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    lo $r32r33r34r35 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    lo $r0r1r2r3 = 96[$r0]
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    fmind $r4 = $r4, $r8
; CV2-NEXT:    fmind $r5 = $r5, $r9
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    fmind $r0 = $r32, $r0
; CV2-NEXT:    fmind $r1 = $r33, $r1
; CV2-NEXT:    fmind $r2 = $r34, $r2
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    fmind $r0 = $r4, $r0
; CV2-NEXT:    fmind $r1 = $r5, $r1
; CV2-NEXT:    fmind $r4 = $r6, $r10
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    fmind $r0 = $r0, $r1
; CV2-NEXT:    fmind $r1 = $r4, $r2
; CV2-NEXT:    fmind $r2 = $r35, $r3
; CV2-NEXT:    fmind $r3 = $r7, $r11
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    fmind $r0 = $r0, $r1
; CV2-NEXT:    fmind $r1 = $r3, $r2
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    fmind $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 10)
  %2 = load <16 x double>, ptr %0
  %3 = tail call double @llvm.vector.reduce.fmin.v16f64(<16 x double> %2)
  ret double %3
}

declare double @llvm.vector.reduce.fmin.v16f64(<16 x double>)

define half @red_min_half2(<2 x half> %0) {
; CV1-LABEL: red_min_half2:
; CV1:       # %bb.0:
; CV1-NEXT:    srlw $r1 = $r0, 16
; CV1-NEXT:    fcompnhq.une $r3 = $r0, $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    fcompnhq.olt $r2 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    iorw $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    cmovehq.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: red_min_half2:
; CV2:       # %bb.0:
; CV2-NEXT:    srlw $r1 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    fminhq $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 1)
  %2 = tail call half @llvm.vector.reduce.fmin.v2f16(<2 x half> %0)
  ret half %2
}

declare half @llvm.vector.reduce.fmin.v2f16(<2 x half>)

define half @red_min_half4(<4 x half> %0) {
; CV1-LABEL: red_min_half4:
; CV1:       # %bb.0:
; CV1-NEXT:    srld $r1 = $r0, 32
; CV1-NEXT:    fcompnhq.une $r3 = $r0, $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    fcompnhq.olt $r2 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    iorw $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    cmovehq.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srlw $r1 = $r0, 16
; CV1-NEXT:    fcompnhq.une $r3 = $r0, $r0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    fcompnhq.olt $r2 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    iorw $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    cmovehq.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: red_min_half4:
; CV2:       # %bb.0:
; CV2-NEXT:    srld $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    fminhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    srlw $r1 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    fminhq $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 3)
  %2 = tail call half @llvm.vector.reduce.fmin.v4f16(<4 x half> %0)
  ret half %2
}

declare half @llvm.vector.reduce.fmin.v4f16(<4 x half>)

define half @red_min_half8(<8 x half> %0) {
; CV1-LABEL: red_min_half8:
; CV1:       # %bb.0:
; CV1-NEXT:    fcompnhq.olt $r2 = $r1, $r0
; CV1-NEXT:    fcompnhq.une $r3 = $r0, $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    iord $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    cmovehq.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    srld $r1 = $r0, 32
; CV1-NEXT:    fcompnhq.une $r3 = $r0, $r0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    fcompnhq.olt $r2 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    iorw $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    cmovehq.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    srlw $r1 = $r0, 16
; CV1-NEXT:    fcompnhq.une $r3 = $r0, $r0
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    fcompnhq.olt $r2 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    iorw $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    cmovehq.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 10)
;
; CV2-LABEL: red_min_half8:
; CV2:       # %bb.0:
; CV2-NEXT:    fminhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srld $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    fminhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    srlw $r1 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    fminhq $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 4)
  %2 = tail call half @llvm.vector.reduce.fmin.v8f16(<8 x half> %0)
  ret half %2
}

declare half @llvm.vector.reduce.fmin.v8f16(<8 x half>)

define half @red_min_half16(<16 x half> %0) {
; CV1-LABEL: red_min_half16:
; CV1:       # %bb.0:
; CV1-NEXT:    srld $r4 = $r0, 48
; CV1-NEXT:    srld $r5 = $r2, 48
; CV1-NEXT:    srld $r8 = $r0, 32
; CV1-NEXT:    srld $r9 = $r2, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    fcompnhq.olt $r6 = $r5, $r4
; CV1-NEXT:    fcompnhq.une $r7 = $r4, $r4
; CV1-NEXT:    srld $r15 = $r1, 48
; CV1-NEXT:    srld $r16 = $r3, 48
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    iorw $r6 = $r6, $r7
; CV1-NEXT:    fcompnhq.olt $r10 = $r9, $r8
; CV1-NEXT:    fcompnhq.une $r11 = $r8, $r8
; CV1-NEXT:    srld $r33 = $r1, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    cmovehq.odd $r6 ? $r4 = $r5
; CV1-NEXT:    srlw $r5 = $r0, 16
; CV1-NEXT:    srlw $r6 = $r2, 16
; CV1-NEXT:    iorw $r7 = $r10, $r11
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    fcompnhq.olt $r7 = $r6, $r5
; CV1-NEXT:    cmovehq.odd $r7 ? $r8 = $r9
; CV1-NEXT:    srld $r34 = $r3, 32
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    fcompnhq.une $r9 = $r5, $r5
; CV1-NEXT:    fcompnhq.olt $r10 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    iorw $r7 = $r7, $r9
; CV1-NEXT:    fcompnhq.une $r11 = $r0, $r0
; CV1-NEXT:    fcompnhq.olt $r17 = $r16, $r15
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    iorw $r9 = $r10, $r11
; CV1-NEXT:    fcompnhq.olt $r10 = $r34, $r33
; CV1-NEXT:    fcompnhq.une $r32 = $r15, $r15
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    cmovehq.odd $r7 ? $r5 = $r6
; CV1-NEXT:    fcompnhq.une $r11 = $r33, $r33
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    cmovehq.odd $r9 ? $r0 = $r2
; CV1-NEXT:    iorw $r2 = $r17, $r32
; CV1-NEXT:    iorw $r6 = $r10, $r11
; CV1-NEXT:    fcompnhq.une $r10 = $r1, $r1
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    srlw $r2 = $r1, 16
; CV1-NEXT:    srlw $r6 = $r3, 16
; CV1-NEXT:    cmovehq.odd $r2 ? $r15 = $r16
; CV1-NEXT:    cmovehq.odd $r6 ? $r33 = $r34
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    fcompnhq.olt $r7 = $r6, $r2
; CV1-NEXT:    fcompnhq.une $r9 = $r2, $r2
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    iorw $r7 = $r7, $r9
; CV1-NEXT:    insf $r8 = $r4, 31, 16
; CV1-NEXT:    fcompnhq.olt $r9 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    insf $r0 = $r5, 31, 16
; CV1-NEXT:    cmovehq.odd $r7 ? $r2 = $r6
; CV1-NEXT:    iorw $r4 = $r9, $r10
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    cmovehq.odd $r4 ? $r1 = $r3
; CV1-NEXT:    insf $r33 = $r15, 31, 16
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    insf $r0 = $r8, 63, 32
; CV1-NEXT:    insf $r1 = $r2, 31, 16
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    insf $r1 = $r33, 63, 32
; CV1-NEXT:    fcompnhq.une $r3 = $r0, $r0
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    fcompnhq.olt $r2 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    iord $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    cmovehq.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    srld $r1 = $r0, 32
; CV1-NEXT:    fcompnhq.une $r3 = $r0, $r0
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    fcompnhq.olt $r2 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    iorw $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    cmovehq.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    srlw $r1 = $r0, 16
; CV1-NEXT:    fcompnhq.une $r3 = $r0, $r0
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    fcompnhq.olt $r2 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    iorw $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    cmovehq.odd $r2 ? $r0 = $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 27)
;
; CV2-LABEL: red_min_half16:
; CV2:       # %bb.0:
; CV2-NEXT:    srld $r4 = $r3, 48
; CV2-NEXT:    srld $r5 = $r1, 48
; CV2-NEXT:    srld $r6 = $r3, 32
; CV2-NEXT:    srld $r7 = $r1, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    fminhq $r4 = $r5, $r4
; CV2-NEXT:    fminhq $r5 = $r7, $r6
; CV2-NEXT:    srlw $r6 = $r3, 16
; CV2-NEXT:    srlw $r7 = $r1, 16
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    fminhq $r1 = $r1, $r3
; CV2-NEXT:    srld $r3 = $r2, 48
; CV2-NEXT:    fminhq $r6 = $r7, $r6
; CV2-NEXT:    srld $r7 = $r0, 48
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    srld $r8 = $r2, 32
; CV2-NEXT:    srld $r9 = $r0, 32
; CV2-NEXT:    srlw $r10 = $r2, 16
; CV2-NEXT:    srlw $r11 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    fminhq $r0 = $r0, $r2
; CV2-NEXT:    fminhq $r3 = $r7, $r3
; CV2-NEXT:    fminhq $r7 = $r9, $r8
; CV2-NEXT:    fminhq $r8 = $r11, $r10
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r0 = $r8, 31, 16
; CV2-NEXT:    insf $r1 = $r6, 31, 16
; CV2-NEXT:    insf $r5 = $r4, 31, 16
; CV2-NEXT:    insf $r7 = $r3, 31, 16
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r0 = $r7, 63, 32
; CV2-NEXT:    insf $r1 = $r5, 63, 32
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    fminhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    srld $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    fminhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    srlw $r1 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    fminhq $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 11)
  %2 = tail call half @llvm.vector.reduce.fmin.v16f16(<16 x half> %0)
  ret half %2
}

declare half @llvm.vector.reduce.fmin.v16f16(<16 x half>)
