; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -o - %s | FileCheck %s --check-prefixes=CHECK,V1
; RUN: llc -mcpu=kv3-2 -o - %s | FileCheck %s --check-prefixes=CHECK,V2
; RUN: clang -c -o /dev/null %s
; RUN: clang -march=kv3-2 -c -o /dev/null %s

target triple = "kvx-kalray-cos"

; From C++ code:
; complex<int> foo(complex<int> c1, complex<int> c2){
;   return c1 + conj(c2);
; }
define i64 @ADDCWCrr(i64 %c1, i64 %c2) {
; V1-LABEL: ADDCWCrr:
; V1:       # %bb.0: # %entry
; V1-NEXT:    addwc.c $r0 = $r1, $r0
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: ADDCWCrr:
; V2:       # %bb.0: # %entry
; V2-NEXT:    addd $r1 = $r1, $r0
; V2-NEXT:    clrf $r2 = $r1, 31, 0
; V2-NEXT:    ;;
; V2-NEXT:    sbfd $r0 = $r2, $r0
; V2-NEXT:    zxwd $r1 = $r1
; V2-NEXT:    ;;
; V2-NEXT:    clrf $r0 = $r0, 31, 0
; V2-NEXT:    ;;
; V2-NEXT:    ord $r0 = $r0, $r1
; V2-NEXT:    ret
; V2-NEXT:    ;;
entry:
  %i2 = and i64 %c2, -4294967296
  %r = add i64 %c2, %c1
  %i = sub i64 %c1, %i2
  %filti = and i64 %i, -4294967296
  %filtr = and i64 %r, 4294967295
  %retc = or i64 %filti, %filtr
  ret i64 %retc
}

;   return c1 - conj(c2);
define i64 @SBFCWCrr(i64 %c1, i64 %c2) {
; V1-LABEL: SBFCWCrr:
; V1:       # %bb.0: # %entry
; V1-NEXT:    sbfwc.c $r0 = $r1, $r0
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: SBFCWCrr:
; V2:       # %bb.0: # %entry
; V2-NEXT:    sbfd $r0 = $r1, $r0
; V2-NEXT:    clrf $r2 = $r0, 31, 0
; V2-NEXT:    ;;
; V2-NEXT:    zxwd $r0 = $r0
; V2-NEXT:    addd $r1 = $r2, $r1
; V2-NEXT:    ;;
; V2-NEXT:    clrf $r1 = $r1, 31, 0
; V2-NEXT:    ;;
; V2-NEXT:    ord $r0 = $r1, $r0
; V2-NEXT:    ret
; V2-NEXT:    ;;
entry:
  %i1 = and i64 %c1, -4294967296
  %r = sub i64 %c1, %c2
  %i = add i64 %i1, %c2
  %ifilt = and i64 %i, -4294967296
  %rfilt = and i64 %r, 4294967295
  %retc = or i64 %ifilt, %rfilt
  ret i64 %retc
}

%"struct.std::complex.1" = type { i64, i64 }
define i64 @ADDCHCP(i64 %0, i64 %1) {
; V1-LABEL: ADDCHCP:
; V1:       # %bb.0:
; V1-NEXT:    addhcp.c $r0 = $r0, $r1
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: ADDCHCP:
; V2:       # %bb.0:
; V2-NEXT:    clrf $r2 = $r1, 31, 47
; V2-NEXT:    addd $r3 = $r1, $r0
; V2-NEXT:    clrf $r4 = $r0, 47, 0
; V2-NEXT:    ;;
; V2-NEXT:    clrf $r0 = $r0, 15, 31
; V2-NEXT:    addd $r2 = $r2, $r0
; V2-NEXT:    clrf $r3 = $r3, 63, 16
; V2-NEXT:    sbfd $r4 = $r4, $r1
; V2-NEXT:    ;;
; V2-NEXT:    sbfd $r0 = $r0, $r1
; V2-NEXT:    clrf $r2 = $r2, 31, 47
; V2-NEXT:    clrf $r4 = $r4, 47, 0
; V2-NEXT:    ;;
; V2-NEXT:    clrf $r0 = $r0, 15, 31
; V2-NEXT:    ord $r1 = $r2, $r3
; V2-NEXT:    ;;
; V2-NEXT:    ord $r1 = $r1, $r4
; V2-NEXT:    ;;
; V2-NEXT:    ord $r0 = $r1, $r0
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = and i64 %0, 4294901760
  %4 = add i64 %1, %0
  %5 = and i64 %0, -281474976710656
  %6 = and i64 %1, 281470681743360
  %7 = sub i64 %1, %5
  %8 = and i64 %7, -281474976710656
  %9 = add i64 %6, %0
  %10 = and i64 %9, 281470681743360
  %11 = sub i64 %1, %3
  %12 = and i64 %11, 4294901760
  %13 = and i64 %4, 65535
  %14 = or i64 %10, %13
  %15 = or i64 %14, %8
  %16 = or i64 %15, %12
  ret i64 %16
}

define i64 @ADDCHCP_half(i64 %0, i64 %1) {
; V1-LABEL: ADDCHCP_half:
; V1:       # %bb.0:
; V1-NEXT:    addhcp.c $r0 = $r0, $r1
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: ADDCHCP_half:
; V2:       # %bb.0:
; V2-NEXT:    addd $r0 = $r1, $r0
; V2-NEXT:    clrf $r2 = $r0, 15, 31
; V2-NEXT:    ;;
; V2-NEXT:    clrf $r0 = $r0, 63, 16
; V2-NEXT:    sbfd $r1 = $r2, $r1
; V2-NEXT:    ;;
; V2-NEXT:    clrf $r1 = $r1, 15, 31
; V2-NEXT:    ;;
; V2-NEXT:    ord $r0 = $r1, $r0
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = and i64 %0, 4294901760
  %4 = add i64 %1, %0
  %5 = sub i64 %1, %3
  %6 = and i64 %5, 4294901760
  %7 = and i64 %4, 65535
  %8 = or i64 %6, %7
  ret i64 %8
}

define i64 @ADDCWC(i64 %0, i64 %1) {
; V1-LABEL: ADDCWC:
; V1:       # %bb.0:
; V1-NEXT:    addwc.c $r0 = $r1, $r0
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: ADDCWC:
; V2:       # %bb.0:
; V2-NEXT:    addd $r1 = $r1, $r0
; V2-NEXT:    clrf $r2 = $r1, 31, 0
; V2-NEXT:    ;;
; V2-NEXT:    sbfd $r0 = $r2, $r0
; V2-NEXT:    zxwd $r1 = $r1
; V2-NEXT:    ;;
; V2-NEXT:    clrf $r0 = $r0, 31, 0
; V2-NEXT:    ;;
; V2-NEXT:    ord $r0 = $r0, $r1
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = and i64 %1, -4294967296
  %4 = add i64 %1, %0
  %5 = sub i64 %0, %3
  %6 = and i64 %5, -4294967296
  %7 = and i64 %4, 4294967295
  %8 = or i64 %6, %7
  ret i64 %8
}

define i64 @SBFCHCP(i64 %0, i64 %1) {
; V1-LABEL: SBFCHCP:
; V1:       # %bb.0:
; V1-NEXT:    sbfhcp.c $r0 = $r1, $r0
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: SBFCHCP:
; V2:       # %bb.0:
; V2-NEXT:    clrf $r2 = $r0, 31, 47
; V2-NEXT:    clrf $r3 = $r0, 47, 0
; V2-NEXT:    sbfd $r4 = $r0, $r1
; V2-NEXT:    clrf $r5 = $r1, 15, 31
; V2-NEXT:    ;;
; V2-NEXT:    adduwd $r0 = $r5, $r0
; V2-NEXT:    addd $r1 = $r3, $r1
; V2-NEXT:    sbfd $r2 = $r2, $r1
; V2-NEXT:    clrf $r3 = $r4, 63, 16
; V2-NEXT:    ;;
; V2-NEXT:    clrf $r0 = $r0, 15, 31
; V2-NEXT:    clrf $r1 = $r1, 47, 0
; V2-NEXT:    clrf $r2 = $r2, 31, 47
; V2-NEXT:    ;;
; V2-NEXT:    ord $r2 = $r2, $r3
; V2-NEXT:    ;;
; V2-NEXT:    ord $r1 = $r2, $r1
; V2-NEXT:    ;;
; V2-NEXT:    ord $r0 = $r1, $r0
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = and i64 %0, 281470681743360
  %4 = and i64 %0, -281474976710656
  %5 = and i64 %1, 4294901760
  %6 = sub i64 %1, %0
  %7 = add i64 %4, %1
  %8 = and i64 %7, -281474976710656
  %9 = sub i64 %1, %3
  %10 = and i64 %9, 281470681743360
  %11 = add i64 %5, %0
  %12 = and i64 %11, 4294901760
  %13 = and i64 %6, 65535
  %14 = or i64 %10, %13
  %15 = or i64 %14, %8
  %16 = or i64 %15, %12
  ret i64 %16
}

define i64 @SBFCHCP_half(i64 %0, i64 %1) {
; V1-LABEL: SBFCHCP_half:
; V1:       # %bb.0:
; V1-NEXT:    sbfhcp.c $r0 = $r1, $r0
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: SBFCHCP_half:
; V2:       # %bb.0:
; V2-NEXT:    sbfd $r1 = $r0, $r1
; V2-NEXT:    clrf $r2 = $r1, 15, 31
; V2-NEXT:    ;;
; V2-NEXT:    adduwd $r0 = $r2, $r0
; V2-NEXT:    clrf $r1 = $r1, 63, 16
; V2-NEXT:    ;;
; V2-NEXT:    clrf $r0 = $r0, 15, 31
; V2-NEXT:    ;;
; V2-NEXT:    ord $r0 = $r0, $r1
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = and i64 %1, 4294901760
  %4 = sub i64 %1, %0
  %5 = add i64 %3, %0
  %6 = and i64 %5, 4294901760
  %7 = and i64 %4, 65535
  %8 = or i64 %6, %7
  ret i64 %8
}

define i64 @SBFCWC(i64 %0, i64 %1) {
; V1-LABEL: SBFCWC:
; V1:       # %bb.0:
; V1-NEXT:    sbfwc.c $r0 = $r1, $r0
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: SBFCWC:
; V2:       # %bb.0:
; V2-NEXT:    sbfd $r0 = $r1, $r0
; V2-NEXT:    clrf $r2 = $r0, 31, 0
; V2-NEXT:    ;;
; V2-NEXT:    zxwd $r0 = $r0
; V2-NEXT:    addd $r1 = $r2, $r1
; V2-NEXT:    ;;
; V2-NEXT:    clrf $r1 = $r1, 31, 0
; V2-NEXT:    ;;
; V2-NEXT:    ord $r0 = $r1, $r0
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = and i64 %0, -4294967296
  %4 = sub i64 %0, %1
  %5 = add i64 %3, %1
  %6 = and i64 %5, -4294967296
  %7 = and i64 %4, 4294967295
  %8 = or i64 %6, %7
  ret i64 %8
}

define i64 @MULCWC(i64 %0, i64 %1) {
; CHECK-LABEL: MULCWC:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srld $r2 = $r1, 32
; CHECK-NEXT:    srld $r3 = $r0, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    muluwd $r4 = $r2, $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    muld $r3 = $r3, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    maddd $r4 = $r1, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    msbfd $r3 = $r2, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r1 = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    slld $r0 = $r3, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ord $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = lshr i64 %1, 32
  %4 = lshr i64 %0, 32
  %5 = mul i64 %1, %0
  %6 = mul nuw i64 %3, %4
  %7 = add i64 %6, %5
  %8 = mul i64 %3, %0
  %9 = mul i64 %4, %1
  %10 = sub i64 %9, %8
  %11 = shl i64 %10, 32
  %12 = and i64 %7, 4294967295
  %13 = or i64 %11, %12
  ret i64 %13
}

define %"struct.std::complex.1" @MULCWDC(i64 %0, i64 %1) {
; CHECK-LABEL: MULCWDC:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mulwd $r2 = $r1, $r0
; CHECK-NEXT:    clrf $r3 = $r1, 31, 0
; CHECK-NEXT:    srld $r4 = $r0, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulwd $r1 = $r1, $r4
; CHECK-NEXT:    negd $r3 = $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    srld $r3 = $r3, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    msbfwd $r2 = $r3, $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    maddwd $r1 = $r3, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r0 = $r2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = and i64 %1, -4294967296
  %4 = shl i64 %0, 32
  %5 = ashr exact i64 %4, 32
  %6 = ashr i64 %0, 32
  %7 = shl i64 %1, 32
  %8 = ashr exact i64 %7, 32
  %9 = sub i64 0, %3
  %10 = ashr exact i64 %9, 32
  %11 = mul nsw i64 %8, %5
  %12 = mul nsw i64 %10, %6
  %13 = sub nsw i64 %11, %12
  %14 = mul nsw i64 %10, %5
  %15 = mul nsw i64 %8, %6
  %16 = add nsw i64 %14, %15
  %17 = insertvalue %"struct.std::complex.1" undef, i64 %13, 0
  %18 = insertvalue %"struct.std::complex.1" %17, i64 %16, 1
  ret %"struct.std::complex.1" %18
}

define i64 @MULWC(i64 %0, i64 %1) {
; CHECK-LABEL: MULWC:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srld $r2 = $r1, 32
; CHECK-NEXT:    srld $r3 = $r0, 32
; CHECK-NEXT:    muld $r4 = $r1, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    muld $r0 = $r2, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    msbfuwd $r4 = $r2, $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    maddd $r0 = $r3, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r1 = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    slld $r0 = $r0, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ord $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = lshr i64 %1, 32
  %4 = lshr i64 %0, 32
  %5 = mul i64 %1, %0
  %6 = mul nuw i64 %3, %4
  %7 = sub i64 %5, %6
  %8 = mul i64 %3, %0
  %9 = mul i64 %4, %1
  %10 = add i64 %8, %9
  %11 = shl i64 %10, 32
  %12 = and i64 %7, 4294967295
  %13 = or i64 %11, %12
  ret i64 %13
}

define %"struct.std::complex.1" @MULWDC(i64 %0, i64 %1) {
; CHECK-LABEL: MULWDC:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mulwd $r2 = $r1, $r0
; CHECK-NEXT:    srld $r3 = $r1, 32
; CHECK-NEXT:    srld $r4 = $r0, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    msbfwd $r2 = $r3, $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulwd $r3 = $r3, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r0 = $r2
; CHECK-NEXT:    maddwd $r3 = $r1, $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r1 = $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = shl i64 %0, 32
  %4 = ashr exact i64 %3, 32
  %5 = ashr i64 %0, 32
  %6 = shl i64 %1, 32
  %7 = ashr exact i64 %6, 32
  %8 = ashr i64 %1, 32
  %9 = mul nsw i64 %7, %4
  %10 = mul nsw i64 %8, %5
  %11 = sub nsw i64 %9, %10
  %12 = mul nsw i64 %8, %4
  %13 = mul nsw i64 %7, %5
  %14 = add nsw i64 %13, %12
  %15 = insertvalue %"struct.std::complex.1" undef, i64 %11, 0
  %16 = insertvalue %"struct.std::complex.1" %15, i64 %14, 1
  ret %"struct.std::complex.1" %16
}

define i64 @ADDCHCP_2(i64 %0, i64 %1) {
; V1-LABEL: ADDCHCP_2:
; V1:       # %bb.0:
; V1-NEXT:    addhcp.c $r0 = $r1, $r0
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: ADDCHCP_2:
; V2:       # %bb.0:
; V2-NEXT:    clrf $r2 = $r0, 31, 47
; V2-NEXT:    addd $r3 = $r0, $r1
; V2-NEXT:    clrf $r4 = $r1, 47, 0
; V2-NEXT:    ;;
; V2-NEXT:    clrf $r1 = $r1, 15, 31
; V2-NEXT:    addd $r2 = $r2, $r1
; V2-NEXT:    clrf $r3 = $r3, 63, 16
; V2-NEXT:    sbfd $r4 = $r4, $r0
; V2-NEXT:    ;;
; V2-NEXT:    sbfd $r0 = $r1, $r0
; V2-NEXT:    clrf $r2 = $r2, 31, 47
; V2-NEXT:    clrf $r4 = $r4, 47, 0
; V2-NEXT:    ;;
; V2-NEXT:    clrf $r0 = $r0, 15, 31
; V2-NEXT:    ord $r1 = $r2, $r3
; V2-NEXT:    ;;
; V2-NEXT:    ord $r1 = $r1, $r4
; V2-NEXT:    ;;
; V2-NEXT:    ord $r0 = $r1, $r0
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = and i64 %1, 4294901760
  %4 = add i64 %0, %1
  %5 = and i64 %0, 281470681743360
  %6 = and i64 %1, -281474976710656
  %7 = sub i64 %0, %6
  %8 = and i64 %7, -281474976710656
  %9 = add i64 %5, %1
  %10 = and i64 %9, 281470681743360
  %11 = sub i64 %0, %3
  %12 = and i64 %11, 4294901760
  %13 = and i64 %4, 65535
  %14 = or i64 %10, %13
  %15 = or i64 %14, %8
  %16 = or i64 %15, %12
  ret i64 %16
}

define i64 @ADDCWC_2(i64 %0, i64 %1) {
; V1-LABEL: ADDCWC_2:
; V1:       # %bb.0:
; V1-NEXT:    addwc.c $r0 = $r1, $r0
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: ADDCWC_2:
; V2:       # %bb.0:
; V2-NEXT:    addd $r1 = $r0, $r1
; V2-NEXT:    clrf $r2 = $r1, 31, 0
; V2-NEXT:    ;;
; V2-NEXT:    sbfd $r0 = $r2, $r0
; V2-NEXT:    zxwd $r1 = $r1
; V2-NEXT:    ;;
; V2-NEXT:    clrf $r0 = $r0, 31, 0
; V2-NEXT:    ;;
; V2-NEXT:    ord $r0 = $r0, $r1
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = and i64 %1, -4294967296
  %4 = add i64 %0, %1
  %5 = sub i64 %0, %3
  %6 = and i64 %5, -4294967296
  %7 = and i64 %4, 4294967295
  %8 = or i64 %6, %7
  ret i64 %8
}

define i64 @SBFCHCP_2(i64 %0, i64 %1) {
; V1-LABEL: SBFCHCP_2:
; V1:       # %bb.0:
; V1-NEXT:    clrf $r2 = $r1, 31, 47
; V1-NEXT:    sbfd $r4 = $r1, $r0
; V1-NEXT:    clrf $r5 = $r0, 47, 0
; V1-NEXT:    ;;
; V1-NEXT:    addd $r1 = $r5, $r1
; V1-NEXT:    sbfd $r2 = $r2, $r0
; V1-NEXT:    clrf $r3 = $r1, 15, 31
; V1-NEXT:    clrf $r4 = $r4, 63, 16
; V1-NEXT:    ;;
; V1-NEXT:    clrf $r1 = $r1, 47, 0
; V1-NEXT:    clrf $r2 = $r2, 31, 47
; V1-NEXT:    ;;
; V1-NEXT:    adduwd $r0 = $r3, $r0
; V1-NEXT:    ord $r2 = $r2, $r4
; V1-NEXT:    ;;
; V1-NEXT:    clrf $r0 = $r0, 15, 31
; V1-NEXT:    ord $r1 = $r2, $r1
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r1, $r0
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: SBFCHCP_2:
; V2:       # %bb.0:
; V2-NEXT:    clrf $r2 = $r1, 31, 47
; V2-NEXT:    sbfd $r3 = $r1, $r0
; V2-NEXT:    clrf $r4 = $r0, 47, 0
; V2-NEXT:    clrf $r5 = $r1, 15, 31
; V2-NEXT:    ;;
; V2-NEXT:    adduwd $r0 = $r5, $r0
; V2-NEXT:    addd $r1 = $r4, $r1
; V2-NEXT:    sbfd $r2 = $r2, $r0
; V2-NEXT:    clrf $r3 = $r3, 63, 16
; V2-NEXT:    ;;
; V2-NEXT:    clrf $r0 = $r0, 15, 31
; V2-NEXT:    clrf $r1 = $r1, 47, 0
; V2-NEXT:    clrf $r2 = $r2, 31, 47
; V2-NEXT:    ;;
; V2-NEXT:    ord $r2 = $r2, $r3
; V2-NEXT:    ;;
; V2-NEXT:    ord $r1 = $r2, $r1
; V2-NEXT:    ;;
; V2-NEXT:    ord $r0 = $r1, $r0
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = and i64 %1, 4294901760
  %4 = sub i64 %0, %1
  %5 = and i64 %0, -281474976710656
  %6 = and i64 %1, 281470681743360
  %7 = add i64 %5, %1
  %8 = and i64 %7, -281474976710656
  %9 = sub i64 %0, %6
  %10 = and i64 %9, 281470681743360
  %11 = add i64 %3, %0
  %12 = and i64 %11, 4294901760
  %13 = and i64 %4, 65535
  %14 = or i64 %10, %13
  %15 = or i64 %14, %8
  %16 = or i64 %15, %12
  ret i64 %16
}

define i64 @SBFCWC_2(i64 %0, i64 %1) {
; CHECK-LABEL: SBFCWC_2:
; CHECK:       # %bb.0:
; CHECK-NEXT:    sbfd $r1 = $r1, $r0
; CHECK-NEXT:    clrf $r2 = $r1, 31, 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r0 = $r2, $r0
; CHECK-NEXT:    zxwd $r1 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    clrf $r0 = $r0, 31, 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ord $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = and i64 %1, -4294967296
  %4 = sub i64 %0, %1
  %5 = add i64 %3, %0
  %6 = and i64 %5, -4294967296
  %7 = and i64 %4, 4294967295
  %8 = or i64 %6, %7
  ret i64 %8
}

define i64 @MULCWC_2(i64 %0, i64 %1) {
; CHECK-LABEL: MULCWC_2:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srld $r0 = $r0, 32
; CHECK-NEXT:    srld $r1 = $r1, 32
; CHECK-NEXT:    sxwd $r2 = $r0
; CHECK-NEXT:    sxwd $r3 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sxwd $r0 = $r0
; CHECK-NEXT:    sxwd $r1 = $r1
; CHECK-NEXT:    floatd.rn $r2 = $r2, 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    floatd.rn $r3 = $r3, 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    floatd.rn $r0 = $r0, 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    floatd.rn $r1 = $r1, 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmuld $r4 = $r3, $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmuld $r3 = $r0, $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ffmad $r4 = $r1, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ffmsd $r3 = $r1, $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fixedd.rz $r0 = $r4, 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fixedd.rz $r1 = $r3, 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r0 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    slld $r1 = $r1, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ord $r0 = $r1, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = trunc i64 %0 to i32
  %4 = lshr i64 %0, 32
  %5 = trunc i64 %4 to i32
  %6 = trunc i64 %1 to i32
  %7 = lshr i64 %1, 32
  %8 = trunc i64 %7 to i32
  %9 = sitofp i32 %3 to double
  %10 = sitofp i32 %5 to double
  %11 = sitofp i32 %6 to double
  %12 = sitofp i32 %8 to double
  %13 = fmul fast double %10, %11
  %14 = fmul fast double %12, %9
  %15 = fsub fast double %13, %14
  %16 = fmul fast double %11, %9
  %17 = fmul fast double %12, %10
  %18 = fadd fast double %17, %16
  %19 = fptosi double %18 to i32
  %20 = fptosi double %15 to i32
  %21 = zext i32 %20 to i64
  %22 = shl nuw i64 %21, 32
  %23 = zext i32 %19 to i64
  %24 = or i64 %22, %23
  ret i64 %24
}

define { i64, i64 } @MULCWDC_2(i64 %0, i64 %1) {
; CHECK-LABEL: MULCWDC_2:
; CHECK:       # %bb.0:
; CHECK-NEXT:    dot2wd $r2 = $r1, $r0
; CHECK-NEXT:    srld $r3 = $r0, 32
; CHECK-NEXT:    srld $r4 = $r1, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulwd $r1 = $r1, $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r0 = $r2
; CHECK-NEXT:    msbfwd $r1 = $r4, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = shl i64 %0, 32
  %4 = ashr exact i64 %3, 32
  %5 = ashr i64 %0, 32
  %6 = ashr i64 %1, 32
  %7 = shl i64 %1, 32
  %8 = ashr exact i64 %7, 32
  %9 = mul nsw i64 %8, %4
  %10 = mul nsw i64 %6, %5
  %11 = add i64 %10, %9
  %12 = mul nsw i64 %8, %5
  %13 = mul nsw i64 %6, %4
  %14 = sub i64 %12, %13
  %15 = insertvalue { i64, i64 } undef, i64 %11, 0
  %16 = insertvalue { i64, i64 } %15, i64 %14, 1
  ret { i64, i64 } %16
}

define i64 @MULWC_2(i64 %0, i64 %1) {
; CHECK-LABEL: MULWC_2:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srld $r2 = $r0, 32
; CHECK-NEXT:    srld $r3 = $r1, 32
; CHECK-NEXT:    muld $r4 = $r1, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    muld $r0 = $r3, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    msbfuwd $r4 = $r3, $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    maddd $r0 = $r2, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r1 = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    slld $r0 = $r0, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ord $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = lshr i64 %0, 32
  %4 = lshr i64 %1, 32
  %5 = mul i64 %1, %0
  %6 = mul nuw i64 %4, %3
  %7 = sub i64 %5, %6
  %8 = mul i64 %3, %1
  %9 = mul i64 %4, %0
  %10 = add i64 %9, %8
  %11 = shl i64 %10, 32
  %12 = and i64 %7, 4294967295
  %13 = or i64 %11, %12
  ret i64 %13
}

define { i64, i64 } @MULWDC_2(i64 %0, i64 %1) {
; CHECK-LABEL: MULWDC_2:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mulwd $r2 = $r1, $r0
; CHECK-NEXT:    srld $r3 = $r1, 32
; CHECK-NEXT:    srld $r4 = $r0, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    msbfwd $r2 = $r3, $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulwd $r3 = $r0, $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r0 = $r2
; CHECK-NEXT:    maddwd $r3 = $r1, $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r1 = $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = shl i64 %0, 32
  %4 = ashr exact i64 %3, 32
  %5 = ashr i64 %0, 32
  %6 = shl i64 %1, 32
  %7 = ashr exact i64 %6, 32
  %8 = ashr i64 %1, 32
  %9 = mul nsw i64 %7, %4
  %10 = mul nsw i64 %8, %5
  %11 = sub nsw i64 %9, %10
  %12 = mul nsw i64 %7, %5
  %13 = mul nsw i64 %4, %8
  %14 = add i64 %12, %13
  %15 = insertvalue { i64, i64 } undef, i64 %11, 0
  %16 = insertvalue { i64, i64 } %15, i64 %14, 1
  ret { i64, i64 } %16
}

define <4 x i16> @ADDCHCP_3(<4 x i16> %0, <4 x i16> %1) {
; CHECK-LABEL: ADDCHCP_3:
; CHECK:       # %bb.0:
; CHECK-NEXT:    sbfhq $r0 = $r1, $r0
; CHECK-NEXT:    addhq $r2 = $r0, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxhd $r0 = $r2
; CHECK-NEXT:    srld $r1 = $r0, 48
; CHECK-NEXT:    extfz $r3 = $r2, 47, 32
; CHECK-NEXT:    srlw $r4 = $r0, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r0 = $r4, 31, 16
; CHECK-NEXT:    insf $r3 = $r1, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r0 = $r3, 63, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = add <4 x i16> %0, %1
  %4 = sub <4 x i16> %0, %1
  %5 = shufflevector <4 x i16> %3, <4 x i16> %4, <4 x i32> <i32 0, i32 5, i32 2, i32 7>
  ret <4 x i16> %5
}

define <2 x i32> @ADDCWC_3(<2 x i32> %0, <2 x i32> %1) {
; CHECK-LABEL: ADDCWC_3:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srad $r2 = $r1, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    negw $r2 = $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r1 = $r2, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addwp $r0 = $r1, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = extractelement <2 x i32> %1, i32 1
  %4 = sub nsw i32 0, %3
  %5 = insertelement <2 x i32> %1, i32 %4, i32 1
  %6 = add <2 x i32> %5, %0
  ret <2 x i32> %6
}

define <4 x i16> @SBFCHCP_3(<4 x i16> %0, <4 x i16> %1) {
; CHECK-LABEL: SBFCHCP_3:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addhq $r0 = $r0, $r1
; CHECK-NEXT:    sbfhq $r2 = $r1, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxhd $r0 = $r2
; CHECK-NEXT:    srld $r1 = $r0, 48
; CHECK-NEXT:    extfz $r3 = $r2, 47, 32
; CHECK-NEXT:    srlw $r4 = $r0, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r0 = $r4, 31, 16
; CHECK-NEXT:    insf $r3 = $r1, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r0 = $r3, 63, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = sub <4 x i16> %0, %1
  %4 = add <4 x i16> %0, %1
  %5 = shufflevector <4 x i16> %3, <4 x i16> %4, <4 x i32> <i32 0, i32 5, i32 2, i32 7>
  ret <4 x i16> %5
}

define <2 x i32> @SBFCWC_3(<2 x i32> %0, <2 x i32> %1) {
; CHECK-LABEL: SBFCWC_3:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srad $r2 = $r1, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    negw $r2 = $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r1 = $r2, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfwp $r0 = $r1, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = extractelement <2 x i32> %1, i32 1
  %4 = sub nsw i32 0, %3
  %5 = insertelement <2 x i32> %1, i32 %4, i32 1
  %6 = sub <2 x i32> %0, %5
  ret <2 x i32> %6
}

define <2 x i32> @MULCWC_3(<2 x i32> %0, <2 x i32> %1) {
; CHECK-LABEL: MULCWC_3:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mulw $r2 = $r1, $r0
; CHECK-NEXT:    srad $r3 = $r0, 32
; CHECK-NEXT:    srad $r4 = $r1, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r0 = $r4, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    maddw $r2 = $r4, $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    msbfw $r0 = $r1, $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r2 = $r0, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r0 = $r2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = extractelement <2 x i32> %0, i32 0
  %4 = extractelement <2 x i32> %1, i32 0
  %5 = mul nsw i32 %4, %3
  %6 = extractelement <2 x i32> %0, i32 1
  %7 = extractelement <2 x i32> %1, i32 1
  %8 = mul nsw i32 %7, %6
  %9 = add nsw i32 %5, %8
  %10 = insertelement <2 x i32> undef, i32 %9, i32 0
  %11 = mul nsw i32 %7, %3
  %12 = mul nsw i32 %4, %6
  %13 = sub nsw i32 %11, %12
  %14 = insertelement <2 x i32> %10, i32 %13, i32 1
  ret <2 x i32> %14
}

define <2 x i64> @MULCWDC_3(<2 x i32> %0, <2 x i32> %1) {
; V1-LABEL: MULCWDC_3:
; V1:       # %bb.0:
; V1-NEXT:    sxwd $r4 = $r0
; V1-NEXT:    sxwd $r6 = $r1
; V1-NEXT:    ;;
; V1-NEXT:    muld $r2 = $r6, $r4
; V1-NEXT:    extfs $r3 = $r1, 63, 32
; V1-NEXT:    extfs $r5 = $r0, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    muld $r1 = $r3, $r4
; V1-NEXT:    ;;
; V1-NEXT:    maddd $r2 = $r3, $r5
; V1-NEXT:    ;;
; V1-NEXT:    msbfd $r1 = $r6, $r5
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r2
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: MULCWDC_3:
; V2:       # %bb.0:
; V2-NEXT:    extfs $r3 = $r1, 63, 32
; V2-NEXT:    sxwd $r4 = $r0
; V2-NEXT:    extfs $r5 = $r0, 63, 32
; V2-NEXT:    sxwd $r6 = $r1
; V2-NEXT:    ;;
; V2-NEXT:    muld $r2 = $r6, $r4
; V2-NEXT:    ;;
; V2-NEXT:    muld $r1 = $r3, $r4
; V2-NEXT:    ;;
; V2-NEXT:    maddd $r2 = $r3, $r5
; V2-NEXT:    ;;
; V2-NEXT:    msbfd $r1 = $r6, $r5
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r2
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = sext <2 x i32> %0 to <2 x i64>
  %4 = sext <2 x i32> %1 to <2 x i64>
  %5 = extractelement <2 x i64> %3, i32 0
  %6 = extractelement <2 x i64> %4, i32 0
  %7 = mul nsw i64 %6, %5
  %8 = extractelement <2 x i64> %3, i32 1
  %9 = extractelement <2 x i64> %4, i32 1
  %10 = mul nsw i64 %9, %8
  %11 = add nsw i64 %7, %10
  %12 = insertelement <2 x i64> undef, i64 %11, i32 0
  %13 = mul nsw i64 %9, %5
  %14 = mul nsw i64 %6, %8
  %15 = sub nsw i64 %13, %14
  %16 = insertelement <2 x i64> %12, i64 %15, i32 1
  ret <2 x i64> %16
}

define <2 x i32> @MULWC_3(<2 x i32> %0, <2 x i32> %1) {
; CHECK-LABEL: MULWC_3:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mulw $r2 = $r1, $r0
; CHECK-NEXT:    srad $r3 = $r0, 32
; CHECK-NEXT:    srad $r4 = $r1, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mulw $r0 = $r4, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    msbfw $r2 = $r4, $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    maddw $r0 = $r1, $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r2 = $r0, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r0 = $r2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = extractelement <2 x i32> %0, i32 0
  %4 = extractelement <2 x i32> %1, i32 0
  %5 = mul nsw i32 %4, %3
  %6 = extractelement <2 x i32> %0, i32 1
  %7 = extractelement <2 x i32> %1, i32 1
  %8 = mul nsw i32 %7, %6
  %9 = sub nsw i32 %5, %8
  %10 = insertelement <2 x i32> undef, i32 %9, i32 0
  %11 = mul nsw i32 %7, %3
  %12 = mul nsw i32 %4, %6
  %13 = add nsw i32 %11, %12
  %14 = insertelement <2 x i32> %10, i32 %13, i32 1
  ret <2 x i32> %14
}

define <2 x i64> @MULWDC_3(<2 x i32> %0, <2 x i32> %1) {
; V1-LABEL: MULWDC_3:
; V1:       # %bb.0:
; V1-NEXT:    sxwd $r4 = $r0
; V1-NEXT:    sxwd $r6 = $r1
; V1-NEXT:    ;;
; V1-NEXT:    muld $r2 = $r6, $r4
; V1-NEXT:    extfs $r3 = $r1, 63, 32
; V1-NEXT:    extfs $r5 = $r0, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    muld $r1 = $r3, $r4
; V1-NEXT:    ;;
; V1-NEXT:    msbfd $r2 = $r3, $r5
; V1-NEXT:    ;;
; V1-NEXT:    maddd $r1 = $r6, $r5
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r2
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: MULWDC_3:
; V2:       # %bb.0:
; V2-NEXT:    extfs $r3 = $r1, 63, 32
; V2-NEXT:    sxwd $r4 = $r0
; V2-NEXT:    extfs $r5 = $r0, 63, 32
; V2-NEXT:    sxwd $r6 = $r1
; V2-NEXT:    ;;
; V2-NEXT:    muld $r2 = $r6, $r4
; V2-NEXT:    ;;
; V2-NEXT:    muld $r1 = $r3, $r4
; V2-NEXT:    ;;
; V2-NEXT:    msbfd $r2 = $r3, $r5
; V2-NEXT:    ;;
; V2-NEXT:    maddd $r1 = $r6, $r5
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r2
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = sext <2 x i32> %0 to <2 x i64>
  %4 = sext <2 x i32> %1 to <2 x i64>
  %5 = extractelement <2 x i64> %3, i32 0
  %6 = extractelement <2 x i64> %4, i32 0
  %7 = mul nsw i64 %6, %5
  %8 = extractelement <2 x i64> %3, i32 1
  %9 = extractelement <2 x i64> %4, i32 1
  %10 = mul nsw i64 %9, %8
  %11 = sub nsw i64 %7, %10
  %12 = insertelement <2 x i64> undef, i64 %11, i32 0
  %13 = mul nsw i64 %9, %5
  %14 = mul nsw i64 %6, %8
  %15 = add nsw i64 %13, %14
  %16 = insertelement <2 x i64> %12, i64 %15, i32 1
  ret <2 x i64> %16
}
