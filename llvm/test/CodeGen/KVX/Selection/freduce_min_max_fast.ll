; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mcpu=kv3-1 -O2 -o - %s | FileCheck %s --check-prefixes=ALL,CV1
; RUN: llc -mcpu=kv3-2 -O2 -o - %s | FileCheck %s --check-prefixes=ALL,CV2
; RUN: clang -O2 -march=kv3-1 -c -o /dev/null %s
; RUN: clang -O2 -march=kv3-2 -c -o /dev/null %s

target triple = "kvx-kalray-cos"

define float @red_max_float2(<2 x float> %0) {
; ALL-LABEL: red_max_float2:
; ALL:       # %bb.0:
; ALL-NEXT:    srad $r1 = $r0, 32
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    insf $r1 = $r0, 63, 32
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    fmaxwp $r0 = $r0, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 2)
  %2 = tail call fast float @llvm.vector.reduce.fmax.v2f32(<2 x float> %0)
  ret float %2
}

declare float @llvm.vector.reduce.fmax.v2f32(<2 x float>)

; TODO: This should be fmaxw(eltExtract(fmaxwp)
define float @red_max_float4(<4 x float> %0) {
; ALL-LABEL: red_max_float4:
; ALL:       # %bb.0:
; ALL-NEXT:    copyd $r2 = $r1
; ALL-NEXT:    fcompnwp.olt $r3 = $r0, $r1
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    fcompnwp.olt $r4 = $r2, $r0
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    cmovewp.even $r4 ? $r0 = $r2
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    cmovewp.even $r3 ? $r1 = $r0
; ALL-NEXT:    srld $r2 = $r0, 32
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    insf $r2 = $r2, 63, 32
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    copyd $r3 = $r2
; ALL-NEXT:    fcompnwp.olt $r4 = $r2, $r0
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    cmovewp.even $r4 ? $r0 = $r2
; ALL-NEXT:    fcompnwp.olt $r5 = $r3, $r1
; ALL-NEXT:    ;; # (end cycle 6)
; ALL-NEXT:    cmovewp.even $r5 ? $r1 = $r3
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 7)
  %2 = tail call fast float @llvm.vector.reduce.fmax.v4f32(<4 x float> %0)
  ret float %2
}

declare float @llvm.vector.reduce.fmax.v4f32(<4 x float>)

define float @red_max_float8(<8 x float> %0) {
; ALL-LABEL: red_max_float8:
; ALL:       # %bb.0:
; ALL-NEXT:    fcompnwp.olt $r4 = $r2, $r0
; ALL-NEXT:    fcompnwp.olt $r5 = $r3, $r1
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    cmovewp.even $r4 ? $r0 = $r2
; ALL-NEXT:    cmovewp.even $r5 ? $r1 = $r3
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    srld $r2 = $r1, 32
; ALL-NEXT:    copyd $r4 = $r1
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    insf $r4 = $r2, 63, 32
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    fcompnwp.olt $r2 = $r4, $r0
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    cmovewp.even $r2 ? $r0 = $r4
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    srld $r2 = $r0, 32
; ALL-NEXT:    fcompnwp.olt $r3 = $r0, $r1
; ALL-NEXT:    ;; # (end cycle 6)
; ALL-NEXT:    cmovewp.even $r3 ? $r1 = $r0
; ALL-NEXT:    insf $r2 = $r2, 63, 32
; ALL-NEXT:    ;; # (end cycle 7)
; ALL-NEXT:    copyd $r3 = $r2
; ALL-NEXT:    fcompnwp.olt $r4 = $r2, $r0
; ALL-NEXT:    ;; # (end cycle 8)
; ALL-NEXT:    cmovewp.even $r4 ? $r0 = $r2
; ALL-NEXT:    fcompnwp.olt $r5 = $r3, $r1
; ALL-NEXT:    ;; # (end cycle 9)
; ALL-NEXT:    cmovewp.even $r5 ? $r1 = $r3
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 10)
  %2 = tail call fast float @llvm.vector.reduce.fmax.v8f32(<8 x float> %0)
  ret float %2
}

declare float @llvm.vector.reduce.fmax.v8f32(<8 x float>)

define float @red_max_float16(ptr %0) {
; CV1-LABEL: red_max_float16:
; CV1:       # %bb.0:
; CV1-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r0r1r2r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    fcompnwp.olt $r9 = $r7, $r3
; CV1-NEXT:    fcompnwp.olt $r11 = $r5, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    fcompnwp.olt $r8 = $r4, $r0
; CV1-NEXT:    fcompnwp.olt $r10 = $r6, $r2
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    cmovewp.even $r11 ? $r1 = $r5
; CV1-NEXT:    cmovewp.even $r9 ? $r3 = $r7
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    cmovewp.even $r8 ? $r0 = $r4
; CV1-NEXT:    cmovewp.even $r10 ? $r2 = $r6
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    fcompnwp.olt $r4 = $r2, $r0
; CV1-NEXT:    fcompnwp.olt $r5 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    cmovewp.even $r4 ? $r0 = $r2
; CV1-NEXT:    cmovewp.even $r5 ? $r1 = $r3
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    srld $r2 = $r1, 32
; CV1-NEXT:    fcompnwp.olt $r3 = $r0, $r1
; CV1-NEXT:    copyd $r4 = $r1
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    cmovewp.even $r3 ? $r1 = $r0
; CV1-NEXT:    insf $r4 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    fcompnwp.olt $r2 = $r4, $r0
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    cmovewp.even $r2 ? $r0 = $r4
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    fcompnwp.olt $r4 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    cmovewp.even $r4 ? $r0 = $r2
; CV1-NEXT:    fcompnwp.olt $r5 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    cmovewp.even $r5 ? $r1 = $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 18)
;
; CV2-LABEL: red_max_float16:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r0r1r2r3 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    fcompnwp.olt $r8 = $r4, $r0
; CV2-NEXT:    fcompnwp.olt $r9 = $r7, $r3
; CV2-NEXT:    fcompnwp.olt $r10 = $r6, $r2
; CV2-NEXT:    fcompnwp.olt $r11 = $r5, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    cmovewp.even $r8 ? $r0 = $r4
; CV2-NEXT:    cmovewp.even $r11 ? $r1 = $r5
; CV2-NEXT:    cmovewp.even $r10 ? $r2 = $r6
; CV2-NEXT:    cmovewp.even $r9 ? $r3 = $r7
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    fcompnwp.olt $r4 = $r2, $r0
; CV2-NEXT:    fcompnwp.olt $r5 = $r3, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    cmovewp.even $r4 ? $r0 = $r2
; CV2-NEXT:    cmovewp.even $r5 ? $r1 = $r3
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    srld $r2 = $r1, 32
; CV2-NEXT:    fcompnwp.olt $r3 = $r0, $r1
; CV2-NEXT:    copyd $r4 = $r1
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    cmovewp.even $r3 ? $r1 = $r0
; CV2-NEXT:    insf $r4 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    fcompnwp.olt $r2 = $r4, $r0
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    cmovewp.even $r2 ? $r0 = $r4
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    fcompnwp.olt $r4 = $r2, $r0
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    cmovewp.even $r4 ? $r0 = $r2
; CV2-NEXT:    fcompnwp.olt $r5 = $r3, $r1
; CV2-NEXT:    ;; # (end cycle 15)
; CV2-NEXT:    cmovewp.even $r5 ? $r1 = $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 16)
  %2 = load <16 x float>, ptr %0
  %3 = tail call fast float @llvm.vector.reduce.fmax.v16f32(<16 x float> %2)
  ret float %3
}

declare float @llvm.vector.reduce.fmax.v16f32(<16 x float>)

define double @red_max_double2(<2 x double> %0) {
; ALL-LABEL: red_max_double2:
; ALL:       # %bb.0:
; ALL-NEXT:    fcompd.olt $r2 = $r1, $r0
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    cmoved.wnez $r2 ? $r1 = $r0
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    copyd $r0 = $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 2)
  %2 = tail call fast double @llvm.vector.reduce.fmax.v2f64(<2 x double> %0)
  ret double %2
}

declare double @llvm.vector.reduce.fmax.v2f64(<2 x double>)

define double @red_max_double4(<4 x double> %0) {
; ALL-LABEL: red_max_double4:
; ALL:       # %bb.0:
; ALL-NEXT:    fcompd.olt $r4 = $r2, $r0
; ALL-NEXT:    fcompd.olt $r5 = $r3, $r1
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    cmoved.wnez $r4 ? $r2 = $r0
; ALL-NEXT:    cmoved.wnez $r5 ? $r3 = $r1
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    fcompd.olt $r0 = $r3, $r2
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    cmoved.wnez $r0 ? $r3 = $r2
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    copyd $r0 = $r3
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 4)
  %2 = tail call fast double @llvm.vector.reduce.fmax.v4f64(<4 x double> %0)
  ret double %2
}

declare double @llvm.vector.reduce.fmax.v4f64(<4 x double>)

define double @red_max_double8(ptr %0) {
; ALL-LABEL: red_max_double8:
; ALL:       # %bb.0:
; ALL-NEXT:    lo $r4r5r6r7 = 32[$r0]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    lo $r0r1r2r3 = 0[$r0]
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    fcompd.olt $r8 = $r5, $r1
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    fcompd.olt $r1 = $r7, $r3
; ALL-NEXT:    cmoved.wnez $r8 ? $r5 = $r1
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    fcompd.olt $r1 = $r4, $r0
; ALL-NEXT:    cmoved.wnez $r1 ? $r7 = $r3
; ALL-NEXT:    ;; # (end cycle 6)
; ALL-NEXT:    fcompd.olt $r0 = $r6, $r2
; ALL-NEXT:    cmoved.wnez $r1 ? $r4 = $r0
; ALL-NEXT:    ;; # (end cycle 7)
; ALL-NEXT:    cmoved.wnez $r0 ? $r6 = $r2
; ALL-NEXT:    ;; # (end cycle 8)
; ALL-NEXT:    fcompd.olt $r0 = $r6, $r4
; ALL-NEXT:    ;; # (end cycle 9)
; ALL-NEXT:    fcompd.olt $r0 = $r7, $r5
; ALL-NEXT:    cmoved.wnez $r0 ? $r6 = $r4
; ALL-NEXT:    ;; # (end cycle 10)
; ALL-NEXT:    cmoved.wnez $r0 ? $r7 = $r5
; ALL-NEXT:    ;; # (end cycle 11)
; ALL-NEXT:    fcompd.olt $r0 = $r7, $r6
; ALL-NEXT:    ;; # (end cycle 12)
; ALL-NEXT:    cmoved.wnez $r0 ? $r7 = $r6
; ALL-NEXT:    ;; # (end cycle 13)
; ALL-NEXT:    copyd $r0 = $r7
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 14)
  %2 = load <8 x double>, ptr %0
  %3 = tail call fast double @llvm.vector.reduce.fmax.v8f64(<8 x double> %2)
  ret double %3
}

declare double @llvm.vector.reduce.fmax.v8f64(<8 x double>)

define double @red_max_double16(ptr %0) {
; CV1-LABEL: red_max_double16:
; CV1:       # %bb.0:
; CV1-NEXT:    lo $r4r5r6r7 = 96[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r32r33r34r35 = 32[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    lo $r8r9r10r11 = 64[$r0]
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    lo $r0r1r2r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    fcompd.olt $r15 = $r6, $r34
; CV1-NEXT:    fcompd.olt $r16 = $r4, $r32
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    cmoved.wnez $r16 ? $r4 = $r32
; CV1-NEXT:    cmoved.wnez $r15 ? $r6 = $r34
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    fcompd.olt $r15 = $r10, $r2
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    fcompd.olt $r2 = $r8, $r0
; CV1-NEXT:    cmoved.wnez $r15 ? $r10 = $r2
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    fcompd.olt $r0 = $r11, $r3
; CV1-NEXT:    cmoved.wnez $r2 ? $r8 = $r0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    fcompd.olt $r2 = $r7, $r35
; CV1-NEXT:    cmoved.wnez $r0 ? $r11 = $r3
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    fcompd.olt $r0 = $r9, $r1
; CV1-NEXT:    cmoved.wnez $r2 ? $r7 = $r35
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    fcompd.olt $r2 = $r5, $r33
; CV1-NEXT:    cmoved.wnez $r0 ? $r9 = $r1
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    cmoved.wnez $r2 ? $r5 = $r33
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    fcompd.olt $r0 = $r5, $r9
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    fcompd.olt $r0 = $r7, $r11
; CV1-NEXT:    cmoved.wnez $r0 ? $r5 = $r9
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    fcompd.olt $r0 = $r4, $r8
; CV1-NEXT:    cmoved.wnez $r0 ? $r7 = $r11
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    fcompd.olt $r0 = $r6, $r10
; CV1-NEXT:    cmoved.wnez $r0 ? $r4 = $r8
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    cmoved.wnez $r0 ? $r6 = $r10
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    fcompd.olt $r0 = $r6, $r4
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    fcompd.olt $r0 = $r7, $r5
; CV1-NEXT:    cmoved.wnez $r0 ? $r6 = $r4
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    cmoved.wnez $r0 ? $r7 = $r5
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    fcompd.olt $r0 = $r7, $r6
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    cmoved.wnez $r0 ? $r7 = $r6
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    copyd $r0 = $r7
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 23)
;
; CV2-LABEL: red_max_double16:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r4r5r6r7 = 96[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r32r33r34r35 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    lo $r8r9r10r11 = 64[$r0]
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    lo $r0r1r2r3 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    fcompd.olt $r15 = $r6, $r34
; CV2-NEXT:    fcompd.olt $r16 = $r4, $r32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    cmoved.wnez $r16 ? $r4 = $r32
; CV2-NEXT:    cmoved.wnez $r15 ? $r6 = $r34
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    fcompd.olt $r15 = $r10, $r2
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    fcompd.olt $r2 = $r8, $r0
; CV2-NEXT:    cmoved.wnez $r15 ? $r10 = $r2
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    fcompd.olt $r0 = $r11, $r3
; CV2-NEXT:    fcompd.olt $r2 = $r7, $r35
; CV2-NEXT:    cmoved.wnez $r2 ? $r8 = $r0
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    fcompd.olt $r0 = $r9, $r1
; CV2-NEXT:    fcompd.olt $r2 = $r5, $r33
; CV2-NEXT:    cmoved.wnez $r2 ? $r7 = $r35
; CV2-NEXT:    cmoved.wnez $r0 ? $r11 = $r3
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    cmoved.wnez $r2 ? $r5 = $r33
; CV2-NEXT:    cmoved.wnez $r0 ? $r9 = $r1
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    fcompd.olt $r0 = $r5, $r9
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    fcompd.olt $r0 = $r7, $r11
; CV2-NEXT:    cmoved.wnez $r0 ? $r5 = $r9
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    fcompd.olt $r0 = $r4, $r8
; CV2-NEXT:    cmoved.wnez $r0 ? $r7 = $r11
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    fcompd.olt $r0 = $r6, $r10
; CV2-NEXT:    cmoved.wnez $r0 ? $r4 = $r8
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    cmoved.wnez $r0 ? $r6 = $r10
; CV2-NEXT:    ;; # (end cycle 15)
; CV2-NEXT:    fcompd.olt $r0 = $r6, $r4
; CV2-NEXT:    ;; # (end cycle 16)
; CV2-NEXT:    fcompd.olt $r0 = $r7, $r5
; CV2-NEXT:    cmoved.wnez $r0 ? $r6 = $r4
; CV2-NEXT:    ;; # (end cycle 17)
; CV2-NEXT:    cmoved.wnez $r0 ? $r7 = $r5
; CV2-NEXT:    ;; # (end cycle 18)
; CV2-NEXT:    fcompd.olt $r0 = $r7, $r6
; CV2-NEXT:    ;; # (end cycle 19)
; CV2-NEXT:    cmoved.wnez $r0 ? $r7 = $r6
; CV2-NEXT:    ;; # (end cycle 20)
; CV2-NEXT:    copyd $r0 = $r7
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 21)
  %2 = load <16 x double>, ptr %0
  %3 = tail call fast double @llvm.vector.reduce.fmax.v16f64(<16 x double> %2)
  ret double %3
}

declare double @llvm.vector.reduce.fmax.v16f64(<16 x double>)

define half @red_max_half2(<2 x half> %0) {
; ALL-LABEL: red_max_half2:
; ALL:       # %bb.0:
; ALL-NEXT:    srlw $r1 = $r0, 16
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    insf $r1 = $r0, 31, 16
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    fmaxhq $r0 = $r0, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 2)
  %2 = tail call fast half @llvm.vector.reduce.fmax.v2f16(<2 x half> %0)
  ret half %2
}

declare half @llvm.vector.reduce.fmax.v2f16(<2 x half>)

define half @red_max_half4(<4 x half> %0) {
; ALL-LABEL: red_max_half4:
; ALL:       # %bb.0:
; ALL-NEXT:    copyd $r1 = $r0
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    insf $r1 = $r0, 15, 0
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    fmaxhq $r0 = $r0, $r1
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    srlw $r1 = $r0, 16
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    zxhd $r1 = $r1
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    fmaxhq $r0 = $r0, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 6)
  %2 = tail call fast half @llvm.vector.reduce.fmax.v4f16(<4 x half> %0)
  ret half %2
}

declare half @llvm.vector.reduce.fmax.v4f16(<4 x half>)

define half @red_max_half8(<8 x half> %0) {
; ALL-LABEL: red_max_half8:
; ALL:       # %bb.0:
; ALL-NEXT:    fcompnhq.olt $r2 = $r1, $r0
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    cmovehq.even $r2 ? $r0 = $r1
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    copyd $r1 = $r0
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    insf $r1 = $r0, 15, 0
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    fcompnhq.olt $r2 = $r1, $r0
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    cmovehq.even $r2 ? $r0 = $r1
; ALL-NEXT:    ;; # (end cycle 6)
; ALL-NEXT:    srlw $r1 = $r0, 16
; ALL-NEXT:    ;; # (end cycle 7)
; ALL-NEXT:    zxhd $r1 = $r1
; ALL-NEXT:    ;; # (end cycle 8)
; ALL-NEXT:    fcompnhq.olt $r2 = $r1, $r0
; ALL-NEXT:    ;; # (end cycle 9)
; ALL-NEXT:    cmovehq.even $r2 ? $r0 = $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 10)
  %2 = tail call fast half @llvm.vector.reduce.fmax.v8f16(<8 x half> %0)
  ret half %2
}

declare half @llvm.vector.reduce.fmax.v8f16(<8 x half>)

define half @red_max_half16(<16 x half> %0) {
; ALL-LABEL: red_max_half16:
; ALL:       # %bb.0:
; ALL-NEXT:    fcompnhq.olt $r4 = $r3, $r1
; ALL-NEXT:    fcompnhq.olt $r5 = $r2, $r0
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    cmovehq.even $r5 ? $r0 = $r2
; ALL-NEXT:    cmovehq.even $r4 ? $r1 = $r3
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    fcompnhq.olt $r2 = $r1, $r0
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    cmovehq.even $r2 ? $r0 = $r1
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    copyd $r1 = $r0
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    insf $r1 = $r0, 15, 0
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; ALL-NEXT:    ;; # (end cycle 6)
; ALL-NEXT:    fcompnhq.olt $r2 = $r1, $r0
; ALL-NEXT:    ;; # (end cycle 7)
; ALL-NEXT:    cmovehq.even $r2 ? $r0 = $r1
; ALL-NEXT:    ;; # (end cycle 8)
; ALL-NEXT:    srlw $r1 = $r0, 16
; ALL-NEXT:    ;; # (end cycle 9)
; ALL-NEXT:    zxhd $r1 = $r1
; ALL-NEXT:    ;; # (end cycle 10)
; ALL-NEXT:    fcompnhq.olt $r2 = $r1, $r0
; ALL-NEXT:    ;; # (end cycle 11)
; ALL-NEXT:    cmovehq.even $r2 ? $r0 = $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 12)
  %2 = tail call fast half @llvm.vector.reduce.fmax.v16f16(<16 x half> %0)
  ret half %2
}

declare half @llvm.vector.reduce.fmax.v16f16(<16 x half>)

define float @red_min_float2(<2 x float> %0) {
; ALL-LABEL: red_min_float2:
; ALL:       # %bb.0:
; ALL-NEXT:    srad $r1 = $r0, 32
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    insf $r1 = $r0, 63, 32
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    fminwp $r0 = $r0, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 2)
  %2 = tail call fast float @llvm.vector.reduce.fmin.v2f32(<2 x float> %0)
  ret float %2
}

declare float @llvm.vector.reduce.fmin.v2f32(<2 x float>)

define float @red_min_float4(<4 x float> %0) {
; ALL-LABEL: red_min_float4:
; ALL:       # %bb.0:
; ALL-NEXT:    copyd $r2 = $r1
; ALL-NEXT:    fcompnwp.olt $r3 = $r1, $r0
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    fcompnwp.olt $r4 = $r0, $r2
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    cmovewp.even $r4 ? $r0 = $r2
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    cmovewp.even $r3 ? $r1 = $r0
; ALL-NEXT:    srld $r2 = $r0, 32
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    insf $r2 = $r2, 63, 32
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    copyd $r3 = $r2
; ALL-NEXT:    fcompnwp.olt $r4 = $r0, $r2
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    cmovewp.even $r4 ? $r0 = $r2
; ALL-NEXT:    fcompnwp.olt $r5 = $r1, $r3
; ALL-NEXT:    ;; # (end cycle 6)
; ALL-NEXT:    cmovewp.even $r5 ? $r1 = $r3
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 7)
  %2 = tail call fast float @llvm.vector.reduce.fmin.v4f32(<4 x float> %0)
  ret float %2
}

declare float @llvm.vector.reduce.fmin.v4f32(<4 x float>)

define float @red_min_float8(<8 x float> %0) {
; ALL-LABEL: red_min_float8:
; ALL:       # %bb.0:
; ALL-NEXT:    fcompnwp.olt $r4 = $r0, $r2
; ALL-NEXT:    fcompnwp.olt $r5 = $r1, $r3
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    cmovewp.even $r4 ? $r0 = $r2
; ALL-NEXT:    cmovewp.even $r5 ? $r1 = $r3
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    srld $r2 = $r1, 32
; ALL-NEXT:    copyd $r4 = $r1
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    insf $r4 = $r2, 63, 32
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    fcompnwp.olt $r2 = $r0, $r4
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    cmovewp.even $r2 ? $r0 = $r4
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    srld $r2 = $r0, 32
; ALL-NEXT:    fcompnwp.olt $r3 = $r1, $r0
; ALL-NEXT:    ;; # (end cycle 6)
; ALL-NEXT:    cmovewp.even $r3 ? $r1 = $r0
; ALL-NEXT:    insf $r2 = $r2, 63, 32
; ALL-NEXT:    ;; # (end cycle 7)
; ALL-NEXT:    copyd $r3 = $r2
; ALL-NEXT:    fcompnwp.olt $r4 = $r0, $r2
; ALL-NEXT:    ;; # (end cycle 8)
; ALL-NEXT:    cmovewp.even $r4 ? $r0 = $r2
; ALL-NEXT:    fcompnwp.olt $r5 = $r1, $r3
; ALL-NEXT:    ;; # (end cycle 9)
; ALL-NEXT:    cmovewp.even $r5 ? $r1 = $r3
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 10)
  %2 = tail call fast float @llvm.vector.reduce.fmin.v8f32(<8 x float> %0)
  ret float %2
}

declare float @llvm.vector.reduce.fmin.v8f32(<8 x float>)

define float @red_min_float16(ptr %0) {
; CV1-LABEL: red_min_float16:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r4 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r0r1r2r3 = 0[$r4]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    lo $r4r5r6r7 = 32[$r4]
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    fcompnwp.olt $r9 = $r1, $r5
; CV1-NEXT:    fcompnwp.olt $r11 = $r3, $r7
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    fcompnwp.olt $r8 = $r2, $r6
; CV1-NEXT:    fcompnwp.olt $r10 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    cmovewp.even $r9 ? $r1 = $r5
; CV1-NEXT:    cmovewp.even $r11 ? $r3 = $r7
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    cmovewp.even $r10 ? $r0 = $r4
; CV1-NEXT:    cmovewp.even $r8 ? $r2 = $r6
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    fcompnwp.olt $r4 = $r0, $r2
; CV1-NEXT:    fcompnwp.olt $r5 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    cmovewp.even $r4 ? $r0 = $r2
; CV1-NEXT:    cmovewp.even $r5 ? $r1 = $r3
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    srld $r2 = $r1, 32
; CV1-NEXT:    fcompnwp.olt $r3 = $r1, $r0
; CV1-NEXT:    copyd $r4 = $r1
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    cmovewp.even $r3 ? $r1 = $r0
; CV1-NEXT:    insf $r4 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    fcompnwp.olt $r2 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    cmovewp.even $r2 ? $r0 = $r4
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    fcompnwp.olt $r4 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    cmovewp.even $r4 ? $r0 = $r2
; CV1-NEXT:    fcompnwp.olt $r5 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    cmovewp.even $r5 ? $r1 = $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 19)
;
; CV2-LABEL: red_min_float16:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r4 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r0r1r2r3 = 0[$r4]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    lo $r4r5r6r7 = 32[$r4]
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    fcompnwp.olt $r8 = $r2, $r6
; CV2-NEXT:    fcompnwp.olt $r9 = $r1, $r5
; CV2-NEXT:    fcompnwp.olt $r10 = $r0, $r4
; CV2-NEXT:    fcompnwp.olt $r11 = $r3, $r7
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    cmovewp.even $r10 ? $r0 = $r4
; CV2-NEXT:    cmovewp.even $r9 ? $r1 = $r5
; CV2-NEXT:    cmovewp.even $r8 ? $r2 = $r6
; CV2-NEXT:    cmovewp.even $r11 ? $r3 = $r7
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    fcompnwp.olt $r4 = $r0, $r2
; CV2-NEXT:    fcompnwp.olt $r5 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    cmovewp.even $r4 ? $r0 = $r2
; CV2-NEXT:    cmovewp.even $r5 ? $r1 = $r3
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    srld $r2 = $r1, 32
; CV2-NEXT:    fcompnwp.olt $r3 = $r1, $r0
; CV2-NEXT:    copyd $r4 = $r1
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    cmovewp.even $r3 ? $r1 = $r0
; CV2-NEXT:    insf $r4 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    fcompnwp.olt $r2 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    cmovewp.even $r2 ? $r0 = $r4
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    fcompnwp.olt $r4 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 15)
; CV2-NEXT:    cmovewp.even $r4 ? $r0 = $r2
; CV2-NEXT:    fcompnwp.olt $r5 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 16)
; CV2-NEXT:    cmovewp.even $r5 ? $r1 = $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 17)
  %2 = load <16 x float>, ptr %0
  %3 = tail call fast float @llvm.vector.reduce.fmin.v16f32(<16 x float> %2)
  ret float %3
}

declare float @llvm.vector.reduce.fmin.v16f32(<16 x float>)

define double @red_min_double2(<2 x double> %0) {
; ALL-LABEL: red_min_double2:
; ALL:       # %bb.0:
; ALL-NEXT:    fcompd.olt $r2 = $r0, $r1
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    cmoved.wnez $r2 ? $r1 = $r0
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    copyd $r0 = $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 2)
  %2 = tail call fast double @llvm.vector.reduce.fmin.v2f64(<2 x double> %0)
  ret double %2
}

declare double @llvm.vector.reduce.fmin.v2f64(<2 x double>)

define double @red_min_double4(<4 x double> %0) {
; ALL-LABEL: red_min_double4:
; ALL:       # %bb.0:
; ALL-NEXT:    fcompd.olt $r4 = $r1, $r3
; ALL-NEXT:    fcompd.olt $r5 = $r0, $r2
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    cmoved.wnez $r5 ? $r2 = $r0
; ALL-NEXT:    cmoved.wnez $r4 ? $r3 = $r1
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    fcompd.olt $r0 = $r2, $r3
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    cmoved.wnez $r0 ? $r3 = $r2
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    copyd $r0 = $r3
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 4)
  %2 = tail call fast double @llvm.vector.reduce.fmin.v4f64(<4 x double> %0)
  ret double %2
}

declare double @llvm.vector.reduce.fmin.v4f64(<4 x double>)

define double @red_min_double8(ptr %0) {
; ALL-LABEL: red_min_double8:
; ALL:       # %bb.0:
; ALL-NEXT:    lo $r4r5r6r7 = 0[$r0]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    lo $r0r1r2r3 = 32[$r0]
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    fcompd.olt $r8 = $r6, $r2
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    cmoved.wnez $r8 ? $r2 = $r6
; ALL-NEXT:    fcompd.olt $r6 = $r4, $r0
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    cmoved.wnez $r6 ? $r0 = $r4
; ALL-NEXT:    fcompd.olt $r4 = $r7, $r3
; ALL-NEXT:    ;; # (end cycle 6)
; ALL-NEXT:    cmoved.wnez $r4 ? $r3 = $r7
; ALL-NEXT:    fcompd.olt $r4 = $r5, $r1
; ALL-NEXT:    ;; # (end cycle 7)
; ALL-NEXT:    cmoved.wnez $r4 ? $r1 = $r5
; ALL-NEXT:    ;; # (end cycle 8)
; ALL-NEXT:    fcompd.olt $r4 = $r1, $r3
; ALL-NEXT:    ;; # (end cycle 9)
; ALL-NEXT:    fcompd.olt $r1 = $r0, $r2
; ALL-NEXT:    cmoved.wnez $r4 ? $r3 = $r1
; ALL-NEXT:    ;; # (end cycle 10)
; ALL-NEXT:    cmoved.wnez $r1 ? $r2 = $r0
; ALL-NEXT:    ;; # (end cycle 11)
; ALL-NEXT:    fcompd.olt $r0 = $r2, $r3
; ALL-NEXT:    ;; # (end cycle 12)
; ALL-NEXT:    cmoved.wnez $r0 ? $r3 = $r2
; ALL-NEXT:    ;; # (end cycle 13)
; ALL-NEXT:    copyd $r0 = $r3
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 14)
  %2 = load <8 x double>, ptr %0
  %3 = tail call fast double @llvm.vector.reduce.fmin.v8f64(<8 x double> %2)
  ret double %3
}

declare double @llvm.vector.reduce.fmin.v8f64(<8 x double>)

define double @red_min_double16(ptr %0) {
; CV1-LABEL: red_min_double16:
; CV1:       # %bb.0:
; CV1-NEXT:    lo $r8r9r10r11 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r4r5r6r7 = 64[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    lo $r32r33r34r35 = 32[$r0]
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    lo $r0r1r2r3 = 96[$r0]
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    fcompd.olt $r15 = $r9, $r5
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    cmoved.wnez $r15 ? $r5 = $r9
; CV1-NEXT:    fcompd.olt $r15 = $r11, $r7
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    cmoved.wnez $r15 ? $r7 = $r11
; CV1-NEXT:    fcompd.olt $r9 = $r33, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    cmoved.wnez $r9 ? $r1 = $r33
; CV1-NEXT:    fcompd.olt $r9 = $r35, $r3
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    cmoved.wnez $r9 ? $r3 = $r35
; CV1-NEXT:    fcompd.olt $r9 = $r32, $r0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    cmoved.wnez $r9 ? $r0 = $r32
; CV1-NEXT:    fcompd.olt $r11 = $r8, $r4
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    cmoved.wnez $r11 ? $r4 = $r8
; CV1-NEXT:    fcompd.olt $r8 = $r34, $r2
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    cmoved.wnez $r8 ? $r2 = $r34
; CV1-NEXT:    fcompd.olt $r9 = $r10, $r6
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    cmoved.wnez $r9 ? $r6 = $r10
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    fcompd.olt $r8 = $r6, $r2
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    cmoved.wnez $r8 ? $r2 = $r6
; CV1-NEXT:    fcompd.olt $r6 = $r4, $r0
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    cmoved.wnez $r6 ? $r0 = $r4
; CV1-NEXT:    fcompd.olt $r4 = $r7, $r3
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    cmoved.wnez $r4 ? $r3 = $r7
; CV1-NEXT:    fcompd.olt $r4 = $r5, $r1
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    cmoved.wnez $r4 ? $r1 = $r5
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    fcompd.olt $r4 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    fcompd.olt $r1 = $r0, $r2
; CV1-NEXT:    cmoved.wnez $r4 ? $r3 = $r1
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    cmoved.wnez $r1 ? $r2 = $r0
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    fcompd.olt $r0 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    cmoved.wnez $r0 ? $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    copyd $r0 = $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 23)
;
; CV2-LABEL: red_min_double16:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r8r9r10r11 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r4r5r6r7 = 64[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    lo $r32r33r34r35 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    lo $r0r1r2r3 = 96[$r0]
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    fcompd.olt $r15 = $r9, $r5
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    cmoved.wnez $r15 ? $r5 = $r9
; CV2-NEXT:    fcompd.olt $r15 = $r11, $r7
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    cmoved.wnez $r15 ? $r7 = $r11
; CV2-NEXT:    fcompd.olt $r9 = $r33, $r1
; CV2-NEXT:    fcompd.olt $r11 = $r8, $r4
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    cmoved.wnez $r9 ? $r1 = $r33
; CV2-NEXT:    cmoved.wnez $r11 ? $r4 = $r8
; CV2-NEXT:    fcompd.olt $r8 = $r34, $r2
; CV2-NEXT:    fcompd.olt $r9 = $r35, $r3
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    cmoved.wnez $r8 ? $r2 = $r34
; CV2-NEXT:    cmoved.wnez $r9 ? $r3 = $r35
; CV2-NEXT:    fcompd.olt $r9 = $r32, $r0
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    cmoved.wnez $r9 ? $r0 = $r32
; CV2-NEXT:    fcompd.olt $r9 = $r10, $r6
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    cmoved.wnez $r9 ? $r6 = $r10
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    fcompd.olt $r8 = $r6, $r2
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    cmoved.wnez $r8 ? $r2 = $r6
; CV2-NEXT:    fcompd.olt $r6 = $r4, $r0
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    cmoved.wnez $r6 ? $r0 = $r4
; CV2-NEXT:    fcompd.olt $r4 = $r7, $r3
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    cmoved.wnez $r4 ? $r3 = $r7
; CV2-NEXT:    fcompd.olt $r4 = $r5, $r1
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    cmoved.wnez $r4 ? $r1 = $r5
; CV2-NEXT:    ;; # (end cycle 15)
; CV2-NEXT:    fcompd.olt $r4 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 16)
; CV2-NEXT:    fcompd.olt $r1 = $r0, $r2
; CV2-NEXT:    cmoved.wnez $r4 ? $r3 = $r1
; CV2-NEXT:    ;; # (end cycle 17)
; CV2-NEXT:    cmoved.wnez $r1 ? $r2 = $r0
; CV2-NEXT:    ;; # (end cycle 18)
; CV2-NEXT:    fcompd.olt $r0 = $r2, $r3
; CV2-NEXT:    ;; # (end cycle 19)
; CV2-NEXT:    cmoved.wnez $r0 ? $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 20)
; CV2-NEXT:    copyd $r0 = $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 21)
  %2 = load <16 x double>, ptr %0
  %3 = tail call fast double @llvm.vector.reduce.fmin.v16f64(<16 x double> %2)
  ret double %3
}

declare double @llvm.vector.reduce.fmin.v16f64(<16 x double>)

define half @red_min_half2(<2 x half> %0) {
; ALL-LABEL: red_min_half2:
; ALL:       # %bb.0:
; ALL-NEXT:    srlw $r1 = $r0, 16
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    insf $r1 = $r0, 31, 16
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    fminhq $r0 = $r0, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 2)
  %2 = tail call fast half @llvm.vector.reduce.fmin.v2f16(<2 x half> %0)
  ret half %2
}

declare half @llvm.vector.reduce.fmin.v2f16(<2 x half>)

define half @red_min_half4(<4 x half> %0) {
; ALL-LABEL: red_min_half4:
; ALL:       # %bb.0:
; ALL-NEXT:    copyd $r1 = $r0
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    insf $r1 = $r0, 15, 0
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    fminhq $r0 = $r0, $r1
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    srlw $r1 = $r0, 16
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    zxhd $r1 = $r1
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    fminhq $r0 = $r0, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 6)
  %2 = tail call fast half @llvm.vector.reduce.fmin.v4f16(<4 x half> %0)
  ret half %2
}

declare half @llvm.vector.reduce.fmin.v4f16(<4 x half>)

define half @red_min_half8(<8 x half> %0) {
; ALL-LABEL: red_min_half8:
; ALL:       # %bb.0:
; ALL-NEXT:    fcompnhq.olt $r2 = $r0, $r1
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    cmovehq.even $r2 ? $r0 = $r1
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    copyd $r1 = $r0
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    insf $r1 = $r0, 15, 0
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    fcompnhq.olt $r2 = $r0, $r1
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    cmovehq.even $r2 ? $r0 = $r1
; ALL-NEXT:    ;; # (end cycle 6)
; ALL-NEXT:    srlw $r1 = $r0, 16
; ALL-NEXT:    ;; # (end cycle 7)
; ALL-NEXT:    zxhd $r1 = $r1
; ALL-NEXT:    ;; # (end cycle 8)
; ALL-NEXT:    fcompnhq.olt $r2 = $r0, $r1
; ALL-NEXT:    ;; # (end cycle 9)
; ALL-NEXT:    cmovehq.even $r2 ? $r0 = $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 10)
  %2 = tail call fast half @llvm.vector.reduce.fmin.v8f16(<8 x half> %0)
  ret half %2
}

declare half @llvm.vector.reduce.fmin.v8f16(<8 x half>)

define half @red_min_half16(<16 x half> %0) {
; ALL-LABEL: red_min_half16:
; ALL:       # %bb.0:
; ALL-NEXT:    fcompnhq.olt $r4 = $r0, $r2
; ALL-NEXT:    fcompnhq.olt $r5 = $r1, $r3
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    cmovehq.even $r4 ? $r0 = $r2
; ALL-NEXT:    cmovehq.even $r5 ? $r1 = $r3
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    fcompnhq.olt $r2 = $r0, $r1
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    cmovehq.even $r2 ? $r0 = $r1
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    copyd $r1 = $r0
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    insf $r1 = $r0, 15, 0
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; ALL-NEXT:    ;; # (end cycle 6)
; ALL-NEXT:    fcompnhq.olt $r2 = $r0, $r1
; ALL-NEXT:    ;; # (end cycle 7)
; ALL-NEXT:    cmovehq.even $r2 ? $r0 = $r1
; ALL-NEXT:    ;; # (end cycle 8)
; ALL-NEXT:    srlw $r1 = $r0, 16
; ALL-NEXT:    ;; # (end cycle 9)
; ALL-NEXT:    zxhd $r1 = $r1
; ALL-NEXT:    ;; # (end cycle 10)
; ALL-NEXT:    fcompnhq.olt $r2 = $r0, $r1
; ALL-NEXT:    ;; # (end cycle 11)
; ALL-NEXT:    cmovehq.even $r2 ? $r0 = $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 12)
  %2 = tail call fast half @llvm.vector.reduce.fmin.v16f16(<16 x half> %0)
  ret half %2
}

declare half @llvm.vector.reduce.fmin.v16f16(<16 x half>)

