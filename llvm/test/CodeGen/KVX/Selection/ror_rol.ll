; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -o - %s -O2 | FileCheck %s --check-prefixes=CHECK,CV1
; RUN: llc -mcpu=kv3-2 -o - %s -O2 | FileCheck %s --check-prefixes=CHECK,CV2
; RUN: clang -O2 -c -o /dev/null %s
; RUN: clang -O2 -march=kv3-2 -c -o /dev/null %s

target triple = "kvx-kalray-cos"

define i32 @rol_i32_ri(i32 %in) {
; CHECK-LABEL: rol_i32_ri:
; CHECK:       # %bb.0:
; CHECK-NEXT:    rolw $r0 = $r0, 9
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
    %left = shl i32 %in, 9
    %right = lshr i32 %in, 23
    %val5 = or i32 %left, %right
    ret i32 %val5
}

; ror and rol are semantically the same thing.
; For the immediates, llvm decides to use rol
define i32 @ror_i32_ri(i32 %in) {
; CHECK-LABEL: ror_i32_ri:
; CHECK:       # %bb.0:
; CHECK-NEXT:    rolw $r0 = $r0, 23
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
    %left = lshr i32 %in, 9
    %right = shl i32 %in, 23
    %val5 = or i32 %right, %left
    ret i32 %val5
}

define i32 @ror_i32_rr(i32 %in, i32 %r) {
; CHECK-LABEL: ror_i32_rr:
; CHECK:       # %bb.0:
; CHECK-NEXT:    rorw $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
    %l  = sub i32 32, %r
    %left = shl i32 %in, %l
    %right = lshr i32 %in, %r
    %ror = or i32 %left, %right
    ret i32 %ror
}

define i32 @rol_i32_rr(i32 %in, i32 %l) {
; CHECK-LABEL: rol_i32_rr:
; CHECK:       # %bb.0:
; CHECK-NEXT:    rolw $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
    %r  = sub i32 32, %l
    %left = shl i32 %in, %l
    %right = lshr i32 %in, %r
    %rol = or i32 %left, %right
    ret i32 %rol
}

declare <2 x i32> @llvm.fshl.v2i32(<2 x i32>, <2 x i32>, <2 x i32>)
define <2 x i32> @test_fshl_v2i32(<2 x i32> %a, <2 x i32> %b) {
; CHECK-LABEL: test_fshl_v2i32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    rolw $r1 = $r0, $r1
; CHECK-NEXT:    extfz $r2 = $r1, 36, 32
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    rolwps $r0 = $r0, $r2
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    insf $r0 = $r1, 31, 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 2)
entry:
  %or = tail call <2 x i32> @llvm.fshl.v2i32(<2 x i32> %a, <2 x i32> %a, <2 x i32> %b)
  ret <2 x i32> %or
}


declare <2 x i16> @llvm.fshl.v2i16(<2 x i16>, <2 x i16>, <2 x i16>)
define <2 x i16> @test_fshl_v2i16(<2 x i16> %a, <2 x i16> %b) {
; CV1-LABEL: test_fshl_v2i16:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    neghq $r1 = $r1
; CV1-NEXT:    andw $r2 = $r1, 0xf000f
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andw $r1 = $r1, 0xf000f
; CV1-NEXT:    extfz $r2 = $r2, 19, 16
; CV1-NEXT:    sllhqs $r3 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sllhqs $r2 = $r0, $r2
; CV1-NEXT:    extfz $r4 = $r1, 19, 16
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    srlhqs $r0 = $r0, $r4
; CV1-NEXT:    srlhqs $r1 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    insf $r0 = $r1, 15, 0
; CV1-NEXT:    insf $r2 = $r3, 15, 0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    orw $r0 = $r2, $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 5)
;
; CV2-LABEL: test_fshl_v2i16:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    neghq $r1 = $r1
; CV2-NEXT:    andw $r2 = $r1, 0xf000f
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    andw $r1 = $r1, 0xf000f
; CV2-NEXT:    extfz $r2 = $r2, 19, 16
; CV2-NEXT:    sllhqs $r3 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    srlhqs $r1 = $r0, $r1
; CV2-NEXT:    sllhqs $r2 = $r0, $r2
; CV2-NEXT:    extfz $r4 = $r1, 19, 16
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    srlhqs $r0 = $r0, $r4
; CV2-NEXT:    insf $r2 = $r3, 15, 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    insf $r0 = $r1, 15, 0
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    orw $r0 = $r2, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 5)
entry:
  %or = tail call <2 x i16> @llvm.fshl.v2i16(<2 x i16> %a, <2 x i16> %a, <2 x i16> %b)
  ret <2 x i16> %or
}

declare <3 x i16> @llvm.fshl.v3i16(<3 x i16>, <3 x i16>, <3 x i16>)
define <3 x i16> @test_fshl_v3i16(<3 x i16> %a, <3 x i16> %b) {
; CV1-LABEL: test_fshl_v3i16:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    neghq $r1 = $r1
; CV1-NEXT:    andd $r2 = $r1, 0xf000f.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd $r1 = $r1, 0xf000f.@
; CV1-NEXT:    sllhqs $r3 = $r0, $r2
; CV1-NEXT:    extfz $r4 = $r2, 19, 16
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sllhqs $r4 = $r0, $r4
; CV1-NEXT:    extfz $r5 = $r2, 35, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sllhqs $r3 = $r0, $r5
; CV1-NEXT:    insf $r4 = $r3, 15, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    extfz $r5 = $r1, 19, 16
; CV1-NEXT:    srlhqs $r6 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    srlhqs $r5 = $r0, $r5
; CV1-NEXT:    extfz $r7 = $r1, 35, 32
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    insf $r5 = $r6, 15, 0
; CV1-NEXT:    srlhqs $r6 = $r0, $r7
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    extfz $r1 = $r1, 51, 48
; CV1-NEXT:    extfz $r2 = $r2, 51, 48
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r3 = $r4, 31, 0
; CV1-NEXT:    insf $r6 = $r5, 31, 0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    srlhqs $r0 = $r0, $r1
; CV1-NEXT:    sllhqs $r2 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r0 = $r6, 47, 0
; CV1-NEXT:    insf $r2 = $r3, 47, 0
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    ord $r0 = $r2, $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 11)
;
; CV2-LABEL: test_fshl_v3i16:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    neghq $r1 = $r1
; CV2-NEXT:    andd $r2 = $r1, 0xf000f.@
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    andd $r1 = $r1, 0xf000f.@
; CV2-NEXT:    sllhqs $r3 = $r0, $r2
; CV2-NEXT:    extfz $r4 = $r2, 19, 16
; CV2-NEXT:    extfz $r5 = $r2, 35, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sllhqs $r4 = $r0, $r4
; CV2-NEXT:    sllhqs $r5 = $r0, $r5
; CV2-NEXT:    srlhqs $r6 = $r0, $r1
; CV2-NEXT:    extfz $r7 = $r1, 19, 16
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    extfz $r2 = $r2, 51, 48
; CV2-NEXT:    srlhqs $r3 = $r0, $r7
; CV2-NEXT:    insf $r4 = $r3, 15, 0
; CV2-NEXT:    extfz $r7 = $r1, 35, 32
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    extfz $r1 = $r1, 51, 48
; CV2-NEXT:    insf $r3 = $r6, 15, 0
; CV2-NEXT:    insf $r5 = $r4, 31, 0
; CV2-NEXT:    srlhqs $r6 = $r0, $r7
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    srlhqs $r0 = $r0, $r1
; CV2-NEXT:    sllhqs $r2 = $r0, $r2
; CV2-NEXT:    insf $r6 = $r3, 31, 0
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r0 = $r6, 47, 0
; CV2-NEXT:    insf $r2 = $r5, 47, 0
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    ord $r0 = $r2, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
entry:
  %or = tail call <3 x i16> @llvm.fshl.v3i16(<3 x i16> %a, <3 x i16> %a, <3 x i16> %b)
  ret <3 x i16> %or
}

declare <4 x i16> @llvm.fshl.v4i16(<4 x i16>, <4 x i16>, <4 x i16>)
define <4 x i16> @test_fshl_v4i16(<4 x i16> %a, <4 x i16> %b) {
; CV1-LABEL: test_fshl_v4i16:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    neghq $r1 = $r1
; CV1-NEXT:    andd $r2 = $r1, 0xf000f.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd $r1 = $r1, 0xf000f.@
; CV1-NEXT:    sllhqs $r3 = $r0, $r2
; CV1-NEXT:    extfz $r4 = $r2, 19, 16
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sllhqs $r4 = $r0, $r4
; CV1-NEXT:    extfz $r5 = $r2, 35, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sllhqs $r3 = $r0, $r5
; CV1-NEXT:    insf $r4 = $r3, 15, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    extfz $r5 = $r1, 19, 16
; CV1-NEXT:    srlhqs $r6 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    srlhqs $r5 = $r0, $r5
; CV1-NEXT:    extfz $r7 = $r1, 35, 32
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    insf $r5 = $r6, 15, 0
; CV1-NEXT:    srlhqs $r6 = $r0, $r7
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    extfz $r1 = $r1, 51, 48
; CV1-NEXT:    extfz $r2 = $r2, 51, 48
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r3 = $r4, 31, 0
; CV1-NEXT:    insf $r6 = $r5, 31, 0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    srlhqs $r0 = $r0, $r1
; CV1-NEXT:    sllhqs $r2 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r0 = $r6, 47, 0
; CV1-NEXT:    insf $r2 = $r3, 47, 0
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    ord $r0 = $r2, $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 11)
;
; CV2-LABEL: test_fshl_v4i16:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    neghq $r1 = $r1
; CV2-NEXT:    andd $r2 = $r1, 0xf000f.@
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    andd $r1 = $r1, 0xf000f.@
; CV2-NEXT:    sllhqs $r3 = $r0, $r2
; CV2-NEXT:    extfz $r4 = $r2, 19, 16
; CV2-NEXT:    extfz $r5 = $r2, 35, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sllhqs $r4 = $r0, $r4
; CV2-NEXT:    sllhqs $r5 = $r0, $r5
; CV2-NEXT:    srlhqs $r6 = $r0, $r1
; CV2-NEXT:    extfz $r7 = $r1, 19, 16
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    extfz $r2 = $r2, 51, 48
; CV2-NEXT:    srlhqs $r3 = $r0, $r7
; CV2-NEXT:    insf $r4 = $r3, 15, 0
; CV2-NEXT:    extfz $r7 = $r1, 35, 32
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    extfz $r1 = $r1, 51, 48
; CV2-NEXT:    insf $r3 = $r6, 15, 0
; CV2-NEXT:    insf $r5 = $r4, 31, 0
; CV2-NEXT:    srlhqs $r6 = $r0, $r7
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    srlhqs $r0 = $r0, $r1
; CV2-NEXT:    sllhqs $r2 = $r0, $r2
; CV2-NEXT:    insf $r6 = $r3, 31, 0
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r0 = $r6, 47, 0
; CV2-NEXT:    insf $r2 = $r5, 47, 0
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    ord $r0 = $r2, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
entry:
  %or = tail call <4 x i16> @llvm.fshl.v4i16(<4 x i16> %a, <4 x i16> %a, <4 x i16> %b)
  ret <4 x i16> %or
}

declare <8 x i16> @llvm.fshl.v8i16(<8 x i16>, <8 x i16>, <8 x i16>)
define <8 x i16> @test_fshl_v8i16(<8 x i16> %a, <8 x i16> %b) {
; CV1-LABEL: test_fshl_v8i16:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    neghq $r2 = $r2
; CV1-NEXT:    neghq $r3 = $r3
; CV1-NEXT:    andd $r4 = $r2, 0xf000f.@
; CV1-NEXT:    andd $r5 = $r3, 0xf000f.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd $r2 = $r2, 0xf000f.@
; CV1-NEXT:    andd $r3 = $r3, 0xf000f.@
; CV1-NEXT:    sllhqs $r6 = $r0, $r4
; CV1-NEXT:    extfz $r7 = $r4, 19, 16
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sllhqs $r7 = $r0, $r7
; CV1-NEXT:    extfz $r8 = $r4, 35, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sllhqs $r6 = $r0, $r8
; CV1-NEXT:    insf $r7 = $r6, 15, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    extfz $r4 = $r4, 51, 48
; CV1-NEXT:    insf $r6 = $r7, 31, 0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sllhqs $r4 = $r0, $r4
; CV1-NEXT:    srlhqs $r7 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    insf $r4 = $r6, 47, 0
; CV1-NEXT:    extfz $r6 = $r2, 19, 16
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    srlhqs $r6 = $r0, $r6
; CV1-NEXT:    extfz $r8 = $r2, 35, 32
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r6 = $r7, 15, 0
; CV1-NEXT:    srlhqs $r7 = $r0, $r8
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    extfz $r2 = $r2, 51, 48
; CV1-NEXT:    insf $r7 = $r6, 31, 0
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    srlhqs $r0 = $r0, $r2
; CV1-NEXT:    extfz $r2 = $r5, 19, 16
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sllhqs $r2 = $r1, $r2
; CV1-NEXT:    sllhqs $r6 = $r1, $r5
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    insf $r0 = $r7, 47, 0
; CV1-NEXT:    extfz $r7 = $r3, 19, 16
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    ord $r0 = $r4, $r0
; CV1-NEXT:    insf $r2 = $r6, 15, 0
; CV1-NEXT:    extfz $r6 = $r5, 35, 32
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    srlhqs $r7 = $r1, $r7
; CV1-NEXT:    srlhqs $r8 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    insf $r7 = $r8, 15, 0
; CV1-NEXT:    extfz $r8 = $r3, 35, 32
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    sllhqs $r6 = $r1, $r6
; CV1-NEXT:    srlhqs $r8 = $r1, $r8
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    extfz $r2 = $r5, 51, 48
; CV1-NEXT:    insf $r6 = $r2, 31, 0
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    extfz $r3 = $r3, 51, 48
; CV1-NEXT:    insf $r8 = $r7, 31, 0
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    srlhqs $r1 = $r1, $r3
; CV1-NEXT:    sllhqs $r2 = $r1, $r2
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    insf $r1 = $r8, 47, 0
; CV1-NEXT:    insf $r2 = $r6, 47, 0
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    ord $r1 = $r2, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 21)
;
; CV2-LABEL: test_fshl_v8i16:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    neghq $r2 = $r2
; CV2-NEXT:    neghq $r3 = $r3
; CV2-NEXT:    andd $r4 = $r2, 0xf000f.@
; CV2-NEXT:    andd $r5 = $r3, 0xf000f.@
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    andd $r2 = $r2, 0xf000f.@
; CV2-NEXT:    sllhqs $r6 = $r0, $r4
; CV2-NEXT:    extfz $r7 = $r4, 19, 16
; CV2-NEXT:    extfz $r8 = $r4, 35, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    andd $r3 = $r3, 0xf000f.@
; CV2-NEXT:    extfz $r4 = $r4, 51, 48
; CV2-NEXT:    sllhqs $r7 = $r0, $r7
; CV2-NEXT:    srlhqs $r9 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sllhqs $r4 = $r0, $r4
; CV2-NEXT:    sllhqs $r6 = $r0, $r8
; CV2-NEXT:    insf $r7 = $r6, 15, 0
; CV2-NEXT:    extfz $r8 = $r2, 19, 16
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    extfz $r2 = $r2, 51, 48
; CV2-NEXT:    insf $r6 = $r7, 31, 0
; CV2-NEXT:    srlhqs $r7 = $r0, $r8
; CV2-NEXT:    extfz $r8 = $r2, 35, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    srlhqs $r0 = $r0, $r2
; CV2-NEXT:    extfz $r2 = $r5, 19, 16
; CV2-NEXT:    insf $r7 = $r9, 15, 0
; CV2-NEXT:    srlhqs $r8 = $r0, $r8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    sllhqs $r2 = $r1, $r2
; CV2-NEXT:    extfz $r7 = $r3, 19, 16
; CV2-NEXT:    insf $r8 = $r7, 31, 0
; CV2-NEXT:    sllhqs $r9 = $r1, $r5
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r9, 15, 0
; CV2-NEXT:    srlhqs $r7 = $r1, $r7
; CV2-NEXT:    extfz $r9 = $r5, 35, 32
; CV2-NEXT:    srlhqs $r10 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    extfz $r5 = $r5, 51, 48
; CV2-NEXT:    insf $r7 = $r10, 15, 0
; CV2-NEXT:    sllhqs $r9 = $r1, $r9
; CV2-NEXT:    extfz $r10 = $r3, 35, 32
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    sllhqs $r2 = $r1, $r5
; CV2-NEXT:    extfz $r3 = $r3, 51, 48
; CV2-NEXT:    insf $r9 = $r2, 31, 0
; CV2-NEXT:    srlhqs $r10 = $r1, $r10
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r0 = $r8, 47, 0
; CV2-NEXT:    srlhqs $r1 = $r1, $r3
; CV2-NEXT:    insf $r4 = $r6, 47, 0
; CV2-NEXT:    insf $r10 = $r7, 31, 0
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    ord $r0 = $r4, $r0
; CV2-NEXT:    insf $r1 = $r10, 47, 0
; CV2-NEXT:    insf $r2 = $r9, 47, 0
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    ord $r1 = $r2, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 12)
entry:
  %or = tail call <8 x i16> @llvm.fshl.v8i16(<8 x i16> %a, <8 x i16> %a, <8 x i16> %b)
  ret <8 x i16> %or
}

declare <16 x i16> @llvm.fshl.v16i16(<16 x i16>, <16 x i16>, <16 x i16>)
define <16 x i16> @test_fshl_v16i16(<16 x i16> %a, <16 x i16> %b) {
; CV1-LABEL: test_fshl_v16i16:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    neghq $r4 = $r4
; CV1-NEXT:    neghq $r5 = $r5
; CV1-NEXT:    andd $r8 = $r4, 0xf000f.@
; CV1-NEXT:    andd $r9 = $r5, 0xf000f.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd $r4 = $r4, 0xf000f.@
; CV1-NEXT:    andd $r5 = $r5, 0xf000f.@
; CV1-NEXT:    sllhqs $r10 = $r0, $r8
; CV1-NEXT:    extfz $r11 = $r8, 19, 16
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    neghq $r6 = $r6
; CV1-NEXT:    sllhqs $r11 = $r0, $r11
; CV1-NEXT:    extfz $r15 = $r8, 35, 32
; CV1-NEXT:    andd $r16 = $r6, 0xf000f.@
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    andd $r6 = $r6, 0xf000f.@
; CV1-NEXT:    sllhqs $r10 = $r0, $r15
; CV1-NEXT:    insf $r11 = $r10, 15, 0
; CV1-NEXT:    andd $r15 = $r7, 0xf000f.@
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    neghq $r7 = $r7
; CV1-NEXT:    extfz $r8 = $r8, 51, 48
; CV1-NEXT:    insf $r10 = $r11, 31, 0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sllhqs $r8 = $r0, $r8
; CV1-NEXT:    srlhqs $r11 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    insf $r8 = $r10, 47, 0
; CV1-NEXT:    extfz $r10 = $r4, 19, 16
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    srlhqs $r10 = $r0, $r10
; CV1-NEXT:    extfz $r17 = $r4, 35, 32
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r10 = $r11, 15, 0
; CV1-NEXT:    srlhqs $r11 = $r0, $r17
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    extfz $r4 = $r4, 51, 48
; CV1-NEXT:    insf $r11 = $r10, 31, 0
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    srlhqs $r0 = $r0, $r4
; CV1-NEXT:    extfz $r10 = $r9, 19, 16
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    insf $r0 = $r11, 47, 0
; CV1-NEXT:    sllhqs $r4 = $r1, $r9
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    ord $r0 = $r8, $r0
; CV1-NEXT:    sllhqs $r10 = $r1, $r10
; CV1-NEXT:    extfz $r11 = $r9, 35, 32
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    sllhqs $r4 = $r1, $r11
; CV1-NEXT:    insf $r10 = $r4, 15, 0
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    insf $r4 = $r10, 31, 0
; CV1-NEXT:    extfz $r9 = $r9, 51, 48
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    sllhqs $r9 = $r1, $r9
; CV1-NEXT:    srlhqs $r10 = $r1, $r5
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    extfz $r4 = $r5, 19, 16
; CV1-NEXT:    insf $r9 = $r4, 47, 0
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    srlhqs $r4 = $r1, $r4
; CV1-NEXT:    extfz $r11 = $r5, 35, 32
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    insf $r4 = $r10, 15, 0
; CV1-NEXT:    srlhqs $r10 = $r1, $r11
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    extfz $r4 = $r5, 51, 48
; CV1-NEXT:    insf $r10 = $r4, 31, 0
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    srlhqs $r1 = $r1, $r4
; CV1-NEXT:    extfz $r4 = $r16, 19, 16
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    sllhqs $r4 = $r2, $r4
; CV1-NEXT:    sllhqs $r5 = $r2, $r16
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    insf $r4 = $r5, 15, 0
; CV1-NEXT:    extfz $r5 = $r16, 35, 32
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    sllhqs $r5 = $r2, $r5
; CV1-NEXT:    extfz $r11 = $r16, 51, 48
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    sllhqs $r4 = $r2, $r11
; CV1-NEXT:    insf $r5 = $r4, 31, 0
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    insf $r1 = $r10, 47, 0
; CV1-NEXT:    extfz $r10 = $r6, 19, 16
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    ord $r1 = $r9, $r1
; CV1-NEXT:    insf $r4 = $r5, 47, 0
; CV1-NEXT:    srlhqs $r5 = $r2, $r6
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    srlhqs $r10 = $r2, $r10
; CV1-NEXT:    extfz $r11 = $r6, 35, 32
; CV1-NEXT:    ;; # (end cycle 27)
; CV1-NEXT:    srlhqs $r5 = $r2, $r11
; CV1-NEXT:    insf $r10 = $r5, 15, 0
; CV1-NEXT:    ;; # (end cycle 28)
; CV1-NEXT:    insf $r5 = $r10, 31, 0
; CV1-NEXT:    extfz $r6 = $r6, 51, 48
; CV1-NEXT:    ;; # (end cycle 29)
; CV1-NEXT:    srlhqs $r2 = $r2, $r6
; CV1-NEXT:    extfz $r6 = $r15, 19, 16
; CV1-NEXT:    ;; # (end cycle 30)
; CV1-NEXT:    sllhqs $r6 = $r3, $r6
; CV1-NEXT:    sllhqs $r10 = $r3, $r15
; CV1-NEXT:    ;; # (end cycle 31)
; CV1-NEXT:    insf $r2 = $r5, 47, 0
; CV1-NEXT:    andd $r5 = $r7, 0xf000f.@
; CV1-NEXT:    insf $r6 = $r10, 15, 0
; CV1-NEXT:    ;; # (end cycle 32)
; CV1-NEXT:    ord $r2 = $r4, $r2
; CV1-NEXT:    extfz $r7 = $r15, 35, 32
; CV1-NEXT:    extfz $r10 = $r5, 19, 16
; CV1-NEXT:    ;; # (end cycle 33)
; CV1-NEXT:    srlhqs $r10 = $r3, $r10
; CV1-NEXT:    srlhqs $r11 = $r3, $r5
; CV1-NEXT:    ;; # (end cycle 34)
; CV1-NEXT:    insf $r10 = $r11, 15, 0
; CV1-NEXT:    extfz $r11 = $r5, 35, 32
; CV1-NEXT:    ;; # (end cycle 35)
; CV1-NEXT:    sllhqs $r7 = $r3, $r7
; CV1-NEXT:    srlhqs $r11 = $r3, $r11
; CV1-NEXT:    ;; # (end cycle 36)
; CV1-NEXT:    extfz $r6 = $r15, 51, 48
; CV1-NEXT:    insf $r7 = $r6, 31, 0
; CV1-NEXT:    ;; # (end cycle 37)
; CV1-NEXT:    extfz $r5 = $r5, 51, 48
; CV1-NEXT:    insf $r11 = $r10, 31, 0
; CV1-NEXT:    ;; # (end cycle 38)
; CV1-NEXT:    srlhqs $r3 = $r3, $r5
; CV1-NEXT:    sllhqs $r6 = $r3, $r6
; CV1-NEXT:    ;; # (end cycle 39)
; CV1-NEXT:    insf $r3 = $r11, 47, 0
; CV1-NEXT:    insf $r6 = $r7, 47, 0
; CV1-NEXT:    ;; # (end cycle 40)
; CV1-NEXT:    ord $r3 = $r6, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 41)
;
; CV2-LABEL: test_fshl_v16i16:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    neghq $r4 = $r4
; CV2-NEXT:    neghq $r5 = $r5
; CV2-NEXT:    andd $r8 = $r4, 0xf000f.@
; CV2-NEXT:    andd $r9 = $r5, 0xf000f.@
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    andd $r4 = $r4, 0xf000f.@
; CV2-NEXT:    sllhqs $r10 = $r0, $r8
; CV2-NEXT:    extfz $r11 = $r8, 19, 16
; CV2-NEXT:    extfz $r15 = $r8, 35, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    andd $r5 = $r5, 0xf000f.@
; CV2-NEXT:    extfz $r8 = $r8, 51, 48
; CV2-NEXT:    sllhqs $r11 = $r0, $r11
; CV2-NEXT:    srlhqs $r16 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sllhqs $r8 = $r0, $r8
; CV2-NEXT:    sllhqs $r10 = $r0, $r15
; CV2-NEXT:    insf $r11 = $r10, 15, 0
; CV2-NEXT:    extfz $r15 = $r4, 19, 16
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    extfz $r4 = $r4, 51, 48
; CV2-NEXT:    insf $r10 = $r11, 31, 0
; CV2-NEXT:    srlhqs $r11 = $r0, $r15
; CV2-NEXT:    extfz $r15 = $r4, 35, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    srlhqs $r0 = $r0, $r4
; CV2-NEXT:    insf $r8 = $r10, 47, 0
; CV2-NEXT:    srlhqs $r10 = $r0, $r15
; CV2-NEXT:    insf $r11 = $r16, 15, 0
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    sllhqs $r4 = $r1, $r9
; CV2-NEXT:    insf $r10 = $r11, 31, 0
; CV2-NEXT:    extfz $r11 = $r9, 19, 16
; CV2-NEXT:    extfz $r15 = $r9, 35, 32
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r0 = $r10, 47, 0
; CV2-NEXT:    extfz $r9 = $r9, 51, 48
; CV2-NEXT:    sllhqs $r10 = $r1, $r11
; CV2-NEXT:    sllhqs $r11 = $r1, $r15
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    sllhqs $r4 = $r1, $r9
; CV2-NEXT:    srlhqs $r9 = $r1, $r5
; CV2-NEXT:    insf $r10 = $r4, 15, 0
; CV2-NEXT:    extfz $r15 = $r5, 19, 16
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r5 = $r5, 51, 48
; CV2-NEXT:    srlhqs $r10 = $r1, $r15
; CV2-NEXT:    insf $r11 = $r10, 31, 0
; CV2-NEXT:    extfz $r15 = $r5, 35, 32
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    srlhqs $r1 = $r1, $r5
; CV2-NEXT:    andd $r5 = $r6, 0xf000f.@
; CV2-NEXT:    srlhqs $r9 = $r1, $r15
; CV2-NEXT:    insf $r10 = $r9, 15, 0
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    insf $r4 = $r11, 47, 0
; CV2-NEXT:    neghq $r6 = $r6
; CV2-NEXT:    insf $r9 = $r10, 31, 0
; CV2-NEXT:    extfz $r10 = $r5, 19, 16
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    andd $r6 = $r6, 0xf000f.@
; CV2-NEXT:    sllhqs $r10 = $r2, $r10
; CV2-NEXT:    sllhqs $r11 = $r2, $r5
; CV2-NEXT:    extfz $r15 = $r5, 35, 32
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    insf $r1 = $r9, 47, 0
; CV2-NEXT:    sllhqs $r9 = $r2, $r15
; CV2-NEXT:    insf $r10 = $r11, 15, 0
; CV2-NEXT:    extfz $r11 = $r6, 19, 16
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    insf $r9 = $r10, 31, 0
; CV2-NEXT:    srlhqs $r10 = $r2, $r6
; CV2-NEXT:    srlhqs $r11 = $r2, $r11
; CV2-NEXT:    neghq $r15 = $r7
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    extfz $r5 = $r5, 51, 48
; CV2-NEXT:    extfz $r6 = $r6, 51, 48
; CV2-NEXT:    extfz $r10 = $r6, 35, 32
; CV2-NEXT:    insf $r11 = $r10, 15, 0
; CV2-NEXT:    ;; # (end cycle 15)
; CV2-NEXT:    sllhqs $r5 = $r2, $r5
; CV2-NEXT:    andd $r7 = $r7, 0xf000f.@
; CV2-NEXT:    srlhqs $r10 = $r2, $r10
; CV2-NEXT:    andd $r15 = $r15, 0xf000f.@
; CV2-NEXT:    ;; # (end cycle 16)
; CV2-NEXT:    srlhqs $r2 = $r2, $r6
; CV2-NEXT:    extfz $r6 = $r7, 19, 16
; CV2-NEXT:    insf $r10 = $r11, 31, 0
; CV2-NEXT:    extfz $r11 = $r15, 19, 16
; CV2-NEXT:    ;; # (end cycle 17)
; CV2-NEXT:    sllhqs $r6 = $r3, $r6
; CV2-NEXT:    srlhqs $r11 = $r3, $r11
; CV2-NEXT:    sllhqs $r16 = $r3, $r7
; CV2-NEXT:    srlhqs $r17 = $r3, $r15
; CV2-NEXT:    ;; # (end cycle 18)
; CV2-NEXT:    insf $r6 = $r16, 15, 0
; CV2-NEXT:    insf $r11 = $r17, 15, 0
; CV2-NEXT:    extfz $r16 = $r7, 35, 32
; CV2-NEXT:    extfz $r17 = $r15, 35, 32
; CV2-NEXT:    ;; # (end cycle 19)
; CV2-NEXT:    extfz $r7 = $r7, 51, 48
; CV2-NEXT:    extfz $r15 = $r15, 51, 48
; CV2-NEXT:    sllhqs $r16 = $r3, $r16
; CV2-NEXT:    srlhqs $r17 = $r3, $r17
; CV2-NEXT:    ;; # (end cycle 20)
; CV2-NEXT:    srlhqs $r3 = $r3, $r15
; CV2-NEXT:    sllhqs $r6 = $r3, $r7
; CV2-NEXT:    insf $r16 = $r6, 31, 0
; CV2-NEXT:    insf $r17 = $r11, 31, 0
; CV2-NEXT:    ;; # (end cycle 21)
; CV2-NEXT:    insf $r2 = $r10, 47, 0
; CV2-NEXT:    insf $r3 = $r17, 47, 0
; CV2-NEXT:    insf $r5 = $r9, 47, 0
; CV2-NEXT:    insf $r6 = $r16, 47, 0
; CV2-NEXT:    ;; # (end cycle 22)
; CV2-NEXT:    ord $r0 = $r8, $r0
; CV2-NEXT:    ord $r1 = $r4, $r1
; CV2-NEXT:    ord $r2 = $r5, $r2
; CV2-NEXT:    ord $r3 = $r6, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 23)
entry:
  %or = tail call <16 x i16> @llvm.fshl.v16i16(<16 x i16> %a, <16 x i16> %a, <16 x i16> %b)
  ret <16 x i16> %or
}
