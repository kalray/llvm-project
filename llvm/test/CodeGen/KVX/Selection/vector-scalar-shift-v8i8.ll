; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mcpu=kv3-1 -O2 -o - %s | FileCheck %s --check-prefix=CV1
; RUN: llc -mcpu=kv3-2 -O2 -o - %s | FileCheck %s --check-prefix=CV2
; RUN: clang -O2 -c -o /dev/null %s
; RUN: clang -O2 -march=kv3-2 -c -o /dev/null %s

target triple = "kvx-kalray-cos"

define <8 x i8> @ashiftR_imm(<8 x i8> %a){
; CV1-LABEL: ashiftR_imm:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    srld $r1 = $r0, 56
; CV1-NEXT:    extfz $r2 = $r0, 55, 48
; CV1-NEXT:    extfz $r3 = $r0, 47, 40
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    extfs $r1 = $r1, 7, 3
; CV1-NEXT:    extfz $r4 = $r0, 39, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    extfs $r2 = $r2, 7, 3
; CV1-NEXT:    extfs $r3 = $r3, 7, 3
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    extfs $r4 = $r4, 7, 3
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    extfz $r3 = $r0, 23, 16
; CV1-NEXT:    insf $r4 = $r3, 15, 8
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    extfs $r1 = $r1, 7, 3
; CV1-NEXT:    extfz $r5 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    extfs $r3 = $r3, 7, 3
; CV1-NEXT:    extfs $r5 = $r5, 7, 3
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    extfs $r0 = $r0, 7, 3
; CV1-NEXT:    insf $r3 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r0 = $r5, 15, 8
; CV1-NEXT:    insf $r4 = $r2, 31, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r0 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r0 = $r4, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 10)
;
; CV2-LABEL: ashiftR_imm:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    srabos $r0 = $r0, 3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
entry:
  %shr = ashr <8 x i8> %a, <i8 3, i8 3, i8 3, i8 3, i8 3, i8 3, i8 3, i8 3>
  ret <8 x i8> %shr
}

define <8 x i8> @lshiftR_imm(<8 x i8> %a){
; CV1-LABEL: lshiftR_imm:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r1 = $r0, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srld $r0 = $r0, 3
; CV1-NEXT:    srld $r1 = $r1, 3
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: lshiftR_imm:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    srlbos $r0 = $r0, 3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
entry:
  %shr = lshr <8 x i8> %a, <i8 3, i8 3, i8 3, i8 3, i8 3, i8 3, i8 3, i8 3>
  ret <8 x i8> %shr
}

define <8 x i8> @shiftL_imm(<8 x i8> %a){
; CV1-LABEL: shiftL_imm:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r1 = $r0, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    slld $r0 = $r0, 3
; CV1-NEXT:    slld $r1 = $r1, 3
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: shiftL_imm:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    sllbos $r0 = $r0, 3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
entry:
  %shl = shl <8 x i8> %a, <i8 3, i8 3, i8 3, i8 3, i8 3, i8 3, i8 3, i8 3>
  ret <8 x i8> %shl
}

define <8 x i8> @ashiftR(<8 x i8> %a, i32 %c){
; CV1-LABEL: ashiftR:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    srld $r2 = $r0, 56
; CV1-NEXT:    extfz $r3 = $r0, 55, 48
; CV1-NEXT:    extfz $r4 = $r0, 47, 40
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    extfz $r5 = $r0, 39, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sraw $r2 = $r2, $r1
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sraw $r3 = $r3, $r1
; CV1-NEXT:    sraw $r4 = $r4, $r1
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    extfz $r6 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srlw $r2 = $r0, 24
; CV1-NEXT:    insf $r3 = $r2, 15, 8
; CV1-NEXT:    sraw $r5 = $r5, $r1
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    extfz $r4 = $r0, 23, 16
; CV1-NEXT:    insf $r5 = $r4, 15, 8
; CV1-NEXT:    sraw $r6 = $r6, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sraw $r2 = $r2, $r1
; CV1-NEXT:    sraw $r4 = $r4, $r1
; CV1-NEXT:    insf $r5 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    sraw $r0 = $r0, $r1
; CV1-NEXT:    insf $r4 = $r2, 15, 8
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r0 = $r6, 15, 8
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r0 = $r4, 31, 16
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    insf $r0 = $r5, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 11)
;
; CV2-LABEL: ashiftR:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    srabos $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
entry:
  %0 = trunc i32 %c to i8
  %1 = insertelement <8 x i8> undef, i8 %0, i32 0
  %sh_prom = shufflevector <8 x i8> %1, <8 x i8> undef, <8 x i32> zeroinitializer
  %shr = ashr <8 x i8> %a, %sh_prom
  ret <8 x i8> %shr
}

define <8 x i8> @lshiftR(<8 x i8> %a, i32 %c){
; CV1-LABEL: lshiftR:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r2 = $r0, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srld $r0 = $r0, $r1
; CV1-NEXT:    srld $r2 = $r2, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r1 = $r2, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: lshiftR:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    srlbos $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
entry:
  %0 = trunc i32 %c to i8
  %1 = insertelement <8 x i8> undef, i8 %0, i32 0
  %sh_prom = shufflevector <8 x i8> %1, <8 x i8> undef, <8 x i32> zeroinitializer
  %shr = lshr <8 x i8> %a, %sh_prom
  ret <8 x i8> %shr
}

define <8 x i8> @shiftL(<8 x i8> %a, i32 %c){
; CV1-LABEL: shiftL:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r2 = $r0, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    slld $r0 = $r0, $r1
; CV1-NEXT:    slld $r2 = $r2, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r1 = $r2, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: shiftL:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    sllbos $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
entry:
  %0 = trunc i32 %c to i8
  %1 = insertelement <8 x i8> undef, i8 %0, i32 0
  %sh_prom = shufflevector <8 x i8> %1, <8 x i8> undef, <8 x i32> zeroinitializer
  %shl = shl <8 x i8> %a, %sh_prom
  ret <8 x i8> %shl
}

define <8 x i8> @ashiftR_imm_vec(<8 x i8> %a){
; CV1-LABEL: ashiftR_imm_vec:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    srld $r1 = $r0, 56
; CV1-NEXT:    extfz $r2 = $r0, 55, 48
; CV1-NEXT:    extfz $r3 = $r0, 47, 40
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    extfs $r1 = $r1, 7, 2
; CV1-NEXT:    extfz $r4 = $r0, 39, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    extfs $r2 = $r2, 7, 3
; CV1-NEXT:    extfs $r3 = $r3, 7, 3
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    extfs $r4 = $r4, 7, 4
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    extfz $r3 = $r0, 23, 16
; CV1-NEXT:    insf $r4 = $r3, 15, 8
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    extfs $r1 = $r1, 7, 2
; CV1-NEXT:    extfz $r5 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    extfs $r3 = $r3, 7, 3
; CV1-NEXT:    extfs $r5 = $r5, 7, 3
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    extfs $r0 = $r0, 7, 4
; CV1-NEXT:    insf $r3 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r0 = $r5, 15, 8
; CV1-NEXT:    insf $r4 = $r2, 31, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r0 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r0 = $r4, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 10)
;
; CV2-LABEL: ashiftR_imm_vec:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    make $r1 = 0x203030402030304
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    extfz $r2 = $r1, 10, 8
; CV2-NEXT:    srabos $r3 = $r0, $r1
; CV2-NEXT:    extfz $r4 = $r1, 18, 16
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    srabos $r2 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r2 = $r3, 7, 0
; CV2-NEXT:    srabos $r3 = $r0, $r4
; CV2-NEXT:    extfz $r4 = $r1, 26, 24
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srabos $r2 = $r0, $r4
; CV2-NEXT:    insf $r3 = $r2, 15, 0
; CV2-NEXT:    extfz $r4 = $r1, 34, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r2 = $r3, 23, 0
; CV2-NEXT:    srabos $r3 = $r0, $r4
; CV2-NEXT:    extfz $r4 = $r1, 42, 40
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    extfz $r1 = $r1, 58, 56
; CV2-NEXT:    srabos $r2 = $r0, $r4
; CV2-NEXT:    insf $r3 = $r2, 31, 0
; CV2-NEXT:    extfz $r4 = $r1, 50, 48
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    srabos $r0 = $r0, $r1
; CV2-NEXT:    insf $r2 = $r3, 39, 0
; CV2-NEXT:    srabos $r3 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r3 = $r2, 47, 0
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    insf $r0 = $r3, 55, 0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 9)
entry:
  %shr = ashr <8 x i8> %a, <i8 4, i8 3, i8 3, i8 2, i8 4, i8 3, i8 3, i8 2>
  ret <8 x i8> %shr
}

define <8 x i8> @lshiftR_imm_vec(<8 x i8> %a){
; CV1-LABEL: lshiftR_imm_vec:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    extfz $r1 = $r0, 55, 48
; CV1-NEXT:    srld $r2 = $r0, 56
; CV1-NEXT:    extfz $r3 = $r0, 47, 40
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andw $r1 = $r1, 248
; CV1-NEXT:    andw $r2 = $r2, 252
; CV1-NEXT:    andw $r3 = $r3, 248
; CV1-NEXT:    extfz $r4 = $r0, 39, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srlw $r1 = $r1, 3
; CV1-NEXT:    srlw $r2 = $r2, 2
; CV1-NEXT:    srlw $r3 = $r3, 3
; CV1-NEXT:    andw $r4 = $r4, 240
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r1 = $r2, 15, 8
; CV1-NEXT:    srlw $r2 = $r0, 24
; CV1-NEXT:    srlw $r4 = $r4, 4
; CV1-NEXT:    extfz $r5 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    andw $r2 = $r2, 240
; CV1-NEXT:    extfz $r3 = $r0, 23, 16
; CV1-NEXT:    insf $r4 = $r3, 15, 8
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    andw $r0 = $r0, 252
; CV1-NEXT:    srlw $r2 = $r2, 4
; CV1-NEXT:    andw $r3 = $r3, 248
; CV1-NEXT:    andw $r5 = $r5, 254
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r0 = $r0, 2
; CV1-NEXT:    srlw $r3 = $r3, 3
; CV1-NEXT:    insf $r4 = $r1, 31, 16
; CV1-NEXT:    srlw $r5 = $r5, 1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r0 = $r5, 15, 8
; CV1-NEXT:    insf $r3 = $r2, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r0 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r0 = $r4, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 9)
;
; CV2-LABEL: lshiftR_imm_vec:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    make $r1 = 0x203030404030102
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    extfz $r2 = $r1, 10, 8
; CV2-NEXT:    srlbos $r3 = $r0, $r1
; CV2-NEXT:    extfz $r4 = $r1, 18, 16
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    srlbos $r2 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r2 = $r3, 7, 0
; CV2-NEXT:    srlbos $r3 = $r0, $r4
; CV2-NEXT:    extfz $r4 = $r1, 26, 24
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlbos $r2 = $r0, $r4
; CV2-NEXT:    insf $r3 = $r2, 15, 0
; CV2-NEXT:    extfz $r4 = $r1, 34, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r2 = $r3, 23, 0
; CV2-NEXT:    srlbos $r3 = $r0, $r4
; CV2-NEXT:    extfz $r4 = $r1, 42, 40
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    extfz $r1 = $r1, 58, 56
; CV2-NEXT:    srlbos $r2 = $r0, $r4
; CV2-NEXT:    insf $r3 = $r2, 31, 0
; CV2-NEXT:    extfz $r4 = $r1, 50, 48
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    srlbos $r0 = $r0, $r1
; CV2-NEXT:    insf $r2 = $r3, 39, 0
; CV2-NEXT:    srlbos $r3 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r3 = $r2, 47, 0
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    insf $r0 = $r3, 55, 0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 9)
entry:
  %shr = lshr <8 x i8> %a, <i8 2, i8 1, i8 3, i8 4, i8 4, i8 3, i8 3, i8 2>
  ret <8 x i8> %shr
}

define <8 x i8> @shiftL_imm_vec(<8 x i8> %a){
; CV1-LABEL: shiftL_imm_vec:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    extfz $r1 = $r0, 55, 48
; CV1-NEXT:    srld $r2 = $r0, 56
; CV1-NEXT:    extfz $r3 = $r0, 47, 40
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sllw $r1 = $r1, 3
; CV1-NEXT:    sllw $r2 = $r2, 2
; CV1-NEXT:    sllw $r3 = $r3, 3
; CV1-NEXT:    extfz $r4 = $r0, 39, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r1 = $r2, 15, 8
; CV1-NEXT:    srlw $r2 = $r0, 24
; CV1-NEXT:    sllw $r4 = $r4, 4
; CV1-NEXT:    extfz $r5 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    sllw $r2 = $r2, 5
; CV1-NEXT:    extfz $r3 = $r0, 23, 16
; CV1-NEXT:    insf $r4 = $r3, 15, 8
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sllw $r0 = $r0, 3
; CV1-NEXT:    sllw $r3 = $r3, 3
; CV1-NEXT:    insf $r4 = $r1, 31, 16
; CV1-NEXT:    sllw $r5 = $r5, 3
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    insf $r0 = $r5, 15, 8
; CV1-NEXT:    insf $r3 = $r2, 15, 8
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    insf $r0 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r0 = $r4, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: shiftL_imm_vec:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    make $r1 = 0x203030405030303
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    extfz $r2 = $r1, 10, 8
; CV2-NEXT:    sllbos $r3 = $r0, $r1
; CV2-NEXT:    extfz $r4 = $r1, 18, 16
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sllbos $r2 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r2 = $r3, 7, 0
; CV2-NEXT:    sllbos $r3 = $r0, $r4
; CV2-NEXT:    extfz $r4 = $r1, 26, 24
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sllbos $r2 = $r0, $r4
; CV2-NEXT:    insf $r3 = $r2, 15, 0
; CV2-NEXT:    extfz $r4 = $r1, 34, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r2 = $r3, 23, 0
; CV2-NEXT:    sllbos $r3 = $r0, $r4
; CV2-NEXT:    extfz $r4 = $r1, 42, 40
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    extfz $r1 = $r1, 58, 56
; CV2-NEXT:    sllbos $r2 = $r0, $r4
; CV2-NEXT:    insf $r3 = $r2, 31, 0
; CV2-NEXT:    extfz $r4 = $r1, 50, 48
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    sllbos $r0 = $r0, $r1
; CV2-NEXT:    insf $r2 = $r3, 39, 0
; CV2-NEXT:    sllbos $r3 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r3 = $r2, 47, 0
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    insf $r0 = $r3, 55, 0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 9)
entry:
  %shl = shl <8 x i8> %a, <i8 3, i8 3, i8 3, i8 5, i8 4, i8 3, i8 3, i8 2>
  ret <8 x i8> %shl
}

define <8 x i8> @ashiftR_rr_vec(<8 x i8> %a, <8 x i8> %b){
; CV1-LABEL: ashiftR_rr_vec:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    srld $r2 = $r0, 56
; CV1-NEXT:    extfz $r4 = $r0, 55, 48
; CV1-NEXT:    extfz $r6 = $r0, 47, 40
; CV1-NEXT:    srld $r8 = $r1, 56
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    extfz $r3 = $r1, 55, 48
; CV1-NEXT:    extfz $r7 = $r0, 39, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sraw $r2 = $r2, $r8
; CV1-NEXT:    sraw $r3 = $r4, $r3
; CV1-NEXT:    extfz $r5 = $r1, 47, 40
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sraw $r4 = $r6, $r5
; CV1-NEXT:    sxbd $r7 = $r7
; CV1-NEXT:    extfz $r9 = $r1, 39, 32
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    srlw $r2 = $r0, 24
; CV1-NEXT:    insf $r3 = $r2, 15, 8
; CV1-NEXT:    sraw $r5 = $r7, $r9
; CV1-NEXT:    extfz $r8 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    extfz $r4 = $r0, 23, 16
; CV1-NEXT:    insf $r5 = $r4, 15, 8
; CV1-NEXT:    srlw $r9 = $r1, 24
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    extfz $r6 = $r1, 23, 16
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    zxbd $r1 = $r1
; CV1-NEXT:    sraw $r2 = $r2, $r9
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    extfz $r7 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sraw $r4 = $r4, $r6
; CV1-NEXT:    sxbd $r8 = $r8
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    sraw $r0 = $r0, $r1
; CV1-NEXT:    insf $r4 = $r2, 15, 8
; CV1-NEXT:    insf $r5 = $r3, 31, 16
; CV1-NEXT:    sraw $r6 = $r8, $r7
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    insf $r0 = $r6, 15, 8
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    insf $r0 = $r4, 31, 16
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    insf $r0 = $r5, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 13)
;
; CV2-LABEL: ashiftR_rr_vec:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    extfz $r2 = $r1, 10, 8
; CV2-NEXT:    srabos $r3 = $r0, $r1
; CV2-NEXT:    extfz $r4 = $r1, 18, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srabos $r2 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r2 = $r3, 7, 0
; CV2-NEXT:    srabos $r3 = $r0, $r4
; CV2-NEXT:    extfz $r4 = $r1, 26, 24
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    srabos $r2 = $r0, $r4
; CV2-NEXT:    insf $r3 = $r2, 15, 0
; CV2-NEXT:    extfz $r4 = $r1, 34, 32
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    insf $r2 = $r3, 23, 0
; CV2-NEXT:    srabos $r3 = $r0, $r4
; CV2-NEXT:    extfz $r4 = $r1, 42, 40
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    extfz $r1 = $r1, 58, 56
; CV2-NEXT:    srabos $r2 = $r0, $r4
; CV2-NEXT:    insf $r3 = $r2, 31, 0
; CV2-NEXT:    extfz $r4 = $r1, 50, 48
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    srabos $r0 = $r0, $r1
; CV2-NEXT:    insf $r2 = $r3, 39, 0
; CV2-NEXT:    srabos $r3 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r3 = $r2, 47, 0
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r0 = $r3, 55, 0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
entry:
  %shr = ashr <8 x i8> %a, %b
  ret <8 x i8> %shr
}

define <8 x i8> @lshiftR_rr_vec(<8 x i8> %a, <8 x i8> %b){
; CV1-LABEL: lshiftR_rr_vec:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    srld $r2 = $r0, 56
; CV1-NEXT:    extfz $r3 = $r0, 55, 48
; CV1-NEXT:    srld $r4 = $r1, 56
; CV1-NEXT:    extfz $r5 = $r1, 55, 48
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    zxbd $r2 = $r2
; CV1-NEXT:    zxbd $r3 = $r3
; CV1-NEXT:    extfz $r6 = $r0, 47, 40
; CV1-NEXT:    extfz $r7 = $r0, 39, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srlw $r2 = $r2, $r4
; CV1-NEXT:    srlw $r3 = $r3, $r5
; CV1-NEXT:    extfz $r4 = $r1, 47, 40
; CV1-NEXT:    zxbd $r5 = $r6
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    srlw $r2 = $r5, $r4
; CV1-NEXT:    insf $r3 = $r2, 15, 8
; CV1-NEXT:    extfz $r6 = $r1, 39, 32
; CV1-NEXT:    zxbd $r7 = $r7
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srlw $r4 = $r7, $r6
; CV1-NEXT:    srlw $r5 = $r0, 24
; CV1-NEXT:    extfz $r6 = $r0, 23, 16
; CV1-NEXT:    extfz $r8 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    srlw $r2 = $r1, 24
; CV1-NEXT:    insf $r4 = $r2, 15, 8
; CV1-NEXT:    zxbd $r5 = $r5
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    zxbd $r6 = $r6
; CV1-NEXT:    extfz $r7 = $r1, 23, 16
; CV1-NEXT:    zxbd $r8 = $r8
; CV1-NEXT:    extfz $r9 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    zxbd $r1 = $r1
; CV1-NEXT:    srlw $r2 = $r5, $r2
; CV1-NEXT:    srlw $r5 = $r6, $r7
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    srlw $r0 = $r0, $r1
; CV1-NEXT:    insf $r4 = $r3, 31, 16
; CV1-NEXT:    insf $r5 = $r2, 15, 8
; CV1-NEXT:    srlw $r6 = $r8, $r9
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r0 = $r6, 15, 8
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r0 = $r5, 31, 16
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    insf $r0 = $r4, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 11)
;
; CV2-LABEL: lshiftR_rr_vec:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    extfz $r2 = $r1, 10, 8
; CV2-NEXT:    srlbos $r3 = $r0, $r1
; CV2-NEXT:    extfz $r4 = $r1, 18, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srlbos $r2 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r2 = $r3, 7, 0
; CV2-NEXT:    srlbos $r3 = $r0, $r4
; CV2-NEXT:    extfz $r4 = $r1, 26, 24
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    srlbos $r2 = $r0, $r4
; CV2-NEXT:    insf $r3 = $r2, 15, 0
; CV2-NEXT:    extfz $r4 = $r1, 34, 32
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    insf $r2 = $r3, 23, 0
; CV2-NEXT:    srlbos $r3 = $r0, $r4
; CV2-NEXT:    extfz $r4 = $r1, 42, 40
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    extfz $r1 = $r1, 58, 56
; CV2-NEXT:    srlbos $r2 = $r0, $r4
; CV2-NEXT:    insf $r3 = $r2, 31, 0
; CV2-NEXT:    extfz $r4 = $r1, 50, 48
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    srlbos $r0 = $r0, $r1
; CV2-NEXT:    insf $r2 = $r3, 39, 0
; CV2-NEXT:    srlbos $r3 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r3 = $r2, 47, 0
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r0 = $r3, 55, 0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
entry:
  %shr = lshr <8 x i8> %a, %b
  ret <8 x i8> %shr
}

define <8 x i8> @shiftL_rr_vec(<8 x i8> %a, <8 x i8> %b){
; CV1-LABEL: shiftL_rr_vec:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    extfz $r2 = $r1, 55, 48
; CV1-NEXT:    extfz $r3 = $r0, 55, 48
; CV1-NEXT:    srld $r6 = $r1, 56
; CV1-NEXT:    srld $r7 = $r0, 56
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sllw $r2 = $r3, $r2
; CV1-NEXT:    extfz $r4 = $r1, 47, 40
; CV1-NEXT:    extfz $r5 = $r0, 47, 40
; CV1-NEXT:    sllw $r6 = $r7, $r6
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sllw $r3 = $r5, $r4
; CV1-NEXT:    srlw $r5 = $r0, 24
; CV1-NEXT:    extfz $r8 = $r1, 39, 32
; CV1-NEXT:    extfz $r9 = $r0, 39, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r2 = $r6, 15, 8
; CV1-NEXT:    sllw $r4 = $r9, $r8
; CV1-NEXT:    extfz $r6 = $r1, 23, 16
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srlw $r3 = $r1, 24
; CV1-NEXT:    insf $r4 = $r3, 15, 8
; CV1-NEXT:    extfz $r7 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    zxbd $r1 = $r1
; CV1-NEXT:    extfz $r8 = $r1, 15, 8
; CV1-NEXT:    extfz $r9 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sllw $r0 = $r0, $r1
; CV1-NEXT:    sllw $r3 = $r5, $r3
; CV1-NEXT:    sllw $r5 = $r7, $r6
; CV1-NEXT:    sllw $r6 = $r9, $r8
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r0 = $r6, 15, 8
; CV1-NEXT:    insf $r5 = $r3, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r0 = $r5, 31, 16
; CV1-NEXT:    insf $r4 = $r2, 31, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r0 = $r4, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 9)
;
; CV2-LABEL: shiftL_rr_vec:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    extfz $r2 = $r1, 10, 8
; CV2-NEXT:    sllbos $r3 = $r0, $r1
; CV2-NEXT:    extfz $r4 = $r1, 18, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sllbos $r2 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r2 = $r3, 7, 0
; CV2-NEXT:    sllbos $r3 = $r0, $r4
; CV2-NEXT:    extfz $r4 = $r1, 26, 24
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sllbos $r2 = $r0, $r4
; CV2-NEXT:    insf $r3 = $r2, 15, 0
; CV2-NEXT:    extfz $r4 = $r1, 34, 32
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    insf $r2 = $r3, 23, 0
; CV2-NEXT:    sllbos $r3 = $r0, $r4
; CV2-NEXT:    extfz $r4 = $r1, 42, 40
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    extfz $r1 = $r1, 58, 56
; CV2-NEXT:    sllbos $r2 = $r0, $r4
; CV2-NEXT:    insf $r3 = $r2, 31, 0
; CV2-NEXT:    extfz $r4 = $r1, 50, 48
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    sllbos $r0 = $r0, $r1
; CV2-NEXT:    insf $r2 = $r3, 39, 0
; CV2-NEXT:    sllbos $r3 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r3 = $r2, 47, 0
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r0 = $r3, 55, 0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
entry:
  %shl = shl <8 x i8> %a, %b
  ret <8 x i8> %shl
}
