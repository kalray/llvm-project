; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc --frame-pointer=none < %s | FileCheck -check-prefix=FP-NONE %s
; RUN: llc --frame-pointer=all < %s | FileCheck -check-prefix=FP-ALL %s

target triple = "kvx-kalray-cos"

; Original C code:
;
; #include <stdarg.h>
;
; int add(int n, ...) {
; int sum = 0;
;
; va_list args;
; va_start(args, n);
;
; for(int i = 0; i < n; i++ )
; sum += va_arg(args, int);
;
; va_end(args);
;
; return sum;
; }

define i32 @add(i32 %n, ...) {
; FP-NONE-LABEL: add:
; FP-NONE:       # %bb.0: # %entry
; FP-NONE-NEXT:    addd $r12 = $r12, -96
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    .cfi_def_cfa_offset 96
; FP-NONE-NEXT:    sd 88[$r12] = $r11
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    sd 80[$r12] = $r10
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    sd 72[$r12] = $r9
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    sd 64[$r12] = $r8
; FP-NONE-NEXT:    addd $r8 = $r12, 8
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    sd 56[$r12] = $r7
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    sd 48[$r12] = $r6
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    sd 40[$r12] = $r5
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    sd 32[$r12] = $r4
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    sd 24[$r12] = $r3
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    sd 16[$r12] = $r2
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    sd 8[$r12] = $r1
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    sd 0[$r12] = $r8
; FP-NONE-NEXT:    cb.wlez $r0 ? .LBB0_1
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:  # %bb.2: # %for.body.preheader
; FP-NONE-NEXT:    ld $r4 = 0[$r12]
; FP-NONE-NEXT:    addw $r1 = $r0, -1
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    compw.ltu $r2 = $r1, 7
; FP-NONE-NEXT:    andw $r1 = $r0, 7
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    cb.even $r2 ? .LBB0_8
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:  # %bb.3:
; FP-NONE-NEXT:    make $r0 = 0
; FP-NONE-NEXT:    goto .LBB0_4
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:  .LBB0_1:
; FP-NONE-NEXT:    make $r0 = 0
; FP-NONE-NEXT:    addd $r12 = $r12, 96
; FP-NONE-NEXT:    ret
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:  .LBB0_8: # %for.body.preheader.new
; FP-NONE-NEXT:    sbfw $r2 = $r0, $r1
; FP-NONE-NEXT:    make $r0 = 0
; FP-NONE-NEXT:    copyd $r3 = $r4
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:  .LBB0_9: # %for.body
; FP-NONE-NEXT:    # =>This Inner Loop Header: Depth=1
; FP-NONE-NEXT:    addd $r4 = $r3, 8
; FP-NONE-NEXT:    addw $r2 = $r2, 8
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    sd 0[$r12] = $r4
; FP-NONE-NEXT:    addd $r4 = $r3, 16
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    lwz $r5 = 0[$r3]
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    sd 0[$r12] = $r4
; FP-NONE-NEXT:    addd $r4 = $r3, 24
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    lwz $r6 = 8[$r3]
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    sd 0[$r12] = $r4
; FP-NONE-NEXT:    addd $r4 = $r3, 32
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    lwz $r7 = 16[$r3]
; FP-NONE-NEXT:    addw $r0 = $r5, $r0
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    sd 0[$r12] = $r4
; FP-NONE-NEXT:    addd $r4 = $r3, 40
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    lwz $r8 = 24[$r3]
; FP-NONE-NEXT:    addw $r0 = $r6, $r0
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    sd 0[$r12] = $r4
; FP-NONE-NEXT:    addd $r4 = $r3, 48
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    lwz $r5 = 32[$r3]
; FP-NONE-NEXT:    addw $r0 = $r7, $r0
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    sd 0[$r12] = $r4
; FP-NONE-NEXT:    addd $r4 = $r3, 56
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    lwz $r6 = 40[$r3]
; FP-NONE-NEXT:    addw $r0 = $r8, $r0
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    sd 0[$r12] = $r4
; FP-NONE-NEXT:    addd $r4 = $r3, 64
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    lwz $r7 = 48[$r3]
; FP-NONE-NEXT:    addw $r0 = $r5, $r0
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    sd 0[$r12] = $r4
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    lwz $r3 = 56[$r3]
; FP-NONE-NEXT:    addw $r0 = $r6, $r0
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    addw $r0 = $r7, $r0
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    addw $r0 = $r3, $r0
; FP-NONE-NEXT:    copyd $r3 = $r4
; FP-NONE-NEXT:    cb.wnez $r2 ? .LBB0_9
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:  .LBB0_4: # %for.cond.cleanup.loopexit.unr-lcssa
; FP-NONE-NEXT:    cb.weqz $r1 ? .LBB0_7
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:  # %bb.5: # %for.body.epil.preheader
; FP-NONE-NEXT:    addd $r2 = $r4, 8
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:  .LBB0_6: # %for.body.epil
; FP-NONE-NEXT:    # =>This Inner Loop Header: Depth=1
; FP-NONE-NEXT:    sd 0[$r12] = $r2
; FP-NONE-NEXT:    addw $r1 = $r1, -1
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    lwz $r3 = -8[$r2]
; FP-NONE-NEXT:    addd $r2 = $r2, 8
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    addw $r0 = $r3, $r0
; FP-NONE-NEXT:    cb.wnez $r1 ? .LBB0_6
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:  .LBB0_7: # %for.cond.cleanup
; FP-NONE-NEXT:    addd $r12 = $r12, 96
; FP-NONE-NEXT:    ret
; FP-NONE-NEXT:    ;;
;
; FP-ALL-LABEL: add:
; FP-ALL:       # %bb.0: # %entry
; FP-ALL-NEXT:    addd $r12 = $r12, -128
; FP-ALL-NEXT:    get $r16 = $ra
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    .cfi_def_cfa_offset 128
; FP-ALL-NEXT:    .cfi_register 67, 16
; FP-ALL-NEXT:    sd 32[$r12] = $r16
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    .cfi_offset 67, -96
; FP-ALL-NEXT:    sd 24[$r12] = $r14
; FP-ALL-NEXT:    addd $r14 = $r12, 24
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    .cfi_offset 14, -104
; FP-ALL-NEXT:    .cfi_def_cfa 14, 104
; FP-ALL-NEXT:    sd 96[$r14] = $r11
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    sd 88[$r14] = $r10
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    sd 80[$r14] = $r9
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    sd 72[$r14] = $r8
; FP-ALL-NEXT:    addd $r8 = $r14, 16
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    sd 64[$r14] = $r7
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    sd 56[$r14] = $r6
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    sd 48[$r14] = $r5
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    sd 40[$r14] = $r4
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    sd 32[$r14] = $r3
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    sd 24[$r14] = $r2
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    sd 16[$r14] = $r1
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    sd -8[$r14] = $r8
; FP-ALL-NEXT:    cb.wlez $r0 ? .LBB0_1
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:  # %bb.2: # %for.body.preheader
; FP-ALL-NEXT:    ld $r4 = -8[$r14]
; FP-ALL-NEXT:    addw $r1 = $r0, -1
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    compw.ltu $r2 = $r1, 7
; FP-ALL-NEXT:    andw $r1 = $r0, 7
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    cb.even $r2 ? .LBB0_8
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:  # %bb.3:
; FP-ALL-NEXT:    make $r0 = 0
; FP-ALL-NEXT:    goto .LBB0_4
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:  .LBB0_1:
; FP-ALL-NEXT:    make $r0 = 0
; FP-ALL-NEXT:    goto .LBB0_7
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:  .LBB0_8: # %for.body.preheader.new
; FP-ALL-NEXT:    sbfw $r2 = $r0, $r1
; FP-ALL-NEXT:    make $r0 = 0
; FP-ALL-NEXT:    copyd $r3 = $r4
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:  .LBB0_9: # %for.body
; FP-ALL-NEXT:    # =>This Inner Loop Header: Depth=1
; FP-ALL-NEXT:    addd $r4 = $r3, 8
; FP-ALL-NEXT:    addw $r2 = $r2, 8
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    sd -8[$r14] = $r4
; FP-ALL-NEXT:    addd $r4 = $r3, 16
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    lwz $r5 = 0[$r3]
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    sd -8[$r14] = $r4
; FP-ALL-NEXT:    addd $r4 = $r3, 24
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    lwz $r6 = 8[$r3]
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    sd -8[$r14] = $r4
; FP-ALL-NEXT:    addd $r4 = $r3, 32
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    lwz $r7 = 16[$r3]
; FP-ALL-NEXT:    addw $r0 = $r5, $r0
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    sd -8[$r14] = $r4
; FP-ALL-NEXT:    addd $r4 = $r3, 40
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    lwz $r8 = 24[$r3]
; FP-ALL-NEXT:    addw $r0 = $r6, $r0
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    sd -8[$r14] = $r4
; FP-ALL-NEXT:    addd $r4 = $r3, 48
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    lwz $r5 = 32[$r3]
; FP-ALL-NEXT:    addw $r0 = $r7, $r0
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    sd -8[$r14] = $r4
; FP-ALL-NEXT:    addd $r4 = $r3, 56
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    lwz $r6 = 40[$r3]
; FP-ALL-NEXT:    addw $r0 = $r8, $r0
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    sd -8[$r14] = $r4
; FP-ALL-NEXT:    addd $r4 = $r3, 64
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    lwz $r7 = 48[$r3]
; FP-ALL-NEXT:    addw $r0 = $r5, $r0
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    sd -8[$r14] = $r4
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    lwz $r3 = 56[$r3]
; FP-ALL-NEXT:    addw $r0 = $r6, $r0
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    addw $r0 = $r7, $r0
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    addw $r0 = $r3, $r0
; FP-ALL-NEXT:    copyd $r3 = $r4
; FP-ALL-NEXT:    cb.wnez $r2 ? .LBB0_9
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:  .LBB0_4: # %for.cond.cleanup.loopexit.unr-lcssa
; FP-ALL-NEXT:    cb.weqz $r1 ? .LBB0_7
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:  # %bb.5: # %for.body.epil.preheader
; FP-ALL-NEXT:    addd $r2 = $r4, 8
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:  .LBB0_6: # %for.body.epil
; FP-ALL-NEXT:    # =>This Inner Loop Header: Depth=1
; FP-ALL-NEXT:    sd -8[$r14] = $r2
; FP-ALL-NEXT:    addw $r1 = $r1, -1
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    lwz $r3 = -8[$r2]
; FP-ALL-NEXT:    addd $r2 = $r2, 8
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    addw $r0 = $r3, $r0
; FP-ALL-NEXT:    cb.wnez $r1 ? .LBB0_6
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:  .LBB0_7: # %for.cond.cleanup
; FP-ALL-NEXT:    addd $r12 = $r14, -24
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    ld $r14 = 24[$r12]
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    ld $r16 = 32[$r12]
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    set $ra = $r16
; FP-ALL-NEXT:    addd $r12 = $r12, 128
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    ret
; FP-ALL-NEXT:    ;;
entry:
  %args = alloca i8*, align 8
  %0 = bitcast i8** %args to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %0)
  call void @llvm.va_start(i8* nonnull %0)
  %cmp7 = icmp sgt i32 %n, 0
  br i1 %cmp7, label %for.body.preheader, label %for.cond.cleanup

for.body.preheader:                               ; preds = %entry
  %ap.cur.pre = load i8*, i8** %args, align 8
  %1 = add i32 %n, -1
  %xtraiter = and i32 %n, 7
  %2 = icmp ult i32 %1, 7
  br i1 %2, label %for.cond.cleanup.loopexit.unr-lcssa, label %for.body.preheader.new

for.body.preheader.new:                           ; preds = %for.body.preheader
  %unroll_iter = sub i32 %n, %xtraiter
  br label %for.body

for.cond.cleanup.loopexit.unr-lcssa:              ; preds = %for.body, %for.body.preheader
  %add.lcssa.ph = phi i32 [ undef, %for.body.preheader ], [ %add.7, %for.body ]
  %ap.cur.unr = phi i8* [ %ap.cur.pre, %for.body.preheader ], [ %ap.next.7, %for.body ]
  %sum.08.unr = phi i32 [ 0, %for.body.preheader ], [ %add.7, %for.body ]
  %lcmp.mod = icmp eq i32 %xtraiter, 0
  br i1 %lcmp.mod, label %for.cond.cleanup, label %for.body.epil

for.body.epil:                                    ; preds = %for.cond.cleanup.loopexit.unr-lcssa, %for.body.epil
  %ap.cur.epil = phi i8* [ %ap.next.epil, %for.body.epil ], [ %ap.cur.unr, %for.cond.cleanup.loopexit.unr-lcssa ]
  %sum.08.epil = phi i32 [ %add.epil, %for.body.epil ], [ %sum.08.unr, %for.cond.cleanup.loopexit.unr-lcssa ]
  %epil.iter = phi i32 [ %epil.iter.sub, %for.body.epil ], [ %xtraiter, %for.cond.cleanup.loopexit.unr-lcssa ]
  %ap.next.epil = getelementptr inbounds i8, i8* %ap.cur.epil, i64 8
  store i8* %ap.next.epil, i8** %args, align 8
  %arg.addr.epil = bitcast i8* %ap.cur.epil to i32*
  %3 = load i32, i32* %arg.addr.epil, align 8
  %add.epil = add nsw i32 %3, %sum.08.epil
  %epil.iter.sub = add i32 %epil.iter, -1
  %epil.iter.cmp = icmp eq i32 %epil.iter.sub, 0
  br i1 %epil.iter.cmp, label %for.cond.cleanup, label %for.body.epil

for.cond.cleanup:                                 ; preds = %for.cond.cleanup.loopexit.unr-lcssa, %for.body.epil, %entry
  %sum.0.lcssa = phi i32 [ 0, %entry ], [ %add.lcssa.ph, %for.cond.cleanup.loopexit.unr-lcssa ], [ %add.epil, %for.body.epil ]
  call void @llvm.va_end(i8* nonnull %0)
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %0)
  ret i32 %sum.0.lcssa

for.body:                                         ; preds = %for.body, %for.body.preheader.new
  %ap.cur = phi i8* [ %ap.cur.pre, %for.body.preheader.new ], [ %ap.next.7, %for.body ]
  %sum.08 = phi i32 [ 0, %for.body.preheader.new ], [ %add.7, %for.body ]
  %niter = phi i32 [ %unroll_iter, %for.body.preheader.new ], [ %niter.nsub.7, %for.body ]
  %ap.next = getelementptr inbounds i8, i8* %ap.cur, i64 8
  store i8* %ap.next, i8** %args, align 8
  %arg.addr = bitcast i8* %ap.cur to i32*
  %4 = load i32, i32* %arg.addr, align 8
  %add = add nsw i32 %4, %sum.08
  %ap.next.1 = getelementptr inbounds i8, i8* %ap.cur, i64 16
  store i8* %ap.next.1, i8** %args, align 8
  %arg.addr.1 = bitcast i8* %ap.next to i32*
  %5 = load i32, i32* %arg.addr.1, align 8
  %add.1 = add nsw i32 %5, %add
  %ap.next.2 = getelementptr inbounds i8, i8* %ap.cur, i64 24
  store i8* %ap.next.2, i8** %args, align 8
  %arg.addr.2 = bitcast i8* %ap.next.1 to i32*
  %6 = load i32, i32* %arg.addr.2, align 8
  %add.2 = add nsw i32 %6, %add.1
  %ap.next.3 = getelementptr inbounds i8, i8* %ap.cur, i64 32
  store i8* %ap.next.3, i8** %args, align 8
  %arg.addr.3 = bitcast i8* %ap.next.2 to i32*
  %7 = load i32, i32* %arg.addr.3, align 8
  %add.3 = add nsw i32 %7, %add.2
  %ap.next.4 = getelementptr inbounds i8, i8* %ap.cur, i64 40
  store i8* %ap.next.4, i8** %args, align 8
  %arg.addr.4 = bitcast i8* %ap.next.3 to i32*
  %8 = load i32, i32* %arg.addr.4, align 8
  %add.4 = add nsw i32 %8, %add.3
  %ap.next.5 = getelementptr inbounds i8, i8* %ap.cur, i64 48
  store i8* %ap.next.5, i8** %args, align 8
  %arg.addr.5 = bitcast i8* %ap.next.4 to i32*
  %9 = load i32, i32* %arg.addr.5, align 8
  %add.5 = add nsw i32 %9, %add.4
  %ap.next.6 = getelementptr inbounds i8, i8* %ap.cur, i64 56
  store i8* %ap.next.6, i8** %args, align 8
  %arg.addr.6 = bitcast i8* %ap.next.5 to i32*
  %10 = load i32, i32* %arg.addr.6, align 8
  %add.6 = add nsw i32 %10, %add.5
  %ap.next.7 = getelementptr inbounds i8, i8* %ap.cur, i64 64
  store i8* %ap.next.7, i8** %args, align 8
  %arg.addr.7 = bitcast i8* %ap.next.6 to i32*
  %11 = load i32, i32* %arg.addr.7, align 8
  %add.7 = add nsw i32 %11, %add.6
  %niter.nsub.7 = add i32 %niter, -8
  %niter.ncmp.7 = icmp eq i32 %niter.nsub.7, 0
  br i1 %niter.ncmp.7, label %for.cond.cleanup.loopexit.unr-lcssa, label %for.body
; CHECK: addd $r12 = $r12, 96
}

declare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture)

declare void @llvm.va_start(i8*)

declare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture)

declare void @llvm.va_end(i8*)
