; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc --frame-pointer=none < %s | FileCheck -check-prefix=FP-NONE %s
; RUN: llc --frame-pointer=all < %s | FileCheck -check-prefix=FP-ALL %s

target triple = "kvx-kalray-cos"

; int fn1(int a, int b, int c, int d, int e,
;         int f, int g, int h, int i, int j,
;         int k, int l, int m, int n, int o) {
;   return a + b + c + d + e +
;          f + g + h + i + j +
;          k + l + m + n + o;
; }

define i32 @fn1(i32 %a, i32 %b, i32 %c, i32 %d, i32 %e, i32 %f, i32 %g, i32 %h, i32 %i, i32 %j, i32 %k, i32 %l, i32 %m, i32 %n, i32 %o) {
; FP-NONE-LABEL: fn1:
; FP-NONE:       # %bb.0: # %entry
; FP-NONE-NEXT:    addw $r0 = $r1, $r0
; FP-NONE-NEXT:    lwz $r1 = 0[$r12]
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    addw $r0 = $r0, $r2
; FP-NONE-NEXT:    lwz $r2 = 8[$r12]
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    addw $r0 = $r0, $r3
; FP-NONE-NEXT:    lwz $r3 = 16[$r12]
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    addw $r0 = $r0, $r4
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    addw $r0 = $r0, $r5
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    addw $r0 = $r0, $r6
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    addw $r0 = $r0, $r7
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    addw $r0 = $r0, $r8
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    addw $r0 = $r0, $r9
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    addw $r0 = $r0, $r10
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    addw $r0 = $r0, $r11
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    addw $r0 = $r0, $r1
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    addw $r0 = $r0, $r2
; FP-NONE-NEXT:    ;;
; FP-NONE-NEXT:    addw $r0 = $r0, $r3
; FP-NONE-NEXT:    ret
; FP-NONE-NEXT:    ;;
;
; FP-ALL-LABEL: fn1:
; FP-ALL:       # %bb.0: # %entry
; FP-ALL-NEXT:    addd $r12 = $r12, -32
; FP-ALL-NEXT:    get $r16 = $ra
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    sd 24[$r12] = $r16
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    sd 16[$r12] = $r14
; FP-ALL-NEXT:    addw $r0 = $r1, $r0
; FP-ALL-NEXT:    addd $r14 = $r12, 16
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    addw $r0 = $r0, $r2
; FP-ALL-NEXT:    lwz $r1 = 16[$r14]
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    addw $r0 = $r0, $r3
; FP-ALL-NEXT:    lwz $r2 = 24[$r14]
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    addw $r0 = $r0, $r4
; FP-ALL-NEXT:    lwz $r3 = 32[$r14]
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    addw $r0 = $r0, $r5
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    addw $r0 = $r0, $r6
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    addw $r0 = $r0, $r7
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    addw $r0 = $r0, $r8
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    addw $r0 = $r0, $r9
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    addw $r0 = $r0, $r10
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    addw $r0 = $r0, $r11
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    addw $r0 = $r0, $r1
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    addw $r0 = $r0, $r2
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    addw $r0 = $r0, $r3
; FP-ALL-NEXT:    addd $r12 = $r14, -16
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    ld $r14 = 16[$r12]
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    ld $r16 = 24[$r12]
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    set $ra = $r16
; FP-ALL-NEXT:    addd $r12 = $r12, 32
; FP-ALL-NEXT:    ;;
; FP-ALL-NEXT:    ret
; FP-ALL-NEXT:    ;;
entry:
  %add = add nsw i32 %b, %a
  %add1 = add nsw i32 %add, %c
  %add2 = add nsw i32 %add1, %d
  %add3 = add nsw i32 %add2, %e
  %add4 = add nsw i32 %add3, %f
  %add5 = add nsw i32 %add4, %g
  %add6 = add nsw i32 %add5, %h
  %add7 = add nsw i32 %add6, %i
  %add8 = add nsw i32 %add7, %j
  %add9 = add nsw i32 %add8, %k
  %add10 = add nsw i32 %add9, %l
  %add11 = add nsw i32 %add10, %m
  %add12 = add nsw i32 %add11, %n
  %add13 = add nsw i32 %add12, %o
  ret i32 %add13
}
