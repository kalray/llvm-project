; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --extra_scrub
; RUN: llc -O2 -o - %s | FileCheck %s -check-prefixes=ALL
; RUN: llc -mcpu=kv3-2 -O2 -o - %s | FileCheck %s -check-prefixes=ALL,NEWBUND
; RUN: llc -switch-old-bundling -mcpu=kv3-2 -O2 -o - %s | FileCheck %s -check-prefixes=ALL,OLDBUND
; RUN: clang -O2 -c -o /dev/null %s
; RUN: clang -O2 -march=kv3-2 -c -o /dev/null %s
; RUN: clang -O2 -mllvm -switch-old-bundling -march=kv3-2 -c -o /dev/null %s

; Code given from @mfeurgard, see T20458
; It uses mainly muludt, addcd and madduzdt instructions

target triple = "kvx-kalray-cos"

define i128 @double_reg_unite(<2 x i64> %v) {
; ALL-LABEL: double_reg_unite:
; ALL:       # %bb.0: # %entry
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
entry:
  %vecext = extractelement <2 x i64> %v, i32 0
  %conv = zext i64 %vecext to i128
  %vecext1 = extractelement <2 x i64> %v, i32 1
  %conv2 = zext i64 %vecext1 to i128
  %shl = shl nuw i128 %conv2, 64
  %or = or i128 %shl, %conv
  ret i128 %or
}

define <2 x i64> @double_reg_split(i128 %v) {
; ALL-LABEL: double_reg_split:
; ALL:       # %bb.0: # %entry
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
entry:
  %conv = trunc i128 %v to i64
  %vecins = insertelement <2 x i64> undef, i64 %conv, i32 0
  %shr = lshr i128 %v, 64
  %conv1 = trunc i128 %shr to i64
  %vecins2 = insertelement <2 x i64> %vecins, i64 %conv1, i32 1
  ret <2 x i64> %vecins2
}

define { i64, i64 } @muludt(i64 %a, i64 %b) {
; NEWBUND-LABEL: muludt:
; NEWBUND:       # %bb.0: # %entry
; NEWBUND-NEXT:    muludt $r0r1 = $r1, $r0
; NEWBUND-NEXT:    ret
; NEWBUND-NEXT:    ;; # (end cycle 0)
;
; OLDBUND-LABEL: muludt:
; OLDBUND:       # %bb.0: # %entry
; OLDBUND-NEXT:    muludt $r0r1 = $r1, $r0
; OLDBUND-NEXT:    ret
; OLDBUND-NEXT:    ;;
entry:
  %conv = zext i64 %a to i128
  %conv1 = zext i64 %b to i128
  %mul = mul nuw i128 %conv1, %conv
  %retval.sroa.0.0.extract.trunc = trunc i128 %mul to i64
  %retval.sroa.2.0.extract.shift = lshr i128 %mul, 64
  %retval.sroa.2.0.extract.trunc = trunc i128 %retval.sroa.2.0.extract.shift to i64
  %.fca.0.insert = insertvalue { i64, i64 } undef, i64 %retval.sroa.0.0.extract.trunc, 0
  %.fca.1.insert = insertvalue { i64, i64 } %.fca.0.insert, i64 %retval.sroa.2.0.extract.trunc, 1
  ret { i64, i64 } %.fca.1.insert
}

define { i64, i64 } @madduzdt(i64 %a, i64 %b, i64 %c.coerce0, i64 %c.coerce1) {
; NEWBUND-LABEL: madduzdt:
; NEWBUND:       # %bb.0: # %entry
; NEWBUND-NEXT:    madduzdt $r2r3 = $r0, $r1
; NEWBUND-NEXT:    ;; # (end cycle 0)
; NEWBUND-NEXT:    copyd $r0 = $r2
; NEWBUND-NEXT:    copyd $r1 = $r3
; NEWBUND-NEXT:    ret
; NEWBUND-NEXT:    ;; # (end cycle 2)
;
; OLDBUND-LABEL: madduzdt:
; OLDBUND:       # %bb.0: # %entry
; OLDBUND-NEXT:    madduzdt $r2r3 = $r0, $r1
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    copyd $r0 = $r2
; OLDBUND-NEXT:    copyd $r1 = $r3
; OLDBUND-NEXT:    ret
; OLDBUND-NEXT:    ;;
entry:
  %c.sroa.0.0.vec.insert = insertelement <2 x i64> undef, i64 %c.coerce0, i32 0
  %c.sroa.0.8.vec.insert = insertelement <2 x i64> %c.sroa.0.0.vec.insert, i64 %c.coerce1, i32 1
  %0 = tail call <2 x i64> @llvm.kvx.madduzdt(<2 x i64> %c.sroa.0.8.vec.insert, i64 %a, i64 %b)
  %retval.sroa.0.0.vec.extract = extractelement <2 x i64> %0, i32 0
  %.fca.0.insert = insertvalue { i64, i64 } undef, i64 %retval.sroa.0.0.vec.extract, 0
  %retval.sroa.0.8.vec.extract = extractelement <2 x i64> %0, i32 1
  %.fca.1.insert = insertvalue { i64, i64 } %.fca.0.insert, i64 %retval.sroa.0.8.vec.extract, 1
  ret { i64, i64 } %.fca.1.insert
}

declare <2 x i64> @llvm.kvx.madduzdt(<2 x i64>, i64, i64)

define { i64, i64 } @triple_add(i64 %a, i64 %b, i64 %c) {
; NEWBUND-LABEL: triple_add:
; NEWBUND:       # %bb.0: # %entry
; NEWBUND-NEXT:    addd $r3 = $r1, $r0
; NEWBUND-NEXT:    ;; # (end cycle 0)
; NEWBUND-NEXT:    addd $r0 = $r3, $r2
; NEWBUND-NEXT:    compd.ltu $r1 = $r3, $r1
; NEWBUND-NEXT:    ;; # (end cycle 1)
; NEWBUND-NEXT:    compd.ltu $r2 = $r0, $r3
; NEWBUND-NEXT:    ;; # (end cycle 2)
; NEWBUND-NEXT:    iorw $r1 = $r1, $r2
; NEWBUND-NEXT:    ret
; NEWBUND-NEXT:    ;; # (end cycle 3)
;
; OLDBUND-LABEL: triple_add:
; OLDBUND:       # %bb.0: # %entry
; OLDBUND-NEXT:    addd $r3 = $r1, $r0
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    addd $r0 = $r3, $r2
; OLDBUND-NEXT:    compd.ltu $r1 = $r3, $r1
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    compd.ltu $r2 = $r0, $r3
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    iorw $r1 = $r1, $r2
; OLDBUND-NEXT:    ret
; OLDBUND-NEXT:    ;;
entry:
  %add = add i64 %b, %a
  %add1 = add i64 %add, %c
  %cmp = icmp ult i64 %add, %a
  %cmp3 = icmp ult i64 %add1, %c
  %or15 = or i1 %cmp, %cmp3
  %conv5 = zext i1 %or15 to i64
  %.fca.0.insert = insertvalue { i64, i64 } undef, i64 %add1, 0
  %.fca.1.insert = insertvalue { i64, i64 } %.fca.0.insert, i64 %conv5, 1
  ret { i64, i64 } %.fca.1.insert
}

define i32 @macc_256(<8 x i64>* nocapture %r, <4 x i64>* nocapture readonly %a, <4 x i64>* nocapture readonly %b) {
; NEWBUND-LABEL: macc_256:
; NEWBUND:       # %bb.0: # %entry
; NEWBUND-NEXT:    lo $r32r33r34r35 = 0[$r2]
; NEWBUND-NEXT:    ;; # (end cycle 0)
; NEWBUND-NEXT:    ld $r2 = 0[$r1]
; NEWBUND-NEXT:    ;; # (end cycle 1)
; NEWBUND-NEXT:    lo $r4r5r6r7 = 0[$r0]
; NEWBUND-NEXT:    ;; # (end cycle 2)
; NEWBUND-NEXT:    muludt $r8r9 = $r2, $r32
; NEWBUND-NEXT:    ;; # (end cycle 4)
; NEWBUND-NEXT:    muludt $r10r11 = $r2, $r33
; NEWBUND-NEXT:    ;; # (end cycle 5)
; NEWBUND-NEXT:    muludt $r16r17 = $r2, $r34
; NEWBUND-NEXT:    addcd.i $r4 = $r8, $r4
; NEWBUND-NEXT:    ;; # (end cycle 6)
; NEWBUND-NEXT:    muludt $r2r3 = $r2, $r35
; NEWBUND-NEXT:    addcd $r5 = $r10, $r5
; NEWBUND-NEXT:    ;; # (end cycle 7)
; NEWBUND-NEXT:    addcd $r6 = $r16, $r6
; NEWBUND-NEXT:    ;; # (end cycle 8)
; NEWBUND-NEXT:    addcd $r7 = $r2, $r7
; NEWBUND-NEXT:    ;; # (end cycle 9)
; NEWBUND-NEXT:    ld $r15 = 8[$r1]
; NEWBUND-NEXT:    compd.ltu $r36 = $r7, $r2
; NEWBUND-NEXT:    ;; # (end cycle 10)
; NEWBUND-NEXT:    madduzdt $r8r9 = $r15, $r32
; NEWBUND-NEXT:    ;; # (end cycle 13)
; NEWBUND-NEXT:    madduzdt $r10r11 = $r15, $r33
; NEWBUND-NEXT:    ;; # (end cycle 14)
; NEWBUND-NEXT:    madduzdt $r16r17 = $r15, $r34
; NEWBUND-NEXT:    addcd.i $r5 = $r8, $r5
; NEWBUND-NEXT:    ;; # (end cycle 15)
; NEWBUND-NEXT:    madduzdt $r2r3 = $r15, $r35
; NEWBUND-NEXT:    addcd $r6 = $r10, $r6
; NEWBUND-NEXT:    ;; # (end cycle 16)
; NEWBUND-NEXT:    addcd $r7 = $r16, $r7
; NEWBUND-NEXT:    ;; # (end cycle 17)
; NEWBUND-NEXT:    addcd $r15 = $r2, $r36
; NEWBUND-NEXT:    ;; # (end cycle 18)
; NEWBUND-NEXT:    ld $r36 = 16[$r1]
; NEWBUND-NEXT:    compd.ltu $r37 = $r15, $r2
; NEWBUND-NEXT:    ;; # (end cycle 19)
; NEWBUND-NEXT:    madduzdt $r8r9 = $r36, $r32
; NEWBUND-NEXT:    ;; # (end cycle 22)
; NEWBUND-NEXT:    madduzdt $r10r11 = $r36, $r33
; NEWBUND-NEXT:    ;; # (end cycle 23)
; NEWBUND-NEXT:    madduzdt $r16r17 = $r36, $r34
; NEWBUND-NEXT:    addcd.i $r6 = $r8, $r6
; NEWBUND-NEXT:    ;; # (end cycle 24)
; NEWBUND-NEXT:    madduzdt $r2r3 = $r36, $r35
; NEWBUND-NEXT:    addcd $r7 = $r10, $r7
; NEWBUND-NEXT:    ;; # (end cycle 25)
; NEWBUND-NEXT:    addcd $r15 = $r16, $r15
; NEWBUND-NEXT:    ;; # (end cycle 26)
; NEWBUND-NEXT:    addcd $r36 = $r2, $r37
; NEWBUND-NEXT:    ;; # (end cycle 27)
; NEWBUND-NEXT:    ld $r1 = 24[$r1]
; NEWBUND-NEXT:    ;; # (end cycle 28)
; NEWBUND-NEXT:    madduzdt $r8r9 = $r1, $r32
; NEWBUND-NEXT:    ;; # (end cycle 31)
; NEWBUND-NEXT:    madduzdt $r10r11 = $r1, $r33
; NEWBUND-NEXT:    ;; # (end cycle 32)
; NEWBUND-NEXT:    madduzdt $r16r17 = $r1, $r34
; NEWBUND-NEXT:    addcd.i $r7 = $r8, $r7
; NEWBUND-NEXT:    compd.ltu $r8 = $r36, $r2
; NEWBUND-NEXT:    ;; # (end cycle 33)
; NEWBUND-NEXT:    madduzdt $r2r3 = $r1, $r35
; NEWBUND-NEXT:    addcd $r1 = $r10, $r15
; NEWBUND-NEXT:    ;; # (end cycle 34)
; NEWBUND-NEXT:    addcd $r10 = $r16, $r36
; NEWBUND-NEXT:    ;; # (end cycle 35)
; NEWBUND-NEXT:    addcd $r8 = $r2, $r8
; NEWBUND-NEXT:    ;; # (end cycle 36)
; NEWBUND-NEXT:    compd.ltu $r1 = $r8, $r2
; NEWBUND-NEXT:    addcd.i $r32 = $r9, $r1
; NEWBUND-NEXT:    ;; # (end cycle 37)
; NEWBUND-NEXT:    addcd $r33 = $r11, $r10
; NEWBUND-NEXT:    ;; # (end cycle 38)
; NEWBUND-NEXT:    addcd $r34 = $r17, $r8
; NEWBUND-NEXT:    ;; # (end cycle 39)
; NEWBUND-NEXT:    so 0[$r0] = $r4r5r6r7
; NEWBUND-NEXT:    addcd $r35 = $r3, $r1
; NEWBUND-NEXT:    ;; # (end cycle 40)
; NEWBUND-NEXT:    so 32[$r0] = $r32r33r34r35
; NEWBUND-NEXT:    compd.ltu $r1 = $r35, $r3
; NEWBUND-NEXT:    ;; # (end cycle 41)
; NEWBUND-NEXT:    copyd $r0 = $r1
; NEWBUND-NEXT:    ret
; NEWBUND-NEXT:    ;; # (end cycle 42)
;
; OLDBUND-LABEL: macc_256:
; OLDBUND:       # %bb.0: # %entry
; OLDBUND-NEXT:    lo $r32r33r34r35 = 0[$r2]
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    ld $r2 = 0[$r1]
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    muludt $r8r9 = $r2, $r32
; OLDBUND-NEXT:    lo $r4r5r6r7 = 0[$r0]
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    muludt $r10r11 = $r2, $r33
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    muludt $r16r17 = $r2, $r34
; OLDBUND-NEXT:    addcd.i $r4 = $r8, $r4
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    muludt $r2r3 = $r2, $r35
; OLDBUND-NEXT:    addcd $r5 = $r10, $r5
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    addcd $r6 = $r16, $r6
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    addcd $r7 = $r2, $r7
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    ld $r15 = 8[$r1]
; OLDBUND-NEXT:    compd.ltu $r36 = $r7, $r2
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    madduzdt $r8r9 = $r15, $r32
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    madduzdt $r10r11 = $r15, $r33
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    madduzdt $r16r17 = $r15, $r34
; OLDBUND-NEXT:    addcd.i $r5 = $r8, $r5
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    madduzdt $r2r3 = $r15, $r35
; OLDBUND-NEXT:    addcd $r6 = $r10, $r6
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    addcd $r7 = $r16, $r7
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    addcd $r15 = $r2, $r36
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    ld $r36 = 16[$r1]
; OLDBUND-NEXT:    compd.ltu $r37 = $r15, $r2
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    madduzdt $r8r9 = $r36, $r32
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    madduzdt $r10r11 = $r36, $r33
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    madduzdt $r16r17 = $r36, $r34
; OLDBUND-NEXT:    addcd.i $r6 = $r8, $r6
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    madduzdt $r2r3 = $r36, $r35
; OLDBUND-NEXT:    addcd $r7 = $r10, $r7
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    addcd $r15 = $r16, $r15
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    addcd $r36 = $r2, $r37
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    ld $r1 = 24[$r1]
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    madduzdt $r8r9 = $r1, $r32
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    madduzdt $r10r11 = $r1, $r33
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    madduzdt $r16r17 = $r1, $r34
; OLDBUND-NEXT:    addcd.i $r7 = $r8, $r7
; OLDBUND-NEXT:    compd.ltu $r8 = $r36, $r2
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    madduzdt $r2r3 = $r1, $r35
; OLDBUND-NEXT:    addcd $r1 = $r10, $r15
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    addcd $r10 = $r16, $r36
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    addcd $r8 = $r2, $r8
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    compd.ltu $r1 = $r8, $r2
; OLDBUND-NEXT:    addcd.i $r32 = $r9, $r1
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    addcd $r33 = $r11, $r10
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    addcd $r34 = $r17, $r8
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    so 0[$r0] = $r4r5r6r7
; OLDBUND-NEXT:    addcd $r35 = $r3, $r1
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    so 32[$r0] = $r32r33r34r35
; OLDBUND-NEXT:    compd.ltu $r1 = $r35, $r3
; OLDBUND-NEXT:    ;;
; OLDBUND-NEXT:    copyd $r0 = $r1
; OLDBUND-NEXT:    ret
; OLDBUND-NEXT:    ;;
entry:
  %0 = load <8 x i64>, <8 x i64>* %r
  %1 = load <4 x i64>, <4 x i64>* %b
  %2 = load <4 x i64>, <4 x i64>* %a
  %vecext = extractelement <4 x i64> %2, i32 0
  %vecext1 = extractelement <4 x i64> %1, i32 0
  %conv.i = zext i64 %vecext to i128
  %conv1.i = zext i64 %vecext1 to i128
  %mul.i = mul nuw i128 %conv.i, %conv1.i
  %retval.sroa.0.0.extract.trunc.i = trunc i128 %mul.i to i64
  %retval.sroa.2.0.extract.shift.i = lshr i128 %mul.i, 64
  %retval.sroa.2.0.extract.trunc.i = trunc i128 %retval.sroa.2.0.extract.shift.i to i64
  %vecext5 = extractelement <4 x i64> %1, i32 1
  %conv1.i571 = zext i64 %vecext5 to i128
  %mul.i572 = mul nuw i128 %conv.i, %conv1.i571
  %retval.sroa.0.0.extract.trunc.i573 = trunc i128 %mul.i572 to i64
  %retval.sroa.2.0.extract.shift.i574 = lshr i128 %mul.i572, 64
  %retval.sroa.2.0.extract.trunc.i575 = trunc i128 %retval.sroa.2.0.extract.shift.i574 to i64
  %vecext11 = extractelement <4 x i64> %1, i32 2
  %conv1.i563 = zext i64 %vecext11 to i128
  %mul.i564 = mul nuw i128 %conv.i, %conv1.i563
  %retval.sroa.0.0.extract.trunc.i565 = trunc i128 %mul.i564 to i64
  %retval.sroa.2.0.extract.shift.i566 = lshr i128 %mul.i564, 64
  %retval.sroa.2.0.extract.trunc.i567 = trunc i128 %retval.sroa.2.0.extract.shift.i566 to i64
  %vecext17 = extractelement <4 x i64> %1, i32 3
  %conv1.i555 = zext i64 %vecext17 to i128
  %mul.i556 = mul nuw i128 %conv.i, %conv1.i555
  %retval.sroa.0.0.extract.trunc.i557 = trunc i128 %mul.i556 to i64
  %retval.sroa.2.0.extract.shift.i558 = lshr i128 %mul.i556, 64
  %retval.sroa.2.0.extract.trunc.i559 = trunc i128 %retval.sroa.2.0.extract.shift.i558 to i64
  %vecext23 = extractelement <8 x i64> %0, i32 0
  %3 = tail call i64 @llvm.kvx.addcd(i64 %retval.sroa.0.0.extract.trunc.i, i64 %vecext23, i32 1)
  %vecins = insertelement <8 x i64> poison, i64 %3, i32 0
  %vecext27 = extractelement <8 x i64> %0, i32 1
  %4 = tail call i64 @llvm.kvx.addcd(i64 %retval.sroa.0.0.extract.trunc.i573, i64 %vecext27, i32 0)
  %vecext32 = extractelement <8 x i64> %0, i32 2
  %5 = tail call i64 @llvm.kvx.addcd(i64 %retval.sroa.0.0.extract.trunc.i565, i64 %vecext32, i32 0)
  %vecext37 = extractelement <8 x i64> %0, i32 3
  %6 = tail call i64 @llvm.kvx.addcd(i64 %retval.sroa.0.0.extract.trunc.i557, i64 %vecext37, i32 0)
  %cmp = icmp ult i64 %6, %retval.sroa.0.0.extract.trunc.i557
  %conv43 = zext i1 %cmp to i64
  %7 = load <4 x i64>, <4 x i64>* %a
  %vecext47 = extractelement <4 x i64> %7, i32 1
  %c.sroa.0.0.vec.insert.i548 = insertelement <2 x i64> undef, i64 %retval.sroa.0.0.extract.trunc.i, i32 0
  %c.sroa.0.8.vec.insert.i549 = insertelement <2 x i64> %c.sroa.0.0.vec.insert.i548, i64 %retval.sroa.2.0.extract.trunc.i, i32 1
  %8 = tail call <2 x i64> @llvm.kvx.madduzdt(<2 x i64> %c.sroa.0.8.vec.insert.i549, i64 %vecext47, i64 %vecext1)
  %retval.sroa.0.0.vec.extract.i550 = extractelement <2 x i64> %8, i32 0
  %c.sroa.0.0.vec.insert.i542 = insertelement <2 x i64> undef, i64 %retval.sroa.0.0.extract.trunc.i573, i32 0
  %c.sroa.0.8.vec.insert.i543 = insertelement <2 x i64> %c.sroa.0.0.vec.insert.i542, i64 %retval.sroa.2.0.extract.trunc.i575, i32 1
  %9 = tail call <2 x i64> @llvm.kvx.madduzdt(<2 x i64> %c.sroa.0.8.vec.insert.i543, i64 %vecext47, i64 %vecext5)
  %retval.sroa.0.0.vec.extract.i544 = extractelement <2 x i64> %9, i32 0
  %c.sroa.0.0.vec.insert.i536 = insertelement <2 x i64> undef, i64 %retval.sroa.0.0.extract.trunc.i565, i32 0
  %c.sroa.0.8.vec.insert.i537 = insertelement <2 x i64> %c.sroa.0.0.vec.insert.i536, i64 %retval.sroa.2.0.extract.trunc.i567, i32 1
  %10 = tail call <2 x i64> @llvm.kvx.madduzdt(<2 x i64> %c.sroa.0.8.vec.insert.i537, i64 %vecext47, i64 %vecext11)
  %retval.sroa.0.0.vec.extract.i538 = extractelement <2 x i64> %10, i32 0
  %c.sroa.0.0.vec.insert.i530 = insertelement <2 x i64> undef, i64 %retval.sroa.0.0.extract.trunc.i557, i32 0
  %c.sroa.0.8.vec.insert.i531 = insertelement <2 x i64> %c.sroa.0.0.vec.insert.i530, i64 %retval.sroa.2.0.extract.trunc.i559, i32 1
  %11 = tail call <2 x i64> @llvm.kvx.madduzdt(<2 x i64> %c.sroa.0.8.vec.insert.i531, i64 %vecext47, i64 %vecext17)
  %retval.sroa.0.0.vec.extract.i532 = extractelement <2 x i64> %11, i32 0
  %12 = tail call i64 @llvm.kvx.addcd(i64 %retval.sroa.0.0.vec.extract.i550, i64 %4, i32 1)
  %vecins77 = insertelement <8 x i64> %vecins, i64 %12, i32 1
  %13 = tail call i64 @llvm.kvx.addcd(i64 %retval.sroa.0.0.vec.extract.i544, i64 %5, i32 0)
  %14 = tail call i64 @llvm.kvx.addcd(i64 %retval.sroa.0.0.vec.extract.i538, i64 %6, i32 0)
  %15 = tail call i64 @llvm.kvx.addcd(i64 %retval.sroa.0.0.vec.extract.i532, i64 %conv43, i32 0)
  %cmp97 = icmp ult i64 %15, %retval.sroa.0.0.vec.extract.i532
  %conv99 = zext i1 %cmp97 to i64
  %16 = load <4 x i64>, <4 x i64>* %a
  %vecext103 = extractelement <4 x i64> %16, i32 2
  %17 = tail call <2 x i64> @llvm.kvx.madduzdt(<2 x i64> %8, i64 %vecext103, i64 %vecext1)
  %retval.sroa.0.0.vec.extract.i526 = extractelement <2 x i64> %17, i32 0
  %18 = tail call <2 x i64> @llvm.kvx.madduzdt(<2 x i64> %9, i64 %vecext103, i64 %vecext5)
  %retval.sroa.0.0.vec.extract.i520 = extractelement <2 x i64> %18, i32 0
  %19 = tail call <2 x i64> @llvm.kvx.madduzdt(<2 x i64> %10, i64 %vecext103, i64 %vecext11)
  %retval.sroa.0.0.vec.extract.i514 = extractelement <2 x i64> %19, i32 0
  %20 = tail call <2 x i64> @llvm.kvx.madduzdt(<2 x i64> %11, i64 %vecext103, i64 %vecext17)
  %retval.sroa.0.0.vec.extract.i508 = extractelement <2 x i64> %20, i32 0
  %21 = tail call i64 @llvm.kvx.addcd(i64 %retval.sroa.0.0.vec.extract.i526, i64 %13, i32 1)
  %vecins133 = insertelement <8 x i64> %vecins77, i64 %21, i32 2
  %22 = tail call i64 @llvm.kvx.addcd(i64 %retval.sroa.0.0.vec.extract.i520, i64 %14, i32 0)
  %23 = tail call i64 @llvm.kvx.addcd(i64 %retval.sroa.0.0.vec.extract.i514, i64 %15, i32 0)
  %24 = tail call i64 @llvm.kvx.addcd(i64 %retval.sroa.0.0.vec.extract.i508, i64 %conv99, i32 0)
  %cmp153 = icmp ult i64 %24, %retval.sroa.0.0.vec.extract.i508
  %conv155 = zext i1 %cmp153 to i64
  %25 = load <4 x i64>, <4 x i64>* %a
  %vecext159 = extractelement <4 x i64> %25, i32 3
  %26 = tail call <2 x i64> @llvm.kvx.madduzdt(<2 x i64> %17, i64 %vecext159, i64 %vecext1)
  %retval.sroa.0.0.vec.extract.i502 = extractelement <2 x i64> %26, i32 0
  %retval.sroa.0.8.vec.extract.i504 = extractelement <2 x i64> %26, i32 1
  %27 = tail call <2 x i64> @llvm.kvx.madduzdt(<2 x i64> %18, i64 %vecext159, i64 %vecext5)
  %retval.sroa.0.0.vec.extract.i496 = extractelement <2 x i64> %27, i32 0
  %retval.sroa.0.8.vec.extract.i498 = extractelement <2 x i64> %27, i32 1
  %28 = tail call <2 x i64> @llvm.kvx.madduzdt(<2 x i64> %19, i64 %vecext159, i64 %vecext11)
  %retval.sroa.0.0.vec.extract.i490 = extractelement <2 x i64> %28, i32 0
  %retval.sroa.0.8.vec.extract.i492 = extractelement <2 x i64> %28, i32 1
  %29 = tail call <2 x i64> @llvm.kvx.madduzdt(<2 x i64> %20, i64 %vecext159, i64 %vecext17)
  %retval.sroa.0.0.vec.extract.i = extractelement <2 x i64> %29, i32 0
  %retval.sroa.0.8.vec.extract.i = extractelement <2 x i64> %29, i32 1
  %30 = tail call i64 @llvm.kvx.addcd(i64 %retval.sroa.0.0.vec.extract.i502, i64 %22, i32 1)
  %vecins189 = insertelement <8 x i64> %vecins133, i64 %30, i32 3
  %31 = tail call i64 @llvm.kvx.addcd(i64 %retval.sroa.0.0.vec.extract.i496, i64 %23, i32 0)
  %32 = tail call i64 @llvm.kvx.addcd(i64 %retval.sroa.0.0.vec.extract.i490, i64 %24, i32 0)
  %33 = tail call i64 @llvm.kvx.addcd(i64 %retval.sroa.0.0.vec.extract.i, i64 %conv155, i32 0)
  %cmp209 = icmp ult i64 %33, %retval.sroa.0.0.vec.extract.i
  %conv211 = zext i1 %cmp209 to i64
  %34 = tail call i64 @llvm.kvx.addcd(i64 %retval.sroa.0.8.vec.extract.i504, i64 %31, i32 1)
  %vecins217 = insertelement <8 x i64> %vecins189, i64 %34, i32 4
  %35 = tail call i64 @llvm.kvx.addcd(i64 %retval.sroa.0.8.vec.extract.i498, i64 %32, i32 0)
  %vecins222 = insertelement <8 x i64> %vecins217, i64 %35, i32 5
  %36 = tail call i64 @llvm.kvx.addcd(i64 %retval.sroa.0.8.vec.extract.i492, i64 %33, i32 0)
  %vecins227 = insertelement <8 x i64> %vecins222, i64 %36, i32 6
  %37 = tail call i64 @llvm.kvx.addcd(i64 %retval.sroa.0.8.vec.extract.i, i64 %conv211, i32 0)
  %vecins232 = insertelement <8 x i64> %vecins227, i64 %37, i32 7
  store <8 x i64> %vecins232, <8 x i64>* %r
  %cmp237 = icmp ult i64 %37, %retval.sroa.0.8.vec.extract.i
  %conv238 = zext i1 %cmp237 to i32
  ret i32 %conv238
}

declare i64 @llvm.kvx.addcd(i64, i64, i32)

