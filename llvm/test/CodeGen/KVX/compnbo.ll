; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mcpu=kv3-1 -O2 -o - %s | FileCheck %s --check-prefixes=CV1
; RUN: llc -mcpu=kv3-2 -O2 -o - %s | FileCheck %s --check-prefixes=CV2
target triple = "kvx-kalray-cos"

; TODO: Could use HQ instructions for cv1 here
define <8 x i8> @eq(<8 x i8> %a, <8 x i8> %b){
; CV1-LABEL: eq:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    sbmm8 $r2 = $r1, 0x80004000200010
; CV1-NEXT:    sbmm8 $r3 = $r0, 0x80004000200010
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x8000400020001
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x8000400020001
; CV1-NEXT:    compnhq.eq $r2 = $r3, $r2
; CV1-NEXT:    ;;
; CV1-NEXT:    compnhq.eq $r0 = $r0, $r1
; CV1-NEXT:    sbmm8 $r2 = $r2, 0x40100401
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    srlw $r1 = $r2, 24
; CV1-NEXT:    extfz $r3 = $r2, 23, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r2 = $r2
; CV1-NEXT:    extfz $r4 = $r2, 15, 8
; CV1-NEXT:    srlw $r5 = $r0, 24
; CV1-NEXT:    extfz $r6 = $r0, 23, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    extfs $r1 = $r1, 0, 0
; CV1-NEXT:    extfz $r7 = $r0, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r3 = $r3, 0, 0
; CV1-NEXT:    extfs $r4 = $r4, 0, 0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r2 = $r2, 0, 0
; CV1-NEXT:    extfs $r5 = $r5, 0, 0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r6 = $r6, 0, 0
; CV1-NEXT:    extfs $r7 = $r7, 0, 0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r0 = $r0, 0, 0
; CV1-NEXT:    insf $r3 = $r1, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r4, 15, 8
; CV1-NEXT:    insf $r6 = $r5, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r7, 15, 8
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r6, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r2, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: eq:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    compnbo.eq $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
entry:
  %cmp = icmp eq <8 x i8> %a, %b
  %sext = sext <8 x i1> %cmp to <8 x i8>
  ret <8 x i8> %sext
}

define <8 x i8> @neq(<8 x i8> %a, <8 x i8> %b){
; CV1-LABEL: neq:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    sbmm8 $r2 = $r1, 0x80004000200010
; CV1-NEXT:    sbmm8 $r3 = $r0, 0x80004000200010
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x8000400020001
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x8000400020001
; CV1-NEXT:    compnhq.ne $r2 = $r3, $r2
; CV1-NEXT:    ;;
; CV1-NEXT:    compnhq.ne $r0 = $r0, $r1
; CV1-NEXT:    sbmm8 $r2 = $r2, 0x40100401
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    srlw $r1 = $r2, 24
; CV1-NEXT:    extfz $r3 = $r2, 23, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r2 = $r2
; CV1-NEXT:    extfz $r4 = $r2, 15, 8
; CV1-NEXT:    srlw $r5 = $r0, 24
; CV1-NEXT:    extfz $r6 = $r0, 23, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    extfs $r1 = $r1, 0, 0
; CV1-NEXT:    extfz $r7 = $r0, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r3 = $r3, 0, 0
; CV1-NEXT:    extfs $r4 = $r4, 0, 0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r2 = $r2, 0, 0
; CV1-NEXT:    extfs $r5 = $r5, 0, 0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r6 = $r6, 0, 0
; CV1-NEXT:    extfs $r7 = $r7, 0, 0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r0 = $r0, 0, 0
; CV1-NEXT:    insf $r3 = $r1, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r4, 15, 8
; CV1-NEXT:    insf $r6 = $r5, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r7, 15, 8
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r6, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r2, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: neq:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    compnbo.ne $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
entry:
  %cmp = icmp ne <8 x i8> %a, %b
  %sext = sext <8 x i1> %cmp to <8 x i8>
  ret <8 x i8> %sext
}

define <8 x i8> @gt(<8 x i8> %a, <8 x i8> %b){
; CV1-LABEL: gt:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    sxmbhq $r2 = $r1
; CV1-NEXT:    sxmbhq $r3 = $r0
; CV1-NEXT:    ;;
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    compnhq.gt $r2 = $r3, $r2
; CV1-NEXT:    ;;
; CV1-NEXT:    compnhq.gt $r0 = $r0, $r1
; CV1-NEXT:    sbmm8 $r2 = $r2, 0x40100401
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    srlw $r1 = $r2, 24
; CV1-NEXT:    extfz $r3 = $r2, 23, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r2 = $r2
; CV1-NEXT:    extfz $r4 = $r2, 15, 8
; CV1-NEXT:    srlw $r5 = $r0, 24
; CV1-NEXT:    extfz $r6 = $r0, 23, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    extfs $r1 = $r1, 0, 0
; CV1-NEXT:    extfz $r7 = $r0, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r3 = $r3, 0, 0
; CV1-NEXT:    extfs $r4 = $r4, 0, 0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r2 = $r2, 0, 0
; CV1-NEXT:    extfs $r5 = $r5, 0, 0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r6 = $r6, 0, 0
; CV1-NEXT:    extfs $r7 = $r7, 0, 0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r0 = $r0, 0, 0
; CV1-NEXT:    insf $r3 = $r1, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r4, 15, 8
; CV1-NEXT:    insf $r6 = $r5, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r7, 15, 8
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r6, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r2, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: gt:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    compnbo.gt $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
entry:
  %cmp = icmp sgt <8 x i8> %a, %b
  %sext = sext <8 x i1> %cmp to <8 x i8>
  ret <8 x i8> %sext
}

define <8 x i8> @lt(<8 x i8> %a, <8 x i8> %b){
; CV1-LABEL: lt:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    sxmbhq $r2 = $r1
; CV1-NEXT:    sxmbhq $r3 = $r0
; CV1-NEXT:    ;;
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    compnhq.lt $r2 = $r3, $r2
; CV1-NEXT:    ;;
; CV1-NEXT:    compnhq.lt $r0 = $r0, $r1
; CV1-NEXT:    sbmm8 $r2 = $r2, 0x40100401
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    srlw $r1 = $r2, 24
; CV1-NEXT:    extfz $r3 = $r2, 23, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r2 = $r2
; CV1-NEXT:    extfz $r4 = $r2, 15, 8
; CV1-NEXT:    srlw $r5 = $r0, 24
; CV1-NEXT:    extfz $r6 = $r0, 23, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    extfs $r1 = $r1, 0, 0
; CV1-NEXT:    extfz $r7 = $r0, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r3 = $r3, 0, 0
; CV1-NEXT:    extfs $r4 = $r4, 0, 0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r2 = $r2, 0, 0
; CV1-NEXT:    extfs $r5 = $r5, 0, 0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r6 = $r6, 0, 0
; CV1-NEXT:    extfs $r7 = $r7, 0, 0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r0 = $r0, 0, 0
; CV1-NEXT:    insf $r3 = $r1, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r4, 15, 8
; CV1-NEXT:    insf $r6 = $r5, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r7, 15, 8
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r6, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r2, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: lt:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    compnbo.lt $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
entry:
  %cmp = icmp slt <8 x i8> %a, %b
  %sext = sext <8 x i1> %cmp to <8 x i8>
  ret <8 x i8> %sext
}

define <8 x i8> @gte(<8 x i8> %a, <8 x i8> %b){
; CV1-LABEL: gte:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    sxmbhq $r2 = $r1
; CV1-NEXT:    sxmbhq $r3 = $r0
; CV1-NEXT:    ;;
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    compnhq.ge $r2 = $r3, $r2
; CV1-NEXT:    ;;
; CV1-NEXT:    compnhq.ge $r0 = $r0, $r1
; CV1-NEXT:    sbmm8 $r2 = $r2, 0x40100401
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    srlw $r1 = $r2, 24
; CV1-NEXT:    extfz $r3 = $r2, 23, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r2 = $r2
; CV1-NEXT:    extfz $r4 = $r2, 15, 8
; CV1-NEXT:    srlw $r5 = $r0, 24
; CV1-NEXT:    extfz $r6 = $r0, 23, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    extfs $r1 = $r1, 0, 0
; CV1-NEXT:    extfz $r7 = $r0, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r3 = $r3, 0, 0
; CV1-NEXT:    extfs $r4 = $r4, 0, 0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r2 = $r2, 0, 0
; CV1-NEXT:    extfs $r5 = $r5, 0, 0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r6 = $r6, 0, 0
; CV1-NEXT:    extfs $r7 = $r7, 0, 0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r0 = $r0, 0, 0
; CV1-NEXT:    insf $r3 = $r1, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r4, 15, 8
; CV1-NEXT:    insf $r6 = $r5, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r7, 15, 8
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r6, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r2, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: gte:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    compnbo.ge $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
entry:
  %cmp = icmp sge <8 x i8> %a, %b
  %sext = sext <8 x i1> %cmp to <8 x i8>
  ret <8 x i8> %sext
}

define <8 x i8> @lte(<8 x i8> %a, <8 x i8> %b){
; CV1-LABEL: lte:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    sxmbhq $r2 = $r1
; CV1-NEXT:    sxmbhq $r3 = $r0
; CV1-NEXT:    ;;
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    compnhq.le $r2 = $r3, $r2
; CV1-NEXT:    ;;
; CV1-NEXT:    compnhq.le $r0 = $r0, $r1
; CV1-NEXT:    sbmm8 $r2 = $r2, 0x40100401
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    srlw $r1 = $r2, 24
; CV1-NEXT:    extfz $r3 = $r2, 23, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r2 = $r2
; CV1-NEXT:    extfz $r4 = $r2, 15, 8
; CV1-NEXT:    srlw $r5 = $r0, 24
; CV1-NEXT:    extfz $r6 = $r0, 23, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    extfs $r1 = $r1, 0, 0
; CV1-NEXT:    extfz $r7 = $r0, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r3 = $r3, 0, 0
; CV1-NEXT:    extfs $r4 = $r4, 0, 0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r2 = $r2, 0, 0
; CV1-NEXT:    extfs $r5 = $r5, 0, 0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r6 = $r6, 0, 0
; CV1-NEXT:    extfs $r7 = $r7, 0, 0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r0 = $r0, 0, 0
; CV1-NEXT:    insf $r3 = $r1, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r4, 15, 8
; CV1-NEXT:    insf $r6 = $r5, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r7, 15, 8
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r6, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r2, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: lte:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    compnbo.le $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
entry:
  %cmp = icmp sle <8 x i8> %a, %b
  %sext = sext <8 x i1> %cmp to <8 x i8>
  ret <8 x i8> %sext
}

define <4 x i8> @v2_eq(<4 x i8> %a, <4 x i8> %b){
; CV1-LABEL: v2_eq:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x8000400020001
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x8000400020001
; CV1-NEXT:    ;;
; CV1-NEXT:    compnhq.eq $r0 = $r0, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    extfz $r3 = $r0, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r1 = $r1, 0, 0
; CV1-NEXT:    extfs $r2 = $r2, 0, 0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r0 = $r0, 0, 0
; CV1-NEXT:    extfs $r3 = $r3, 0, 0
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r3, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r2, 31, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: v2_eq:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    compnbo.eq $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
entry:
  %cmp = icmp eq <4 x i8> %a, %b
  %sext = sext <4 x i1> %cmp to <4 x i8>
  ret <4 x i8> %sext
}

define <4 x i8> @v2_neq(<4 x i8> %a, <4 x i8> %b){
; CV1-LABEL: v2_neq:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x8000400020001
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x8000400020001
; CV1-NEXT:    ;;
; CV1-NEXT:    compnhq.ne $r0 = $r0, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    extfz $r3 = $r0, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r1 = $r1, 0, 0
; CV1-NEXT:    extfs $r2 = $r2, 0, 0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r0 = $r0, 0, 0
; CV1-NEXT:    extfs $r3 = $r3, 0, 0
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r3, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r2, 31, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: v2_neq:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    compnbo.ne $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
entry:
  %cmp = icmp ne <4 x i8> %a, %b
  %sext = sext <4 x i1> %cmp to <4 x i8>
  ret <4 x i8> %sext
}

define <4 x i8> @v2_gt(<4 x i8> %a, <4 x i8> %b){
; CV1-LABEL: v2_gt:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    compnhq.gt $r0 = $r0, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    extfz $r3 = $r0, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r1 = $r1, 0, 0
; CV1-NEXT:    extfs $r2 = $r2, 0, 0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r0 = $r0, 0, 0
; CV1-NEXT:    extfs $r3 = $r3, 0, 0
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r3, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r2, 31, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: v2_gt:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    compnbo.gt $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
entry:
  %cmp = icmp sgt <4 x i8> %a, %b
  %sext = sext <4 x i1> %cmp to <4 x i8>
  ret <4 x i8> %sext
}

define <4 x i8> @v2_lt(<4 x i8> %a, <4 x i8> %b){
; CV1-LABEL: v2_lt:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    compnhq.lt $r0 = $r0, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    extfz $r3 = $r0, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r1 = $r1, 0, 0
; CV1-NEXT:    extfs $r2 = $r2, 0, 0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r0 = $r0, 0, 0
; CV1-NEXT:    extfs $r3 = $r3, 0, 0
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r3, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r2, 31, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: v2_lt:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    compnbo.lt $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
entry:
  %cmp = icmp slt <4 x i8> %a, %b
  %sext = sext <4 x i1> %cmp to <4 x i8>
  ret <4 x i8> %sext
}

define <4 x i8> @v2_gte(<4 x i8> %a, <4 x i8> %b){
; CV1-LABEL: v2_gte:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    compnhq.ge $r0 = $r0, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    extfz $r3 = $r0, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r1 = $r1, 0, 0
; CV1-NEXT:    extfs $r2 = $r2, 0, 0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r0 = $r0, 0, 0
; CV1-NEXT:    extfs $r3 = $r3, 0, 0
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r3, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r2, 31, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: v2_gte:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    compnbo.ge $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
entry:
  %cmp = icmp sge <4 x i8> %a, %b
  %sext = sext <4 x i1> %cmp to <4 x i8>
  ret <4 x i8> %sext
}

define <4 x i8> @v2_lte(<4 x i8> %a, <4 x i8> %b){
; CV1-LABEL: v2_lte:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    compnhq.le $r0 = $r0, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    extfz $r3 = $r0, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r1 = $r1, 0, 0
; CV1-NEXT:    extfs $r2 = $r2, 0, 0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r0 = $r0, 0, 0
; CV1-NEXT:    extfs $r3 = $r3, 0, 0
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r3, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r2, 31, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: v2_lte:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    compnbo.le $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
entry:
  %cmp = icmp sle <4 x i8> %a, %b
  %sext = sext <4 x i1> %cmp to <4 x i8>
  ret <4 x i8> %sext
}



define <8 x i8> @azext(<8 x i8> %a, <8 x i8> %b){
; CV1-LABEL: azext:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    sbmm8 $r2 = $r1, 0x80004000200010
; CV1-NEXT:    sbmm8 $r3 = $r0, 0x80004000200010
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x8000400020001
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x8000400020001
; CV1-NEXT:    compnhq.eq $r2 = $r3, $r2
; CV1-NEXT:    ;;
; CV1-NEXT:    compnhq.eq $r0 = $r0, $r1
; CV1-NEXT:    sbmm8 $r1 = $r2, 0x40100401
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r1, 63, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    andd $r0 = $r0, 0x101010101010101
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: azext:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    compnbo.eq $r0 = $r0, $r1
; CV2-NEXT:    ;;
; CV2-NEXT:    andd $r0 = $r0, 0x101010101010101
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
entry:
  %cmp = icmp eq <8 x i8> %a, %b
  %zext = zext <8 x i1> %cmp to <8 x i8>
  ret <8 x i8> %zext
}
