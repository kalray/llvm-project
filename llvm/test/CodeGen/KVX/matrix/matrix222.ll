; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mcpu=kv3-1 -O2 -o - %s | FileCheck %s --check-prefix=V1
; RUN: llc -mcpu=kv3-2 -O2 -o - %s | FileCheck %s --check-prefix=V2

target triple = "kvx-kalray-cos"
declare <4 x float> @llvm.matrix.multiply.v4f32.v4f32.v4f32(<4 x float>, <4 x float>, i32 immarg, i32 immarg, i32 immarg)
declare <4 x float> @llvm.matrix.transpose.v4f32(<4 x float>, i32 immarg, i32 immarg)
declare <4 x float> @llvm.kvx.fmm222w(<4 x float>, <4 x float>, i32, i32, i32)
declare <4 x float> @llvm.kvx.fmma222w(<4 x float>, <4 x float>, <4 x float>, i32, i32, i32)
declare <4 x float> @llvm.kvx.fmms222w(<4 x float>, <4 x float>, <4 x float>, i32, i32, i32)

define <4 x float> @fmm(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm:
; V1:       # %bb.0:
; V1-NEXT:    srld $r4 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    andd $r5 = $r1, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    fmm212w $r0r1 = $r0, $r2
; V1-NEXT:    ord $r2 = $r4, $r5
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w $r0r1 = $r2, $r3
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.matrix.multiply.v4f32.v4f32.v4f32(<4 x float> %0, <4 x float> %1, i32 2, i32 2, i32 2)
  ret <4 x float> %3
}


define <4 x float> @fmm_2(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_2:
; V1:       # %bb.0:
; V1-NEXT:    srld $r4 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    andd $r5 = $r1, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    fmm212w $r0r1 = $r0, $r2
; V1-NEXT:    ord $r2 = $r4, $r5
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w $r0r1 = $r2, $r3
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_2:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.matrix.multiply.v4f32.v4f32.v4f32(<4 x float> %0, <4 x float> %1, i32 2, i32 2, i32 2)
  ret <4 x float> %3
}

define <4 x float> @fmma222w(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmma222w:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r1, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w $r4r5 = $r0, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmma222w:
; V2:       # %bb.0:
; V2-NEXT:    fmma222w $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.matrix.multiply.v4f32.v4f32.v4f32(<4 x float> %0, <4 x float> %1, i32 2, i32 2, i32 2)
  %5 = fadd fast <4 x float> %4, %2
  ret <4 x float> %5
}

define <4 x float> @fmma222w_2(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmma222w_2:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r1, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w $r4r5 = $r0, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmma222w_2:
; V2:       # %bb.0:
; V2-NEXT:    fmma222w $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.matrix.multiply.v4f32.v4f32.v4f32(<4 x float> %0, <4 x float> %1, i32 2, i32 2, i32 2)
  %5 = fadd fast <4 x float> %4, %2
  ret <4 x float> %5
}

define <4 x float> @fmms222w(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmms222w:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r1, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w $r4r5 = $r0, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmms222w:
; V2:       # %bb.0:
; V2-NEXT:    fmms222w $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.matrix.multiply.v4f32.v4f32.v4f32(<4 x float> %0, <4 x float> %1, i32 2, i32 2, i32 2)
  %5 = fsub fast <4 x float> %2, %4
  ret <4 x float> %5
}

define <4 x float> @fmms222w_2(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmms222w_2:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r1, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w $r4r5 = $r0, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmms222w_2:
; V2:       # %bb.0:
; V2-NEXT:    fmms222w $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.matrix.multiply.v4f32.v4f32.v4f32(<4 x float> %0, <4 x float> %1, i32 2, i32 2, i32 2)
  %5 = fsub fast <4 x float> %2, %4
  ret <4 x float> %5
}

define <4 x float> @fmm_tn(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_tn:
; V1:       # %bb.0:
; V1-NEXT:    fmm212w $r4r5 = $r0, $r2
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w $r4r5 = $r1, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_tn:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.tn $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.matrix.transpose.v4f32(<4 x float> %0, i32 2, i32 2)
  %4 = tail call <4 x float> @llvm.matrix.multiply.v4f32.v4f32.v4f32(<4 x float> %3, <4 x float> %1, i32 2, i32 2, i32 2)
  ret <4 x float> %4
}


define <4 x float> @fmm_2_tn(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_2_tn:
; V1:       # %bb.0:
; V1-NEXT:    fmm212w $r4r5 = $r0, $r2
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w $r4r5 = $r1, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_2_tn:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.tn $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.matrix.transpose.v4f32(<4 x float> %0, i32 2, i32 2)
  %4 = tail call <4 x float> @llvm.matrix.multiply.v4f32.v4f32.v4f32(<4 x float> %3, <4 x float> %1, i32 2, i32 2, i32 2)
  ret <4 x float> %4
}

define <4 x float> @fmma222w_2_tn(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmma222w_2_tn:
; V1:       # %bb.0:
; V1-NEXT:    fmma212w $r4r5 = $r0, $r2
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w $r4r5 = $r1, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmma222w_2_tn:
; V2:       # %bb.0:
; V2-NEXT:    fmma222w.tn $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.matrix.transpose.v4f32(<4 x float> %0, i32 2, i32 2)
  %5 = tail call <4 x float> @llvm.matrix.multiply.v4f32.v4f32.v4f32(<4 x float> %4, <4 x float> %1, i32 2, i32 2, i32 2)
  %6 = fadd fast <4 x float> %5, %2
  ret <4 x float> %6
}

define <4 x float> @fmms222w_2_tn(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmms222w_2_tn:
; V1:       # %bb.0:
; V1-NEXT:    fmms212w $r4r5 = $r0, $r2
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w $r4r5 = $r1, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmms222w_2_tn:
; V2:       # %bb.0:
; V2-NEXT:    fmms222w.tn $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.matrix.transpose.v4f32(<4 x float> %0, i32 2, i32 2)
  %5 = tail call <4 x float> @llvm.matrix.multiply.v4f32.v4f32.v4f32(<4 x float> %4, <4 x float> %1, i32 2, i32 2, i32 2)
  %6 = fsub fast <4 x float> %2, %5
  ret <4 x float> %6
}

define <4 x float> @fmm_nt(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_nt:
; V1:       # %bb.0:
; V1-NEXT:    srld $r4 = $r2, 32
; V1-NEXT:    srld $r5 = $r0, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    andd $r6 = $r1, 0xffffffff00000000
; V1-NEXT:    fmm212w $r0r1 = $r0, $r2
; V1-NEXT:    andd $r2 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r3 = $r5, $r6
; V1-NEXT:    ord $r2 = $r4, $r2
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w $r0r1 = $r3, $r2
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_nt:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.nt $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.matrix.transpose.v4f32(<4 x float> %1, i32 2, i32 2)
  %4 = tail call <4 x float> @llvm.matrix.multiply.v4f32.v4f32.v4f32(<4 x float> %0, <4 x float> %3, i32 2, i32 2, i32 2)
  ret <4 x float> %4
}

define <4 x float> @fmm_2_nt(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_2_nt:
; V1:       # %bb.0:
; V1-NEXT:    srld $r4 = $r2, 32
; V1-NEXT:    srld $r5 = $r0, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    andd $r6 = $r1, 0xffffffff00000000
; V1-NEXT:    fmm212w $r0r1 = $r0, $r2
; V1-NEXT:    andd $r2 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r3 = $r5, $r6
; V1-NEXT:    ord $r2 = $r4, $r2
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w $r0r1 = $r3, $r2
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_2_nt:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.nt $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.matrix.transpose.v4f32(<4 x float> %1, i32 2, i32 2)
  %4 = tail call <4 x float> @llvm.matrix.multiply.v4f32.v4f32.v4f32(<4 x float> %0, <4 x float> %3, i32 2, i32 2, i32 2)
  ret <4 x float> %4
}

define <4 x float> @fmma222w_2_nt(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmma222w_2_nt:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    srld $r7 = $r2, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r1, 0xffffffff00000000
; V1-NEXT:    andd $r1 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ord $r1 = $r7, $r1
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w $r4r5 = $r0, $r1
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmma222w_2_nt:
; V2:       # %bb.0:
; V2-NEXT:    fmma222w.nt $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.matrix.transpose.v4f32(<4 x float> %1, i32 2, i32 2)
  %5 = tail call <4 x float> @llvm.matrix.multiply.v4f32.v4f32.v4f32(<4 x float> %0, <4 x float> %4, i32 2, i32 2, i32 2)
  %6 = fadd fast <4 x float> %5, %2
  ret <4 x float> %6
}

define <4 x float> @fmms222w_2_nt(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmms222w_2_nt:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    srld $r7 = $r2, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r1, 0xffffffff00000000
; V1-NEXT:    andd $r1 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ord $r1 = $r7, $r1
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w $r4r5 = $r0, $r1
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmms222w_2_nt:
; V2:       # %bb.0:
; V2-NEXT:    fmms222w.nt $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.matrix.transpose.v4f32(<4 x float> %1, i32 2, i32 2)
  %5 = tail call <4 x float> @llvm.matrix.multiply.v4f32.v4f32.v4f32(<4 x float> %0, <4 x float> %4, i32 2, i32 2, i32 2)
  %6 = fsub fast <4 x float> %2, %5
  ret <4 x float> %6
}

define <4 x float> @fmm_tt(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_tt:
; V1:       # %bb.0:
; V1-NEXT:    srld $r4 = $r2, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    andd $r5 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    fmm212w $r2r3 = $r0, $r2
; V1-NEXT:    ord $r0 = $r4, $r5
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w $r2r3 = $r1, $r0
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r2
; V1-NEXT:    copyd $r1 = $r3
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_tt:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.tt $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.matrix.transpose.v4f32(<4 x float> %0, i32 2, i32 2)
  %4 = tail call <4 x float> @llvm.matrix.transpose.v4f32(<4 x float> %1, i32 2, i32 2)
  %5 = tail call <4 x float> @llvm.matrix.multiply.v4f32.v4f32.v4f32(<4 x float> %3, <4 x float> %4, i32 2, i32 2, i32 2)
  ret <4 x float> %5
}

define <4 x float> @fmm_2_tt(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_2_tt:
; V1:       # %bb.0:
; V1-NEXT:    srld $r4 = $r2, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    andd $r5 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    fmm212w $r2r3 = $r0, $r2
; V1-NEXT:    ord $r0 = $r4, $r5
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w $r2r3 = $r1, $r0
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r2
; V1-NEXT:    copyd $r1 = $r3
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_2_tt:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.tt $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.matrix.transpose.v4f32(<4 x float> %0, i32 2, i32 2)
  %4 = tail call <4 x float> @llvm.matrix.transpose.v4f32(<4 x float> %1, i32 2, i32 2)
  %5 = tail call <4 x float> @llvm.matrix.multiply.v4f32.v4f32.v4f32(<4 x float> %3, <4 x float> %4, i32 2, i32 2, i32 2)
  ret <4 x float> %5
}

define <4 x float> @fmma222w_2_tt(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmma222w_2_tt:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r2, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w $r4r5 = $r1, $r0
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmma222w_2_tt:
; V2:       # %bb.0:
; V2-NEXT:    fmma222w.tt $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.matrix.transpose.v4f32(<4 x float> %0, i32 2, i32 2)
  %5 = tail call <4 x float> @llvm.matrix.transpose.v4f32(<4 x float> %1, i32 2, i32 2)
  %6 = tail call <4 x float> @llvm.matrix.multiply.v4f32.v4f32.v4f32(<4 x float> %4, <4 x float> %5, i32 2, i32 2, i32 2)
  %7 = fadd fast <4 x float> %6, %2
  ret <4 x float> %7
}

define <4 x float> @fmms222w_2_tt(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmms222w_2_tt:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r2, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w $r4r5 = $r1, $r0
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmms222w_2_tt:
; V2:       # %bb.0:
; V2-NEXT:    fmms222w.tt $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.matrix.transpose.v4f32(<4 x float> %0, i32 2, i32 2)
  %5 = tail call <4 x float> @llvm.matrix.transpose.v4f32(<4 x float> %1, i32 2, i32 2)
  %6 = tail call <4 x float> @llvm.matrix.multiply.v4f32.v4f32.v4f32(<4 x float> %4, <4 x float> %5, i32 2, i32 2, i32 2)
  %7 = fsub fast <4 x float> %2, %6
  ret <4 x float> %7
}

define <4 x float> @fmm_rn(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_rn:
; V1:       # %bb.0:
; V1-NEXT:    srld $r4 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    andd $r5 = $r1, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    fmm212w.rn $r0r1 = $r0, $r2
; V1-NEXT:    ord $r2 = $r4, $r5
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rn $r0r1 = $r2, $r3
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_rn:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.rn $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.kvx.fmm222w(<4 x float> %0, <4 x float> %1, i32 0, i32 0, i32 0)
  ret <4 x float> %3
}


define <4 x float> @fmm_2_rn(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_2_rn:
; V1:       # %bb.0:
; V1-NEXT:    srld $r4 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    andd $r5 = $r1, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    fmm212w.rn $r0r1 = $r0, $r2
; V1-NEXT:    ord $r2 = $r4, $r5
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rn $r0r1 = $r2, $r3
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_2_rn:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.rn $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.kvx.fmm222w(<4 x float> %0, <4 x float> %1, i32 0, i32 0, i32 0)
  ret <4 x float> %3
}

define <4 x float> @fmma222w_rn(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmma222w_rn:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rn $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r1, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rn $r4r5 = $r0, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmma222w_rn:
; V2:       # %bb.0:
; V2-NEXT:    fmma222w.rn $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 0, i32 0, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmma222w_2_rn(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmma222w_2_rn:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rn $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r1, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rn $r4r5 = $r0, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmma222w_2_rn:
; V2:       # %bb.0:
; V2-NEXT:    fmma222w.rn $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 0, i32 0, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmms222w_rn(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmms222w_rn:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w.rn $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r1, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w.rn $r4r5 = $r0, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmms222w_rn:
; V2:       # %bb.0:
; V2-NEXT:    fmms222w.rn $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmms222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 0, i32 0, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmms222w_2_rn(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmms222w_2_rn:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w.rn $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r1, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w.rn $r4r5 = $r0, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmms222w_2_rn:
; V2:       # %bb.0:
; V2-NEXT:    fmms222w.rn $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmms222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 0, i32 0, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmm_tn_rn(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_tn_rn:
; V1:       # %bb.0:
; V1-NEXT:    fmm212w.rn $r4r5 = $r0, $r2
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rn $r4r5 = $r1, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_tn_rn:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.tn.rn $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.kvx.fmm222w(<4 x float> %0, <4 x float> %1, i32 1, i32 0, i32 0)
  ret <4 x float> %3
}

define <4 x float> @fmm_2_tn_rn(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_2_tn_rn:
; V1:       # %bb.0:
; V1-NEXT:    fmm212w.rn $r4r5 = $r0, $r2
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rn $r4r5 = $r1, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_2_tn_rn:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.tn.rn $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.kvx.fmm222w(<4 x float> %0, <4 x float> %1, i32 1, i32 0, i32 0)
  ret <4 x float> %3
}

define <4 x float> @fmma222w_2_tn_rn(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmma222w_2_tn_rn:
; V1:       # %bb.0:
; V1-NEXT:    fmma212w.rn $r4r5 = $r0, $r2
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rn $r4r5 = $r1, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmma222w_2_tn_rn:
; V2:       # %bb.0:
; V2-NEXT:    fmma222w.tn.rn $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 1, i32 0, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmms222w_2_tn_rn(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmms222w_2_tn_rn:
; V1:       # %bb.0:
; V1-NEXT:    fmms212w.rn $r4r5 = $r0, $r2
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w.rn $r4r5 = $r1, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmms222w_2_tn_rn:
; V2:       # %bb.0:
; V2-NEXT:    fmms222w.tn.rn $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmms222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 1, i32 0, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmm_nt_rn(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_nt_rn:
; V1:       # %bb.0:
; V1-NEXT:    srld $r4 = $r2, 32
; V1-NEXT:    srld $r5 = $r0, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    andd $r6 = $r1, 0xffffffff00000000
; V1-NEXT:    fmm212w.rn $r0r1 = $r0, $r2
; V1-NEXT:    andd $r2 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r3 = $r5, $r6
; V1-NEXT:    ord $r2 = $r4, $r2
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rn $r0r1 = $r3, $r2
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_nt_rn:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.nt.rn $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.kvx.fmm222w(<4 x float> %0, <4 x float> %1, i32 2, i32 0, i32 0)
  ret <4 x float> %3
}

define <4 x float> @fmm_2_nt_rn(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_2_nt_rn:
; V1:       # %bb.0:
; V1-NEXT:    srld $r4 = $r2, 32
; V1-NEXT:    srld $r5 = $r0, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    andd $r6 = $r1, 0xffffffff00000000
; V1-NEXT:    fmm212w.rn $r0r1 = $r0, $r2
; V1-NEXT:    andd $r2 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r3 = $r5, $r6
; V1-NEXT:    ord $r2 = $r4, $r2
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rn $r0r1 = $r3, $r2
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_2_nt_rn:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.nt.rn $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.kvx.fmm222w(<4 x float> %0, <4 x float> %1, i32 2, i32 0, i32 0)
  ret <4 x float> %3
}

define <4 x float> @fmma222w_2_nt_rn(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmma222w_2_nt_rn:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    srld $r7 = $r2, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rn $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r1, 0xffffffff00000000
; V1-NEXT:    andd $r1 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ord $r1 = $r7, $r1
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rn $r4r5 = $r0, $r1
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmma222w_2_nt_rn:
; V2:       # %bb.0:
; V2-NEXT:    fmma222w.nt.rn $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 2, i32 0, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmms222w_2_nt_rn(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmms222w_2_nt_rn:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    srld $r7 = $r2, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w.rn $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r1, 0xffffffff00000000
; V1-NEXT:    andd $r1 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ord $r1 = $r7, $r1
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w.rn $r4r5 = $r0, $r1
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmms222w_2_nt_rn:
; V2:       # %bb.0:
; V2-NEXT:    fmms222w.nt.rn $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmms222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 2, i32 0, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmm_tt_rn(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_tt_rn:
; V1:       # %bb.0:
; V1-NEXT:    srld $r4 = $r2, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    andd $r5 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    fmm212w.rn $r2r3 = $r0, $r2
; V1-NEXT:    ord $r0 = $r4, $r5
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rn $r2r3 = $r1, $r0
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r2
; V1-NEXT:    copyd $r1 = $r3
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_tt_rn:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.tt.rn $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.kvx.fmm222w(<4 x float> %0, <4 x float> %1, i32 3, i32 0, i32 0)
  ret <4 x float> %3
}

define <4 x float> @fmm_2_tt_rn(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_2_tt_rn:
; V1:       # %bb.0:
; V1-NEXT:    srld $r4 = $r2, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    andd $r5 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    fmm212w.rn $r2r3 = $r0, $r2
; V1-NEXT:    ord $r0 = $r4, $r5
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rn $r2r3 = $r1, $r0
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r2
; V1-NEXT:    copyd $r1 = $r3
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_2_tt_rn:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.tt.rn $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.kvx.fmm222w(<4 x float> %0, <4 x float> %1, i32 3, i32 0, i32 0)
  ret <4 x float> %3
}

define <4 x float> @fmma222w_2_tt_rn(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmma222w_2_tt_rn:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r2, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rn $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rn $r4r5 = $r1, $r0
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmma222w_2_tt_rn:
; V2:       # %bb.0:
; V2-NEXT:    fmma222w.tt.rn $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 3, i32 0, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmms222w_2_tt_rn(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmms222w_2_tt_rn:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r2, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w.rn $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w.rn $r4r5 = $r1, $r0
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmms222w_2_tt_rn:
; V2:       # %bb.0:
; V2-NEXT:    fmms222w.tt.rn $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmms222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 3, i32 0, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmm_ru(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_ru:
; V1:       # %bb.0:
; V1-NEXT:    srld $r4 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    andd $r5 = $r1, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    fmm212w.ru $r0r1 = $r0, $r2
; V1-NEXT:    ord $r2 = $r4, $r5
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.ru $r0r1 = $r2, $r3
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_ru:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.ru $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.kvx.fmm222w(<4 x float> %0, <4 x float> %1, i32 0, i32 1, i32 0)
  ret <4 x float> %3
}

define <4 x float> @fmm_2_ru(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_2_ru:
; V1:       # %bb.0:
; V1-NEXT:    srld $r4 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    andd $r5 = $r1, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    fmm212w.ru $r0r1 = $r0, $r2
; V1-NEXT:    ord $r2 = $r4, $r5
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.ru $r0r1 = $r2, $r3
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_2_ru:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.ru $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.kvx.fmm222w(<4 x float> %0, <4 x float> %1, i32 0, i32 1, i32 0)
  ret <4 x float> %3
}

define <4 x float> @fmma222w_ru(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmma222w_ru:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.ru $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r1, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.ru $r4r5 = $r0, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmma222w_ru:
; V2:       # %bb.0:
; V2-NEXT:    fmma222w.ru $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 0, i32 1, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmma222w_2_ru(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmma222w_2_ru:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.ru $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r1, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.ru $r4r5 = $r0, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmma222w_2_ru:
; V2:       # %bb.0:
; V2-NEXT:    fmma222w.ru $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 0, i32 1, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmms222w_ru(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmms222w_ru:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w.ru $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r1, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w.ru $r4r5 = $r0, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmms222w_ru:
; V2:       # %bb.0:
; V2-NEXT:    fmms222w.ru $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmms222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 0, i32 1, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmms222w_2_ru(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmms222w_2_ru:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w.ru $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r1, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w.ru $r4r5 = $r0, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmms222w_2_ru:
; V2:       # %bb.0:
; V2-NEXT:    fmms222w.ru $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmms222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 0, i32 1, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmm_tn_ru(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_tn_ru:
; V1:       # %bb.0:
; V1-NEXT:    fmm212w.ru $r4r5 = $r0, $r2
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.ru $r4r5 = $r1, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_tn_ru:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.tn.ru $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.kvx.fmm222w(<4 x float> %0, <4 x float> %1, i32 1, i32 1, i32 0)
  ret <4 x float> %3
}

define <4 x float> @fmm_2_tn_ru(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_2_tn_ru:
; V1:       # %bb.0:
; V1-NEXT:    fmm212w.ru $r4r5 = $r0, $r2
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.ru $r4r5 = $r1, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_2_tn_ru:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.tn.ru $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.kvx.fmm222w(<4 x float> %0, <4 x float> %1, i32 1, i32 1, i32 0)
  ret <4 x float> %3
}

define <4 x float> @fmma222w_2_tn_ru(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmma222w_2_tn_ru:
; V1:       # %bb.0:
; V1-NEXT:    fmma212w.ru $r4r5 = $r0, $r2
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.ru $r4r5 = $r1, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmma222w_2_tn_ru:
; V2:       # %bb.0:
; V2-NEXT:    fmma222w.tn.ru $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 1, i32 1, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmms222w_2_tn_ru(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmms222w_2_tn_ru:
; V1:       # %bb.0:
; V1-NEXT:    fmms212w.ru $r4r5 = $r0, $r2
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w.ru $r4r5 = $r1, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmms222w_2_tn_ru:
; V2:       # %bb.0:
; V2-NEXT:    fmms222w.tn.ru $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmms222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 1, i32 1, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmm_nt_ru(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_nt_ru:
; V1:       # %bb.0:
; V1-NEXT:    srld $r4 = $r2, 32
; V1-NEXT:    srld $r5 = $r0, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    andd $r6 = $r1, 0xffffffff00000000
; V1-NEXT:    fmm212w.ru $r0r1 = $r0, $r2
; V1-NEXT:    andd $r2 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r3 = $r5, $r6
; V1-NEXT:    ord $r2 = $r4, $r2
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.ru $r0r1 = $r3, $r2
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_nt_ru:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.nt.ru $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.kvx.fmm222w(<4 x float> %0, <4 x float> %1, i32 2, i32 1, i32 0)
  ret <4 x float> %3
}

define <4 x float> @fmm_2_nt_ru(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_2_nt_ru:
; V1:       # %bb.0:
; V1-NEXT:    srld $r4 = $r2, 32
; V1-NEXT:    srld $r5 = $r0, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    andd $r6 = $r1, 0xffffffff00000000
; V1-NEXT:    fmm212w.ru $r0r1 = $r0, $r2
; V1-NEXT:    andd $r2 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r3 = $r5, $r6
; V1-NEXT:    ord $r2 = $r4, $r2
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.ru $r0r1 = $r3, $r2
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_2_nt_ru:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.nt.ru $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.kvx.fmm222w(<4 x float> %0, <4 x float> %1, i32 2, i32 1, i32 0)
  ret <4 x float> %3
}

define <4 x float> @fmma222w_2_nt_ru(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmma222w_2_nt_ru:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    srld $r7 = $r2, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.ru $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r1, 0xffffffff00000000
; V1-NEXT:    andd $r1 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ord $r1 = $r7, $r1
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.ru $r4r5 = $r0, $r1
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmma222w_2_nt_ru:
; V2:       # %bb.0:
; V2-NEXT:    fmma222w.nt.ru $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 2, i32 1, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmms222w_2_nt_ru(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmms222w_2_nt_ru:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    srld $r7 = $r2, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w.ru $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r1, 0xffffffff00000000
; V1-NEXT:    andd $r1 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ord $r1 = $r7, $r1
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w.ru $r4r5 = $r0, $r1
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmms222w_2_nt_ru:
; V2:       # %bb.0:
; V2-NEXT:    fmms222w.nt.ru $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmms222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 2, i32 1, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmm_tt_ru(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_tt_ru:
; V1:       # %bb.0:
; V1-NEXT:    srld $r4 = $r2, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    andd $r5 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    fmm212w.ru $r2r3 = $r0, $r2
; V1-NEXT:    ord $r0 = $r4, $r5
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.ru $r2r3 = $r1, $r0
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r2
; V1-NEXT:    copyd $r1 = $r3
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_tt_ru:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.tt.ru $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.kvx.fmm222w(<4 x float> %0, <4 x float> %1, i32 3, i32 1, i32 0)
  ret <4 x float> %3
}

define <4 x float> @fmm_2_tt_ru(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_2_tt_ru:
; V1:       # %bb.0:
; V1-NEXT:    srld $r4 = $r2, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    andd $r5 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    fmm212w.ru $r2r3 = $r0, $r2
; V1-NEXT:    ord $r0 = $r4, $r5
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.ru $r2r3 = $r1, $r0
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r2
; V1-NEXT:    copyd $r1 = $r3
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_2_tt_ru:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.tt.ru $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.kvx.fmm222w(<4 x float> %0, <4 x float> %1, i32 3, i32 1, i32 0)
  ret <4 x float> %3
}

define <4 x float> @fmma222w_2_tt_ru(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmma222w_2_tt_ru:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r2, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.ru $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.ru $r4r5 = $r1, $r0
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmma222w_2_tt_ru:
; V2:       # %bb.0:
; V2-NEXT:    fmma222w.tt.ru $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 3, i32 1, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmms222w_2_tt_ru(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmms222w_2_tt_ru:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r2, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w.ru $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w.ru $r4r5 = $r1, $r0
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmms222w_2_tt_ru:
; V2:       # %bb.0:
; V2-NEXT:    fmms222w.tt.ru $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmms222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 3, i32 1, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmm_rd(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_rd:
; V1:       # %bb.0:
; V1-NEXT:    srld $r4 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    andd $r5 = $r1, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    fmm212w.rd $r0r1 = $r0, $r2
; V1-NEXT:    ord $r2 = $r4, $r5
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rd $r0r1 = $r2, $r3
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_rd:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.rd $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.kvx.fmm222w(<4 x float> %0, <4 x float> %1, i32 0, i32 2, i32 0)
  ret <4 x float> %3
}

define <4 x float> @fmm_2_rd(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_2_rd:
; V1:       # %bb.0:
; V1-NEXT:    srld $r4 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    andd $r5 = $r1, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    fmm212w.rd $r0r1 = $r0, $r2
; V1-NEXT:    ord $r2 = $r4, $r5
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rd $r0r1 = $r2, $r3
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_2_rd:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.rd $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.kvx.fmm222w(<4 x float> %0, <4 x float> %1, i32 0, i32 2, i32 0)
  ret <4 x float> %3
}

define <4 x float> @fmma222w_rd(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmma222w_rd:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rd $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r1, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rd $r4r5 = $r0, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmma222w_rd:
; V2:       # %bb.0:
; V2-NEXT:    fmma222w.rd $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 0, i32 2, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmma222w_2_rd(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmma222w_2_rd:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rd $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r1, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rd $r4r5 = $r0, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmma222w_2_rd:
; V2:       # %bb.0:
; V2-NEXT:    fmma222w.rd $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 0, i32 2, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmms222w_rd(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmms222w_rd:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w.rd $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r1, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w.rd $r4r5 = $r0, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmms222w_rd:
; V2:       # %bb.0:
; V2-NEXT:    fmms222w.rd $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmms222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 0, i32 2, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmms222w_2_rd(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmms222w_2_rd:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w.rd $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r1, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w.rd $r4r5 = $r0, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmms222w_2_rd:
; V2:       # %bb.0:
; V2-NEXT:    fmms222w.rd $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmms222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 0, i32 2, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmm_tn_rd(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_tn_rd:
; V1:       # %bb.0:
; V1-NEXT:    fmm212w.rd $r4r5 = $r0, $r2
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rd $r4r5 = $r1, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_tn_rd:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.tn.rd $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.kvx.fmm222w(<4 x float> %0, <4 x float> %1, i32 1, i32 2, i32 0)
  ret <4 x float> %3
}

define <4 x float> @fmm_2_tn_rd(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_2_tn_rd:
; V1:       # %bb.0:
; V1-NEXT:    fmm212w.rd $r4r5 = $r0, $r2
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rd $r4r5 = $r1, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_2_tn_rd:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.tn.rd $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.kvx.fmm222w(<4 x float> %0, <4 x float> %1, i32 1, i32 2, i32 0)
  ret <4 x float> %3
}

define <4 x float> @fmma222w_2_tn_rd(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmma222w_2_tn_rd:
; V1:       # %bb.0:
; V1-NEXT:    fmma212w.rd $r4r5 = $r0, $r2
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rd $r4r5 = $r1, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmma222w_2_tn_rd:
; V2:       # %bb.0:
; V2-NEXT:    fmma222w.tn.rd $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 1, i32 2, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmms222w_2_tn_rd(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmms222w_2_tn_rd:
; V1:       # %bb.0:
; V1-NEXT:    fmms212w.rd $r4r5 = $r0, $r2
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w.rd $r4r5 = $r1, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmms222w_2_tn_rd:
; V2:       # %bb.0:
; V2-NEXT:    fmms222w.tn.rd $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmms222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 1, i32 2, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmm_nt_rd(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_nt_rd:
; V1:       # %bb.0:
; V1-NEXT:    srld $r4 = $r2, 32
; V1-NEXT:    srld $r5 = $r0, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    andd $r6 = $r1, 0xffffffff00000000
; V1-NEXT:    fmm212w.rd $r0r1 = $r0, $r2
; V1-NEXT:    andd $r2 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r3 = $r5, $r6
; V1-NEXT:    ord $r2 = $r4, $r2
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rd $r0r1 = $r3, $r2
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_nt_rd:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.nt.rd $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.kvx.fmm222w(<4 x float> %0, <4 x float> %1, i32 2, i32 2, i32 0)
  ret <4 x float> %3
}

define <4 x float> @fmm_2_nt_rd(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_2_nt_rd:
; V1:       # %bb.0:
; V1-NEXT:    srld $r4 = $r2, 32
; V1-NEXT:    srld $r5 = $r0, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    andd $r6 = $r1, 0xffffffff00000000
; V1-NEXT:    fmm212w.rd $r0r1 = $r0, $r2
; V1-NEXT:    andd $r2 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r3 = $r5, $r6
; V1-NEXT:    ord $r2 = $r4, $r2
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rd $r0r1 = $r3, $r2
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_2_nt_rd:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.nt.rd $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.kvx.fmm222w(<4 x float> %0, <4 x float> %1, i32 2, i32 2, i32 0)
  ret <4 x float> %3
}

define <4 x float> @fmma222w_2_nt_rd(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmma222w_2_nt_rd:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    srld $r7 = $r2, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rd $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r1, 0xffffffff00000000
; V1-NEXT:    andd $r1 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ord $r1 = $r7, $r1
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rd $r4r5 = $r0, $r1
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmma222w_2_nt_rd:
; V2:       # %bb.0:
; V2-NEXT:    fmma222w.nt.rd $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 2, i32 2, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmms222w_2_nt_rd(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmms222w_2_nt_rd:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    srld $r7 = $r2, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w.rd $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r1, 0xffffffff00000000
; V1-NEXT:    andd $r1 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ord $r1 = $r7, $r1
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w.rd $r4r5 = $r0, $r1
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmms222w_2_nt_rd:
; V2:       # %bb.0:
; V2-NEXT:    fmms222w.nt.rd $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmms222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 2, i32 2, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmm_tt_rd(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_tt_rd:
; V1:       # %bb.0:
; V1-NEXT:    srld $r4 = $r2, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    andd $r5 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    fmm212w.rd $r2r3 = $r0, $r2
; V1-NEXT:    ord $r0 = $r4, $r5
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rd $r2r3 = $r1, $r0
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r2
; V1-NEXT:    copyd $r1 = $r3
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_tt_rd:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.tt.rd $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.kvx.fmm222w(<4 x float> %0, <4 x float> %1, i32 3, i32 2, i32 0)
  ret <4 x float> %3
}

define <4 x float> @fmm_2_tt_rd(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_2_tt_rd:
; V1:       # %bb.0:
; V1-NEXT:    srld $r4 = $r2, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    andd $r5 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    fmm212w.rd $r2r3 = $r0, $r2
; V1-NEXT:    ord $r0 = $r4, $r5
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rd $r2r3 = $r1, $r0
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r2
; V1-NEXT:    copyd $r1 = $r3
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_2_tt_rd:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.tt.rd $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.kvx.fmm222w(<4 x float> %0, <4 x float> %1, i32 3, i32 2, i32 0)
  ret <4 x float> %3
}

define <4 x float> @fmma222w_2_tt_rd(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmma222w_2_tt_rd:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r2, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rd $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rd $r4r5 = $r1, $r0
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmma222w_2_tt_rd:
; V2:       # %bb.0:
; V2-NEXT:    fmma222w.tt.rd $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 3, i32 2, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmms222w_2_tt_rd(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmms222w_2_tt_rd:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r2, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w.rd $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w.rd $r4r5 = $r1, $r0
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmms222w_2_tt_rd:
; V2:       # %bb.0:
; V2-NEXT:    fmms222w.tt.rd $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmms222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 3, i32 2, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmm_rz(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_rz:
; V1:       # %bb.0:
; V1-NEXT:    srld $r4 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    andd $r5 = $r1, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    fmm212w.rz $r0r1 = $r0, $r2
; V1-NEXT:    ord $r2 = $r4, $r5
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rz $r0r1 = $r2, $r3
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_rz:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.rz $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.kvx.fmm222w(<4 x float> %0, <4 x float> %1, i32 0, i32 3, i32 0)
  ret <4 x float> %3
}

define <4 x float> @fmm_2_rz(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_2_rz:
; V1:       # %bb.0:
; V1-NEXT:    srld $r4 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    andd $r5 = $r1, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    fmm212w.rz $r0r1 = $r0, $r2
; V1-NEXT:    ord $r2 = $r4, $r5
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rz $r0r1 = $r2, $r3
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_2_rz:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.rz $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.kvx.fmm222w(<4 x float> %0, <4 x float> %1, i32 0, i32 3, i32 0)
  ret <4 x float> %3
}

define <4 x float> @fmma222w_rz(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmma222w_rz:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rz $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r1, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rz $r4r5 = $r0, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmma222w_rz:
; V2:       # %bb.0:
; V2-NEXT:    fmma222w.rz $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 0, i32 3, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmma222w_2_rz(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmma222w_2_rz:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rz $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r1, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rz $r4r5 = $r0, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmma222w_2_rz:
; V2:       # %bb.0:
; V2-NEXT:    fmma222w.rz $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 0, i32 3, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmms222w_rz(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmms222w_rz:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w.rz $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r1, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w.rz $r4r5 = $r0, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmms222w_rz:
; V2:       # %bb.0:
; V2-NEXT:    fmms222w.rz $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmms222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 0, i32 3, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmms222w_2_rz(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmms222w_2_rz:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w.rz $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r1, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w.rz $r4r5 = $r0, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmms222w_2_rz:
; V2:       # %bb.0:
; V2-NEXT:    fmms222w.rz $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmms222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 0, i32 3, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmm_tn_rz(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_tn_rz:
; V1:       # %bb.0:
; V1-NEXT:    fmm212w.rz $r4r5 = $r0, $r2
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rz $r4r5 = $r1, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_tn_rz:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.tn.rz $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.kvx.fmm222w(<4 x float> %0, <4 x float> %1, i32 1, i32 3, i32 0)
  ret <4 x float> %3
}

define <4 x float> @fmm_2_tn_rz(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_2_tn_rz:
; V1:       # %bb.0:
; V1-NEXT:    fmm212w.rz $r4r5 = $r0, $r2
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rz $r4r5 = $r1, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_2_tn_rz:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.tn.rz $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.kvx.fmm222w(<4 x float> %0, <4 x float> %1, i32 1, i32 3, i32 0)
  ret <4 x float> %3
}

define <4 x float> @fmma222w_2_tn_rz(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmma222w_2_tn_rz:
; V1:       # %bb.0:
; V1-NEXT:    fmma212w.rz $r4r5 = $r0, $r2
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rz $r4r5 = $r1, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmma222w_2_tn_rz:
; V2:       # %bb.0:
; V2-NEXT:    fmma222w.tn.rz $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 1, i32 3, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmms222w_2_tn_rz(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmms222w_2_tn_rz:
; V1:       # %bb.0:
; V1-NEXT:    fmms212w.rz $r4r5 = $r0, $r2
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w.rz $r4r5 = $r1, $r3
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmms222w_2_tn_rz:
; V2:       # %bb.0:
; V2-NEXT:    fmms222w.tn.rz $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmms222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 1, i32 3, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmm_nt_rz(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_nt_rz:
; V1:       # %bb.0:
; V1-NEXT:    srld $r4 = $r2, 32
; V1-NEXT:    srld $r5 = $r0, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    andd $r6 = $r1, 0xffffffff00000000
; V1-NEXT:    fmm212w.rz $r0r1 = $r0, $r2
; V1-NEXT:    andd $r2 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r3 = $r5, $r6
; V1-NEXT:    ord $r2 = $r4, $r2
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rz $r0r1 = $r3, $r2
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_nt_rz:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.nt.rz $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.kvx.fmm222w(<4 x float> %0, <4 x float> %1, i32 2, i32 3, i32 0)
  ret <4 x float> %3
}

define <4 x float> @fmm_2_nt_rz(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_2_nt_rz:
; V1:       # %bb.0:
; V1-NEXT:    srld $r4 = $r2, 32
; V1-NEXT:    srld $r5 = $r0, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    andd $r6 = $r1, 0xffffffff00000000
; V1-NEXT:    fmm212w.rz $r0r1 = $r0, $r2
; V1-NEXT:    andd $r2 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r3 = $r5, $r6
; V1-NEXT:    ord $r2 = $r4, $r2
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rz $r0r1 = $r3, $r2
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_2_nt_rz:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.nt.rz $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.kvx.fmm222w(<4 x float> %0, <4 x float> %1, i32 2, i32 3, i32 0)
  ret <4 x float> %3
}

define <4 x float> @fmma222w_2_nt_rz(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmma222w_2_nt_rz:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    srld $r7 = $r2, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rz $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r1, 0xffffffff00000000
; V1-NEXT:    andd $r1 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ord $r1 = $r7, $r1
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rz $r4r5 = $r0, $r1
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmma222w_2_nt_rz:
; V2:       # %bb.0:
; V2-NEXT:    fmma222w.nt.rz $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 2, i32 3, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmms222w_2_nt_rz(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmms222w_2_nt_rz:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    srld $r7 = $r2, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w.rz $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r1, 0xffffffff00000000
; V1-NEXT:    andd $r1 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ord $r1 = $r7, $r1
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w.rz $r4r5 = $r0, $r1
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmms222w_2_nt_rz:
; V2:       # %bb.0:
; V2-NEXT:    fmms222w.nt.rz $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmms222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 2, i32 3, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmm_tt_rz(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_tt_rz:
; V1:       # %bb.0:
; V1-NEXT:    srld $r4 = $r2, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    andd $r5 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    fmm212w.rz $r2r3 = $r0, $r2
; V1-NEXT:    ord $r0 = $r4, $r5
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rz $r2r3 = $r1, $r0
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r2
; V1-NEXT:    copyd $r1 = $r3
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_tt_rz:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.tt.rz $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.kvx.fmm222w(<4 x float> %0, <4 x float> %1, i32 3, i32 3, i32 0)
  ret <4 x float> %3
}

define <4 x float> @fmm_2_tt_rz(<4 x float> %0, <4 x float> %1) {
; V1-LABEL: fmm_2_tt_rz:
; V1:       # %bb.0:
; V1-NEXT:    srld $r4 = $r2, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    andd $r5 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    fmm212w.rz $r2r3 = $r0, $r2
; V1-NEXT:    ord $r0 = $r4, $r5
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rz $r2r3 = $r1, $r0
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r2
; V1-NEXT:    copyd $r1 = $r3
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmm_2_tt_rz:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w.tt.rz $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x float> @llvm.kvx.fmm222w(<4 x float> %0, <4 x float> %1, i32 3, i32 3, i32 0)
  ret <4 x float> %3
}

define <4 x float> @fmma222w_2_tt_rz(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmma222w_2_tt_rz:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r2, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rz $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w.rz $r4r5 = $r1, $r0
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmma222w_2_tt_rz:
; V2:       # %bb.0:
; V2-NEXT:    fmma222w.tt.rz $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 3, i32 3, i32 0)
  ret <4 x float> %4
}

define <4 x float> @fmms222w_2_tt_rz(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: fmms222w_2_tt_rz:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r2, 32
; V1-NEXT:    insf $r2 = $r3, 63, 32
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w.rz $r4r5 = $r0, $r2
; V1-NEXT:    andd $r0 = $r3, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r6, $r0
; V1-NEXT:    ;;
; V1-NEXT:    fmms212w.rz $r4r5 = $r1, $r0
; V1-NEXT:    ;;
; V1-NEXT:    copyd $r0 = $r4
; V1-NEXT:    copyd $r1 = $r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: fmms222w_2_tt_rz:
; V2:       # %bb.0:
; V2-NEXT:    fmms222w.tt.rz $r4r5 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    copyd $r0 = $r4
; V2-NEXT:    copyd $r1 = $r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.kvx.fmms222w(<4 x float> %0, <4 x float> %1, <4 x float> %2, i32 3, i32 3, i32 0)
  ret <4 x float> %4
}

define <4 x float> @not_fmma222w(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: not_fmma222w:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    andd $r1 = $r1, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    fmm212w $r8r9 = $r0, $r2
; V1-NEXT:    ord $r0 = $r6, $r1
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w $r8r9 = $r0, $r3
; V1-NEXT:    ;;
; V1-NEXT:    faddwq $r0r1 = $r8r9, $r4r5
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: not_fmma222w:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    faddwq $r0r1 = $r0r1, $r4r5
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.matrix.multiply.v4f32.v4f32.v4f32(<4 x float> %0, <4 x float> %1, i32 2, i32 2, i32 2)
  %5 = fadd <4 x float> %4, %2
  ret <4 x float> %5
}

define <4 x float> @not_fmms222w(<4 x float> %0, <4 x float> %1, <4 x float> %2) {
; V1-LABEL: not_fmms222w:
; V1:       # %bb.0:
; V1-NEXT:    srld $r6 = $r0, 32
; V1-NEXT:    insf $r0 = $r1, 63, 32
; V1-NEXT:    andd $r1 = $r1, 0xffffffff00000000
; V1-NEXT:    ;;
; V1-NEXT:    fmm212w $r8r9 = $r0, $r2
; V1-NEXT:    ord $r0 = $r6, $r1
; V1-NEXT:    ;;
; V1-NEXT:    fmma212w $r8r9 = $r0, $r3
; V1-NEXT:    ;;
; V1-NEXT:    fsbfwq $r0r1 = $r4r5, $r8r9
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: not_fmms222w:
; V2:       # %bb.0:
; V2-NEXT:    fmm222w $r0r1 = $r0r1, $r2r3
; V2-NEXT:    ;;
; V2-NEXT:    fsbfwq $r0r1 = $r4r5, $r0r1
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %4 = tail call <4 x float> @llvm.matrix.multiply.v4f32.v4f32.v4f32(<4 x float> %0, <4 x float> %1, i32 2, i32 2, i32 2)
  %5 = fsub <4 x float> %4, %2
  ret <4 x float> %5
}
