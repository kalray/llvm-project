; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -o - %s -mtriple=kvx-kalray-cos | FileCheck %s --check-prefixes=CHECK,CV1
; RUN: llc -mcpu=kv3-2 -o - %s | FileCheck %s --check-prefixes=CHECK,CV2
; RUN: clang -c -o /dev/null %s
; RUN: clang -march=kv3-2 -c -o /dev/null %s

target triple = "kvx-kalray-cos"

define i64 @shrd(i64 %0, i32 %1) {
; CHECK-LABEL: shrd:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srld $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %3 = tail call i64 @llvm.kvx.shr.i64(i64 %0, i32 %1, i32 0)
  ret i64 %3
}

declare i64 @llvm.kvx.shr.i64(i64, i32, i32)

define i64 @shrd_s(i64 %0, i32 %1) {
; CHECK-LABEL: shrd_s:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srad $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %3 = tail call i64 @llvm.kvx.shr.i64(i64 %0, i32 %1, i32 1)
  ret i64 %3
}

define i64 @shrd_us(i64 %0, i32 %1) {
; CHECK-LABEL: shrd_us:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srsd $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %3 = tail call i64 @llvm.kvx.shr.i64(i64 %0, i32 %1, i32 2)
  ret i64 %3
}

define i64 @shrd_r(i64 %0, i32 %1) {
; CHECK-LABEL: shrd_r:
; CHECK:       # %bb.0:
; CHECK-NEXT:    negw $r1 = $r1
; CHECK-NEXT:    andw $r2 = $r1, 63
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    andw $r1 = $r1, 63
; CHECK-NEXT:    srld $r2 = $r0, $r2
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    slld $r0 = $r0, $r1
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    ord $r0 = $r2, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
  %3 = tail call i64 @llvm.kvx.shr.i64(i64 %0, i32 %1, i32 3)
  ret i64 %3
}

define <2 x i64> @shrdp(<2 x i64> %0, <2 x i32> %1) {
; CHECK-LABEL: shrdp:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srld $r0 = $r0, $r2
; CHECK-NEXT:    srad $r3 = $r2, 32
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    srld $r1 = $r1, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
  %3 = extractelement <2 x i64> %0, i64 0
  %4 = extractelement <2 x i32> %1, i64 0
  %5 = tail call i64 @llvm.kvx.shr.i64(i64 %3, i32 %4, i32 0)
  %6 = extractelement <2 x i64> %0, i64 1
  %7 = extractelement <2 x i32> %1, i64 1
  %8 = tail call i64 @llvm.kvx.shr.i64(i64 %6, i32 %7, i32 0)
  %9 = insertelement <2 x i64> undef, i64 %5, i32 0
  %10 = insertelement <2 x i64> %9, i64 %8, i32 1
  ret <2 x i64> %10
}

define <2 x i64> @shrdp_s(<2 x i64> %0, <2 x i32> %1) {
; CHECK-LABEL: shrdp_s:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srad $r0 = $r0, $r2
; CHECK-NEXT:    srad $r3 = $r2, 32
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    srad $r1 = $r1, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
  %3 = extractelement <2 x i64> %0, i64 0
  %4 = extractelement <2 x i32> %1, i64 0
  %5 = tail call i64 @llvm.kvx.shr.i64(i64 %3, i32 %4, i32 1)
  %6 = extractelement <2 x i64> %0, i64 1
  %7 = extractelement <2 x i32> %1, i64 1
  %8 = tail call i64 @llvm.kvx.shr.i64(i64 %6, i32 %7, i32 1)
  %9 = insertelement <2 x i64> undef, i64 %5, i32 0
  %10 = insertelement <2 x i64> %9, i64 %8, i32 1
  ret <2 x i64> %10
}

define <2 x i64> @shrdp_us(<2 x i64> %0, <2 x i32> %1) {
; CHECK-LABEL: shrdp_us:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srsd $r0 = $r0, $r2
; CHECK-NEXT:    srad $r3 = $r2, 32
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    srsd $r1 = $r1, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
  %3 = extractelement <2 x i64> %0, i64 0
  %4 = extractelement <2 x i32> %1, i64 0
  %5 = tail call i64 @llvm.kvx.shr.i64(i64 %3, i32 %4, i32 2)
  %6 = extractelement <2 x i64> %0, i64 1
  %7 = extractelement <2 x i32> %1, i64 1
  %8 = tail call i64 @llvm.kvx.shr.i64(i64 %6, i32 %7, i32 2)
  %9 = insertelement <2 x i64> undef, i64 %5, i32 0
  %10 = insertelement <2 x i64> %9, i64 %8, i32 1
  ret <2 x i64> %10
}

define <2 x i64> @shrdp_r(<2 x i64> %0, <2 x i32> %1) {
; CHECK-LABEL: shrdp_r:
; CHECK:       # %bb.0:
; CHECK-NEXT:    negw $r2 = $r2
; CHECK-NEXT:    andw $r3 = $r2, 63
; CHECK-NEXT:    srad $r4 = $r2, 32
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    andw $r2 = $r2, 63
; CHECK-NEXT:    srld $r3 = $r0, $r3
; CHECK-NEXT:    andw $r4 = $r4, 63
; CHECK-NEXT:    negw $r5 = $r4
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    slld $r0 = $r0, $r2
; CHECK-NEXT:    srld $r2 = $r1, $r4
; CHECK-NEXT:    andw $r5 = $r5, 63
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    ord $r0 = $r3, $r0
; CHECK-NEXT:    slld $r1 = $r1, $r5
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    ord $r1 = $r2, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 4)
  %3 = extractelement <2 x i64> %0, i64 0
  %4 = extractelement <2 x i32> %1, i64 0
  %5 = tail call i64 @llvm.kvx.shr.i64(i64 %3, i32 %4, i32 3)
  %6 = extractelement <2 x i64> %0, i64 1
  %7 = extractelement <2 x i32> %1, i64 1
  %8 = tail call i64 @llvm.kvx.shr.i64(i64 %6, i32 %7, i32 3)
  %9 = insertelement <2 x i64> undef, i64 %5, i32 0
  %10 = insertelement <2 x i64> %9, i64 %8, i32 1
  ret <2 x i64> %10
}

define <2 x i64> @shrdps(<2 x i64> %0, i32 %1) {
; CHECK-LABEL: shrdps:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srld $r0 = $r0, $r2
; CHECK-NEXT:    srld $r1 = $r1, $r2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %3 = extractelement <2 x i64> %0, i64 0
  %4 = tail call i64 @llvm.kvx.shr.i64(i64 %3, i32 %1, i32 0)
  %5 = extractelement <2 x i64> %0, i64 1
  %6 = tail call i64 @llvm.kvx.shr.i64(i64 %5, i32 %1, i32 0)
  %7 = insertelement <2 x i64> undef, i64 %4, i32 0
  %8 = insertelement <2 x i64> %7, i64 %6, i32 1
  ret <2 x i64> %8
}

define <2 x i64> @shrdps_s(<2 x i64> %0, i32 %1) {
; CHECK-LABEL: shrdps_s:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srad $r0 = $r0, $r2
; CHECK-NEXT:    srad $r1 = $r1, $r2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %3 = extractelement <2 x i64> %0, i64 0
  %4 = tail call i64 @llvm.kvx.shr.i64(i64 %3, i32 %1, i32 1)
  %5 = extractelement <2 x i64> %0, i64 1
  %6 = tail call i64 @llvm.kvx.shr.i64(i64 %5, i32 %1, i32 1)
  %7 = insertelement <2 x i64> undef, i64 %4, i32 0
  %8 = insertelement <2 x i64> %7, i64 %6, i32 1
  ret <2 x i64> %8
}

define <2 x i64> @shrdps_us(<2 x i64> %0, i32 %1) {
; CHECK-LABEL: shrdps_us:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srsd $r0 = $r0, $r2
; CHECK-NEXT:    srsd $r1 = $r1, $r2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %3 = extractelement <2 x i64> %0, i64 0
  %4 = tail call i64 @llvm.kvx.shr.i64(i64 %3, i32 %1, i32 2)
  %5 = extractelement <2 x i64> %0, i64 1
  %6 = tail call i64 @llvm.kvx.shr.i64(i64 %5, i32 %1, i32 2)
  %7 = insertelement <2 x i64> undef, i64 %4, i32 0
  %8 = insertelement <2 x i64> %7, i64 %6, i32 1
  ret <2 x i64> %8
}

define <2 x i64> @shrdps_r(<2 x i64> %0, i32 %1) {
; CHECK-LABEL: shrdps_r:
; CHECK:       # %bb.0:
; CHECK-NEXT:    negw $r2 = $r2
; CHECK-NEXT:    andw $r3 = $r2, 63
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    andw $r2 = $r2, 63
; CHECK-NEXT:    srld $r3 = $r1, $r3
; CHECK-NEXT:    srld $r4 = $r0, $r3
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    slld $r0 = $r0, $r2
; CHECK-NEXT:    slld $r1 = $r1, $r2
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    ord $r0 = $r4, $r0
; CHECK-NEXT:    ord $r1 = $r3, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
  %3 = extractelement <2 x i64> %0, i64 0
  %4 = tail call i64 @llvm.kvx.shr.i64(i64 %3, i32 %1, i32 3)
  %5 = extractelement <2 x i64> %0, i64 1
  %6 = tail call i64 @llvm.kvx.shr.i64(i64 %5, i32 %1, i32 3)
  %7 = insertelement <2 x i64> undef, i64 %4, i32 0
  %8 = insertelement <2 x i64> %7, i64 %6, i32 1
  ret <2 x i64> %8
}

define <4 x i64> @shrdq(<4 x i64> %0, <4 x i32> %1) {
; CHECK-LABEL: shrdq:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srld $r0 = $r0, $r4
; CHECK-NEXT:    srld $r2 = $r2, $r5
; CHECK-NEXT:    srad $r4 = $r4, 32
; CHECK-NEXT:    srad $r6 = $r5, 32
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    srld $r1 = $r1, $r4
; CHECK-NEXT:    srld $r3 = $r3, $r6
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
  %3 = extractelement <4 x i64> %0, i64 0
  %4 = extractelement <4 x i32> %1, i64 0
  %5 = tail call i64 @llvm.kvx.shr.i64(i64 %3, i32 %4, i32 0)
  %6 = extractelement <4 x i64> %0, i64 1
  %7 = extractelement <4 x i32> %1, i64 1
  %8 = tail call i64 @llvm.kvx.shr.i64(i64 %6, i32 %7, i32 0)
  %9 = extractelement <4 x i64> %0, i64 2
  %10 = extractelement <4 x i32> %1, i64 2
  %11 = tail call i64 @llvm.kvx.shr.i64(i64 %9, i32 %10, i32 0)
  %12 = extractelement <4 x i64> %0, i64 3
  %13 = extractelement <4 x i32> %1, i64 3
  %14 = tail call i64 @llvm.kvx.shr.i64(i64 %12, i32 %13, i32 0)
  %15 = insertelement <4 x i64> undef, i64 %5, i32 0
  %16 = insertelement <4 x i64> %15, i64 %8, i32 1
  %17 = insertelement <4 x i64> %16, i64 %11, i32 2
  %18 = insertelement <4 x i64> %17, i64 %14, i32 3
  ret <4 x i64> %18
}

define <4 x i64> @shrdq_s(<4 x i64> %0, <4 x i32> %1) {
; CHECK-LABEL: shrdq_s:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srad $r0 = $r0, $r4
; CHECK-NEXT:    srad $r2 = $r2, $r5
; CHECK-NEXT:    srad $r4 = $r4, 32
; CHECK-NEXT:    srad $r6 = $r5, 32
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    srad $r1 = $r1, $r4
; CHECK-NEXT:    srad $r3 = $r3, $r6
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
  %3 = extractelement <4 x i64> %0, i64 0
  %4 = extractelement <4 x i32> %1, i64 0
  %5 = tail call i64 @llvm.kvx.shr.i64(i64 %3, i32 %4, i32 1)
  %6 = extractelement <4 x i64> %0, i64 1
  %7 = extractelement <4 x i32> %1, i64 1
  %8 = tail call i64 @llvm.kvx.shr.i64(i64 %6, i32 %7, i32 1)
  %9 = extractelement <4 x i64> %0, i64 2
  %10 = extractelement <4 x i32> %1, i64 2
  %11 = tail call i64 @llvm.kvx.shr.i64(i64 %9, i32 %10, i32 1)
  %12 = extractelement <4 x i64> %0, i64 3
  %13 = extractelement <4 x i32> %1, i64 3
  %14 = tail call i64 @llvm.kvx.shr.i64(i64 %12, i32 %13, i32 1)
  %15 = insertelement <4 x i64> undef, i64 %5, i32 0
  %16 = insertelement <4 x i64> %15, i64 %8, i32 1
  %17 = insertelement <4 x i64> %16, i64 %11, i32 2
  %18 = insertelement <4 x i64> %17, i64 %14, i32 3
  ret <4 x i64> %18
}

define <4 x i64> @shrdq_us(<4 x i64> %0, <4 x i32> %1) {
; CHECK-LABEL: shrdq_us:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srsd $r0 = $r0, $r4
; CHECK-NEXT:    srsd $r2 = $r2, $r5
; CHECK-NEXT:    srad $r4 = $r4, 32
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    srsd $r1 = $r1, $r4
; CHECK-NEXT:    srad $r4 = $r5, 32
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    srsd $r3 = $r3, $r4
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 2)
  %3 = extractelement <4 x i64> %0, i64 0
  %4 = extractelement <4 x i32> %1, i64 0
  %5 = tail call i64 @llvm.kvx.shr.i64(i64 %3, i32 %4, i32 2)
  %6 = extractelement <4 x i64> %0, i64 1
  %7 = extractelement <4 x i32> %1, i64 1
  %8 = tail call i64 @llvm.kvx.shr.i64(i64 %6, i32 %7, i32 2)
  %9 = extractelement <4 x i64> %0, i64 2
  %10 = extractelement <4 x i32> %1, i64 2
  %11 = tail call i64 @llvm.kvx.shr.i64(i64 %9, i32 %10, i32 2)
  %12 = extractelement <4 x i64> %0, i64 3
  %13 = extractelement <4 x i32> %1, i64 3
  %14 = tail call i64 @llvm.kvx.shr.i64(i64 %12, i32 %13, i32 2)
  %15 = insertelement <4 x i64> undef, i64 %5, i32 0
  %16 = insertelement <4 x i64> %15, i64 %8, i32 1
  %17 = insertelement <4 x i64> %16, i64 %11, i32 2
  %18 = insertelement <4 x i64> %17, i64 %14, i32 3
  ret <4 x i64> %18
}

define <4 x i64> @shrdq_r(<4 x i64> %0, <4 x i32> %1) {
; CHECK-LABEL: shrdq_r:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srad $r4 = $r4, 32
; CHECK-NEXT:    andw $r6 = $r4, 63
; CHECK-NEXT:    negw $r7 = $r4
; CHECK-NEXT:    andw $r8 = $r5, 63
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    andw $r4 = $r4, 63
; CHECK-NEXT:    srld $r6 = $r0, $r6
; CHECK-NEXT:    andw $r7 = $r7, 63
; CHECK-NEXT:    negw $r9 = $r4
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    slld $r0 = $r0, $r7
; CHECK-NEXT:    negw $r5 = $r5
; CHECK-NEXT:    srad $r7 = $r5, 32
; CHECK-NEXT:    andw $r9 = $r9, 63
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    slld $r1 = $r1, $r9
; CHECK-NEXT:    srld $r4 = $r1, $r4
; CHECK-NEXT:    andw $r5 = $r5, 63
; CHECK-NEXT:    negw $r10 = $r7
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    slld $r2 = $r2, $r5
; CHECK-NEXT:    andw $r7 = $r7, 63
; CHECK-NEXT:    srld $r8 = $r2, $r8
; CHECK-NEXT:    andw $r9 = $r10, 63
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    ord $r0 = $r6, $r0
; CHECK-NEXT:    ord $r1 = $r4, $r1
; CHECK-NEXT:    slld $r3 = $r3, $r9
; CHECK-NEXT:    srld $r5 = $r3, $r7
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    ord $r2 = $r8, $r2
; CHECK-NEXT:    ord $r3 = $r5, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 6)
  %3 = extractelement <4 x i64> %0, i64 0
  %4 = extractelement <4 x i32> %1, i64 0
  %5 = tail call i64 @llvm.kvx.shr.i64(i64 %3, i32 %4, i32 3)
  %6 = extractelement <4 x i64> %0, i64 1
  %7 = extractelement <4 x i32> %1, i64 1
  %8 = tail call i64 @llvm.kvx.shr.i64(i64 %6, i32 %7, i32 3)
  %9 = extractelement <4 x i64> %0, i64 2
  %10 = extractelement <4 x i32> %1, i64 2
  %11 = tail call i64 @llvm.kvx.shr.i64(i64 %9, i32 %10, i32 3)
  %12 = extractelement <4 x i64> %0, i64 3
  %13 = extractelement <4 x i32> %1, i64 3
  %14 = tail call i64 @llvm.kvx.shr.i64(i64 %12, i32 %13, i32 3)
  %15 = insertelement <4 x i64> undef, i64 %5, i32 0
  %16 = insertelement <4 x i64> %15, i64 %8, i32 1
  %17 = insertelement <4 x i64> %16, i64 %11, i32 2
  %18 = insertelement <4 x i64> %17, i64 %14, i32 3
  ret <4 x i64> %18
}

define <4 x i64> @shrdqs(<4 x i64> %0, i32 %1) {
; CHECK-LABEL: shrdqs:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srld $r0 = $r0, $r4
; CHECK-NEXT:    srld $r1 = $r1, $r4
; CHECK-NEXT:    srld $r2 = $r2, $r4
; CHECK-NEXT:    srld $r3 = $r3, $r4
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %3 = extractelement <4 x i64> %0, i64 0
  %4 = tail call i64 @llvm.kvx.shr.i64(i64 %3, i32 %1, i32 0)
  %5 = extractelement <4 x i64> %0, i64 1
  %6 = tail call i64 @llvm.kvx.shr.i64(i64 %5, i32 %1, i32 0)
  %7 = extractelement <4 x i64> %0, i64 2
  %8 = tail call i64 @llvm.kvx.shr.i64(i64 %7, i32 %1, i32 0)
  %9 = extractelement <4 x i64> %0, i64 3
  %10 = tail call i64 @llvm.kvx.shr.i64(i64 %9, i32 %1, i32 0)
  %11 = insertelement <4 x i64> undef, i64 %4, i32 0
  %12 = insertelement <4 x i64> %11, i64 %6, i32 1
  %13 = insertelement <4 x i64> %12, i64 %8, i32 2
  %14 = insertelement <4 x i64> %13, i64 %10, i32 3
  ret <4 x i64> %14
}

define <4 x i64> @shrdqs_s(<4 x i64> %0, i32 %1) {
; CHECK-LABEL: shrdqs_s:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srad $r0 = $r0, $r4
; CHECK-NEXT:    srad $r1 = $r1, $r4
; CHECK-NEXT:    srad $r2 = $r2, $r4
; CHECK-NEXT:    srad $r3 = $r3, $r4
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %3 = extractelement <4 x i64> %0, i64 0
  %4 = tail call i64 @llvm.kvx.shr.i64(i64 %3, i32 %1, i32 1)
  %5 = extractelement <4 x i64> %0, i64 1
  %6 = tail call i64 @llvm.kvx.shr.i64(i64 %5, i32 %1, i32 1)
  %7 = extractelement <4 x i64> %0, i64 2
  %8 = tail call i64 @llvm.kvx.shr.i64(i64 %7, i32 %1, i32 1)
  %9 = extractelement <4 x i64> %0, i64 3
  %10 = tail call i64 @llvm.kvx.shr.i64(i64 %9, i32 %1, i32 1)
  %11 = insertelement <4 x i64> undef, i64 %4, i32 0
  %12 = insertelement <4 x i64> %11, i64 %6, i32 1
  %13 = insertelement <4 x i64> %12, i64 %8, i32 2
  %14 = insertelement <4 x i64> %13, i64 %10, i32 3
  ret <4 x i64> %14
}

define <4 x i64> @shrdqs_us(<4 x i64> %0, i32 %1) {
; CHECK-LABEL: shrdqs_us:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srsd $r0 = $r0, $r4
; CHECK-NEXT:    srsd $r1 = $r1, $r4
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    srsd $r2 = $r2, $r4
; CHECK-NEXT:    srsd $r3 = $r3, $r4
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
  %3 = extractelement <4 x i64> %0, i64 0
  %4 = tail call i64 @llvm.kvx.shr.i64(i64 %3, i32 %1, i32 2)
  %5 = extractelement <4 x i64> %0, i64 1
  %6 = tail call i64 @llvm.kvx.shr.i64(i64 %5, i32 %1, i32 2)
  %7 = extractelement <4 x i64> %0, i64 2
  %8 = tail call i64 @llvm.kvx.shr.i64(i64 %7, i32 %1, i32 2)
  %9 = extractelement <4 x i64> %0, i64 3
  %10 = tail call i64 @llvm.kvx.shr.i64(i64 %9, i32 %1, i32 2)
  %11 = insertelement <4 x i64> undef, i64 %4, i32 0
  %12 = insertelement <4 x i64> %11, i64 %6, i32 1
  %13 = insertelement <4 x i64> %12, i64 %8, i32 2
  %14 = insertelement <4 x i64> %13, i64 %10, i32 3
  ret <4 x i64> %14
}

define <4 x i64> @shrdqs_r(<4 x i64> %0, i32 %1) {
; CHECK-LABEL: shrdqs_r:
; CHECK:       # %bb.0:
; CHECK-NEXT:    andw $r4 = $r4, 63
; CHECK-NEXT:    negw $r5 = $r4
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    andw $r5 = $r5, 63
; CHECK-NEXT:    srld $r6 = $r0, $r4
; CHECK-NEXT:    srld $r7 = $r1, $r4
; CHECK-NEXT:    srld $r8 = $r2, $r4
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    slld $r0 = $r0, $r5
; CHECK-NEXT:    slld $r1 = $r1, $r5
; CHECK-NEXT:    slld $r2 = $r2, $r5
; CHECK-NEXT:    srld $r4 = $r3, $r4
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    ord $r0 = $r6, $r0
; CHECK-NEXT:    ord $r1 = $r7, $r1
; CHECK-NEXT:    ord $r2 = $r8, $r2
; CHECK-NEXT:    slld $r3 = $r3, $r5
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    ord $r3 = $r4, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 4)
  %3 = extractelement <4 x i64> %0, i64 0
  %4 = tail call i64 @llvm.kvx.shr.i64(i64 %3, i32 %1, i32 3)
  %5 = extractelement <4 x i64> %0, i64 1
  %6 = tail call i64 @llvm.kvx.shr.i64(i64 %5, i32 %1, i32 3)
  %7 = extractelement <4 x i64> %0, i64 2
  %8 = tail call i64 @llvm.kvx.shr.i64(i64 %7, i32 %1, i32 3)
  %9 = extractelement <4 x i64> %0, i64 3
  %10 = tail call i64 @llvm.kvx.shr.i64(i64 %9, i32 %1, i32 3)
  %11 = insertelement <4 x i64> undef, i64 %4, i32 0
  %12 = insertelement <4 x i64> %11, i64 %6, i32 1
  %13 = insertelement <4 x i64> %12, i64 %8, i32 2
  %14 = insertelement <4 x i64> %13, i64 %10, i32 3
  ret <4 x i64> %14
}

define <8 x i16> @shrhos(<8 x i16> %0, i32 %1) {
; CHECK-LABEL: shrhos:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srlhqs $r0 = $r0, $r2
; CHECK-NEXT:    srlhqs $r1 = $r1, $r2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %3 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4 = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> %3, i32 %1, i32 0)
  %5 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6 = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> %5, i32 %1, i32 0)
  %7 = shufflevector <4 x i16> %4, <4 x i16> %6, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i16> %7
}

declare <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16>, i32, i32)

define <8 x i16> @shrhos_s(<8 x i16> %0, i32 %1) {
; CHECK-LABEL: shrhos_s:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srahqs $r0 = $r0, $r2
; CHECK-NEXT:    srahqs $r1 = $r1, $r2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %3 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4 = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> %3, i32 %1, i32 1)
  %5 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6 = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> %5, i32 %1, i32 1)
  %7 = shufflevector <4 x i16> %4, <4 x i16> %6, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i16> %7
}

define <8 x i16> @shrhos_us(<8 x i16> %0, i32 %1) {
; CHECK-LABEL: shrhos_us:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srshqs $r0 = $r0, $r2
; CHECK-NEXT:    srshqs $r1 = $r1, $r2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %3 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4 = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> %3, i32 %1, i32 2)
  %5 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6 = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> %5, i32 %1, i32 2)
  %7 = shufflevector <4 x i16> %4, <4 x i16> %6, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i16> %7
}

define <8 x i16> @shrhos_r(<8 x i16> %0, i32 %1) {
; CV1-LABEL: shrhos_r:
; CV1:       # %bb.0:
; CV1-NEXT:    sbmm8 $r2 = $r2, 0x2010201.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    neghq $r2 = $r2
; CV1-NEXT:    andd $r3 = $r2, 0xf000f.@
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andd $r2 = $r2, 0xf000f.@
; CV1-NEXT:    srlhqs $r4 = $r0, $r3
; CV1-NEXT:    extfz $r5 = $r3, 19, 16
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    srlhqs $r6 = $r0, $r5
; CV1-NEXT:    extfz $r7 = $r3, 35, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srlhqs $r4 = $r0, $r7
; CV1-NEXT:    insf $r6 = $r4, 15, 0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    insf $r4 = $r6, 31, 0
; CV1-NEXT:    extfz $r6 = $r3, 51, 48
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlhqs $r8 = $r0, $r6
; CV1-NEXT:    sllhqs $r9 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    extfz $r4 = $r2, 19, 16
; CV1-NEXT:    insf $r8 = $r4, 47, 0
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    sllhqs $r10 = $r0, $r4
; CV1-NEXT:    extfz $r11 = $r2, 35, 32
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    srlhqs $r3 = $r1, $r3
; CV1-NEXT:    extfz $r15 = $r2, 51, 48
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    sllhqs $r2 = $r1, $r2
; CV1-NEXT:    srlhqs $r5 = $r1, $r5
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sllhqs $r4 = $r1, $r4
; CV1-NEXT:    insf $r10 = $r9, 15, 0
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    insf $r5 = $r3, 15, 0
; CV1-NEXT:    sllhqs $r9 = $r0, $r11
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    srlhqs $r3 = $r1, $r7
; CV1-NEXT:    insf $r4 = $r2, 15, 0
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    sllhqs $r2 = $r1, $r11
; CV1-NEXT:    insf $r9 = $r10, 31, 0
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    sllhqs $r0 = $r0, $r15
; CV1-NEXT:    insf $r3 = $r5, 31, 0
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    insf $r2 = $r4, 31, 0
; CV1-NEXT:    srlhqs $r5 = $r1, $r6
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    insf $r0 = $r9, 47, 0
; CV1-NEXT:    sllhqs $r1 = $r1, $r15
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    ord $r0 = $r8, $r0
; CV1-NEXT:    insf $r1 = $r2, 47, 0
; CV1-NEXT:    insf $r5 = $r3, 47, 0
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    ord $r1 = $r5, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 19)
;
; CV2-LABEL: shrhos_r:
; CV2:       # %bb.0:
; CV2-NEXT:    sbmm8 $r2 = $r2, 0x2010201.@
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    neghq $r2 = $r2
; CV2-NEXT:    andd $r3 = $r2, 0xf000f.@
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    andd $r2 = $r2, 0xf000f.@
; CV2-NEXT:    srlhqs $r4 = $r0, $r3
; CV2-NEXT:    extfz $r5 = $r3, 19, 16
; CV2-NEXT:    extfz $r6 = $r3, 35, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    extfz $r7 = $r3, 51, 48
; CV2-NEXT:    srlhqs $r8 = $r0, $r5
; CV2-NEXT:    srlhqs $r9 = $r0, $r6
; CV2-NEXT:    extfz $r11 = $r2, 19, 16
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlhqs $r3 = $r1, $r3
; CV2-NEXT:    sllhqs $r4 = $r0, $r2
; CV2-NEXT:    srlhqs $r5 = $r1, $r5
; CV2-NEXT:    insf $r8 = $r4, 15, 0
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r5 = $r3, 15, 0
; CV2-NEXT:    sllhqs $r8 = $r0, $r11
; CV2-NEXT:    insf $r9 = $r8, 31, 0
; CV2-NEXT:    extfz $r15 = $r2, 35, 32
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    sllhqs $r2 = $r1, $r2
; CV2-NEXT:    sllhqs $r3 = $r0, $r15
; CV2-NEXT:    extfz $r4 = $r2, 51, 48
; CV2-NEXT:    insf $r8 = $r4, 15, 0
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r3 = $r8, 31, 0
; CV2-NEXT:    srlhqs $r6 = $r1, $r6
; CV2-NEXT:    sllhqs $r8 = $r1, $r11
; CV2-NEXT:    srlhqs $r10 = $r0, $r7
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    sllhqs $r0 = $r0, $r4
; CV2-NEXT:    sllhqs $r2 = $r1, $r15
; CV2-NEXT:    insf $r6 = $r5, 31, 0
; CV2-NEXT:    insf $r8 = $r2, 15, 0
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    sllhqs $r1 = $r1, $r4
; CV2-NEXT:    insf $r2 = $r8, 31, 0
; CV2-NEXT:    srlhqs $r5 = $r1, $r7
; CV2-NEXT:    insf $r10 = $r9, 47, 0
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r0 = $r3, 47, 0
; CV2-NEXT:    insf $r1 = $r2, 47, 0
; CV2-NEXT:    insf $r5 = $r6, 47, 0
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    ord $r0 = $r10, $r0
; CV2-NEXT:    ord $r1 = $r5, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 11)
  %3 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4 = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> %3, i32 %1, i32 3)
  %5 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6 = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> %5, i32 %1, i32 3)
  %7 = shufflevector <4 x i16> %4, <4 x i16> %6, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i16> %7
}

define <2 x i16> @shrhps(<2 x i16> %0, i32 %1) {
; CHECK-LABEL: shrhps:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srlhqs $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %3 = tail call <2 x i16> @llvm.kvx.shr.v2i16(<2 x i16> %0, i32 %1, i32 0)
  ret <2 x i16> %3
}

declare <2 x i16> @llvm.kvx.shr.v2i16(<2 x i16>, i32, i32)

define <2 x i16> @shrhps_s(<2 x i16> %0, i32 %1) {
; CHECK-LABEL: shrhps_s:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srahqs $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %3 = tail call <2 x i16> @llvm.kvx.shr.v2i16(<2 x i16> %0, i32 %1, i32 1)
  ret <2 x i16> %3
}

define <2 x i16> @shrhps_us(<2 x i16> %0, i32 %1) {
; CHECK-LABEL: shrhps_us:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srshqs $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %3 = tail call <2 x i16> @llvm.kvx.shr.v2i16(<2 x i16> %0, i32 %1, i32 2)
  ret <2 x i16> %3
}

define <2 x i16> @shrhps_r(<2 x i16> %0, i32 %1) {
; CV1-LABEL: shrhps_r:
; CV1:       # %bb.0:
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    neghq $r1 = $r1
; CV1-NEXT:    andw $r2 = $r1, 0xf000f
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andw $r1 = $r1, 0xf000f
; CV1-NEXT:    extfz $r2 = $r2, 19, 16
; CV1-NEXT:    srlhqs $r3 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    srlhqs $r2 = $r0, $r2
; CV1-NEXT:    extfz $r4 = $r1, 19, 16
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sllhqs $r0 = $r0, $r4
; CV1-NEXT:    sllhqs $r1 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    insf $r0 = $r1, 15, 0
; CV1-NEXT:    insf $r2 = $r3, 15, 0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    orw $r0 = $r2, $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: shrhps_r:
; CV2:       # %bb.0:
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    neghq $r1 = $r1
; CV2-NEXT:    andw $r2 = $r1, 0xf000f
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    andw $r1 = $r1, 0xf000f
; CV2-NEXT:    extfz $r2 = $r2, 19, 16
; CV2-NEXT:    srlhqs $r3 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sllhqs $r1 = $r0, $r1
; CV2-NEXT:    srlhqs $r2 = $r0, $r2
; CV2-NEXT:    extfz $r4 = $r1, 19, 16
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sllhqs $r0 = $r0, $r4
; CV2-NEXT:    insf $r2 = $r3, 15, 0
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r0 = $r1, 15, 0
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    orw $r0 = $r2, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 6)
  %3 = tail call <2 x i16> @llvm.kvx.shr.v2i16(<2 x i16> %0, i32 %1, i32 3)
  ret <2 x i16> %3
}

define <4 x i16> @shrhqs(<4 x i16> %0, i32 %1) {
; CHECK-LABEL: shrhqs:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srlhqs $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %3 = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> %0, i32 %1, i32 0)
  ret <4 x i16> %3
}

define <4 x i16> @shrhqs_s(<4 x i16> %0, i32 %1) {
; CHECK-LABEL: shrhqs_s:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srahqs $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %3 = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> %0, i32 %1, i32 1)
  ret <4 x i16> %3
}

define <4 x i16> @shrhqs_us(<4 x i16> %0, i32 %1) {
; CHECK-LABEL: shrhqs_us:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srshqs $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %3 = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> %0, i32 %1, i32 2)
  ret <4 x i16> %3
}

define <4 x i16> @shrhqs_r(<4 x i16> %0, i32 %1) {
; CV1-LABEL: shrhqs_r:
; CV1:       # %bb.0:
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x2010201.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    neghq $r1 = $r1
; CV1-NEXT:    andd $r2 = $r1, 0xf000f.@
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andd $r1 = $r1, 0xf000f.@
; CV1-NEXT:    srlhqs $r3 = $r0, $r2
; CV1-NEXT:    extfz $r4 = $r2, 19, 16
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    srlhqs $r4 = $r0, $r4
; CV1-NEXT:    extfz $r5 = $r2, 35, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srlhqs $r3 = $r0, $r5
; CV1-NEXT:    insf $r4 = $r3, 15, 0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    extfz $r5 = $r1, 19, 16
; CV1-NEXT:    sllhqs $r6 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sllhqs $r5 = $r0, $r5
; CV1-NEXT:    extfz $r7 = $r1, 35, 32
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r5 = $r6, 15, 0
; CV1-NEXT:    sllhqs $r6 = $r0, $r7
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    extfz $r1 = $r1, 51, 48
; CV1-NEXT:    extfz $r2 = $r2, 51, 48
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r3 = $r4, 31, 0
; CV1-NEXT:    insf $r6 = $r5, 31, 0
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    sllhqs $r0 = $r0, $r1
; CV1-NEXT:    srlhqs $r2 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    insf $r0 = $r6, 47, 0
; CV1-NEXT:    insf $r2 = $r3, 47, 0
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    ord $r0 = $r2, $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 12)
;
; CV2-LABEL: shrhqs_r:
; CV2:       # %bb.0:
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x2010201.@
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    neghq $r1 = $r1
; CV2-NEXT:    andd $r2 = $r1, 0xf000f.@
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    andd $r1 = $r1, 0xf000f.@
; CV2-NEXT:    srlhqs $r3 = $r0, $r2
; CV2-NEXT:    extfz $r4 = $r2, 19, 16
; CV2-NEXT:    extfz $r5 = $r2, 35, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    srlhqs $r4 = $r0, $r4
; CV2-NEXT:    srlhqs $r5 = $r0, $r5
; CV2-NEXT:    sllhqs $r6 = $r0, $r1
; CV2-NEXT:    extfz $r7 = $r1, 19, 16
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    extfz $r2 = $r2, 51, 48
; CV2-NEXT:    sllhqs $r3 = $r0, $r7
; CV2-NEXT:    insf $r4 = $r3, 15, 0
; CV2-NEXT:    extfz $r7 = $r1, 35, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    extfz $r1 = $r1, 51, 48
; CV2-NEXT:    insf $r3 = $r6, 15, 0
; CV2-NEXT:    insf $r5 = $r4, 31, 0
; CV2-NEXT:    sllhqs $r6 = $r0, $r7
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    sllhqs $r0 = $r0, $r1
; CV2-NEXT:    srlhqs $r2 = $r0, $r2
; CV2-NEXT:    insf $r6 = $r3, 31, 0
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r0 = $r6, 47, 0
; CV2-NEXT:    insf $r2 = $r5, 47, 0
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    ord $r0 = $r2, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
  %3 = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> %0, i32 %1, i32 3)
  ret <4 x i16> %3
}

define <16 x i16> @shrhxs(<16 x i16> %0, i32 %1) {
; CV1-LABEL: shrhxs:
; CV1:       # %bb.0:
; CV1-NEXT:    srlhqs $r0 = $r0, $r4
; CV1-NEXT:    srlhqs $r1 = $r1, $r4
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srlhqs $r2 = $r2, $r4
; CV1-NEXT:    srlhqs $r3 = $r3, $r4
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 1)
;
; CV2-LABEL: shrhxs:
; CV2:       # %bb.0:
; CV2-NEXT:    srlhqs $r0 = $r0, $r4
; CV2-NEXT:    srlhqs $r1 = $r1, $r4
; CV2-NEXT:    srlhqs $r2 = $r2, $r4
; CV2-NEXT:    srlhqs $r3 = $r3, $r4
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %3 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4 = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> %3, i32 %1, i32 0)
  %5 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6 = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> %5, i32 %1, i32 0)
  %7 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8 = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> %7, i32 %1, i32 0)
  %9 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %10 = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> %9, i32 %1, i32 0)
  %11 = shufflevector <4 x i16> %4, <4 x i16> %6, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %12 = shufflevector <4 x i16> %8, <4 x i16> %10, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %13 = shufflevector <8 x i16> %11, <8 x i16> %12, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  ret <16 x i16> %13
}

define <16 x i16> @shrhxs_s(<16 x i16> %0, i32 %1) {
; CV1-LABEL: shrhxs_s:
; CV1:       # %bb.0:
; CV1-NEXT:    srahqs $r0 = $r0, $r4
; CV1-NEXT:    srahqs $r1 = $r1, $r4
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srahqs $r2 = $r2, $r4
; CV1-NEXT:    srahqs $r3 = $r3, $r4
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 1)
;
; CV2-LABEL: shrhxs_s:
; CV2:       # %bb.0:
; CV2-NEXT:    srahqs $r0 = $r0, $r4
; CV2-NEXT:    srahqs $r1 = $r1, $r4
; CV2-NEXT:    srahqs $r2 = $r2, $r4
; CV2-NEXT:    srahqs $r3 = $r3, $r4
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %3 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4 = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> %3, i32 %1, i32 1)
  %5 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6 = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> %5, i32 %1, i32 1)
  %7 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8 = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> %7, i32 %1, i32 1)
  %9 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %10 = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> %9, i32 %1, i32 1)
  %11 = shufflevector <4 x i16> %4, <4 x i16> %6, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %12 = shufflevector <4 x i16> %8, <4 x i16> %10, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %13 = shufflevector <8 x i16> %11, <8 x i16> %12, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  ret <16 x i16> %13
}

define <16 x i16> @shrhxs_us(<16 x i16> %0, i32 %1) {
; CHECK-LABEL: shrhxs_us:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srshqs $r0 = $r0, $r4
; CHECK-NEXT:    srshqs $r1 = $r1, $r4
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    srshqs $r2 = $r2, $r4
; CHECK-NEXT:    srshqs $r3 = $r3, $r4
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
  %3 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4 = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> %3, i32 %1, i32 2)
  %5 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6 = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> %5, i32 %1, i32 2)
  %7 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8 = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> %7, i32 %1, i32 2)
  %9 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %10 = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> %9, i32 %1, i32 2)
  %11 = shufflevector <4 x i16> %4, <4 x i16> %6, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %12 = shufflevector <4 x i16> %8, <4 x i16> %10, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %13 = shufflevector <8 x i16> %11, <8 x i16> %12, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  ret <16 x i16> %13
}

define <16 x i16> @shrhxs_r(<16 x i16> %0, i32 %1) {
; CV1-LABEL: shrhxs_r:
; CV1:       # %bb.0:
; CV1-NEXT:    sbmm8 $r4 = $r4, 0x2010201.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    neghq $r4 = $r4
; CV1-NEXT:    andd $r5 = $r4, 0xf000f.@
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andd $r4 = $r4, 0xf000f.@
; CV1-NEXT:    srlhqs $r6 = $r0, $r5
; CV1-NEXT:    extfz $r7 = $r5, 19, 16
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    srlhqs $r8 = $r0, $r7
; CV1-NEXT:    extfz $r9 = $r5, 35, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srlhqs $r6 = $r0, $r9
; CV1-NEXT:    insf $r8 = $r6, 15, 0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    insf $r6 = $r8, 31, 0
; CV1-NEXT:    extfz $r8 = $r5, 51, 48
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlhqs $r10 = $r0, $r8
; CV1-NEXT:    sllhqs $r11 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    extfz $r6 = $r4, 19, 16
; CV1-NEXT:    insf $r10 = $r6, 47, 0
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    sllhqs $r15 = $r0, $r6
; CV1-NEXT:    extfz $r16 = $r4, 35, 32
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    sllhqs $r11 = $r0, $r16
; CV1-NEXT:    insf $r15 = $r11, 15, 0
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r11 = $r15, 31, 0
; CV1-NEXT:    extfz $r15 = $r4, 51, 48
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sllhqs $r0 = $r0, $r15
; CV1-NEXT:    srlhqs $r17 = $r1, $r5
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    insf $r0 = $r11, 47, 0
; CV1-NEXT:    srlhqs $r11 = $r1, $r7
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    ord $r0 = $r10, $r0
; CV1-NEXT:    insf $r11 = $r17, 15, 0
; CV1-NEXT:    srlhqs $r17 = $r1, $r9
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    srlhqs $r11 = $r1, $r8
; CV1-NEXT:    insf $r17 = $r11, 31, 0
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    insf $r11 = $r17, 47, 0
; CV1-NEXT:    sllhqs $r17 = $r1, $r4
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    sllhqs $r32 = $r1, $r6
; CV1-NEXT:    sllhqs $r33 = $r1, $r16
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    sllhqs $r1 = $r1, $r15
; CV1-NEXT:    insf $r32 = $r17, 15, 0
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    srlhqs $r17 = $r2, $r5
; CV1-NEXT:    insf $r33 = $r32, 31, 0
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    insf $r1 = $r33, 47, 0
; CV1-NEXT:    srlhqs $r32 = $r2, $r7
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    ord $r1 = $r11, $r1
; CV1-NEXT:    sllhqs $r33 = $r2, $r4
; CV1-NEXT:    sllhqs $r34 = $r2, $r6
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    srlhqs $r5 = $r3, $r5
; CV1-NEXT:    srlhqs $r7 = $r3, $r7
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    sllhqs $r4 = $r3, $r4
; CV1-NEXT:    sllhqs $r6 = $r3, $r6
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    srlhqs $r17 = $r2, $r9
; CV1-NEXT:    insf $r32 = $r17, 15, 0
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    sllhqs $r33 = $r2, $r16
; CV1-NEXT:    insf $r34 = $r33, 15, 0
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    srlhqs $r5 = $r3, $r9
; CV1-NEXT:    insf $r7 = $r5, 15, 0
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    sllhqs $r4 = $r3, $r16
; CV1-NEXT:    insf $r6 = $r4, 15, 0
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    insf $r17 = $r32, 31, 0
; CV1-NEXT:    insf $r33 = $r34, 31, 0
; CV1-NEXT:    ;; # (end cycle 27)
; CV1-NEXT:    sllhqs $r2 = $r2, $r15
; CV1-NEXT:    srlhqs $r32 = $r2, $r8
; CV1-NEXT:    ;; # (end cycle 28)
; CV1-NEXT:    insf $r5 = $r7, 31, 0
; CV1-NEXT:    srlhqs $r7 = $r3, $r8
; CV1-NEXT:    ;; # (end cycle 29)
; CV1-NEXT:    sllhqs $r3 = $r3, $r15
; CV1-NEXT:    insf $r4 = $r6, 31, 0
; CV1-NEXT:    ;; # (end cycle 30)
; CV1-NEXT:    insf $r2 = $r33, 47, 0
; CV1-NEXT:    insf $r32 = $r17, 47, 0
; CV1-NEXT:    ;; # (end cycle 31)
; CV1-NEXT:    ord $r2 = $r32, $r2
; CV1-NEXT:    insf $r3 = $r4, 47, 0
; CV1-NEXT:    insf $r7 = $r5, 47, 0
; CV1-NEXT:    ;; # (end cycle 32)
; CV1-NEXT:    ord $r3 = $r7, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 33)
;
; CV2-LABEL: shrhxs_r:
; CV2:       # %bb.0:
; CV2-NEXT:    sbmm8 $r4 = $r4, 0x2010201.@
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    neghq $r4 = $r4
; CV2-NEXT:    andd $r5 = $r4, 0xf000f.@
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    andd $r4 = $r4, 0xf000f.@
; CV2-NEXT:    srlhqs $r6 = $r0, $r5
; CV2-NEXT:    extfz $r7 = $r5, 19, 16
; CV2-NEXT:    extfz $r8 = $r5, 35, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    srlhqs $r10 = $r0, $r7
; CV2-NEXT:    srlhqs $r11 = $r0, $r8
; CV2-NEXT:    extfz $r16 = $r4, 19, 16
; CV2-NEXT:    extfz $r17 = $r4, 35, 32
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sllhqs $r6 = $r0, $r4
; CV2-NEXT:    extfz $r9 = $r5, 51, 48
; CV2-NEXT:    insf $r10 = $r6, 15, 0
; CV2-NEXT:    sllhqs $r32 = $r0, $r17
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sllhqs $r10 = $r0, $r16
; CV2-NEXT:    insf $r11 = $r10, 31, 0
; CV2-NEXT:    srlhqs $r15 = $r0, $r9
; CV2-NEXT:    extfz $r33 = $r4, 51, 48
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    sllhqs $r0 = $r0, $r33
; CV2-NEXT:    srlhqs $r6 = $r1, $r5
; CV2-NEXT:    insf $r10 = $r6, 15, 0
; CV2-NEXT:    insf $r15 = $r11, 47, 0
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    srlhqs $r10 = $r1, $r7
; CV2-NEXT:    srlhqs $r11 = $r1, $r8
; CV2-NEXT:    insf $r32 = $r10, 31, 0
; CV2-NEXT:    srlhqs $r34 = $r1, $r9
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r0 = $r32, 47, 0
; CV2-NEXT:    sllhqs $r6 = $r1, $r4
; CV2-NEXT:    insf $r10 = $r6, 15, 0
; CV2-NEXT:    sllhqs $r32 = $r1, $r16
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    sllhqs $r1 = $r1, $r33
; CV2-NEXT:    sllhqs $r6 = $r1, $r17
; CV2-NEXT:    insf $r11 = $r10, 31, 0
; CV2-NEXT:    insf $r32 = $r6, 15, 0
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r6 = $r32, 31, 0
; CV2-NEXT:    srlhqs $r10 = $r2, $r7
; CV2-NEXT:    sllhqs $r11 = $r2, $r4
; CV2-NEXT:    insf $r34 = $r11, 47, 0
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    insf $r1 = $r6, 47, 0
; CV2-NEXT:    srlhqs $r5 = $r3, $r5
; CV2-NEXT:    srlhqs $r6 = $r2, $r5
; CV2-NEXT:    sllhqs $r32 = $r2, $r16
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    sllhqs $r4 = $r3, $r4
; CV2-NEXT:    srlhqs $r7 = $r3, $r7
; CV2-NEXT:    insf $r10 = $r6, 15, 0
; CV2-NEXT:    sllhqs $r16 = $r3, $r16
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    srlhqs $r6 = $r2, $r8
; CV2-NEXT:    insf $r7 = $r5, 15, 0
; CV2-NEXT:    sllhqs $r11 = $r2, $r17
; CV2-NEXT:    insf $r32 = $r11, 15, 0
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    srlhqs $r5 = $r3, $r8
; CV2-NEXT:    insf $r6 = $r10, 31, 0
; CV2-NEXT:    sllhqs $r8 = $r3, $r17
; CV2-NEXT:    insf $r16 = $r4, 15, 0
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    sllhqs $r2 = $r2, $r33
; CV2-NEXT:    srlhqs $r4 = $r2, $r9
; CV2-NEXT:    insf $r5 = $r7, 31, 0
; CV2-NEXT:    insf $r11 = $r32, 31, 0
; CV2-NEXT:    ;; # (end cycle 15)
; CV2-NEXT:    sllhqs $r3 = $r3, $r33
; CV2-NEXT:    insf $r4 = $r6, 47, 0
; CV2-NEXT:    srlhqs $r7 = $r3, $r9
; CV2-NEXT:    insf $r8 = $r16, 31, 0
; CV2-NEXT:    ;; # (end cycle 16)
; CV2-NEXT:    ord $r0 = $r15, $r0
; CV2-NEXT:    insf $r2 = $r11, 47, 0
; CV2-NEXT:    insf $r3 = $r8, 47, 0
; CV2-NEXT:    insf $r7 = $r5, 47, 0
; CV2-NEXT:    ;; # (end cycle 17)
; CV2-NEXT:    ord $r1 = $r34, $r1
; CV2-NEXT:    ord $r2 = $r4, $r2
; CV2-NEXT:    ord $r3 = $r7, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 18)
  %3 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4 = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> %3, i32 %1, i32 3)
  %5 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6 = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> %5, i32 %1, i32 3)
  %7 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8 = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> %7, i32 %1, i32 3)
  %9 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %10 = tail call <4 x i16> @llvm.kvx.shr.v4i16(<4 x i16> %9, i32 %1, i32 3)
  %11 = shufflevector <4 x i16> %4, <4 x i16> %6, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %12 = shufflevector <4 x i16> %8, <4 x i16> %10, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %13 = shufflevector <8 x i16> %11, <8 x i16> %12, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  ret <16 x i16> %13
}

define i32 @shrw(i32 %0, i32 %1) {
; CHECK-LABEL: shrw:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srlw $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %3 = tail call i32 @llvm.kvx.shr.i32(i32 %0, i32 %1, i32 0)
  ret i32 %3
}

declare i32 @llvm.kvx.shr.i32(i32, i32, i32)

define i32 @shrw_s(i32 %0, i32 %1) {
; CHECK-LABEL: shrw_s:
; CHECK:       # %bb.0:
; CHECK-NEXT:    sraw $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %3 = tail call i32 @llvm.kvx.shr.i32(i32 %0, i32 %1, i32 1)
  ret i32 %3
}

define i32 @shrw_us(i32 %0, i32 %1) {
; CHECK-LABEL: shrw_us:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srsw $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %3 = tail call i32 @llvm.kvx.shr.i32(i32 %0, i32 %1, i32 2)
  ret i32 %3
}

define i32 @shrw_r(i32 %0, i32 %1) {
; CHECK-LABEL: shrw_r:
; CHECK:       # %bb.0:
; CHECK-NEXT:    rorw $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %3 = tail call i32 @llvm.kvx.shr.i32(i32 %0, i32 %1, i32 3)
  ret i32 %3
}

define <8 x i32> @shrwo(<8 x i32> %0, <8 x i32> %1) {
; CV1-LABEL: shrwo:
; CV1:       # %bb.0:
; CV1-NEXT:    srad $r0 = $r0, 32
; CV1-NEXT:    srlw $r4 = $r1, $r5
; CV1-NEXT:    srlw $r8 = $r0, $r4
; CV1-NEXT:    srad $r9 = $r4, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srlw $r0 = $r0, $r9
; CV1-NEXT:    srad $r1 = $r1, 32
; CV1-NEXT:    srlw $r5 = $r2, $r6
; CV1-NEXT:    srad $r9 = $r5, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srlw $r1 = $r1, $r9
; CV1-NEXT:    srad $r2 = $r2, 32
; CV1-NEXT:    srad $r6 = $r6, 32
; CV1-NEXT:    srad $r9 = $r7, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    srlw $r2 = $r2, $r6
; CV1-NEXT:    srlw $r3 = $r3, $r7
; CV1-NEXT:    srad $r6 = $r3, 32
; CV1-NEXT:    insf $r8 = $r0, 63, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    copyd $r0 = $r8
; CV1-NEXT:    insf $r4 = $r1, 63, 32
; CV1-NEXT:    insf $r5 = $r2, 63, 32
; CV1-NEXT:    srlw $r6 = $r6, $r9
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    copyd $r1 = $r4
; CV1-NEXT:    copyd $r2 = $r5
; CV1-NEXT:    insf $r3 = $r6, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 5)
;
; CV2-LABEL: shrwo:
; CV2:       # %bb.0:
; CV2-NEXT:    srad $r0 = $r0, 32
; CV2-NEXT:    srlw $r4 = $r1, $r5
; CV2-NEXT:    srlw $r8 = $r0, $r4
; CV2-NEXT:    srad $r9 = $r4, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srlw $r0 = $r0, $r9
; CV2-NEXT:    srad $r1 = $r1, 32
; CV2-NEXT:    srad $r5 = $r5, 32
; CV2-NEXT:    srad $r9 = $r2, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    srlw $r1 = $r1, $r5
; CV2-NEXT:    srad $r5 = $r6, 32
; CV2-NEXT:    srad $r10 = $r3, 32
; CV2-NEXT:    srad $r11 = $r7, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    srlw $r2 = $r2, $r6
; CV2-NEXT:    srlw $r3 = $r3, $r7
; CV2-NEXT:    srlw $r5 = $r9, $r5
; CV2-NEXT:    srlw $r6 = $r10, $r11
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    insf $r2 = $r5, 63, 32
; CV2-NEXT:    insf $r3 = $r6, 63, 32
; CV2-NEXT:    insf $r4 = $r1, 63, 32
; CV2-NEXT:    insf $r8 = $r0, 63, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    copyd $r0 = $r8
; CV2-NEXT:    copyd $r1 = $r4
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 5)
  %3 = extractelement <8 x i32> %0, i64 0
  %4 = extractelement <8 x i32> %1, i64 0
  %5 = tail call i32 @llvm.kvx.shr.i32(i32 %3, i32 %4, i32 0)
  %6 = extractelement <8 x i32> %0, i64 1
  %7 = extractelement <8 x i32> %1, i64 1
  %8 = tail call i32 @llvm.kvx.shr.i32(i32 %6, i32 %7, i32 0)
  %9 = extractelement <8 x i32> %0, i64 2
  %10 = extractelement <8 x i32> %1, i64 2
  %11 = tail call i32 @llvm.kvx.shr.i32(i32 %9, i32 %10, i32 0)
  %12 = extractelement <8 x i32> %0, i64 3
  %13 = extractelement <8 x i32> %1, i64 3
  %14 = tail call i32 @llvm.kvx.shr.i32(i32 %12, i32 %13, i32 0)
  %15 = extractelement <8 x i32> %0, i64 4
  %16 = extractelement <8 x i32> %1, i64 4
  %17 = tail call i32 @llvm.kvx.shr.i32(i32 %15, i32 %16, i32 0)
  %18 = extractelement <8 x i32> %0, i64 5
  %19 = extractelement <8 x i32> %1, i64 5
  %20 = tail call i32 @llvm.kvx.shr.i32(i32 %18, i32 %19, i32 0)
  %21 = extractelement <8 x i32> %0, i64 6
  %22 = extractelement <8 x i32> %1, i64 6
  %23 = tail call i32 @llvm.kvx.shr.i32(i32 %21, i32 %22, i32 0)
  %24 = extractelement <8 x i32> %0, i64 7
  %25 = extractelement <8 x i32> %1, i64 7
  %26 = tail call i32 @llvm.kvx.shr.i32(i32 %24, i32 %25, i32 0)
  %27 = insertelement <8 x i32> undef, i32 %5, i32 0
  %28 = insertelement <8 x i32> %27, i32 %8, i32 1
  %29 = insertelement <8 x i32> %28, i32 %11, i32 2
  %30 = insertelement <8 x i32> %29, i32 %14, i32 3
  %31 = insertelement <8 x i32> %30, i32 %17, i32 4
  %32 = insertelement <8 x i32> %31, i32 %20, i32 5
  %33 = insertelement <8 x i32> %32, i32 %23, i32 6
  %34 = insertelement <8 x i32> %33, i32 %26, i32 7
  ret <8 x i32> %34
}

define <8 x i32> @shrwo_s(<8 x i32> %0, <8 x i32> %1) {
; CV1-LABEL: shrwo_s:
; CV1:       # %bb.0:
; CV1-NEXT:    srad $r0 = $r0, 32
; CV1-NEXT:    sraw $r4 = $r1, $r5
; CV1-NEXT:    sraw $r8 = $r0, $r4
; CV1-NEXT:    srad $r9 = $r4, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sraw $r0 = $r0, $r9
; CV1-NEXT:    srad $r1 = $r1, 32
; CV1-NEXT:    sraw $r5 = $r2, $r6
; CV1-NEXT:    srad $r9 = $r5, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sraw $r1 = $r1, $r9
; CV1-NEXT:    srad $r2 = $r2, 32
; CV1-NEXT:    srad $r6 = $r6, 32
; CV1-NEXT:    srad $r9 = $r7, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sraw $r2 = $r2, $r6
; CV1-NEXT:    sraw $r3 = $r3, $r7
; CV1-NEXT:    srad $r6 = $r3, 32
; CV1-NEXT:    insf $r8 = $r0, 63, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    copyd $r0 = $r8
; CV1-NEXT:    insf $r4 = $r1, 63, 32
; CV1-NEXT:    insf $r5 = $r2, 63, 32
; CV1-NEXT:    sraw $r6 = $r6, $r9
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    copyd $r1 = $r4
; CV1-NEXT:    copyd $r2 = $r5
; CV1-NEXT:    insf $r3 = $r6, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 5)
;
; CV2-LABEL: shrwo_s:
; CV2:       # %bb.0:
; CV2-NEXT:    srad $r0 = $r0, 32
; CV2-NEXT:    sraw $r4 = $r1, $r5
; CV2-NEXT:    sraw $r8 = $r0, $r4
; CV2-NEXT:    srad $r9 = $r4, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sraw $r0 = $r0, $r9
; CV2-NEXT:    srad $r1 = $r1, 32
; CV2-NEXT:    srad $r5 = $r5, 32
; CV2-NEXT:    srad $r9 = $r2, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sraw $r1 = $r1, $r5
; CV2-NEXT:    srad $r5 = $r6, 32
; CV2-NEXT:    srad $r10 = $r3, 32
; CV2-NEXT:    srad $r11 = $r7, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sraw $r2 = $r2, $r6
; CV2-NEXT:    sraw $r3 = $r3, $r7
; CV2-NEXT:    sraw $r5 = $r9, $r5
; CV2-NEXT:    sraw $r6 = $r10, $r11
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    insf $r2 = $r5, 63, 32
; CV2-NEXT:    insf $r3 = $r6, 63, 32
; CV2-NEXT:    insf $r4 = $r1, 63, 32
; CV2-NEXT:    insf $r8 = $r0, 63, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    copyd $r0 = $r8
; CV2-NEXT:    copyd $r1 = $r4
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 5)
  %3 = extractelement <8 x i32> %0, i64 0
  %4 = extractelement <8 x i32> %1, i64 0
  %5 = tail call i32 @llvm.kvx.shr.i32(i32 %3, i32 %4, i32 1)
  %6 = extractelement <8 x i32> %0, i64 1
  %7 = extractelement <8 x i32> %1, i64 1
  %8 = tail call i32 @llvm.kvx.shr.i32(i32 %6, i32 %7, i32 1)
  %9 = extractelement <8 x i32> %0, i64 2
  %10 = extractelement <8 x i32> %1, i64 2
  %11 = tail call i32 @llvm.kvx.shr.i32(i32 %9, i32 %10, i32 1)
  %12 = extractelement <8 x i32> %0, i64 3
  %13 = extractelement <8 x i32> %1, i64 3
  %14 = tail call i32 @llvm.kvx.shr.i32(i32 %12, i32 %13, i32 1)
  %15 = extractelement <8 x i32> %0, i64 4
  %16 = extractelement <8 x i32> %1, i64 4
  %17 = tail call i32 @llvm.kvx.shr.i32(i32 %15, i32 %16, i32 1)
  %18 = extractelement <8 x i32> %0, i64 5
  %19 = extractelement <8 x i32> %1, i64 5
  %20 = tail call i32 @llvm.kvx.shr.i32(i32 %18, i32 %19, i32 1)
  %21 = extractelement <8 x i32> %0, i64 6
  %22 = extractelement <8 x i32> %1, i64 6
  %23 = tail call i32 @llvm.kvx.shr.i32(i32 %21, i32 %22, i32 1)
  %24 = extractelement <8 x i32> %0, i64 7
  %25 = extractelement <8 x i32> %1, i64 7
  %26 = tail call i32 @llvm.kvx.shr.i32(i32 %24, i32 %25, i32 1)
  %27 = insertelement <8 x i32> undef, i32 %5, i32 0
  %28 = insertelement <8 x i32> %27, i32 %8, i32 1
  %29 = insertelement <8 x i32> %28, i32 %11, i32 2
  %30 = insertelement <8 x i32> %29, i32 %14, i32 3
  %31 = insertelement <8 x i32> %30, i32 %17, i32 4
  %32 = insertelement <8 x i32> %31, i32 %20, i32 5
  %33 = insertelement <8 x i32> %32, i32 %23, i32 6
  %34 = insertelement <8 x i32> %33, i32 %26, i32 7
  ret <8 x i32> %34
}

define <8 x i32> @shrwo_us(<8 x i32> %0, <8 x i32> %1) {
; CV1-LABEL: shrwo_us:
; CV1:       # %bb.0:
; CV1-NEXT:    srad $r0 = $r0, 32
; CV1-NEXT:    srsw $r4 = $r1, $r5
; CV1-NEXT:    srsw $r8 = $r0, $r4
; CV1-NEXT:    srad $r9 = $r4, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srsw $r0 = $r0, $r9
; CV1-NEXT:    srad $r1 = $r1, 32
; CV1-NEXT:    srad $r5 = $r5, 32
; CV1-NEXT:    srsw $r9 = $r2, $r6
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srsw $r1 = $r1, $r5
; CV1-NEXT:    srad $r2 = $r2, 32
; CV1-NEXT:    srad $r5 = $r3, 32
; CV1-NEXT:    srad $r6 = $r6, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    srsw $r2 = $r2, $r6
; CV1-NEXT:    srsw $r3 = $r3, $r7
; CV1-NEXT:    srad $r6 = $r7, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srsw $r5 = $r5, $r6
; CV1-NEXT:    insf $r8 = $r0, 63, 32
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    copyd $r0 = $r8
; CV1-NEXT:    insf $r4 = $r1, 63, 32
; CV1-NEXT:    insf $r9 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    copyd $r1 = $r4
; CV1-NEXT:    copyd $r2 = $r9
; CV1-NEXT:    insf $r3 = $r5, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: shrwo_us:
; CV2:       # %bb.0:
; CV2-NEXT:    srad $r0 = $r0, 32
; CV2-NEXT:    srsw $r4 = $r1, $r5
; CV2-NEXT:    srsw $r8 = $r0, $r4
; CV2-NEXT:    srad $r9 = $r4, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srsw $r0 = $r0, $r9
; CV2-NEXT:    srad $r1 = $r1, 32
; CV2-NEXT:    srsw $r5 = $r2, $r6
; CV2-NEXT:    srad $r9 = $r5, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    srsw $r1 = $r1, $r9
; CV2-NEXT:    srad $r2 = $r2, 32
; CV2-NEXT:    srad $r6 = $r6, 32
; CV2-NEXT:    srad $r9 = $r7, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    srsw $r2 = $r2, $r6
; CV2-NEXT:    srsw $r3 = $r3, $r7
; CV2-NEXT:    srad $r6 = $r3, 32
; CV2-NEXT:    insf $r8 = $r0, 63, 32
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    copyd $r0 = $r8
; CV2-NEXT:    insf $r4 = $r1, 63, 32
; CV2-NEXT:    insf $r5 = $r2, 63, 32
; CV2-NEXT:    srsw $r6 = $r6, $r9
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    copyd $r1 = $r4
; CV2-NEXT:    copyd $r2 = $r5
; CV2-NEXT:    insf $r3 = $r6, 63, 32
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 5)
  %3 = extractelement <8 x i32> %0, i64 0
  %4 = extractelement <8 x i32> %1, i64 0
  %5 = tail call i32 @llvm.kvx.shr.i32(i32 %3, i32 %4, i32 2)
  %6 = extractelement <8 x i32> %0, i64 1
  %7 = extractelement <8 x i32> %1, i64 1
  %8 = tail call i32 @llvm.kvx.shr.i32(i32 %6, i32 %7, i32 2)
  %9 = extractelement <8 x i32> %0, i64 2
  %10 = extractelement <8 x i32> %1, i64 2
  %11 = tail call i32 @llvm.kvx.shr.i32(i32 %9, i32 %10, i32 2)
  %12 = extractelement <8 x i32> %0, i64 3
  %13 = extractelement <8 x i32> %1, i64 3
  %14 = tail call i32 @llvm.kvx.shr.i32(i32 %12, i32 %13, i32 2)
  %15 = extractelement <8 x i32> %0, i64 4
  %16 = extractelement <8 x i32> %1, i64 4
  %17 = tail call i32 @llvm.kvx.shr.i32(i32 %15, i32 %16, i32 2)
  %18 = extractelement <8 x i32> %0, i64 5
  %19 = extractelement <8 x i32> %1, i64 5
  %20 = tail call i32 @llvm.kvx.shr.i32(i32 %18, i32 %19, i32 2)
  %21 = extractelement <8 x i32> %0, i64 6
  %22 = extractelement <8 x i32> %1, i64 6
  %23 = tail call i32 @llvm.kvx.shr.i32(i32 %21, i32 %22, i32 2)
  %24 = extractelement <8 x i32> %0, i64 7
  %25 = extractelement <8 x i32> %1, i64 7
  %26 = tail call i32 @llvm.kvx.shr.i32(i32 %24, i32 %25, i32 2)
  %27 = insertelement <8 x i32> undef, i32 %5, i32 0
  %28 = insertelement <8 x i32> %27, i32 %8, i32 1
  %29 = insertelement <8 x i32> %28, i32 %11, i32 2
  %30 = insertelement <8 x i32> %29, i32 %14, i32 3
  %31 = insertelement <8 x i32> %30, i32 %17, i32 4
  %32 = insertelement <8 x i32> %31, i32 %20, i32 5
  %33 = insertelement <8 x i32> %32, i32 %23, i32 6
  %34 = insertelement <8 x i32> %33, i32 %26, i32 7
  ret <8 x i32> %34
}

define <8 x i32> @shrwo_r(<8 x i32> %0, <8 x i32> %1) {
; CV1-LABEL: shrwo_r:
; CV1:       # %bb.0:
; CV1-NEXT:    srad $r0 = $r0, 32
; CV1-NEXT:    rorw $r4 = $r1, $r5
; CV1-NEXT:    rorw $r8 = $r0, $r4
; CV1-NEXT:    srad $r9 = $r4, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    rorw $r0 = $r0, $r9
; CV1-NEXT:    srad $r1 = $r1, 32
; CV1-NEXT:    srad $r5 = $r5, 32
; CV1-NEXT:    rorw $r9 = $r2, $r6
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    rorw $r1 = $r1, $r5
; CV1-NEXT:    srad $r2 = $r2, 32
; CV1-NEXT:    srad $r5 = $r3, 32
; CV1-NEXT:    srad $r6 = $r6, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    rorw $r2 = $r2, $r6
; CV1-NEXT:    rorw $r3 = $r3, $r7
; CV1-NEXT:    srad $r6 = $r7, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    rorw $r5 = $r5, $r6
; CV1-NEXT:    insf $r8 = $r0, 63, 32
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    copyd $r0 = $r8
; CV1-NEXT:    insf $r4 = $r1, 63, 32
; CV1-NEXT:    insf $r9 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    copyd $r1 = $r4
; CV1-NEXT:    copyd $r2 = $r9
; CV1-NEXT:    insf $r3 = $r5, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: shrwo_r:
; CV2:       # %bb.0:
; CV2-NEXT:    srad $r0 = $r0, 32
; CV2-NEXT:    rorw $r4 = $r1, $r5
; CV2-NEXT:    rorw $r8 = $r0, $r4
; CV2-NEXT:    srad $r9 = $r4, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    rorw $r0 = $r0, $r9
; CV2-NEXT:    srad $r1 = $r1, 32
; CV2-NEXT:    rorw $r5 = $r2, $r6
; CV2-NEXT:    srad $r9 = $r5, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    rorw $r1 = $r1, $r9
; CV2-NEXT:    srad $r2 = $r2, 32
; CV2-NEXT:    srad $r6 = $r6, 32
; CV2-NEXT:    srad $r9 = $r7, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    rorw $r2 = $r2, $r6
; CV2-NEXT:    rorw $r3 = $r3, $r7
; CV2-NEXT:    srad $r6 = $r3, 32
; CV2-NEXT:    insf $r8 = $r0, 63, 32
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    copyd $r0 = $r8
; CV2-NEXT:    insf $r4 = $r1, 63, 32
; CV2-NEXT:    insf $r5 = $r2, 63, 32
; CV2-NEXT:    rorw $r6 = $r6, $r9
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    copyd $r1 = $r4
; CV2-NEXT:    copyd $r2 = $r5
; CV2-NEXT:    insf $r3 = $r6, 63, 32
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 5)
  %3 = extractelement <8 x i32> %0, i64 0
  %4 = extractelement <8 x i32> %1, i64 0
  %5 = tail call i32 @llvm.kvx.shr.i32(i32 %3, i32 %4, i32 3)
  %6 = extractelement <8 x i32> %0, i64 1
  %7 = extractelement <8 x i32> %1, i64 1
  %8 = tail call i32 @llvm.kvx.shr.i32(i32 %6, i32 %7, i32 3)
  %9 = extractelement <8 x i32> %0, i64 2
  %10 = extractelement <8 x i32> %1, i64 2
  %11 = tail call i32 @llvm.kvx.shr.i32(i32 %9, i32 %10, i32 3)
  %12 = extractelement <8 x i32> %0, i64 3
  %13 = extractelement <8 x i32> %1, i64 3
  %14 = tail call i32 @llvm.kvx.shr.i32(i32 %12, i32 %13, i32 3)
  %15 = extractelement <8 x i32> %0, i64 4
  %16 = extractelement <8 x i32> %1, i64 4
  %17 = tail call i32 @llvm.kvx.shr.i32(i32 %15, i32 %16, i32 3)
  %18 = extractelement <8 x i32> %0, i64 5
  %19 = extractelement <8 x i32> %1, i64 5
  %20 = tail call i32 @llvm.kvx.shr.i32(i32 %18, i32 %19, i32 3)
  %21 = extractelement <8 x i32> %0, i64 6
  %22 = extractelement <8 x i32> %1, i64 6
  %23 = tail call i32 @llvm.kvx.shr.i32(i32 %21, i32 %22, i32 3)
  %24 = extractelement <8 x i32> %0, i64 7
  %25 = extractelement <8 x i32> %1, i64 7
  %26 = tail call i32 @llvm.kvx.shr.i32(i32 %24, i32 %25, i32 3)
  %27 = insertelement <8 x i32> undef, i32 %5, i32 0
  %28 = insertelement <8 x i32> %27, i32 %8, i32 1
  %29 = insertelement <8 x i32> %28, i32 %11, i32 2
  %30 = insertelement <8 x i32> %29, i32 %14, i32 3
  %31 = insertelement <8 x i32> %30, i32 %17, i32 4
  %32 = insertelement <8 x i32> %31, i32 %20, i32 5
  %33 = insertelement <8 x i32> %32, i32 %23, i32 6
  %34 = insertelement <8 x i32> %33, i32 %26, i32 7
  ret <8 x i32> %34
}

define <8 x i32> @shrwos(<8 x i32> %0, i32 %1) {
; CV1-LABEL: shrwos:
; CV1:       # %bb.0:
; CV1-NEXT:    srlwps $r0 = $r0, $r4
; CV1-NEXT:    srlwps $r1 = $r1, $r4
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srlwps $r2 = $r2, $r4
; CV1-NEXT:    srlwps $r3 = $r3, $r4
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 1)
;
; CV2-LABEL: shrwos:
; CV2:       # %bb.0:
; CV2-NEXT:    srlwps $r0 = $r0, $r4
; CV2-NEXT:    srlwps $r1 = $r1, $r4
; CV2-NEXT:    srlwps $r2 = $r2, $r4
; CV2-NEXT:    srlwps $r3 = $r3, $r4
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %3 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 0, i32 1>
  %4 = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> %3, i32 %1, i32 0)
  %5 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 2, i32 3>
  %6 = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> %5, i32 %1, i32 0)
  %7 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 4, i32 5>
  %8 = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> %7, i32 %1, i32 0)
  %9 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 6, i32 7>
  %10 = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> %9, i32 %1, i32 0)
  %11 = shufflevector <2 x i32> %4, <2 x i32> %6, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %12 = shufflevector <2 x i32> %8, <2 x i32> %10, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %13 = shufflevector <4 x i32> %11, <4 x i32> %12, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i32> %13
}

declare <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32>, i32, i32)

define <8 x i32> @shrwos_s(<8 x i32> %0, i32 %1) {
; CV1-LABEL: shrwos_s:
; CV1:       # %bb.0:
; CV1-NEXT:    srawps $r0 = $r0, $r4
; CV1-NEXT:    srawps $r1 = $r1, $r4
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srawps $r2 = $r2, $r4
; CV1-NEXT:    srawps $r3 = $r3, $r4
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 1)
;
; CV2-LABEL: shrwos_s:
; CV2:       # %bb.0:
; CV2-NEXT:    srawps $r0 = $r0, $r4
; CV2-NEXT:    srawps $r1 = $r1, $r4
; CV2-NEXT:    srawps $r2 = $r2, $r4
; CV2-NEXT:    srawps $r3 = $r3, $r4
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %3 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 0, i32 1>
  %4 = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> %3, i32 %1, i32 1)
  %5 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 2, i32 3>
  %6 = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> %5, i32 %1, i32 1)
  %7 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 4, i32 5>
  %8 = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> %7, i32 %1, i32 1)
  %9 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 6, i32 7>
  %10 = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> %9, i32 %1, i32 1)
  %11 = shufflevector <2 x i32> %4, <2 x i32> %6, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %12 = shufflevector <2 x i32> %8, <2 x i32> %10, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %13 = shufflevector <4 x i32> %11, <4 x i32> %12, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i32> %13
}

define <8 x i32> @shrwos_us(<8 x i32> %0, i32 %1) {
; CHECK-LABEL: shrwos_us:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srswps $r0 = $r0, $r4
; CHECK-NEXT:    srswps $r1 = $r1, $r4
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    srswps $r2 = $r2, $r4
; CHECK-NEXT:    srswps $r3 = $r3, $r4
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
  %3 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 0, i32 1>
  %4 = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> %3, i32 %1, i32 2)
  %5 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 2, i32 3>
  %6 = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> %5, i32 %1, i32 2)
  %7 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 4, i32 5>
  %8 = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> %7, i32 %1, i32 2)
  %9 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 6, i32 7>
  %10 = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> %9, i32 %1, i32 2)
  %11 = shufflevector <2 x i32> %4, <2 x i32> %6, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %12 = shufflevector <2 x i32> %8, <2 x i32> %10, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %13 = shufflevector <4 x i32> %11, <4 x i32> %12, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i32> %13
}

define <8 x i32> @shrwos_r(<8 x i32> %0, i32 %1) {
; CHECK-LABEL: shrwos_r:
; CHECK:       # %bb.0:
; CHECK-NEXT:    rorwps $r0 = $r0, $r4
; CHECK-NEXT:    rorwps $r1 = $r1, $r4
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    rorwps $r2 = $r2, $r4
; CHECK-NEXT:    rorwps $r3 = $r3, $r4
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
  %3 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 0, i32 1>
  %4 = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> %3, i32 %1, i32 3)
  %5 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 2, i32 3>
  %6 = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> %5, i32 %1, i32 3)
  %7 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 4, i32 5>
  %8 = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> %7, i32 %1, i32 3)
  %9 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 6, i32 7>
  %10 = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> %9, i32 %1, i32 3)
  %11 = shufflevector <2 x i32> %4, <2 x i32> %6, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %12 = shufflevector <2 x i32> %8, <2 x i32> %10, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %13 = shufflevector <4 x i32> %11, <4 x i32> %12, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i32> %13
}

define <2 x i32> @shrwp(<2 x i32> %0, <2 x i32> %1) {
; CHECK-LABEL: shrwp:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srlw $r0 = $r0, $r1
; CHECK-NEXT:    srad $r2 = $r0, 32
; CHECK-NEXT:    srad $r3 = $r1, 32
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    srlw $r1 = $r2, $r3
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    insf $r0 = $r1, 63, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 2)
  %3 = extractelement <2 x i32> %0, i64 0
  %4 = extractelement <2 x i32> %1, i64 0
  %5 = tail call i32 @llvm.kvx.shr.i32(i32 %3, i32 %4, i32 0)
  %6 = extractelement <2 x i32> %0, i64 1
  %7 = extractelement <2 x i32> %1, i64 1
  %8 = tail call i32 @llvm.kvx.shr.i32(i32 %6, i32 %7, i32 0)
  %9 = insertelement <2 x i32> undef, i32 %5, i32 0
  %10 = insertelement <2 x i32> %9, i32 %8, i32 1
  ret <2 x i32> %10
}

define <2 x i32> @shrwp_s(<2 x i32> %0, <2 x i32> %1) {
; CHECK-LABEL: shrwp_s:
; CHECK:       # %bb.0:
; CHECK-NEXT:    sraw $r0 = $r0, $r1
; CHECK-NEXT:    srad $r2 = $r0, 32
; CHECK-NEXT:    srad $r3 = $r1, 32
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    sraw $r1 = $r2, $r3
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    insf $r0 = $r1, 63, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 2)
  %3 = extractelement <2 x i32> %0, i64 0
  %4 = extractelement <2 x i32> %1, i64 0
  %5 = tail call i32 @llvm.kvx.shr.i32(i32 %3, i32 %4, i32 1)
  %6 = extractelement <2 x i32> %0, i64 1
  %7 = extractelement <2 x i32> %1, i64 1
  %8 = tail call i32 @llvm.kvx.shr.i32(i32 %6, i32 %7, i32 1)
  %9 = insertelement <2 x i32> undef, i32 %5, i32 0
  %10 = insertelement <2 x i32> %9, i32 %8, i32 1
  ret <2 x i32> %10
}

define <2 x i32> @shrwp_us(<2 x i32> %0, <2 x i32> %1) {
; CHECK-LABEL: shrwp_us:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srsw $r0 = $r0, $r1
; CHECK-NEXT:    srad $r2 = $r0, 32
; CHECK-NEXT:    srad $r3 = $r1, 32
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    srsw $r1 = $r2, $r3
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    insf $r0 = $r1, 63, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 2)
  %3 = extractelement <2 x i32> %0, i64 0
  %4 = extractelement <2 x i32> %1, i64 0
  %5 = tail call i32 @llvm.kvx.shr.i32(i32 %3, i32 %4, i32 2)
  %6 = extractelement <2 x i32> %0, i64 1
  %7 = extractelement <2 x i32> %1, i64 1
  %8 = tail call i32 @llvm.kvx.shr.i32(i32 %6, i32 %7, i32 2)
  %9 = insertelement <2 x i32> undef, i32 %5, i32 0
  %10 = insertelement <2 x i32> %9, i32 %8, i32 1
  ret <2 x i32> %10
}

define <2 x i32> @shrwp_r(<2 x i32> %0, <2 x i32> %1) {
; CHECK-LABEL: shrwp_r:
; CHECK:       # %bb.0:
; CHECK-NEXT:    rorw $r0 = $r0, $r1
; CHECK-NEXT:    srad $r2 = $r0, 32
; CHECK-NEXT:    srad $r3 = $r1, 32
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    rorw $r1 = $r2, $r3
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    insf $r0 = $r1, 63, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 2)
  %3 = extractelement <2 x i32> %0, i64 0
  %4 = extractelement <2 x i32> %1, i64 0
  %5 = tail call i32 @llvm.kvx.shr.i32(i32 %3, i32 %4, i32 3)
  %6 = extractelement <2 x i32> %0, i64 1
  %7 = extractelement <2 x i32> %1, i64 1
  %8 = tail call i32 @llvm.kvx.shr.i32(i32 %6, i32 %7, i32 3)
  %9 = insertelement <2 x i32> undef, i32 %5, i32 0
  %10 = insertelement <2 x i32> %9, i32 %8, i32 1
  ret <2 x i32> %10
}

define <2 x i32> @shrwps(<2 x i32> %0, i32 %1) {
; CHECK-LABEL: shrwps:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srlwps $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %3 = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> %0, i32 %1, i32 0)
  ret <2 x i32> %3
}

define <2 x i32> @shrwps_s(<2 x i32> %0, i32 %1) {
; CHECK-LABEL: shrwps_s:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srawps $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %3 = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> %0, i32 %1, i32 1)
  ret <2 x i32> %3
}

define <2 x i32> @shrwps_us(<2 x i32> %0, i32 %1) {
; CHECK-LABEL: shrwps_us:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srswps $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %3 = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> %0, i32 %1, i32 2)
  ret <2 x i32> %3
}

define <2 x i32> @shrwps_r(<2 x i32> %0, i32 %1) {
; CHECK-LABEL: shrwps_r:
; CHECK:       # %bb.0:
; CHECK-NEXT:    rorwps $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %3 = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> %0, i32 %1, i32 3)
  ret <2 x i32> %3
}

define <4 x i32> @shrwq(<4 x i32> %0, <4 x i32> %1) {
; CHECK-LABEL: shrwq:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srlw $r0 = $r0, $r2
; CHECK-NEXT:    srad $r2 = $r2, 32
; CHECK-NEXT:    srad $r4 = $r0, 32
; CHECK-NEXT:    srad $r5 = $r1, 32
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    srlw $r1 = $r1, $r3
; CHECK-NEXT:    srlw $r2 = $r4, $r2
; CHECK-NEXT:    srad $r6 = $r3, 32
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    insf $r0 = $r2, 63, 32
; CHECK-NEXT:    srlw $r3 = $r5, $r6
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    insf $r1 = $r3, 63, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
  %3 = extractelement <4 x i32> %0, i64 0
  %4 = extractelement <4 x i32> %1, i64 0
  %5 = tail call i32 @llvm.kvx.shr.i32(i32 %3, i32 %4, i32 0)
  %6 = extractelement <4 x i32> %0, i64 1
  %7 = extractelement <4 x i32> %1, i64 1
  %8 = tail call i32 @llvm.kvx.shr.i32(i32 %6, i32 %7, i32 0)
  %9 = extractelement <4 x i32> %0, i64 2
  %10 = extractelement <4 x i32> %1, i64 2
  %11 = tail call i32 @llvm.kvx.shr.i32(i32 %9, i32 %10, i32 0)
  %12 = extractelement <4 x i32> %0, i64 3
  %13 = extractelement <4 x i32> %1, i64 3
  %14 = tail call i32 @llvm.kvx.shr.i32(i32 %12, i32 %13, i32 0)
  %15 = insertelement <4 x i32> undef, i32 %5, i32 0
  %16 = insertelement <4 x i32> %15, i32 %8, i32 1
  %17 = insertelement <4 x i32> %16, i32 %11, i32 2
  %18 = insertelement <4 x i32> %17, i32 %14, i32 3
  ret <4 x i32> %18
}

define <4 x i32> @shrwq_s(<4 x i32> %0, <4 x i32> %1) {
; CHECK-LABEL: shrwq_s:
; CHECK:       # %bb.0:
; CHECK-NEXT:    sraw $r0 = $r0, $r2
; CHECK-NEXT:    srad $r2 = $r2, 32
; CHECK-NEXT:    srad $r4 = $r0, 32
; CHECK-NEXT:    srad $r5 = $r1, 32
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    sraw $r1 = $r1, $r3
; CHECK-NEXT:    sraw $r2 = $r4, $r2
; CHECK-NEXT:    srad $r6 = $r3, 32
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    insf $r0 = $r2, 63, 32
; CHECK-NEXT:    sraw $r3 = $r5, $r6
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    insf $r1 = $r3, 63, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
  %3 = extractelement <4 x i32> %0, i64 0
  %4 = extractelement <4 x i32> %1, i64 0
  %5 = tail call i32 @llvm.kvx.shr.i32(i32 %3, i32 %4, i32 1)
  %6 = extractelement <4 x i32> %0, i64 1
  %7 = extractelement <4 x i32> %1, i64 1
  %8 = tail call i32 @llvm.kvx.shr.i32(i32 %6, i32 %7, i32 1)
  %9 = extractelement <4 x i32> %0, i64 2
  %10 = extractelement <4 x i32> %1, i64 2
  %11 = tail call i32 @llvm.kvx.shr.i32(i32 %9, i32 %10, i32 1)
  %12 = extractelement <4 x i32> %0, i64 3
  %13 = extractelement <4 x i32> %1, i64 3
  %14 = tail call i32 @llvm.kvx.shr.i32(i32 %12, i32 %13, i32 1)
  %15 = insertelement <4 x i32> undef, i32 %5, i32 0
  %16 = insertelement <4 x i32> %15, i32 %8, i32 1
  %17 = insertelement <4 x i32> %16, i32 %11, i32 2
  %18 = insertelement <4 x i32> %17, i32 %14, i32 3
  ret <4 x i32> %18
}

define <4 x i32> @shrwq_us(<4 x i32> %0, <4 x i32> %1) {
; CHECK-LABEL: shrwq_us:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srsw $r0 = $r0, $r2
; CHECK-NEXT:    srad $r2 = $r2, 32
; CHECK-NEXT:    srad $r4 = $r0, 32
; CHECK-NEXT:    srad $r5 = $r1, 32
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    srsw $r1 = $r1, $r3
; CHECK-NEXT:    srsw $r2 = $r4, $r2
; CHECK-NEXT:    srad $r4 = $r3, 32
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    insf $r0 = $r2, 63, 32
; CHECK-NEXT:    srsw $r3 = $r5, $r4
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    insf $r1 = $r3, 63, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
  %3 = extractelement <4 x i32> %0, i64 0
  %4 = extractelement <4 x i32> %1, i64 0
  %5 = tail call i32 @llvm.kvx.shr.i32(i32 %3, i32 %4, i32 2)
  %6 = extractelement <4 x i32> %0, i64 1
  %7 = extractelement <4 x i32> %1, i64 1
  %8 = tail call i32 @llvm.kvx.shr.i32(i32 %6, i32 %7, i32 2)
  %9 = extractelement <4 x i32> %0, i64 2
  %10 = extractelement <4 x i32> %1, i64 2
  %11 = tail call i32 @llvm.kvx.shr.i32(i32 %9, i32 %10, i32 2)
  %12 = extractelement <4 x i32> %0, i64 3
  %13 = extractelement <4 x i32> %1, i64 3
  %14 = tail call i32 @llvm.kvx.shr.i32(i32 %12, i32 %13, i32 2)
  %15 = insertelement <4 x i32> undef, i32 %5, i32 0
  %16 = insertelement <4 x i32> %15, i32 %8, i32 1
  %17 = insertelement <4 x i32> %16, i32 %11, i32 2
  %18 = insertelement <4 x i32> %17, i32 %14, i32 3
  ret <4 x i32> %18
}

define <4 x i32> @shrwq_r(<4 x i32> %0, <4 x i32> %1) {
; CHECK-LABEL: shrwq_r:
; CHECK:       # %bb.0:
; CHECK-NEXT:    rorw $r0 = $r0, $r2
; CHECK-NEXT:    srad $r2 = $r2, 32
; CHECK-NEXT:    srad $r4 = $r0, 32
; CHECK-NEXT:    srad $r5 = $r1, 32
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    rorw $r1 = $r1, $r3
; CHECK-NEXT:    rorw $r2 = $r4, $r2
; CHECK-NEXT:    srad $r4 = $r3, 32
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    insf $r0 = $r2, 63, 32
; CHECK-NEXT:    rorw $r3 = $r5, $r4
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    insf $r1 = $r3, 63, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
  %3 = extractelement <4 x i32> %0, i64 0
  %4 = extractelement <4 x i32> %1, i64 0
  %5 = tail call i32 @llvm.kvx.shr.i32(i32 %3, i32 %4, i32 3)
  %6 = extractelement <4 x i32> %0, i64 1
  %7 = extractelement <4 x i32> %1, i64 1
  %8 = tail call i32 @llvm.kvx.shr.i32(i32 %6, i32 %7, i32 3)
  %9 = extractelement <4 x i32> %0, i64 2
  %10 = extractelement <4 x i32> %1, i64 2
  %11 = tail call i32 @llvm.kvx.shr.i32(i32 %9, i32 %10, i32 3)
  %12 = extractelement <4 x i32> %0, i64 3
  %13 = extractelement <4 x i32> %1, i64 3
  %14 = tail call i32 @llvm.kvx.shr.i32(i32 %12, i32 %13, i32 3)
  %15 = insertelement <4 x i32> undef, i32 %5, i32 0
  %16 = insertelement <4 x i32> %15, i32 %8, i32 1
  %17 = insertelement <4 x i32> %16, i32 %11, i32 2
  %18 = insertelement <4 x i32> %17, i32 %14, i32 3
  ret <4 x i32> %18
}

define <4 x i32> @shrwqs(<4 x i32> %0, i32 %1) {
; CHECK-LABEL: shrwqs:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srlwps $r0 = $r0, $r2
; CHECK-NEXT:    srlwps $r1 = $r1, $r2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %3 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 0, i32 1>
  %4 = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> %3, i32 %1, i32 0)
  %5 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 2, i32 3>
  %6 = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> %5, i32 %1, i32 0)
  %7 = shufflevector <2 x i32> %4, <2 x i32> %6, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  ret <4 x i32> %7
}

define <4 x i32> @shrwqs_s(<4 x i32> %0, i32 %1) {
; CHECK-LABEL: shrwqs_s:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srawps $r0 = $r0, $r2
; CHECK-NEXT:    srawps $r1 = $r1, $r2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %3 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 0, i32 1>
  %4 = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> %3, i32 %1, i32 1)
  %5 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 2, i32 3>
  %6 = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> %5, i32 %1, i32 1)
  %7 = shufflevector <2 x i32> %4, <2 x i32> %6, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  ret <4 x i32> %7
}

define <4 x i32> @shrwqs_us(<4 x i32> %0, i32 %1) {
; CHECK-LABEL: shrwqs_us:
; CHECK:       # %bb.0:
; CHECK-NEXT:    srswps $r0 = $r0, $r2
; CHECK-NEXT:    srswps $r1 = $r1, $r2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %3 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 0, i32 1>
  %4 = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> %3, i32 %1, i32 2)
  %5 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 2, i32 3>
  %6 = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> %5, i32 %1, i32 2)
  %7 = shufflevector <2 x i32> %4, <2 x i32> %6, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  ret <4 x i32> %7
}

define <4 x i32> @shrwqs_r(<4 x i32> %0, i32 %1) {
; CHECK-LABEL: shrwqs_r:
; CHECK:       # %bb.0:
; CHECK-NEXT:    rorwps $r0 = $r0, $r2
; CHECK-NEXT:    rorwps $r1 = $r1, $r2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %3 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 0, i32 1>
  %4 = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> %3, i32 %1, i32 3)
  %5 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 2, i32 3>
  %6 = tail call <2 x i32> @llvm.kvx.shr.v2i32(<2 x i32> %5, i32 %1, i32 3)
  %7 = shufflevector <2 x i32> %4, <2 x i32> %6, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  ret <4 x i32> %7
}

