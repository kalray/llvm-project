; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -O2 -mcpu=kv3-1 -o - %s | FileCheck --check-prefixes=ALL,CV1 %s
; RUN: llc -O2 -mcpu=kv3-2 -o - %s | FileCheck --check-prefixes=ALL,CV2 %s

; RUN: clang -O2 -march=kv3-1 -c -o /dev/null %s
; RUN: clang -O2 -march=kv3-2 -c -o /dev/null %s

target triple = "kvx-kalray-cos"

define <2 x i64> @stsudp(<2 x i64> %0, <2 x i64> %1) {
; ALL-LABEL: stsudp:
; ALL:       # %bb.0:
; ALL-NEXT:    stsud $r0 = $r0, $r2
; ALL-NEXT:    stsud $r1 = $r1, $r3
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %3 = extractelement <2 x i64> %0, i64 0
  %4 = extractelement <2 x i64> %1, i64 0
  %5 = tail call i64 @llvm.kvx.stsu.i64(i64 %3, i64 %4)
  %6 = extractelement <2 x i64> %0, i64 1
  %7 = extractelement <2 x i64> %1, i64 1
  %8 = tail call i64 @llvm.kvx.stsu.i64(i64 %6, i64 %7)
  %9 = insertelement <2 x i64> undef, i64 %5, i32 0
  %10 = insertelement <2 x i64> %9, i64 %8, i32 1
  ret <2 x i64> %10
}

declare i64 @llvm.kvx.stsu.i64(i64, i64)

define <4 x i64> @stsudq(<4 x i64> %0, <4 x i64> %1) {
; CV1-LABEL: stsudq:
; CV1:       # %bb.0:
; CV1-NEXT:    stsud $r0 = $r0, $r4
; CV1-NEXT:    stsud $r1 = $r1, $r5
; CV1-NEXT:    ;;
; CV1-NEXT:    stsud $r2 = $r2, $r6
; CV1-NEXT:    stsud $r3 = $r3, $r7
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: stsudq:
; CV2:       # %bb.0:
; CV2-NEXT:    stsud $r0 = $r0, $r4
; CV2-NEXT:    stsud $r1 = $r1, $r5
; CV2-NEXT:    stsud $r2 = $r2, $r6
; CV2-NEXT:    stsud $r3 = $r3, $r7
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = extractelement <4 x i64> %0, i64 0
  %4 = extractelement <4 x i64> %1, i64 0
  %5 = tail call i64 @llvm.kvx.stsu.i64(i64 %3, i64 %4)
  %6 = extractelement <4 x i64> %0, i64 1
  %7 = extractelement <4 x i64> %1, i64 1
  %8 = tail call i64 @llvm.kvx.stsu.i64(i64 %6, i64 %7)
  %9 = extractelement <4 x i64> %0, i64 2
  %10 = extractelement <4 x i64> %1, i64 2
  %11 = tail call i64 @llvm.kvx.stsu.i64(i64 %9, i64 %10)
  %12 = extractelement <4 x i64> %0, i64 3
  %13 = extractelement <4 x i64> %1, i64 3
  %14 = tail call i64 @llvm.kvx.stsu.i64(i64 %12, i64 %13)
  %15 = insertelement <4 x i64> undef, i64 %5, i32 0
  %16 = insertelement <4 x i64> %15, i64 %8, i32 1
  %17 = insertelement <4 x i64> %16, i64 %11, i32 2
  %18 = insertelement <4 x i64> %17, i64 %14, i32 3
  ret <4 x i64> %18
}

define void @stsudo(<8 x i64>* noalias nocapture sret(<8 x i64>) align 32 %0, <8 x i64>* nocapture readonly %1, <8 x i64>* nocapture readonly %2) {
; CV1-LABEL: stsudo:
; CV1:       # %bb.0:
; CV1-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV1-NEXT:    ;;
; CV1-NEXT:    lo $r8r9r10r11 = 0[$r0]
; CV1-NEXT:    ;;
; CV1-NEXT:    lo $r32r33r34r35 = 32[$r1]
; CV1-NEXT:    ;;
; CV1-NEXT:    lo $r0r1r2r3 = 0[$r1]
; CV1-NEXT:    ;;
; CV1-NEXT:    stsud $r4 = $r4, $r32
; CV1-NEXT:    stsud $r5 = $r5, $r33
; CV1-NEXT:    ;;
; CV1-NEXT:    stsud $r0 = $r8, $r0
; CV1-NEXT:    stsud $r1 = $r9, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    stsud $r6 = $r6, $r34
; CV1-NEXT:    stsud $r7 = $r7, $r35
; CV1-NEXT:    ;;
; CV1-NEXT:    so 32[$r15] = $r4r5r6r7
; CV1-NEXT:    stsud $r2 = $r10, $r2
; CV1-NEXT:    stsud $r3 = $r11, $r3
; CV1-NEXT:    ;;
; CV1-NEXT:    so 0[$r15] = $r0r1r2r3
; CV1-NEXT:    copyd $r0 = $r15
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: stsudo:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV2-NEXT:    ;;
; CV2-NEXT:    lo $r8r9r10r11 = 0[$r0]
; CV2-NEXT:    ;;
; CV2-NEXT:    lo $r32r33r34r35 = 32[$r1]
; CV2-NEXT:    ;;
; CV2-NEXT:    lo $r0r1r2r3 = 0[$r1]
; CV2-NEXT:    ;;
; CV2-NEXT:    stsud $r4 = $r4, $r32
; CV2-NEXT:    stsud $r5 = $r5, $r33
; CV2-NEXT:    stsud $r6 = $r6, $r34
; CV2-NEXT:    stsud $r7 = $r7, $r35
; CV2-NEXT:    ;;
; CV2-NEXT:    stsud $r0 = $r8, $r0
; CV2-NEXT:    stsud $r1 = $r9, $r1
; CV2-NEXT:    stsud $r2 = $r10, $r2
; CV2-NEXT:    stsud $r3 = $r11, $r3
; CV2-NEXT:    ;;
; CV2-NEXT:    so 32[$r15] = $r4r5r6r7
; CV2-NEXT:    ;;
; CV2-NEXT:    so 0[$r15] = $r0r1r2r3
; CV2-NEXT:    copyd $r0 = $r15
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %4 = load <8 x i64>, <8 x i64>* %1
  %5 = load <8 x i64>, <8 x i64>* %2
  %6 = extractelement <8 x i64> %4, i64 0
  %7 = extractelement <8 x i64> %5, i64 0
  %8 = tail call i64 @llvm.kvx.stsu.i64(i64 %6, i64 %7)
  %9 = extractelement <8 x i64> %4, i64 1
  %10 = extractelement <8 x i64> %5, i64 1
  %11 = tail call i64 @llvm.kvx.stsu.i64(i64 %9, i64 %10)
  %12 = extractelement <8 x i64> %4, i64 2
  %13 = extractelement <8 x i64> %5, i64 2
  %14 = tail call i64 @llvm.kvx.stsu.i64(i64 %12, i64 %13)
  %15 = extractelement <8 x i64> %4, i64 3
  %16 = extractelement <8 x i64> %5, i64 3
  %17 = tail call i64 @llvm.kvx.stsu.i64(i64 %15, i64 %16)
  %18 = extractelement <8 x i64> %4, i64 4
  %19 = extractelement <8 x i64> %5, i64 4
  %20 = tail call i64 @llvm.kvx.stsu.i64(i64 %18, i64 %19)
  %21 = extractelement <8 x i64> %4, i64 5
  %22 = extractelement <8 x i64> %5, i64 5
  %23 = tail call i64 @llvm.kvx.stsu.i64(i64 %21, i64 %22)
  %24 = extractelement <8 x i64> %4, i64 6
  %25 = extractelement <8 x i64> %5, i64 6
  %26 = tail call i64 @llvm.kvx.stsu.i64(i64 %24, i64 %25)
  %27 = extractelement <8 x i64> %4, i64 7
  %28 = extractelement <8 x i64> %5, i64 7
  %29 = tail call i64 @llvm.kvx.stsu.i64(i64 %27, i64 %28)
  %30 = insertelement <8 x i64> undef, i64 %8, i32 0
  %31 = insertelement <8 x i64> %30, i64 %11, i32 1
  %32 = insertelement <8 x i64> %31, i64 %14, i32 2
  %33 = insertelement <8 x i64> %32, i64 %17, i32 3
  %34 = insertelement <8 x i64> %33, i64 %20, i32 4
  %35 = insertelement <8 x i64> %34, i64 %23, i32 5
  %36 = insertelement <8 x i64> %35, i64 %26, i32 6
  %37 = insertelement <8 x i64> %36, i64 %29, i32 7
  store <8 x i64> %37, <8 x i64>* %0
  ret void
}

define i32 @stsuw(i32 %0, i32 %1) {
; ALL-LABEL: stsuw:
; ALL:       # %bb.0:
; ALL-NEXT:    stsuw $r0 = $r0, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %3 = tail call i32 @llvm.kvx.stsu.i32(i32 %0, i32 %1)
  ret i32 %3
}

declare i32 @llvm.kvx.stsu.i32(i32, i32)

