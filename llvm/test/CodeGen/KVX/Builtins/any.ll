; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mcpu=kv3-1 -o - %s -O2 | FileCheck %s --check-prefixes=ALL,CV1
; RUN: llc -mcpu=kv3-2 -o - %s -O2 | FileCheck %s --check-prefixes=ALL,CV2
; RUN: clang -march=kv3-1 -O2 -c -o /dev/null %s
; RUN: clang -march=kv3-2 -O2 -c -o /dev/null %s

target triple = "kvx-kalray-cos"

define i64 @anybo_nez(<8 x i8> %0) {
; ALL-LABEL: anybo_nez:
; ALL:       # %bb.0:
; ALL-NEXT:    compd.ne $r0 = $r0, 0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %2 = tail call i64 @llvm.kvx.any.v8i8(<8 x i8> %0, i32 0)
  ret i64 %2
}

declare <8 x i8>   @llvm.abs.v8i8(<8 x i8>, i1)
define i64 @anybo_nez_abs(<8 x i8> %0) {
; ALL-LABEL: anybo_nez_abs:
; ALL:       # %bb.0:
; ALL-NEXT:    compd.ne $r0 = $r0, 0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %2 = tail call <8 x i8> @llvm.abs.v8i8(<8 x i8> %0, i1 false)
  %3 = tail call i64 @llvm.kvx.any.v8i8(<8 x i8> %2, i32 0)
  ret i64 %3
}

declare i64 @llvm.kvx.any.v8i8(<8 x i8>, i32)

define i64 @anybo_eqz(<8 x i8> %0) {
; CV1-LABEL: anybo_eqz:
; CV1:       # %bb.0:
; CV1-NEXT:    addd $r0 = $r0, 0xfefefefefefefeff
; CV1-NEXT:    andnd $r1 = $r0, 0x80808080.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    compd.ne $r0 = $r0, 0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 2)
;
; CV2-LABEL: anybo_eqz:
; CV2:       # %bb.0:
; CV2-NEXT:    compnbo.eq $r0 = $r0, 0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    compd.ne $r0 = $r0, 0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 1)
  %2 = tail call i64 @llvm.kvx.any.v8i8(<8 x i8> %0, i32 1)
  ret i64 %2
}

define i64 @anybv_nez(<32 x i8> %0) {
; ALL-LABEL: anybv_nez:
; ALL:       # %bb.0:
; ALL-NEXT:    liord $r0 = $r0, $r1
; ALL-NEXT:    liord $r2 = $r2, $r3
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    iorw $r0 = $r0, $r2
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 1)
  %2 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %3 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %4 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %5 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  %6 = tail call i64 @llvm.kvx.any4.v8i8(<8 x i8> %2, <8 x i8> %3, <8 x i8> %4, <8 x i8> %5, i32 0)
  ret i64 %6
}

declare i64 @llvm.kvx.any4.v8i8(<8 x i8>, <8 x i8>, <8 x i8>, <8 x i8>, i32)

define i64 @anybv_eqz(<32 x i8> %0) {
; CV1-LABEL: anybv_eqz:
; CV1:       # %bb.0:
; CV1-NEXT:    addd $r3 = $r3, 0xfefefefefefefeff
; CV1-NEXT:    andnd $r4 = $r3, 0x80808080.@
; CV1-NEXT:    andnd $r5 = $r2, 0x80808080.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    addd $r1 = $r1, 0xfefefefefefefeff
; CV1-NEXT:    addd $r2 = $r2, 0xfefefefefefefeff
; CV1-NEXT:    andnd $r6 = $r1, 0x80808080.@
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    addd $r0 = $r0, 0xfefefefefefefeff
; CV1-NEXT:    andd $r2 = $r2, $r5
; CV1-NEXT:    andd $r3 = $r3, $r4
; CV1-NEXT:    andnd $r7 = $r0, 0x80808080.@
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    andd $r0 = $r0, $r7
; CV1-NEXT:    andd $r1 = $r1, $r6
; CV1-NEXT:    liord $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    liord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    iorw $r0 = $r0, $r2
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 5)
;
; CV2-LABEL: anybv_eqz:
; CV2:       # %bb.0:
; CV2-NEXT:    compnbo.eq $r0 = $r0, 0
; CV2-NEXT:    compnbo.eq $r1 = $r1, 0
; CV2-NEXT:    compnbo.eq $r2 = $r2, 0
; CV2-NEXT:    compnbo.eq $r3 = $r3, 0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    liord $r0 = $r0, $r1
; CV2-NEXT:    liord $r2 = $r2, $r3
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    iorw $r0 = $r0, $r2
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 2)
  %2 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %3 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %4 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %5 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  %6 = tail call i64 @llvm.kvx.any4.v8i8(<8 x i8> %2, <8 x i8> %3, <8 x i8> %4, <8 x i8> %5, i32 1)
  ret i64 %6
}

define i64 @anybx_nez(<16 x i8> %0) {
; ALL-LABEL: anybx_nez:
; ALL:       # %bb.0:
; ALL-NEXT:    liord $r0 = $r0, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %2 = shufflevector <16 x i8> %0, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %3 = shufflevector <16 x i8> %0, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %4 = tail call i64 @llvm.kvx.any2.v8i8(<8 x i8> %2, <8 x i8> %3, i32 0)
  ret i64 %4
}

declare i64 @llvm.kvx.any2.v8i8(<8 x i8>, <8 x i8>, i32)

define i64 @anybx_eqz(<16 x i8> %0) {
; CV1-LABEL: anybx_eqz:
; CV1:       # %bb.0:
; CV1-NEXT:    addd $r1 = $r1, 0xfefefefefefefeff
; CV1-NEXT:    andnd $r2 = $r1, 0x80808080.@
; CV1-NEXT:    andnd $r3 = $r0, 0x80808080.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    addd $r0 = $r0, 0xfefefefefefefeff
; CV1-NEXT:    andd $r1 = $r1, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andd $r0 = $r0, $r3
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    liord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: anybx_eqz:
; CV2:       # %bb.0:
; CV2-NEXT:    compnbo.eq $r0 = $r0, 0
; CV2-NEXT:    compnbo.eq $r1 = $r1, 0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    liord $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 1)
  %2 = shufflevector <16 x i8> %0, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %3 = shufflevector <16 x i8> %0, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %4 = tail call i64 @llvm.kvx.any2.v8i8(<8 x i8> %2, <8 x i8> %3, i32 1)
  ret i64 %4
}

define i64 @anydp_nez(<2 x i64> %0) {
; ALL-LABEL: anydp_nez:
; ALL:       # %bb.0:
; ALL-NEXT:    liord $r0 = $r0, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %2 = extractelement <2 x i64> %0, i64 0
  %3 = extractelement <2 x i64> %0, i64 1
  %4 = tail call i64 @llvm.kvx.any2.i64(i64 %2, i64 %3, i32 0)
  ret i64 %4
}

declare i64 @llvm.kvx.any2.i64(i64, i64, i32)

define i64 @anydp_eqz(<2 x i64> %0) {
; ALL-LABEL: anydp_eqz:
; ALL:       # %bb.0:
; ALL-NEXT:    lnandd $r0 = $r0, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %2 = extractelement <2 x i64> %0, i64 0
  %3 = extractelement <2 x i64> %0, i64 1
  %4 = tail call i64 @llvm.kvx.any2.i64(i64 %2, i64 %3, i32 1)
  ret i64 %4
}

define i64 @anydq_nez(<4 x i64> %0) {
; ALL-LABEL: anydq_nez:
; ALL:       # %bb.0:
; ALL-NEXT:    liord $r0 = $r0, $r1
; ALL-NEXT:    liord $r2 = $r2, $r3
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    iorw $r0 = $r0, $r2
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 1)
  %2 = extractelement <4 x i64> %0, i64 0
  %3 = extractelement <4 x i64> %0, i64 1
  %4 = extractelement <4 x i64> %0, i64 2
  %5 = extractelement <4 x i64> %0, i64 3
  %6 = tail call i64 @llvm.kvx.any4.i64(i64 %2, i64 %3, i64 %4, i64 %5, i32 0)
  ret i64 %6
}

declare i64 @llvm.kvx.any4.i64(i64, i64, i64, i64, i32)

define i64 @anydq_eqz(<4 x i64> %0) {
; ALL-LABEL: anydq_eqz:
; ALL:       # %bb.0:
; ALL-NEXT:    lnandd $r0 = $r0, $r1
; ALL-NEXT:    lnandd $r2 = $r2, $r3
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    iorw $r0 = $r0, $r2
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 1)
  %2 = extractelement <4 x i64> %0, i64 0
  %3 = extractelement <4 x i64> %0, i64 1
  %4 = extractelement <4 x i64> %0, i64 2
  %5 = extractelement <4 x i64> %0, i64 3
  %6 = tail call i64 @llvm.kvx.any4.i64(i64 %2, i64 %3, i64 %4, i64 %5, i32 1)
  ret i64 %6
}

define i64 @anyho_nez(<8 x i16> %0) {
; ALL-LABEL: anyho_nez:
; ALL:       # %bb.0:
; ALL-NEXT:    liord $r0 = $r0, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %2 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4 = tail call i64 @llvm.kvx.any2.v4i16(<4 x i16> %2, <4 x i16> %3, i32 0)
  ret i64 %4
}

declare i64 @llvm.kvx.any2.v4i16(<4 x i16>, <4 x i16>, i32)

define i64 @anyho_eqz(<8 x i16> %0) {
; ALL-LABEL: anyho_eqz:
; ALL:       # %bb.0:
; ALL-NEXT:    compnhq.eq $r0 = $r0, 0
; ALL-NEXT:    compnhq.eq $r1 = $r1, 0
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    liord $r0 = $r0, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 1)
  %2 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4 = tail call i64 @llvm.kvx.any2.v4i16(<4 x i16> %2, <4 x i16> %3, i32 1)
  ret i64 %4
}

define i64 @anyhq_nez(<4 x i16> %0) {
; ALL-LABEL: anyhq_nez:
; ALL:       # %bb.0:
; ALL-NEXT:    compd.ne $r0 = $r0, 0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %2 = tail call i64 @llvm.kvx.any.v4i16(<4 x i16> %0, i32 0)
  ret i64 %2
}

declare i64 @llvm.kvx.any.v4i16(<4 x i16>, i32)

define i64 @anyhq_eqz(<4 x i16> %0) {
; ALL-LABEL: anyhq_eqz:
; ALL:       # %bb.0:
; ALL-NEXT:    compnhq.eq $r0 = $r0, 0
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    compd.ne $r0 = $r0, 0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 1)
  %2 = tail call i64 @llvm.kvx.any.v4i16(<4 x i16> %0, i32 1)
  ret i64 %2
}

define i64 @anyhx_nez(<16 x i16> %0) {
; ALL-LABEL: anyhx_nez:
; ALL:       # %bb.0:
; ALL-NEXT:    liord $r0 = $r0, $r1
; ALL-NEXT:    liord $r2 = $r2, $r3
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    iorw $r0 = $r0, $r2
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 1)
  %2 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %5 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6 = tail call i64 @llvm.kvx.any4.v4i16(<4 x i16> %2, <4 x i16> %3, <4 x i16> %4, <4 x i16> %5, i32 0)
  ret i64 %6
}

declare i64 @llvm.kvx.any4.v4i16(<4 x i16>, <4 x i16>, <4 x i16>, <4 x i16>, i32)

define i64 @anyhx_eqz(<16 x i16> %0) {
; ALL-LABEL: anyhx_eqz:
; ALL:       # %bb.0:
; ALL-NEXT:    compnhq.eq $r0 = $r0, 0
; ALL-NEXT:    compnhq.eq $r1 = $r1, 0
; ALL-NEXT:    compnhq.eq $r2 = $r2, 0
; ALL-NEXT:    compnhq.eq $r3 = $r3, 0
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    liord $r0 = $r0, $r1
; ALL-NEXT:    liord $r2 = $r2, $r3
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    iorw $r0 = $r0, $r2
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 2)
  %2 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %4 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %5 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %6 = tail call i64 @llvm.kvx.any4.v4i16(<4 x i16> %2, <4 x i16> %3, <4 x i16> %4, <4 x i16> %5, i32 1)
  ret i64 %6
}

define i64 @anywo_nez(<8 x i32> %0) {
; ALL-LABEL: anywo_nez:
; ALL:       # %bb.0:
; ALL-NEXT:    liord $r0 = $r0, $r1
; ALL-NEXT:    liord $r2 = $r2, $r3
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    iorw $r0 = $r0, $r2
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 1)
  %2 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 0, i32 1>
  %3 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 2, i32 3>
  %4 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 4, i32 5>
  %5 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 6, i32 7>
  %6 = tail call i64 @llvm.kvx.any4.v2i32(<2 x i32> %2, <2 x i32> %3, <2 x i32> %4, <2 x i32> %5, i32 0)
  ret i64 %6
}

declare i64 @llvm.kvx.any4.v2i32(<2 x i32>, <2 x i32>, <2 x i32>, <2 x i32>, i32)

define i64 @anywo_eqz(<8 x i32> %0) {
; ALL-LABEL: anywo_eqz:
; ALL:       # %bb.0:
; ALL-NEXT:    compnwp.eq $r0 = $r0, 0
; ALL-NEXT:    compnwp.eq $r1 = $r1, 0
; ALL-NEXT:    compnwp.eq $r2 = $r2, 0
; ALL-NEXT:    compnwp.eq $r3 = $r3, 0
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    liord $r0 = $r0, $r1
; ALL-NEXT:    liord $r2 = $r2, $r3
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    iorw $r0 = $r0, $r2
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 2)
  %2 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 0, i32 1>
  %3 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 2, i32 3>
  %4 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 4, i32 5>
  %5 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 6, i32 7>
  %6 = tail call i64 @llvm.kvx.any4.v2i32(<2 x i32> %2, <2 x i32> %3, <2 x i32> %4, <2 x i32> %5, i32 1)
  ret i64 %6
}

define i64 @anywp_nez(<2 x i32> %0) {
; ALL-LABEL: anywp_nez:
; ALL:       # %bb.0:
; ALL-NEXT:    compd.ne $r0 = $r0, 0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %2 = tail call i64 @llvm.kvx.any.v2i32(<2 x i32> %0, i32 0)
  ret i64 %2
}

declare i64 @llvm.kvx.any.v2i32(<2 x i32>, i32)

define i64 @anywp_eqz(<2 x i32> %0) {
; ALL-LABEL: anywp_eqz:
; ALL:       # %bb.0:
; ALL-NEXT:    compnwp.eq $r0 = $r0, 0
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    compd.ne $r0 = $r0, 0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 1)
  %2 = tail call i64 @llvm.kvx.any.v2i32(<2 x i32> %0, i32 1)
  ret i64 %2
}
