; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -o - %s -O2 | FileCheck %s --check-prefixes=CHECK
; RUN: llc -mcpu=kv3-2 -o - %s -O2 | FileCheck %s --check-prefixes=CHECK
; RUN: clang -O2 -c -o /dev/null %s
; RUN: clang -O2 -march=kv3-2 -c -o /dev/null %s

target triple = "kvx-kalray-cos"

define i64 @ready_int(ptr %0) {
; CHECK-LABEL: ready_int:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lwz $r0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    iord $r0 = $r0, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
  %2 = load i32, ptr %0, align 4
  %3 = tail call i64 (...) @llvm.kvx.ready(i32 %2)
  ret i64 %3
}

declare i64 @llvm.kvx.ready(...)

define i64 @ready_long(ptr %0) {
; CHECK-LABEL: ready_long:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ld $r0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    iord $r0 = $r0, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
  %2 = load i64, ptr %0, align 8
  %3 = tail call i64 (...) @llvm.kvx.ready(i64 %2)
  ret i64 %3
}

define i64 @ready_v2i8(ptr %0) {
; CHECK-LABEL: ready_v2i8:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lhz $r0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    iord $r0 = $r0, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
  %2 = load half, ptr %0, align 2
  %3 = tail call i64 (...) @llvm.kvx.ready(half %2)
  ret i64 %3
}

define i64 @ready_v2i16(ptr %0) {
; CHECK-LABEL: ready_v2i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lwz $r0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    iord $r0 = $r0, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
  %2 = load i32, ptr %0, align 4
  %3 = tail call i64 (...) @llvm.kvx.ready(i32 %2)
  ret i64 %3
}

define i64 @ready_v2i32(ptr %0) {
; CHECK-LABEL: ready_v2i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ld $r0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    iord $r0 = $r0, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
  %2 = load i64, ptr %0, align 8
  %3 = tail call i64 (...) @llvm.kvx.ready(i64 %2)
  ret i64 %3
}

define i64 @ready_v2i64(ptr %0) {
; CHECK-LABEL: ready_v2i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lq $r0r1 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    iord $r0 = $r0, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
  %2 = load <2 x i64>, ptr %0, align 16
  %3 = tail call i64 (...) @llvm.kvx.ready(<2 x i64> %2)
  ret i64 %3
}

define i64 @ready_v4i8(ptr %0) {
; CHECK-LABEL: ready_v4i8:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lwz $r0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    iord $r0 = $r0, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
  %2 = load i32, ptr %0, align 4
  %3 = tail call i64 (...) @llvm.kvx.ready(i32 %2)
  ret i64 %3
}

define i64 @ready_v4i16(ptr %0) {
; CHECK-LABEL: ready_v4i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ld $r0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    iord $r0 = $r0, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
  %2 = load i64, ptr %0, align 8
  %3 = tail call i64 (...) @llvm.kvx.ready(i64 %2)
  ret i64 %3
}

define i64 @ready_v4i32(ptr %0) {
; CHECK-LABEL: ready_v4i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lq $r0r1 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    iord $r0 = $r0, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
  %2 = load <2 x i64>, ptr %0, align 16
  %3 = tail call i64 (...) @llvm.kvx.ready(<2 x i64> %2)
  ret i64 %3
}

define i64 @ready_v4i64(ptr %0) {
; CHECK-LABEL: ready_v4i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo $r0r1r2r3 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    iord $r0 = $r0, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
  %2 = load <4 x i64>, ptr %0, align 32
  %3 = tail call i64 (...) @llvm.kvx.ready(<4 x i64> %2)
  ret i64 %3
}

define i64 @ready_v8i8(ptr %0) {
; CHECK-LABEL: ready_v8i8:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ld $r0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    iord $r0 = $r0, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
  %2 = load i64, ptr %0, align 8
  %3 = tail call i64 (...) @llvm.kvx.ready(i64 %2)
  ret i64 %3
}

define i64 @ready__Float16(ptr %0) {
; CHECK-LABEL: ready__Float16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lhz $r0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    iord $r0 = $r0, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
  %2 = load half, ptr %0, align 2
  %3 = tail call i64 (...) @llvm.kvx.ready(half %2)
  ret i64 %3
}

define i64 @ready_float(ptr %0) {
; CHECK-LABEL: ready_float:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lwz $r0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    iord $r0 = $r0, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
  %2 = load float, ptr %0, align 4
  %3 = fpext float %2 to double
  %4 = bitcast double %3 to i64
  %5 = tail call i64 (...) @llvm.kvx.ready(i64 %4)
  ret i64 %5
}

define i64 @ready_double(ptr %0) {
; CHECK-LABEL: ready_double:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ld $r0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    iord $r0 = $r0, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
  %2 = load i64, ptr %0, align 8
  %3 = tail call i64 (...) @llvm.kvx.ready(i64 %2)
  ret i64 %3
}

define i64 @ready_v2f16(ptr %0) {
; CHECK-LABEL: ready_v2f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lwz $r0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    iord $r0 = $r0, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
  %2 = load i32, ptr %0, align 4
  %3 = tail call i64 (...) @llvm.kvx.ready(i32 %2)
  ret i64 %3
}

define i64 @ready_v2f32(ptr %0) {
; CHECK-LABEL: ready_v2f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ld $r0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    iord $r0 = $r0, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
  %2 = load i64, ptr %0, align 8
  %3 = tail call i64 (...) @llvm.kvx.ready(i64 %2)
  ret i64 %3
}

define i64 @ready_v2f64(ptr %0) {
; CHECK-LABEL: ready_v2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lq $r0r1 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    iord $r0 = $r0, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
  %2 = load <2 x i64>, ptr %0, align 16
  %3 = tail call i64 (...) @llvm.kvx.ready(<2 x i64> %2)
  ret i64 %3
}

define i64 @ready_v4f16(ptr %0) {
; CHECK-LABEL: ready_v4f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ld $r0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    iord $r0 = $r0, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
  %2 = load i64, ptr %0, align 8
  %3 = tail call i64 (...) @llvm.kvx.ready(i64 %2)
  ret i64 %3
}

define i64 @ready_v4f32(ptr %0) {
; CHECK-LABEL: ready_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lq $r0r1 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    iord $r0 = $r0, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
  %2 = load <2 x i64>, ptr %0, align 16
  %3 = tail call i64 (...) @llvm.kvx.ready(<2 x i64> %2)
  ret i64 %3
}

define i64 @ready_v4f64(ptr %0) {
; CHECK-LABEL: ready_v4f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo $r0r1r2r3 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    iord $r0 = $r0, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
  %2 = load <4 x i64>, ptr %0, align 32
  %3 = tail call i64 (...) @llvm.kvx.ready(<4 x i64> %2)
  ret i64 %3
}

define i64 @ready___int128(ptr %0) {
; CHECK-LABEL: ready___int128:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lq $r0r1 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    iord $r0 = $r0, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
  %2 = load <2 x i64>, ptr %0, align 16
  %3 = tail call i64 (...) @llvm.kvx.ready(<2 x i64> %2)
  ret i64 %3
}

define i64 @ready_v8i16(ptr %0) {
; CHECK-LABEL: ready_v8i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lq $r0r1 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    iord $r0 = $r0, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
  %2 = load <2 x i64>, ptr %0, align 16
  %3 = tail call i64 (...) @llvm.kvx.ready(<2 x i64> %2)
  ret i64 %3
}

define i64 @ready_v8i32(ptr %0) {
; CHECK-LABEL: ready_v8i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo $r0r1r2r3 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    iord $r0 = $r0, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
  %2 = load <4 x i64>, ptr %0, align 32
  %3 = tail call i64 (...) @llvm.kvx.ready(<4 x i64> %2)
  ret i64 %3
}

define i64 @ready_char(ptr %0) {
; CHECK-LABEL: ready_char:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lbs $r0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    iord $r0 = $r0, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
  %2 = load i8, ptr %0, align 1
  %3 = sext i8 %2 to i32
  %4 = tail call i64 (...) @llvm.kvx.ready(i32 %3)
  ret i64 %4
}

define i64 @ready_int_v4f32(ptr %0, ptr %1) {
; CHECK-LABEL: ready_int_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lwz $r0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    lq $r2r3 = 0[$r1]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    iord $r0 = $r0, $r2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 4)
  %3 = load i32, ptr %0, align 4
  %4 = load <2 x i64>, ptr %1, align 16
  %5 = tail call i64 (...) @llvm.kvx.ready(i32 %3, <2 x i64> %4)
  ret i64 %5
}

define i64 @ready_long_int(ptr %0, ptr %1) {
; CHECK-LABEL: ready_long_int:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ld $r0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    lwz $r1 = 0[$r1]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    iord $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 4)
  %3 = load i64, ptr %0, align 8
  %4 = load i32, ptr %1, align 4
  %5 = tail call i64 (...) @llvm.kvx.ready(i64 %3, i32 %4)
  ret i64 %5
}

define i64 @ready_float_v8i8(ptr %0, ptr %1) {
; CHECK-LABEL: ready_float_v8i8:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lwz $r0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    ld $r1 = 0[$r1]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    iord $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 4)
  %3 = load float, ptr %0, align 4
  %4 = fpext float %3 to double
  %5 = bitcast double %4 to i64
  %6 = load i64, ptr %1, align 8
  %7 = tail call i64 (...) @llvm.kvx.ready(i64 %5, i64 %6)
  ret i64 %7
}

define i64 @ready_int_long_float(ptr %0, ptr %1, ptr %2) {
; CHECK-LABEL: ready_int_long_float:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lwz $r0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    ld $r1 = 0[$r1]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    lwz $r2 = 0[$r2]
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    iord $r0 = $r0, $r1
; CHECK-NEXT:    iord $r0 = $r2, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 5)
  %4 = load i32, ptr %0, align 4
  %5 = load i64, ptr %1, align 8
  %6 = load float, ptr %2, align 4
  %7 = fpext float %6 to double
  %8 = bitcast double %7 to i64
  %9 = tail call i64 (...) @llvm.kvx.ready(i32 %4, i64 %5, i64 %8)
  ret i64 %9
}

define i64 @ready___int128_v8i8_v2i64(ptr %0, ptr %1, ptr %2) {
; CHECK-LABEL: ready___int128_v8i8_v2i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lq $r4r5 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    ld $r0 = 0[$r1]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    lq $r2r3 = 0[$r2]
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    iord $r0 = $r2, $r4
; CHECK-NEXT:    iord $r0 = $r4, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 5)
  %4 = load <2 x i64>, ptr %0, align 16
  %5 = load i64, ptr %1, align 8
  %6 = load <2 x i64>, ptr %2, align 16
  %7 = tail call i64 (...) @llvm.kvx.ready(<2 x i64> %4, i64 %5, <2 x i64> %6)
  ret i64 %7
}

define i64 @ready_char_short_double(ptr %0, ptr %1, ptr %2) {
; CHECK-LABEL: ready_char_short_double:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lbs $r0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    lhs $r1 = 0[$r1]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    ld $r2 = 0[$r2]
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    iord $r0 = $r0, $r1
; CHECK-NEXT:    iord $r0 = $r2, $r0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 5)
  %4 = load i8, ptr %0, align 1
  %5 = sext i8 %4 to i32
  %6 = load i16, ptr %1, align 2
  %7 = sext i16 %6 to i32
  %8 = load i64, ptr %2, align 8
  %9 = tail call i64 (...) @llvm.kvx.ready(i32 %5, i32 %7, i64 %8)
  ret i64 %9
}

define i64 @ready_char_short_int_long(ptr %0, ptr %1, ptr %2, ptr %3) {
; CHECK-LABEL: ready_char_short_int_long:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lbs $r0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    lhs $r1 = 0[$r1]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    lwz $r2 = 0[$r2]
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    ld $r3 = 0[$r3]
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    iord $r0 = $r0, $r1
; CHECK-NEXT:    iord $r0 = $r2, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 6)
  %5 = load i8, ptr %0, align 1
  %6 = sext i8 %5 to i32
  %7 = load i16, ptr %1, align 2
  %8 = sext i16 %7 to i32
  %9 = load i32, ptr %2, align 4
  %10 = load i64, ptr %3, align 8
  %11 = tail call i64 (...) @llvm.kvx.ready(i32 %6, i32 %8, i32 %9, i64 %10)
  ret i64 %11
}

define i64 @ready__Float16_float_double_v4i64(ptr %0, ptr %1, ptr %2, ptr %3) {
; CHECK-LABEL: ready__Float16_float_double_v4i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lhz $r0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    lwz $r1 = 0[$r1]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    ld $r2 = 0[$r2]
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r3]
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    iord $r0 = $r0, $r1
; CHECK-NEXT:    iord $r0 = $r2, $r4
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 6)
  %5 = load half, ptr %0, align 2
  %6 = load float, ptr %1, align 4
  %7 = fpext float %6 to double
  %8 = bitcast double %7 to i64
  %9 = load i64, ptr %2, align 8
  %10 = load <4 x i64>, ptr %3, align 32
  %11 = tail call i64 (...) @llvm.kvx.ready(half %5, i64 %8, i64 %9, <4 x i64> %10)
  ret i64 %11
}

define i64 @ready_v8f32_v4i32___int128_char(ptr %0, ptr %1, ptr %2, ptr %3) {
; CHECK-LABEL: ready_v8f32_v4i32___int128_char:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    lq $r0r1 = 0[$r1]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    lq $r8r9 = 0[$r2]
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    lbs $r2 = 0[$r3]
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    iord $r0 = $r4, $r0
; CHECK-NEXT:    iord $r0 = $r8, $r2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 6)
  %5 = load <4 x i64>, ptr %0, align 32
  %6 = load <2 x i64>, ptr %1, align 16
  %7 = load <2 x i64>, ptr %2, align 16
  %8 = load i8, ptr %3, align 1
  %9 = sext i8 %8 to i32
  %10 = tail call i64 (...) @llvm.kvx.ready(<4 x i64> %5, <2 x i64> %6, <2 x i64> %7, i32 %9)
  ret i64 %10
}

