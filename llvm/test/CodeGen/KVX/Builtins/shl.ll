; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -o - %s -mtriple=kvx-kalray-cos | FileCheck %s --check-prefixes=ALL,CV1
; RUN: llc -mcpu=kv3-2 -o - %s | FileCheck %s --check-prefixes=ALL,CV2
; RUN: clang -c -o /dev/null %s
; RUN: clang -march=kv3-2 -c -o /dev/null %s

target triple = "kvx-kalray-cos"

define i64 @shld(i64 %0, i32 %1) {
; ALL-LABEL: shld:
; ALL:       # %bb.0:
; ALL-NEXT:    slld $r0 = $r0, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %3 = tail call i64 @llvm.kvx.shl.i64(i64 %0, i32 %1, i32 0)
  ret i64 %3
}

declare i64 @llvm.kvx.shl.i64(i64, i32, i32)

define i64 @shld_s(i64 %0, i32 %1) {
; ALL-LABEL: shld_s:
; ALL:       # %bb.0:
; ALL-NEXT:    slsd $r0 = $r0, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %3 = tail call i64 @llvm.kvx.shl.i64(i64 %0, i32 %1, i32 1)
  ret i64 %3
}

define i64 @shld_us(i64 %0, i32 %1) {
; CV1-LABEL: shld_us:
; CV1:       # %bb.0:
; CV1-NEXT:    slld $r2 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srld $r1 = $r2, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    compd.ne $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    cmoved.odd $r0 ? $r2 = -1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    copyd $r0 = $r2
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 4)
;
; CV2-LABEL: shld_us:
; CV2:       # %bb.0:
; CV2-NEXT:    slusd $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %3 = tail call i64 @llvm.kvx.shl.i64(i64 %0, i32 %1, i32 2)
  ret i64 %3
}

define i64 @shld_r(i64 %0, i32 %1) {
; ALL-LABEL: shld_r:
; ALL:       # %bb.0:
; ALL-NEXT:    negw $r1 = $r1
; ALL-NEXT:    andw $r2 = $r1, 63
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    andw $r1 = $r1, 63
; ALL-NEXT:    slld $r2 = $r0, $r2
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    srld $r0 = $r0, $r1
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    ord $r0 = $r2, $r0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 3)
  %3 = tail call i64 @llvm.kvx.shl.i64(i64 %0, i32 %1, i32 3)
  ret i64 %3
}

define <2 x i64> @shldp(<2 x i64> %0, <2 x i32> %1) {
; ALL-LABEL: shldp:
; ALL:       # %bb.0:
; ALL-NEXT:    slld $r0 = $r0, $r2
; ALL-NEXT:    srad $r3 = $r2, 32
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    slld $r1 = $r1, $r3
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 1)
  %3 = extractelement <2 x i64> %0, i64 0
  %4 = extractelement <2 x i32> %1, i64 0
  %5 = tail call i64 @llvm.kvx.shl.i64(i64 %3, i32 %4, i32 0)
  %6 = extractelement <2 x i64> %0, i64 1
  %7 = extractelement <2 x i32> %1, i64 1
  %8 = tail call i64 @llvm.kvx.shl.i64(i64 %6, i32 %7, i32 0)
  %9 = insertelement <2 x i64> undef, i64 %5, i32 0
  %10 = insertelement <2 x i64> %9, i64 %8, i32 1
  ret <2 x i64> %10
}

define <2 x i64> @shldp_s(<2 x i64> %0, <2 x i32> %1) {
; ALL-LABEL: shldp_s:
; ALL:       # %bb.0:
; ALL-NEXT:    slsd $r0 = $r0, $r2
; ALL-NEXT:    srad $r3 = $r2, 32
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    slsd $r1 = $r1, $r3
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 1)
  %3 = extractelement <2 x i64> %0, i64 0
  %4 = extractelement <2 x i32> %1, i64 0
  %5 = tail call i64 @llvm.kvx.shl.i64(i64 %3, i32 %4, i32 1)
  %6 = extractelement <2 x i64> %0, i64 1
  %7 = extractelement <2 x i32> %1, i64 1
  %8 = tail call i64 @llvm.kvx.shl.i64(i64 %6, i32 %7, i32 1)
  %9 = insertelement <2 x i64> undef, i64 %5, i32 0
  %10 = insertelement <2 x i64> %9, i64 %8, i32 1
  ret <2 x i64> %10
}

define <2 x i64> @shldp_us(<2 x i64> %0, <2 x i32> %1) {
; CV1-LABEL: shldp_us:
; CV1:       # %bb.0:
; CV1-NEXT:    slld $r3 = $r0, $r2
; CV1-NEXT:    srad $r4 = $r2, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    slld $r2 = $r1, $r4
; CV1-NEXT:    srld $r5 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    compd.ne $r0 = $r0, $r5
; CV1-NEXT:    srld $r4 = $r2, $r4
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    compd.ne $r1 = $r1, $r4
; CV1-NEXT:    cmoved.odd $r0 ? $r3 = -1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    copyd $r0 = $r3
; CV1-NEXT:    cmoved.odd $r1 ? $r2 = -1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    copyd $r1 = $r2
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 5)
;
; CV2-LABEL: shldp_us:
; CV2:       # %bb.0:
; CV2-NEXT:    slusd $r0 = $r0, $r2
; CV2-NEXT:    srad $r3 = $r2, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    slusd $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 1)
  %3 = extractelement <2 x i64> %0, i64 0
  %4 = extractelement <2 x i32> %1, i64 0
  %5 = tail call i64 @llvm.kvx.shl.i64(i64 %3, i32 %4, i32 2)
  %6 = extractelement <2 x i64> %0, i64 1
  %7 = extractelement <2 x i32> %1, i64 1
  %8 = tail call i64 @llvm.kvx.shl.i64(i64 %6, i32 %7, i32 2)
  %9 = insertelement <2 x i64> undef, i64 %5, i32 0
  %10 = insertelement <2 x i64> %9, i64 %8, i32 1
  ret <2 x i64> %10
}

define <2 x i64> @shldp_r(<2 x i64> %0, <2 x i32> %1) {
; ALL-LABEL: shldp_r:
; ALL:       # %bb.0:
; ALL-NEXT:    negw $r2 = $r2
; ALL-NEXT:    andw $r3 = $r2, 63
; ALL-NEXT:    srad $r4 = $r2, 32
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    andw $r2 = $r2, 63
; ALL-NEXT:    slld $r3 = $r0, $r3
; ALL-NEXT:    andw $r4 = $r4, 63
; ALL-NEXT:    negw $r5 = $r4
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    srld $r0 = $r0, $r2
; ALL-NEXT:    slld $r2 = $r1, $r4
; ALL-NEXT:    andw $r5 = $r5, 63
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    ord $r0 = $r3, $r0
; ALL-NEXT:    srld $r1 = $r1, $r5
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    ord $r1 = $r2, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 4)
  %3 = extractelement <2 x i64> %0, i64 0
  %4 = extractelement <2 x i32> %1, i64 0
  %5 = tail call i64 @llvm.kvx.shl.i64(i64 %3, i32 %4, i32 3)
  %6 = extractelement <2 x i64> %0, i64 1
  %7 = extractelement <2 x i32> %1, i64 1
  %8 = tail call i64 @llvm.kvx.shl.i64(i64 %6, i32 %7, i32 3)
  %9 = insertelement <2 x i64> undef, i64 %5, i32 0
  %10 = insertelement <2 x i64> %9, i64 %8, i32 1
  ret <2 x i64> %10
}

define <2 x i64> @shldps(<2 x i64> %0, i32 %1) {
; ALL-LABEL: shldps:
; ALL:       # %bb.0:
; ALL-NEXT:    slld $r0 = $r0, $r2
; ALL-NEXT:    slld $r1 = $r1, $r2
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %3 = extractelement <2 x i64> %0, i64 0
  %4 = tail call i64 @llvm.kvx.shl.i64(i64 %3, i32 %1, i32 0)
  %5 = extractelement <2 x i64> %0, i64 1
  %6 = tail call i64 @llvm.kvx.shl.i64(i64 %5, i32 %1, i32 0)
  %7 = insertelement <2 x i64> undef, i64 %4, i32 0
  %8 = insertelement <2 x i64> %7, i64 %6, i32 1
  ret <2 x i64> %8
}

define <2 x i64> @shldps_s(<2 x i64> %0, i32 %1) {
; ALL-LABEL: shldps_s:
; ALL:       # %bb.0:
; ALL-NEXT:    slsd $r0 = $r0, $r2
; ALL-NEXT:    slsd $r1 = $r1, $r2
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %3 = extractelement <2 x i64> %0, i64 0
  %4 = tail call i64 @llvm.kvx.shl.i64(i64 %3, i32 %1, i32 1)
  %5 = extractelement <2 x i64> %0, i64 1
  %6 = tail call i64 @llvm.kvx.shl.i64(i64 %5, i32 %1, i32 1)
  %7 = insertelement <2 x i64> undef, i64 %4, i32 0
  %8 = insertelement <2 x i64> %7, i64 %6, i32 1
  ret <2 x i64> %8
}

define <2 x i64> @shldps_us(<2 x i64> %0, i32 %1) {
; CV1-LABEL: shldps_us:
; CV1:       # %bb.0:
; CV1-NEXT:    slld $r3 = $r0, $r2
; CV1-NEXT:    slld $r4 = $r1, $r2
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srld $r2 = $r4, $r2
; CV1-NEXT:    srld $r5 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    compd.ne $r0 = $r0, $r5
; CV1-NEXT:    compd.ne $r1 = $r1, $r2
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    cmoved.odd $r0 ? $r3 = -1
; CV1-NEXT:    cmoved.odd $r1 ? $r4 = -1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    copyd $r0 = $r3
; CV1-NEXT:    copyd $r1 = $r4
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 4)
;
; CV2-LABEL: shldps_us:
; CV2:       # %bb.0:
; CV2-NEXT:    slusd $r0 = $r0, $r2
; CV2-NEXT:    slusd $r1 = $r1, $r2
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %3 = extractelement <2 x i64> %0, i64 0
  %4 = tail call i64 @llvm.kvx.shl.i64(i64 %3, i32 %1, i32 2)
  %5 = extractelement <2 x i64> %0, i64 1
  %6 = tail call i64 @llvm.kvx.shl.i64(i64 %5, i32 %1, i32 2)
  %7 = insertelement <2 x i64> undef, i64 %4, i32 0
  %8 = insertelement <2 x i64> %7, i64 %6, i32 1
  ret <2 x i64> %8
}

define <2 x i64> @shldps_r(<2 x i64> %0, i32 %1) {
; ALL-LABEL: shldps_r:
; ALL:       # %bb.0:
; ALL-NEXT:    negw $r2 = $r2
; ALL-NEXT:    andw $r3 = $r2, 63
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    andw $r2 = $r2, 63
; ALL-NEXT:    slld $r3 = $r1, $r3
; ALL-NEXT:    slld $r4 = $r0, $r3
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    srld $r0 = $r0, $r2
; ALL-NEXT:    srld $r1 = $r1, $r2
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    ord $r0 = $r4, $r0
; ALL-NEXT:    ord $r1 = $r3, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 3)
  %3 = extractelement <2 x i64> %0, i64 0
  %4 = tail call i64 @llvm.kvx.shl.i64(i64 %3, i32 %1, i32 3)
  %5 = extractelement <2 x i64> %0, i64 1
  %6 = tail call i64 @llvm.kvx.shl.i64(i64 %5, i32 %1, i32 3)
  %7 = insertelement <2 x i64> undef, i64 %4, i32 0
  %8 = insertelement <2 x i64> %7, i64 %6, i32 1
  ret <2 x i64> %8
}

define <4 x i64> @shldq(<4 x i64> %0, <4 x i32> %1) {
; ALL-LABEL: shldq:
; ALL:       # %bb.0:
; ALL-NEXT:    slld $r0 = $r0, $r4
; ALL-NEXT:    slld $r2 = $r2, $r5
; ALL-NEXT:    srad $r4 = $r4, 32
; ALL-NEXT:    srad $r6 = $r5, 32
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    slld $r1 = $r1, $r4
; ALL-NEXT:    slld $r3 = $r3, $r6
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 1)
  %3 = extractelement <4 x i64> %0, i64 0
  %4 = extractelement <4 x i32> %1, i64 0
  %5 = tail call i64 @llvm.kvx.shl.i64(i64 %3, i32 %4, i32 0)
  %6 = extractelement <4 x i64> %0, i64 1
  %7 = extractelement <4 x i32> %1, i64 1
  %8 = tail call i64 @llvm.kvx.shl.i64(i64 %6, i32 %7, i32 0)
  %9 = extractelement <4 x i64> %0, i64 2
  %10 = extractelement <4 x i32> %1, i64 2
  %11 = tail call i64 @llvm.kvx.shl.i64(i64 %9, i32 %10, i32 0)
  %12 = extractelement <4 x i64> %0, i64 3
  %13 = extractelement <4 x i32> %1, i64 3
  %14 = tail call i64 @llvm.kvx.shl.i64(i64 %12, i32 %13, i32 0)
  %15 = insertelement <4 x i64> undef, i64 %5, i32 0
  %16 = insertelement <4 x i64> %15, i64 %8, i32 1
  %17 = insertelement <4 x i64> %16, i64 %11, i32 2
  %18 = insertelement <4 x i64> %17, i64 %14, i32 3
  ret <4 x i64> %18
}

define <4 x i64> @shldq_s(<4 x i64> %0, <4 x i32> %1) {
; ALL-LABEL: shldq_s:
; ALL:       # %bb.0:
; ALL-NEXT:    slsd $r0 = $r0, $r4
; ALL-NEXT:    slsd $r2 = $r2, $r5
; ALL-NEXT:    srad $r4 = $r4, 32
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    slsd $r1 = $r1, $r4
; ALL-NEXT:    srad $r4 = $r5, 32
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    slsd $r3 = $r3, $r4
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 2)
  %3 = extractelement <4 x i64> %0, i64 0
  %4 = extractelement <4 x i32> %1, i64 0
  %5 = tail call i64 @llvm.kvx.shl.i64(i64 %3, i32 %4, i32 1)
  %6 = extractelement <4 x i64> %0, i64 1
  %7 = extractelement <4 x i32> %1, i64 1
  %8 = tail call i64 @llvm.kvx.shl.i64(i64 %6, i32 %7, i32 1)
  %9 = extractelement <4 x i64> %0, i64 2
  %10 = extractelement <4 x i32> %1, i64 2
  %11 = tail call i64 @llvm.kvx.shl.i64(i64 %9, i32 %10, i32 1)
  %12 = extractelement <4 x i64> %0, i64 3
  %13 = extractelement <4 x i32> %1, i64 3
  %14 = tail call i64 @llvm.kvx.shl.i64(i64 %12, i32 %13, i32 1)
  %15 = insertelement <4 x i64> undef, i64 %5, i32 0
  %16 = insertelement <4 x i64> %15, i64 %8, i32 1
  %17 = insertelement <4 x i64> %16, i64 %11, i32 2
  %18 = insertelement <4 x i64> %17, i64 %14, i32 3
  ret <4 x i64> %18
}

define <4 x i64> @shldq_us(<4 x i64> %0, <4 x i32> %1) {
; CV1-LABEL: shldq_us:
; CV1:       # %bb.0:
; CV1-NEXT:    slld $r6 = $r0, $r4
; CV1-NEXT:    slld $r8 = $r2, $r5
; CV1-NEXT:    srad $r10 = $r4, 32
; CV1-NEXT:    srad $r11 = $r5, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srld $r4 = $r6, $r4
; CV1-NEXT:    slld $r7 = $r1, $r10
; CV1-NEXT:    slld $r9 = $r3, $r11
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    compd.ne $r0 = $r0, $r4
; CV1-NEXT:    srld $r4 = $r8, $r5
; CV1-NEXT:    srld $r5 = $r9, $r11
; CV1-NEXT:    srld $r10 = $r7, $r10
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    compd.ne $r0 = $r2, $r4
; CV1-NEXT:    compd.ne $r1 = $r1, $r10
; CV1-NEXT:    cmoved.odd $r0 ? $r6 = -1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    copyd $r0 = $r6
; CV1-NEXT:    compd.ne $r1 = $r3, $r5
; CV1-NEXT:    cmoved.odd $r1 ? $r7 = -1
; CV1-NEXT:    cmoved.odd $r0 ? $r8 = -1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    copyd $r1 = $r7
; CV1-NEXT:    copyd $r2 = $r8
; CV1-NEXT:    cmoved.odd $r1 ? $r9 = -1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    copyd $r3 = $r9
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: shldq_us:
; CV2:       # %bb.0:
; CV2-NEXT:    slusd $r0 = $r0, $r4
; CV2-NEXT:    slusd $r2 = $r2, $r5
; CV2-NEXT:    srad $r4 = $r4, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    slusd $r1 = $r1, $r4
; CV2-NEXT:    srad $r4 = $r5, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    slusd $r3 = $r3, $r4
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 2)
  %3 = extractelement <4 x i64> %0, i64 0
  %4 = extractelement <4 x i32> %1, i64 0
  %5 = tail call i64 @llvm.kvx.shl.i64(i64 %3, i32 %4, i32 2)
  %6 = extractelement <4 x i64> %0, i64 1
  %7 = extractelement <4 x i32> %1, i64 1
  %8 = tail call i64 @llvm.kvx.shl.i64(i64 %6, i32 %7, i32 2)
  %9 = extractelement <4 x i64> %0, i64 2
  %10 = extractelement <4 x i32> %1, i64 2
  %11 = tail call i64 @llvm.kvx.shl.i64(i64 %9, i32 %10, i32 2)
  %12 = extractelement <4 x i64> %0, i64 3
  %13 = extractelement <4 x i32> %1, i64 3
  %14 = tail call i64 @llvm.kvx.shl.i64(i64 %12, i32 %13, i32 2)
  %15 = insertelement <4 x i64> undef, i64 %5, i32 0
  %16 = insertelement <4 x i64> %15, i64 %8, i32 1
  %17 = insertelement <4 x i64> %16, i64 %11, i32 2
  %18 = insertelement <4 x i64> %17, i64 %14, i32 3
  ret <4 x i64> %18
}

define <4 x i64> @shldq_r(<4 x i64> %0, <4 x i32> %1) {
; ALL-LABEL: shldq_r:
; ALL:       # %bb.0:
; ALL-NEXT:    srad $r4 = $r4, 32
; ALL-NEXT:    andw $r6 = $r4, 63
; ALL-NEXT:    negw $r7 = $r4
; ALL-NEXT:    andw $r8 = $r5, 63
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    andw $r4 = $r4, 63
; ALL-NEXT:    slld $r6 = $r0, $r6
; ALL-NEXT:    andw $r7 = $r7, 63
; ALL-NEXT:    negw $r9 = $r4
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    srld $r0 = $r0, $r7
; ALL-NEXT:    negw $r5 = $r5
; ALL-NEXT:    srad $r7 = $r5, 32
; ALL-NEXT:    andw $r9 = $r9, 63
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    srld $r1 = $r1, $r9
; ALL-NEXT:    slld $r4 = $r1, $r4
; ALL-NEXT:    andw $r5 = $r5, 63
; ALL-NEXT:    negw $r10 = $r7
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    srld $r2 = $r2, $r5
; ALL-NEXT:    andw $r7 = $r7, 63
; ALL-NEXT:    slld $r8 = $r2, $r8
; ALL-NEXT:    andw $r9 = $r10, 63
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    ord $r0 = $r6, $r0
; ALL-NEXT:    ord $r1 = $r4, $r1
; ALL-NEXT:    srld $r3 = $r3, $r9
; ALL-NEXT:    slld $r5 = $r3, $r7
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    ord $r2 = $r8, $r2
; ALL-NEXT:    ord $r3 = $r5, $r3
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 6)
  %3 = extractelement <4 x i64> %0, i64 0
  %4 = extractelement <4 x i32> %1, i64 0
  %5 = tail call i64 @llvm.kvx.shl.i64(i64 %3, i32 %4, i32 3)
  %6 = extractelement <4 x i64> %0, i64 1
  %7 = extractelement <4 x i32> %1, i64 1
  %8 = tail call i64 @llvm.kvx.shl.i64(i64 %6, i32 %7, i32 3)
  %9 = extractelement <4 x i64> %0, i64 2
  %10 = extractelement <4 x i32> %1, i64 2
  %11 = tail call i64 @llvm.kvx.shl.i64(i64 %9, i32 %10, i32 3)
  %12 = extractelement <4 x i64> %0, i64 3
  %13 = extractelement <4 x i32> %1, i64 3
  %14 = tail call i64 @llvm.kvx.shl.i64(i64 %12, i32 %13, i32 3)
  %15 = insertelement <4 x i64> undef, i64 %5, i32 0
  %16 = insertelement <4 x i64> %15, i64 %8, i32 1
  %17 = insertelement <4 x i64> %16, i64 %11, i32 2
  %18 = insertelement <4 x i64> %17, i64 %14, i32 3
  ret <4 x i64> %18
}

define <4 x i64> @shldqs(<4 x i64> %0, i32 %1) {
; ALL-LABEL: shldqs:
; ALL:       # %bb.0:
; ALL-NEXT:    slld $r0 = $r0, $r4
; ALL-NEXT:    slld $r1 = $r1, $r4
; ALL-NEXT:    slld $r2 = $r2, $r4
; ALL-NEXT:    slld $r3 = $r3, $r4
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %3 = extractelement <4 x i64> %0, i64 0
  %4 = tail call i64 @llvm.kvx.shl.i64(i64 %3, i32 %1, i32 0)
  %5 = extractelement <4 x i64> %0, i64 1
  %6 = tail call i64 @llvm.kvx.shl.i64(i64 %5, i32 %1, i32 0)
  %7 = extractelement <4 x i64> %0, i64 2
  %8 = tail call i64 @llvm.kvx.shl.i64(i64 %7, i32 %1, i32 0)
  %9 = extractelement <4 x i64> %0, i64 3
  %10 = tail call i64 @llvm.kvx.shl.i64(i64 %9, i32 %1, i32 0)
  %11 = insertelement <4 x i64> undef, i64 %4, i32 0
  %12 = insertelement <4 x i64> %11, i64 %6, i32 1
  %13 = insertelement <4 x i64> %12, i64 %8, i32 2
  %14 = insertelement <4 x i64> %13, i64 %10, i32 3
  ret <4 x i64> %14
}

define <4 x i64> @shldqs_s(<4 x i64> %0, i32 %1) {
; ALL-LABEL: shldqs_s:
; ALL:       # %bb.0:
; ALL-NEXT:    slsd $r0 = $r0, $r4
; ALL-NEXT:    slsd $r1 = $r1, $r4
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    slsd $r2 = $r2, $r4
; ALL-NEXT:    slsd $r3 = $r3, $r4
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 1)
  %3 = extractelement <4 x i64> %0, i64 0
  %4 = tail call i64 @llvm.kvx.shl.i64(i64 %3, i32 %1, i32 1)
  %5 = extractelement <4 x i64> %0, i64 1
  %6 = tail call i64 @llvm.kvx.shl.i64(i64 %5, i32 %1, i32 1)
  %7 = extractelement <4 x i64> %0, i64 2
  %8 = tail call i64 @llvm.kvx.shl.i64(i64 %7, i32 %1, i32 1)
  %9 = extractelement <4 x i64> %0, i64 3
  %10 = tail call i64 @llvm.kvx.shl.i64(i64 %9, i32 %1, i32 1)
  %11 = insertelement <4 x i64> undef, i64 %4, i32 0
  %12 = insertelement <4 x i64> %11, i64 %6, i32 1
  %13 = insertelement <4 x i64> %12, i64 %8, i32 2
  %14 = insertelement <4 x i64> %13, i64 %10, i32 3
  ret <4 x i64> %14
}

define <4 x i64> @shldqs_us(<4 x i64> %0, i32 %1) {
; CV1-LABEL: shldqs_us:
; CV1:       # %bb.0:
; CV1-NEXT:    slld $r5 = $r0, $r4
; CV1-NEXT:    slld $r6 = $r1, $r4
; CV1-NEXT:    slld $r7 = $r2, $r4
; CV1-NEXT:    slld $r8 = $r3, $r4
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srld $r4 = $r8, $r4
; CV1-NEXT:    srld $r9 = $r5, $r4
; CV1-NEXT:    srld $r10 = $r6, $r4
; CV1-NEXT:    srld $r11 = $r7, $r4
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    compd.ne $r0 = $r0, $r9
; CV1-NEXT:    compd.ne $r1 = $r1, $r10
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    compd.ne $r0 = $r2, $r11
; CV1-NEXT:    compd.ne $r1 = $r3, $r4
; CV1-NEXT:    cmoved.odd $r0 ? $r5 = -1
; CV1-NEXT:    cmoved.odd $r1 ? $r6 = -1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    copyd $r0 = $r5
; CV1-NEXT:    copyd $r1 = $r6
; CV1-NEXT:    cmoved.odd $r0 ? $r7 = -1
; CV1-NEXT:    cmoved.odd $r1 ? $r8 = -1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    copyd $r2 = $r7
; CV1-NEXT:    copyd $r3 = $r8
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 5)
;
; CV2-LABEL: shldqs_us:
; CV2:       # %bb.0:
; CV2-NEXT:    slusd $r0 = $r0, $r4
; CV2-NEXT:    slusd $r1 = $r1, $r4
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    slusd $r2 = $r2, $r4
; CV2-NEXT:    slusd $r3 = $r3, $r4
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 1)
  %3 = extractelement <4 x i64> %0, i64 0
  %4 = tail call i64 @llvm.kvx.shl.i64(i64 %3, i32 %1, i32 2)
  %5 = extractelement <4 x i64> %0, i64 1
  %6 = tail call i64 @llvm.kvx.shl.i64(i64 %5, i32 %1, i32 2)
  %7 = extractelement <4 x i64> %0, i64 2
  %8 = tail call i64 @llvm.kvx.shl.i64(i64 %7, i32 %1, i32 2)
  %9 = extractelement <4 x i64> %0, i64 3
  %10 = tail call i64 @llvm.kvx.shl.i64(i64 %9, i32 %1, i32 2)
  %11 = insertelement <4 x i64> undef, i64 %4, i32 0
  %12 = insertelement <4 x i64> %11, i64 %6, i32 1
  %13 = insertelement <4 x i64> %12, i64 %8, i32 2
  %14 = insertelement <4 x i64> %13, i64 %10, i32 3
  ret <4 x i64> %14
}

define <4 x i64> @shldqs_r(<4 x i64> %0, i32 %1) {
; ALL-LABEL: shldqs_r:
; ALL:       # %bb.0:
; ALL-NEXT:    andw $r4 = $r4, 63
; ALL-NEXT:    negw $r5 = $r4
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    andw $r5 = $r5, 63
; ALL-NEXT:    slld $r6 = $r0, $r4
; ALL-NEXT:    slld $r7 = $r1, $r4
; ALL-NEXT:    slld $r8 = $r2, $r4
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    srld $r0 = $r0, $r5
; ALL-NEXT:    srld $r1 = $r1, $r5
; ALL-NEXT:    srld $r2 = $r2, $r5
; ALL-NEXT:    slld $r4 = $r3, $r4
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    ord $r0 = $r6, $r0
; ALL-NEXT:    ord $r1 = $r7, $r1
; ALL-NEXT:    ord $r2 = $r8, $r2
; ALL-NEXT:    srld $r3 = $r3, $r5
; ALL-NEXT:    ;; # (end cycle 3)
; ALL-NEXT:    ord $r3 = $r4, $r3
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 4)
  %3 = extractelement <4 x i64> %0, i64 0
  %4 = tail call i64 @llvm.kvx.shl.i64(i64 %3, i32 %1, i32 3)
  %5 = extractelement <4 x i64> %0, i64 1
  %6 = tail call i64 @llvm.kvx.shl.i64(i64 %5, i32 %1, i32 3)
  %7 = extractelement <4 x i64> %0, i64 2
  %8 = tail call i64 @llvm.kvx.shl.i64(i64 %7, i32 %1, i32 3)
  %9 = extractelement <4 x i64> %0, i64 3
  %10 = tail call i64 @llvm.kvx.shl.i64(i64 %9, i32 %1, i32 3)
  %11 = insertelement <4 x i64> undef, i64 %4, i32 0
  %12 = insertelement <4 x i64> %11, i64 %6, i32 1
  %13 = insertelement <4 x i64> %12, i64 %8, i32 2
  %14 = insertelement <4 x i64> %13, i64 %10, i32 3
  ret <4 x i64> %14
}

define <8 x i16> @shlhos(<8 x i16> %0, i32 %1) {
; ALL-LABEL: shlhos:
; ALL:       # %bb.0:
; ALL-NEXT:    sllhqs $r0 = $r0, $r2
; ALL-NEXT:    sllhqs $r1 = $r1, $r2
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %3 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4 = tail call <4 x i16> @llvm.kvx.shl.v4i16(<4 x i16> %3, i32 %1, i32 0)
  %5 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6 = tail call <4 x i16> @llvm.kvx.shl.v4i16(<4 x i16> %5, i32 %1, i32 0)
  %7 = shufflevector <4 x i16> %4, <4 x i16> %6, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i16> %7
}

declare <4 x i16> @llvm.kvx.shl.v4i16(<4 x i16>, i32, i32)

define <8 x i16> @shlhos_s(<8 x i16> %0, i32 %1) {
; ALL-LABEL: shlhos_s:
; ALL:       # %bb.0:
; ALL-NEXT:    slshqs $r0 = $r0, $r2
; ALL-NEXT:    slshqs $r1 = $r1, $r2
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %3 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4 = tail call <4 x i16> @llvm.kvx.shl.v4i16(<4 x i16> %3, i32 %1, i32 1)
  %5 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6 = tail call <4 x i16> @llvm.kvx.shl.v4i16(<4 x i16> %5, i32 %1, i32 1)
  %7 = shufflevector <4 x i16> %4, <4 x i16> %6, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i16> %7
}

define <8 x i16> @shlhos_us(<8 x i16> %0, i32 %1) {
; CV1-LABEL: shlhos_us:
; CV1:       # %bb.0:
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    srld $r3 = $r0, 48
; CV1-NEXT:    extfz $r4 = $r0, 47, 32
; CV1-NEXT:    srlw $r5 = $r0, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sllw $r0 = $r0, 16
; CV1-NEXT:    sllw $r3 = $r3, 16
; CV1-NEXT:    sllw $r4 = $r4, 16
; CV1-NEXT:    sllw $r5 = $r5, 16
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    zxhd $r2 = $r2
; CV1-NEXT:    zxwd $r3 = $r3
; CV1-NEXT:    zxwd $r4 = $r4
; CV1-NEXT:    zxwd $r5 = $r5
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    slld $r3 = $r3, $r2
; CV1-NEXT:    slld $r4 = $r4, $r2
; CV1-NEXT:    slld $r5 = $r5, $r2
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    slld $r0 = $r0, $r2
; CV1-NEXT:    minud $r3 = $r3, 0xffffffff
; CV1-NEXT:    minud $r4 = $r4, 0xffffffff
; CV1-NEXT:    minud $r5 = $r5, 0xffffffff
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    minud $r0 = $r0, 0xffffffff
; CV1-NEXT:    srlw $r3 = $r3, 16
; CV1-NEXT:    srlw $r4 = $r4, 16
; CV1-NEXT:    srlw $r5 = $r5, 16
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r0 = $r0, 16
; CV1-NEXT:    srld $r3 = $r1, 48
; CV1-NEXT:    insf $r4 = $r3, 31, 16
; CV1-NEXT:    srlw $r6 = $r1, 16
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r0 = $r5, 31, 16
; CV1-NEXT:    sllw $r3 = $r3, 16
; CV1-NEXT:    extfz $r5 = $r1, 47, 32
; CV1-NEXT:    sllw $r6 = $r6, 16
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    zxhd $r1 = $r1
; CV1-NEXT:    zxwd $r3 = $r3
; CV1-NEXT:    sllw $r5 = $r5, 16
; CV1-NEXT:    zxwd $r6 = $r6
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    sllw $r1 = $r1, 16
; CV1-NEXT:    slld $r3 = $r3, $r2
; CV1-NEXT:    zxwd $r5 = $r5
; CV1-NEXT:    slld $r6 = $r6, $r2
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r0 = $r4, 63, 32
; CV1-NEXT:    zxwd $r1 = $r1
; CV1-NEXT:    slld $r5 = $r5, $r2
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    slld $r1 = $r1, $r2
; CV1-NEXT:    minud $r2 = $r3, 0xffffffff
; CV1-NEXT:    minud $r3 = $r5, 0xffffffff
; CV1-NEXT:    minud $r5 = $r6, 0xffffffff
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    minud $r1 = $r1, 0xffffffff
; CV1-NEXT:    srlw $r2 = $r2, 16
; CV1-NEXT:    srlw $r3 = $r3, 16
; CV1-NEXT:    srlw $r5 = $r5, 16
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    srlw $r1 = $r1, 16
; CV1-NEXT:    insf $r3 = $r2, 31, 16
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    insf $r1 = $r5, 31, 16
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    insf $r1 = $r3, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 15)
;
; CV2-LABEL: shlhos_us:
; CV2:       # %bb.0:
; CV2-NEXT:    slushqs $r0 = $r0, $r2
; CV2-NEXT:    slushqs $r1 = $r1, $r2
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %3 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4 = tail call <4 x i16> @llvm.kvx.shl.v4i16(<4 x i16> %3, i32 %1, i32 2)
  %5 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6 = tail call <4 x i16> @llvm.kvx.shl.v4i16(<4 x i16> %5, i32 %1, i32 2)
  %7 = shufflevector <4 x i16> %4, <4 x i16> %6, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i16> %7
}

define <8 x i16> @shlhos_r(<8 x i16> %0, i32 %1) {
; CV1-LABEL: shlhos_r:
; CV1:       # %bb.0:
; CV1-NEXT:    sbmm8 $r2 = $r2, 0x2010201.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    neghq $r2 = $r2
; CV1-NEXT:    andd $r3 = $r2, 0xf000f.@
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andd $r2 = $r2, 0xf000f.@
; CV1-NEXT:    sllhqs $r4 = $r0, $r3
; CV1-NEXT:    extfz $r5 = $r3, 19, 16
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sllhqs $r6 = $r0, $r5
; CV1-NEXT:    extfz $r7 = $r3, 35, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sllhqs $r4 = $r0, $r7
; CV1-NEXT:    insf $r6 = $r4, 15, 0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    insf $r4 = $r6, 31, 0
; CV1-NEXT:    extfz $r6 = $r3, 51, 48
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sllhqs $r8 = $r0, $r6
; CV1-NEXT:    srlhqs $r9 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    extfz $r4 = $r2, 19, 16
; CV1-NEXT:    insf $r8 = $r4, 47, 0
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    srlhqs $r10 = $r0, $r4
; CV1-NEXT:    extfz $r11 = $r2, 35, 32
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    sllhqs $r3 = $r1, $r3
; CV1-NEXT:    extfz $r15 = $r2, 51, 48
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    srlhqs $r2 = $r1, $r2
; CV1-NEXT:    sllhqs $r5 = $r1, $r5
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    srlhqs $r4 = $r1, $r4
; CV1-NEXT:    insf $r10 = $r9, 15, 0
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    insf $r5 = $r3, 15, 0
; CV1-NEXT:    srlhqs $r9 = $r0, $r11
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    sllhqs $r3 = $r1, $r7
; CV1-NEXT:    insf $r4 = $r2, 15, 0
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    srlhqs $r2 = $r1, $r11
; CV1-NEXT:    insf $r9 = $r10, 31, 0
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    srlhqs $r0 = $r0, $r15
; CV1-NEXT:    insf $r3 = $r5, 31, 0
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    insf $r2 = $r4, 31, 0
; CV1-NEXT:    sllhqs $r5 = $r1, $r6
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    insf $r0 = $r9, 47, 0
; CV1-NEXT:    srlhqs $r1 = $r1, $r15
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    ord $r0 = $r8, $r0
; CV1-NEXT:    insf $r1 = $r2, 47, 0
; CV1-NEXT:    insf $r5 = $r3, 47, 0
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    ord $r1 = $r5, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 19)
;
; CV2-LABEL: shlhos_r:
; CV2:       # %bb.0:
; CV2-NEXT:    sbmm8 $r2 = $r2, 0x2010201.@
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    neghq $r2 = $r2
; CV2-NEXT:    andd $r3 = $r2, 0xf000f.@
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    andd $r2 = $r2, 0xf000f.@
; CV2-NEXT:    sllhqs $r4 = $r0, $r3
; CV2-NEXT:    extfz $r5 = $r3, 19, 16
; CV2-NEXT:    extfz $r6 = $r3, 35, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    extfz $r7 = $r3, 51, 48
; CV2-NEXT:    sllhqs $r8 = $r0, $r5
; CV2-NEXT:    sllhqs $r9 = $r0, $r6
; CV2-NEXT:    extfz $r11 = $r2, 19, 16
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sllhqs $r3 = $r1, $r3
; CV2-NEXT:    srlhqs $r4 = $r0, $r2
; CV2-NEXT:    sllhqs $r5 = $r1, $r5
; CV2-NEXT:    insf $r8 = $r4, 15, 0
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r5 = $r3, 15, 0
; CV2-NEXT:    srlhqs $r8 = $r0, $r11
; CV2-NEXT:    insf $r9 = $r8, 31, 0
; CV2-NEXT:    extfz $r15 = $r2, 35, 32
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    srlhqs $r2 = $r1, $r2
; CV2-NEXT:    srlhqs $r3 = $r0, $r15
; CV2-NEXT:    extfz $r4 = $r2, 51, 48
; CV2-NEXT:    insf $r8 = $r4, 15, 0
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r3 = $r8, 31, 0
; CV2-NEXT:    sllhqs $r6 = $r1, $r6
; CV2-NEXT:    srlhqs $r8 = $r1, $r11
; CV2-NEXT:    sllhqs $r10 = $r0, $r7
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    srlhqs $r0 = $r0, $r4
; CV2-NEXT:    srlhqs $r2 = $r1, $r15
; CV2-NEXT:    insf $r6 = $r5, 31, 0
; CV2-NEXT:    insf $r8 = $r2, 15, 0
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    srlhqs $r1 = $r1, $r4
; CV2-NEXT:    insf $r2 = $r8, 31, 0
; CV2-NEXT:    sllhqs $r5 = $r1, $r7
; CV2-NEXT:    insf $r10 = $r9, 47, 0
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r0 = $r3, 47, 0
; CV2-NEXT:    insf $r1 = $r2, 47, 0
; CV2-NEXT:    insf $r5 = $r6, 47, 0
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    ord $r0 = $r10, $r0
; CV2-NEXT:    ord $r1 = $r5, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 11)
  %3 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4 = tail call <4 x i16> @llvm.kvx.shl.v4i16(<4 x i16> %3, i32 %1, i32 3)
  %5 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6 = tail call <4 x i16> @llvm.kvx.shl.v4i16(<4 x i16> %5, i32 %1, i32 3)
  %7 = shufflevector <4 x i16> %4, <4 x i16> %6, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i16> %7
}

define <2 x i16> @shlhps(<2 x i16> %0, i32 %1) {
; ALL-LABEL: shlhps:
; ALL:       # %bb.0:
; ALL-NEXT:    sllhqs $r0 = $r0, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %3 = tail call <2 x i16> @llvm.kvx.shl.v2i16(<2 x i16> %0, i32 %1, i32 0)
  ret <2 x i16> %3
}

declare <2 x i16> @llvm.kvx.shl.v2i16(<2 x i16>, i32, i32)

define <2 x i16> @shlhps_s(<2 x i16> %0, i32 %1) {
; ALL-LABEL: shlhps_s:
; ALL:       # %bb.0:
; ALL-NEXT:    slshqs $r0 = $r0, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %3 = tail call <2 x i16> @llvm.kvx.shl.v2i16(<2 x i16> %0, i32 %1, i32 1)
  ret <2 x i16> %3
}

define <2 x i16> @shlhps_us(<2 x i16> %0, i32 %1) {
; CV1-LABEL: shlhps_us:
; CV1:       # %bb.0:
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    zxhd $r1 = $r1
; CV1-NEXT:    srlw $r2 = $r0, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sllw $r0 = $r0, 16
; CV1-NEXT:    sllw $r2 = $r2, 16
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    zxwd $r2 = $r2
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    slld $r0 = $r0, $r1
; CV1-NEXT:    slld $r2 = $r2, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    minud $r0 = $r0, 0xffffffff
; CV1-NEXT:    minud $r1 = $r2, 0xffffffff
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    srlw $r0 = $r0, 16
; CV1-NEXT:    srlw $r1 = $r1, 16
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    insf $r0 = $r1, 31, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: shlhps_us:
; CV2:       # %bb.0:
; CV2-NEXT:    slushqs $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %3 = tail call <2 x i16> @llvm.kvx.shl.v2i16(<2 x i16> %0, i32 %1, i32 2)
  ret <2 x i16> %3
}

define <2 x i16> @shlhps_r(<2 x i16> %0, i32 %1) {
; CV1-LABEL: shlhps_r:
; CV1:       # %bb.0:
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    neghq $r1 = $r1
; CV1-NEXT:    andw $r2 = $r1, 0xf000f
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andw $r1 = $r1, 0xf000f
; CV1-NEXT:    extfz $r2 = $r2, 19, 16
; CV1-NEXT:    sllhqs $r3 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sllhqs $r2 = $r0, $r2
; CV1-NEXT:    extfz $r4 = $r1, 19, 16
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srlhqs $r0 = $r0, $r4
; CV1-NEXT:    srlhqs $r1 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    insf $r0 = $r1, 15, 0
; CV1-NEXT:    insf $r2 = $r3, 15, 0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    orw $r0 = $r2, $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: shlhps_r:
; CV2:       # %bb.0:
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    neghq $r1 = $r1
; CV2-NEXT:    andw $r2 = $r1, 0xf000f
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    andw $r1 = $r1, 0xf000f
; CV2-NEXT:    extfz $r2 = $r2, 19, 16
; CV2-NEXT:    sllhqs $r3 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    srlhqs $r1 = $r0, $r1
; CV2-NEXT:    sllhqs $r2 = $r0, $r2
; CV2-NEXT:    extfz $r4 = $r1, 19, 16
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlhqs $r0 = $r0, $r4
; CV2-NEXT:    insf $r2 = $r3, 15, 0
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r0 = $r1, 15, 0
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    orw $r0 = $r2, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 6)
  %3 = tail call <2 x i16> @llvm.kvx.shl.v2i16(<2 x i16> %0, i32 %1, i32 3)
  ret <2 x i16> %3
}

define <4 x i16> @shlhqs(<4 x i16> %0, i32 %1) {
; ALL-LABEL: shlhqs:
; ALL:       # %bb.0:
; ALL-NEXT:    sllhqs $r0 = $r0, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %3 = tail call <4 x i16> @llvm.kvx.shl.v4i16(<4 x i16> %0, i32 %1, i32 0)
  ret <4 x i16> %3
}

define <4 x i16> @shlhqs_s(<4 x i16> %0, i32 %1) {
; ALL-LABEL: shlhqs_s:
; ALL:       # %bb.0:
; ALL-NEXT:    slshqs $r0 = $r0, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %3 = tail call <4 x i16> @llvm.kvx.shl.v4i16(<4 x i16> %0, i32 %1, i32 1)
  ret <4 x i16> %3
}

define <4 x i16> @shlhqs_us(<4 x i16> %0, i32 %1) {
; CV1-LABEL: shlhqs_us:
; CV1:       # %bb.0:
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    srld $r2 = $r0, 48
; CV1-NEXT:    extfz $r3 = $r0, 47, 32
; CV1-NEXT:    srlw $r4 = $r0, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sllw $r0 = $r0, 16
; CV1-NEXT:    sllw $r2 = $r2, 16
; CV1-NEXT:    sllw $r3 = $r3, 16
; CV1-NEXT:    sllw $r4 = $r4, 16
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    zxhd $r1 = $r1
; CV1-NEXT:    zxwd $r2 = $r2
; CV1-NEXT:    zxwd $r3 = $r3
; CV1-NEXT:    zxwd $r4 = $r4
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    slld $r2 = $r2, $r1
; CV1-NEXT:    slld $r3 = $r3, $r1
; CV1-NEXT:    slld $r4 = $r4, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    slld $r0 = $r0, $r1
; CV1-NEXT:    minud $r1 = $r2, 0xffffffff
; CV1-NEXT:    minud $r2 = $r3, 0xffffffff
; CV1-NEXT:    minud $r3 = $r4, 0xffffffff
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    minud $r0 = $r0, 0xffffffff
; CV1-NEXT:    srlw $r1 = $r1, 16
; CV1-NEXT:    srlw $r2 = $r2, 16
; CV1-NEXT:    srlw $r3 = $r3, 16
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r0 = $r0, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r0 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r0 = $r2, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 8)
;
; CV2-LABEL: shlhqs_us:
; CV2:       # %bb.0:
; CV2-NEXT:    slushqs $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %3 = tail call <4 x i16> @llvm.kvx.shl.v4i16(<4 x i16> %0, i32 %1, i32 2)
  ret <4 x i16> %3
}

define <4 x i16> @shlhqs_r(<4 x i16> %0, i32 %1) {
; CV1-LABEL: shlhqs_r:
; CV1:       # %bb.0:
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x2010201.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    neghq $r1 = $r1
; CV1-NEXT:    andd $r2 = $r1, 0xf000f.@
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andd $r1 = $r1, 0xf000f.@
; CV1-NEXT:    sllhqs $r3 = $r0, $r2
; CV1-NEXT:    extfz $r4 = $r2, 19, 16
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sllhqs $r4 = $r0, $r4
; CV1-NEXT:    extfz $r5 = $r2, 35, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sllhqs $r3 = $r0, $r5
; CV1-NEXT:    insf $r4 = $r3, 15, 0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    extfz $r5 = $r1, 19, 16
; CV1-NEXT:    srlhqs $r6 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlhqs $r5 = $r0, $r5
; CV1-NEXT:    extfz $r7 = $r1, 35, 32
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r5 = $r6, 15, 0
; CV1-NEXT:    srlhqs $r6 = $r0, $r7
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    extfz $r1 = $r1, 51, 48
; CV1-NEXT:    extfz $r2 = $r2, 51, 48
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r3 = $r4, 31, 0
; CV1-NEXT:    insf $r6 = $r5, 31, 0
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    srlhqs $r0 = $r0, $r1
; CV1-NEXT:    sllhqs $r2 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    insf $r0 = $r6, 47, 0
; CV1-NEXT:    insf $r2 = $r3, 47, 0
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    ord $r0 = $r2, $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 12)
;
; CV2-LABEL: shlhqs_r:
; CV2:       # %bb.0:
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x2010201.@
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    neghq $r1 = $r1
; CV2-NEXT:    andd $r2 = $r1, 0xf000f.@
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    andd $r1 = $r1, 0xf000f.@
; CV2-NEXT:    sllhqs $r3 = $r0, $r2
; CV2-NEXT:    extfz $r4 = $r2, 19, 16
; CV2-NEXT:    extfz $r5 = $r2, 35, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sllhqs $r4 = $r0, $r4
; CV2-NEXT:    sllhqs $r5 = $r0, $r5
; CV2-NEXT:    srlhqs $r6 = $r0, $r1
; CV2-NEXT:    extfz $r7 = $r1, 19, 16
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    extfz $r2 = $r2, 51, 48
; CV2-NEXT:    srlhqs $r3 = $r0, $r7
; CV2-NEXT:    insf $r4 = $r3, 15, 0
; CV2-NEXT:    extfz $r7 = $r1, 35, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    extfz $r1 = $r1, 51, 48
; CV2-NEXT:    insf $r3 = $r6, 15, 0
; CV2-NEXT:    insf $r5 = $r4, 31, 0
; CV2-NEXT:    srlhqs $r6 = $r0, $r7
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    srlhqs $r0 = $r0, $r1
; CV2-NEXT:    sllhqs $r2 = $r0, $r2
; CV2-NEXT:    insf $r6 = $r3, 31, 0
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r0 = $r6, 47, 0
; CV2-NEXT:    insf $r2 = $r5, 47, 0
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    ord $r0 = $r2, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
  %3 = tail call <4 x i16> @llvm.kvx.shl.v4i16(<4 x i16> %0, i32 %1, i32 3)
  ret <4 x i16> %3
}

define <16 x i16> @shlhxs(<16 x i16> %0, i32 %1) {
; CV1-LABEL: shlhxs:
; CV1:       # %bb.0:
; CV1-NEXT:    sllhqs $r0 = $r0, $r4
; CV1-NEXT:    sllhqs $r1 = $r1, $r4
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sllhqs $r2 = $r2, $r4
; CV1-NEXT:    sllhqs $r3 = $r3, $r4
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 1)
;
; CV2-LABEL: shlhxs:
; CV2:       # %bb.0:
; CV2-NEXT:    sllhqs $r0 = $r0, $r4
; CV2-NEXT:    sllhqs $r1 = $r1, $r4
; CV2-NEXT:    sllhqs $r2 = $r2, $r4
; CV2-NEXT:    sllhqs $r3 = $r3, $r4
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %3 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4 = tail call <4 x i16> @llvm.kvx.shl.v4i16(<4 x i16> %3, i32 %1, i32 0)
  %5 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6 = tail call <4 x i16> @llvm.kvx.shl.v4i16(<4 x i16> %5, i32 %1, i32 0)
  %7 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8 = tail call <4 x i16> @llvm.kvx.shl.v4i16(<4 x i16> %7, i32 %1, i32 0)
  %9 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %10 = tail call <4 x i16> @llvm.kvx.shl.v4i16(<4 x i16> %9, i32 %1, i32 0)
  %11 = shufflevector <4 x i16> %4, <4 x i16> %6, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %12 = shufflevector <4 x i16> %8, <4 x i16> %10, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %13 = shufflevector <8 x i16> %11, <8 x i16> %12, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  ret <16 x i16> %13
}

define <16 x i16> @shlhxs_s(<16 x i16> %0, i32 %1) {
; ALL-LABEL: shlhxs_s:
; ALL:       # %bb.0:
; ALL-NEXT:    slshqs $r0 = $r0, $r4
; ALL-NEXT:    slshqs $r1 = $r1, $r4
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    slshqs $r2 = $r2, $r4
; ALL-NEXT:    slshqs $r3 = $r3, $r4
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 1)
  %3 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4 = tail call <4 x i16> @llvm.kvx.shl.v4i16(<4 x i16> %3, i32 %1, i32 1)
  %5 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6 = tail call <4 x i16> @llvm.kvx.shl.v4i16(<4 x i16> %5, i32 %1, i32 1)
  %7 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8 = tail call <4 x i16> @llvm.kvx.shl.v4i16(<4 x i16> %7, i32 %1, i32 1)
  %9 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %10 = tail call <4 x i16> @llvm.kvx.shl.v4i16(<4 x i16> %9, i32 %1, i32 1)
  %11 = shufflevector <4 x i16> %4, <4 x i16> %6, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %12 = shufflevector <4 x i16> %8, <4 x i16> %10, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %13 = shufflevector <8 x i16> %11, <8 x i16> %12, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  ret <16 x i16> %13
}

define <16 x i16> @shlhxs_us(<16 x i16> %0, i32 %1) {
; CV1-LABEL: shlhxs_us:
; CV1:       # %bb.0:
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    srld $r5 = $r0, 48
; CV1-NEXT:    extfz $r6 = $r0, 47, 32
; CV1-NEXT:    srlw $r7 = $r0, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    zxhd $r4 = $r4
; CV1-NEXT:    sllw $r5 = $r5, 16
; CV1-NEXT:    sllw $r6 = $r6, 16
; CV1-NEXT:    sllw $r7 = $r7, 16
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sllw $r0 = $r0, 16
; CV1-NEXT:    zxwd $r5 = $r5
; CV1-NEXT:    zxwd $r6 = $r6
; CV1-NEXT:    zxwd $r7 = $r7
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    slld $r5 = $r5, $r4
; CV1-NEXT:    slld $r6 = $r6, $r4
; CV1-NEXT:    slld $r7 = $r7, $r4
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    slld $r0 = $r0, $r4
; CV1-NEXT:    minud $r5 = $r5, 0xffffffff
; CV1-NEXT:    minud $r6 = $r6, 0xffffffff
; CV1-NEXT:    minud $r7 = $r7, 0xffffffff
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    minud $r0 = $r0, 0xffffffff
; CV1-NEXT:    srlw $r5 = $r5, 16
; CV1-NEXT:    srlw $r6 = $r6, 16
; CV1-NEXT:    srlw $r8 = $r1, 16
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r0 = $r0, 16
; CV1-NEXT:    srlw $r5 = $r7, 16
; CV1-NEXT:    insf $r6 = $r5, 31, 16
; CV1-NEXT:    srld $r7 = $r1, 48
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r0 = $r5, 31, 16
; CV1-NEXT:    sllw $r5 = $r7, 16
; CV1-NEXT:    extfz $r7 = $r1, 47, 32
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r0 = $r6, 63, 32
; CV1-NEXT:    zxhd $r1 = $r1
; CV1-NEXT:    sllw $r6 = $r7, 16
; CV1-NEXT:    sllw $r7 = $r8, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    sllw $r1 = $r1, 16
; CV1-NEXT:    zxwd $r5 = $r5
; CV1-NEXT:    zxwd $r6 = $r6
; CV1-NEXT:    zxwd $r7 = $r7
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    zxwd $r1 = $r1
; CV1-NEXT:    slld $r5 = $r5, $r4
; CV1-NEXT:    slld $r6 = $r6, $r4
; CV1-NEXT:    slld $r7 = $r7, $r4
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    slld $r1 = $r1, $r4
; CV1-NEXT:    minud $r5 = $r5, 0xffffffff
; CV1-NEXT:    minud $r6 = $r6, 0xffffffff
; CV1-NEXT:    minud $r7 = $r7, 0xffffffff
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    minud $r1 = $r1, 0xffffffff
; CV1-NEXT:    srlw $r5 = $r5, 16
; CV1-NEXT:    srlw $r6 = $r6, 16
; CV1-NEXT:    srlw $r7 = $r7, 16
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    srlw $r1 = $r1, 16
; CV1-NEXT:    extfz $r5 = $r2, 47, 32
; CV1-NEXT:    insf $r6 = $r5, 31, 16
; CV1-NEXT:    srld $r8 = $r2, 48
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    insf $r1 = $r7, 31, 16
; CV1-NEXT:    sllw $r5 = $r5, 16
; CV1-NEXT:    sllw $r7 = $r8, 16
; CV1-NEXT:    srlw $r8 = $r3, 16
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    insf $r1 = $r6, 63, 32
; CV1-NEXT:    zxhd $r2 = $r2
; CV1-NEXT:    srlw $r6 = $r2, 16
; CV1-NEXT:    zxwd $r7 = $r7
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    sllw $r2 = $r2, 16
; CV1-NEXT:    zxwd $r5 = $r5
; CV1-NEXT:    sllw $r6 = $r6, 16
; CV1-NEXT:    slld $r7 = $r7, $r4
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    zxwd $r2 = $r2
; CV1-NEXT:    slld $r5 = $r5, $r4
; CV1-NEXT:    zxwd $r6 = $r6
; CV1-NEXT:    minud $r7 = $r7, 0xffffffff
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    slld $r2 = $r2, $r4
; CV1-NEXT:    minud $r5 = $r5, 0xffffffff
; CV1-NEXT:    slld $r6 = $r6, $r4
; CV1-NEXT:    srlw $r7 = $r7, 16
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    minud $r2 = $r2, 0xffffffff
; CV1-NEXT:    srlw $r5 = $r5, 16
; CV1-NEXT:    minud $r6 = $r6, 0xffffffff
; CV1-NEXT:    sllw $r8 = $r8, 16
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    srlw $r2 = $r2, 16
; CV1-NEXT:    insf $r5 = $r7, 31, 16
; CV1-NEXT:    srlw $r6 = $r6, 16
; CV1-NEXT:    extfz $r7 = $r3, 47, 32
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    insf $r2 = $r6, 31, 16
; CV1-NEXT:    zxhd $r3 = $r3
; CV1-NEXT:    srld $r6 = $r3, 48
; CV1-NEXT:    sllw $r7 = $r7, 16
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    sllw $r3 = $r3, 16
; CV1-NEXT:    sllw $r6 = $r6, 16
; CV1-NEXT:    zxwd $r7 = $r7
; CV1-NEXT:    zxwd $r8 = $r8
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    zxwd $r3 = $r3
; CV1-NEXT:    zxwd $r6 = $r6
; CV1-NEXT:    slld $r7 = $r7, $r4
; CV1-NEXT:    slld $r8 = $r8, $r4
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    insf $r2 = $r5, 63, 32
; CV1-NEXT:    slld $r3 = $r3, $r4
; CV1-NEXT:    slld $r6 = $r6, $r4
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    minud $r3 = $r3, 0xffffffff
; CV1-NEXT:    minud $r4 = $r6, 0xffffffff
; CV1-NEXT:    minud $r6 = $r7, 0xffffffff
; CV1-NEXT:    minud $r7 = $r8, 0xffffffff
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    srlw $r3 = $r3, 16
; CV1-NEXT:    srlw $r4 = $r4, 16
; CV1-NEXT:    srlw $r6 = $r6, 16
; CV1-NEXT:    srlw $r7 = $r7, 16
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    insf $r3 = $r7, 31, 16
; CV1-NEXT:    insf $r6 = $r4, 31, 16
; CV1-NEXT:    ;; # (end cycle 27)
; CV1-NEXT:    insf $r3 = $r6, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 28)
;
; CV2-LABEL: shlhxs_us:
; CV2:       # %bb.0:
; CV2-NEXT:    slushqs $r0 = $r0, $r4
; CV2-NEXT:    slushqs $r1 = $r1, $r4
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    slushqs $r2 = $r2, $r4
; CV2-NEXT:    slushqs $r3 = $r3, $r4
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 1)
  %3 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4 = tail call <4 x i16> @llvm.kvx.shl.v4i16(<4 x i16> %3, i32 %1, i32 2)
  %5 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6 = tail call <4 x i16> @llvm.kvx.shl.v4i16(<4 x i16> %5, i32 %1, i32 2)
  %7 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8 = tail call <4 x i16> @llvm.kvx.shl.v4i16(<4 x i16> %7, i32 %1, i32 2)
  %9 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %10 = tail call <4 x i16> @llvm.kvx.shl.v4i16(<4 x i16> %9, i32 %1, i32 2)
  %11 = shufflevector <4 x i16> %4, <4 x i16> %6, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %12 = shufflevector <4 x i16> %8, <4 x i16> %10, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %13 = shufflevector <8 x i16> %11, <8 x i16> %12, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  ret <16 x i16> %13
}

define <16 x i16> @shlhxs_r(<16 x i16> %0, i32 %1) {
; CV1-LABEL: shlhxs_r:
; CV1:       # %bb.0:
; CV1-NEXT:    sbmm8 $r4 = $r4, 0x2010201.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    neghq $r4 = $r4
; CV1-NEXT:    andd $r5 = $r4, 0xf000f.@
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andd $r4 = $r4, 0xf000f.@
; CV1-NEXT:    sllhqs $r6 = $r0, $r5
; CV1-NEXT:    extfz $r7 = $r5, 19, 16
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sllhqs $r8 = $r0, $r7
; CV1-NEXT:    extfz $r9 = $r5, 35, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sllhqs $r6 = $r0, $r9
; CV1-NEXT:    insf $r8 = $r6, 15, 0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    insf $r6 = $r8, 31, 0
; CV1-NEXT:    extfz $r8 = $r5, 51, 48
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sllhqs $r10 = $r0, $r8
; CV1-NEXT:    srlhqs $r11 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    extfz $r6 = $r4, 19, 16
; CV1-NEXT:    insf $r10 = $r6, 47, 0
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    srlhqs $r15 = $r0, $r6
; CV1-NEXT:    extfz $r16 = $r4, 35, 32
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    srlhqs $r11 = $r0, $r16
; CV1-NEXT:    insf $r15 = $r11, 15, 0
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r11 = $r15, 31, 0
; CV1-NEXT:    extfz $r15 = $r4, 51, 48
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    srlhqs $r0 = $r0, $r15
; CV1-NEXT:    sllhqs $r17 = $r1, $r5
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    insf $r0 = $r11, 47, 0
; CV1-NEXT:    sllhqs $r11 = $r1, $r7
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    ord $r0 = $r10, $r0
; CV1-NEXT:    insf $r11 = $r17, 15, 0
; CV1-NEXT:    sllhqs $r17 = $r1, $r9
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    sllhqs $r11 = $r1, $r8
; CV1-NEXT:    insf $r17 = $r11, 31, 0
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    insf $r11 = $r17, 47, 0
; CV1-NEXT:    srlhqs $r17 = $r1, $r4
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    srlhqs $r32 = $r1, $r6
; CV1-NEXT:    srlhqs $r33 = $r1, $r16
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    srlhqs $r1 = $r1, $r15
; CV1-NEXT:    insf $r32 = $r17, 15, 0
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    sllhqs $r17 = $r2, $r5
; CV1-NEXT:    insf $r33 = $r32, 31, 0
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    insf $r1 = $r33, 47, 0
; CV1-NEXT:    sllhqs $r32 = $r2, $r7
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    ord $r1 = $r11, $r1
; CV1-NEXT:    srlhqs $r33 = $r2, $r4
; CV1-NEXT:    srlhqs $r34 = $r2, $r6
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    sllhqs $r5 = $r3, $r5
; CV1-NEXT:    sllhqs $r7 = $r3, $r7
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    srlhqs $r4 = $r3, $r4
; CV1-NEXT:    srlhqs $r6 = $r3, $r6
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    sllhqs $r17 = $r2, $r9
; CV1-NEXT:    insf $r32 = $r17, 15, 0
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    srlhqs $r33 = $r2, $r16
; CV1-NEXT:    insf $r34 = $r33, 15, 0
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    sllhqs $r5 = $r3, $r9
; CV1-NEXT:    insf $r7 = $r5, 15, 0
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    srlhqs $r4 = $r3, $r16
; CV1-NEXT:    insf $r6 = $r4, 15, 0
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    insf $r17 = $r32, 31, 0
; CV1-NEXT:    insf $r33 = $r34, 31, 0
; CV1-NEXT:    ;; # (end cycle 27)
; CV1-NEXT:    srlhqs $r2 = $r2, $r15
; CV1-NEXT:    sllhqs $r32 = $r2, $r8
; CV1-NEXT:    ;; # (end cycle 28)
; CV1-NEXT:    insf $r5 = $r7, 31, 0
; CV1-NEXT:    sllhqs $r7 = $r3, $r8
; CV1-NEXT:    ;; # (end cycle 29)
; CV1-NEXT:    srlhqs $r3 = $r3, $r15
; CV1-NEXT:    insf $r4 = $r6, 31, 0
; CV1-NEXT:    ;; # (end cycle 30)
; CV1-NEXT:    insf $r2 = $r33, 47, 0
; CV1-NEXT:    insf $r32 = $r17, 47, 0
; CV1-NEXT:    ;; # (end cycle 31)
; CV1-NEXT:    ord $r2 = $r32, $r2
; CV1-NEXT:    insf $r3 = $r4, 47, 0
; CV1-NEXT:    insf $r7 = $r5, 47, 0
; CV1-NEXT:    ;; # (end cycle 32)
; CV1-NEXT:    ord $r3 = $r7, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 33)
;
; CV2-LABEL: shlhxs_r:
; CV2:       # %bb.0:
; CV2-NEXT:    sbmm8 $r4 = $r4, 0x2010201.@
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    neghq $r4 = $r4
; CV2-NEXT:    andd $r5 = $r4, 0xf000f.@
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    andd $r4 = $r4, 0xf000f.@
; CV2-NEXT:    sllhqs $r6 = $r0, $r5
; CV2-NEXT:    extfz $r7 = $r5, 19, 16
; CV2-NEXT:    extfz $r8 = $r5, 35, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sllhqs $r10 = $r0, $r7
; CV2-NEXT:    sllhqs $r11 = $r0, $r8
; CV2-NEXT:    extfz $r16 = $r4, 19, 16
; CV2-NEXT:    extfz $r17 = $r4, 35, 32
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlhqs $r6 = $r0, $r4
; CV2-NEXT:    extfz $r9 = $r5, 51, 48
; CV2-NEXT:    insf $r10 = $r6, 15, 0
; CV2-NEXT:    srlhqs $r32 = $r0, $r17
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    srlhqs $r10 = $r0, $r16
; CV2-NEXT:    insf $r11 = $r10, 31, 0
; CV2-NEXT:    sllhqs $r15 = $r0, $r9
; CV2-NEXT:    extfz $r33 = $r4, 51, 48
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    srlhqs $r0 = $r0, $r33
; CV2-NEXT:    sllhqs $r6 = $r1, $r5
; CV2-NEXT:    insf $r10 = $r6, 15, 0
; CV2-NEXT:    insf $r15 = $r11, 47, 0
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    sllhqs $r10 = $r1, $r7
; CV2-NEXT:    sllhqs $r11 = $r1, $r8
; CV2-NEXT:    insf $r32 = $r10, 31, 0
; CV2-NEXT:    sllhqs $r34 = $r1, $r9
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r0 = $r32, 47, 0
; CV2-NEXT:    srlhqs $r6 = $r1, $r4
; CV2-NEXT:    insf $r10 = $r6, 15, 0
; CV2-NEXT:    srlhqs $r32 = $r1, $r16
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    srlhqs $r1 = $r1, $r33
; CV2-NEXT:    srlhqs $r6 = $r1, $r17
; CV2-NEXT:    insf $r11 = $r10, 31, 0
; CV2-NEXT:    insf $r32 = $r6, 15, 0
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r6 = $r32, 31, 0
; CV2-NEXT:    sllhqs $r10 = $r2, $r7
; CV2-NEXT:    srlhqs $r11 = $r2, $r4
; CV2-NEXT:    insf $r34 = $r11, 47, 0
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    insf $r1 = $r6, 47, 0
; CV2-NEXT:    sllhqs $r5 = $r3, $r5
; CV2-NEXT:    sllhqs $r6 = $r2, $r5
; CV2-NEXT:    srlhqs $r32 = $r2, $r16
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    srlhqs $r4 = $r3, $r4
; CV2-NEXT:    sllhqs $r7 = $r3, $r7
; CV2-NEXT:    insf $r10 = $r6, 15, 0
; CV2-NEXT:    srlhqs $r16 = $r3, $r16
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    sllhqs $r6 = $r2, $r8
; CV2-NEXT:    insf $r7 = $r5, 15, 0
; CV2-NEXT:    srlhqs $r11 = $r2, $r17
; CV2-NEXT:    insf $r32 = $r11, 15, 0
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    sllhqs $r5 = $r3, $r8
; CV2-NEXT:    insf $r6 = $r10, 31, 0
; CV2-NEXT:    srlhqs $r8 = $r3, $r17
; CV2-NEXT:    insf $r16 = $r4, 15, 0
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    srlhqs $r2 = $r2, $r33
; CV2-NEXT:    sllhqs $r4 = $r2, $r9
; CV2-NEXT:    insf $r5 = $r7, 31, 0
; CV2-NEXT:    insf $r11 = $r32, 31, 0
; CV2-NEXT:    ;; # (end cycle 15)
; CV2-NEXT:    srlhqs $r3 = $r3, $r33
; CV2-NEXT:    insf $r4 = $r6, 47, 0
; CV2-NEXT:    sllhqs $r7 = $r3, $r9
; CV2-NEXT:    insf $r8 = $r16, 31, 0
; CV2-NEXT:    ;; # (end cycle 16)
; CV2-NEXT:    ord $r0 = $r15, $r0
; CV2-NEXT:    insf $r2 = $r11, 47, 0
; CV2-NEXT:    insf $r3 = $r8, 47, 0
; CV2-NEXT:    insf $r7 = $r5, 47, 0
; CV2-NEXT:    ;; # (end cycle 17)
; CV2-NEXT:    ord $r1 = $r34, $r1
; CV2-NEXT:    ord $r2 = $r4, $r2
; CV2-NEXT:    ord $r3 = $r7, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 18)
  %3 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4 = tail call <4 x i16> @llvm.kvx.shl.v4i16(<4 x i16> %3, i32 %1, i32 3)
  %5 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6 = tail call <4 x i16> @llvm.kvx.shl.v4i16(<4 x i16> %5, i32 %1, i32 3)
  %7 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %8 = tail call <4 x i16> @llvm.kvx.shl.v4i16(<4 x i16> %7, i32 %1, i32 3)
  %9 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %10 = tail call <4 x i16> @llvm.kvx.shl.v4i16(<4 x i16> %9, i32 %1, i32 3)
  %11 = shufflevector <4 x i16> %4, <4 x i16> %6, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %12 = shufflevector <4 x i16> %8, <4 x i16> %10, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %13 = shufflevector <8 x i16> %11, <8 x i16> %12, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  ret <16 x i16> %13
}

define i32 @shlw(i32 %0, i32 %1) {
; ALL-LABEL: shlw:
; ALL:       # %bb.0:
; ALL-NEXT:    sllw $r0 = $r0, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %3 = tail call i32 @llvm.kvx.shl.i32(i32 %0, i32 %1, i32 0)
  ret i32 %3
}

declare i32 @llvm.kvx.shl.i32(i32, i32, i32)

define i32 @shlw_s(i32 %0, i32 %1) {
; ALL-LABEL: shlw_s:
; ALL:       # %bb.0:
; ALL-NEXT:    slsw $r0 = $r0, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %3 = tail call i32 @llvm.kvx.shl.i32(i32 %0, i32 %1, i32 1)
  ret i32 %3
}

define i32 @shlw_us(i32 %0, i32 %1) {
; CV1-LABEL: shlw_us:
; CV1:       # %bb.0:
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    slld $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    minud $r0 = $r0, 0xffffffff
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 2)
;
; CV2-LABEL: shlw_us:
; CV2:       # %bb.0:
; CV2-NEXT:    slusw $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %3 = tail call i32 @llvm.kvx.shl.i32(i32 %0, i32 %1, i32 2)
  ret i32 %3
}

define i32 @shlw_r(i32 %0, i32 %1) {
; ALL-LABEL: shlw_r:
; ALL:       # %bb.0:
; ALL-NEXT:    rolw $r0 = $r0, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %3 = tail call i32 @llvm.kvx.shl.i32(i32 %0, i32 %1, i32 3)
  ret i32 %3
}

define <8 x i32> @shlwo(<8 x i32> %0, <8 x i32> %1) {
; CV1-LABEL: shlwo:
; CV1:       # %bb.0:
; CV1-NEXT:    srad $r0 = $r0, 32
; CV1-NEXT:    sllw $r4 = $r1, $r5
; CV1-NEXT:    sllw $r8 = $r0, $r4
; CV1-NEXT:    srad $r9 = $r4, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sllw $r0 = $r0, $r9
; CV1-NEXT:    srad $r1 = $r1, 32
; CV1-NEXT:    sllw $r5 = $r2, $r6
; CV1-NEXT:    srad $r9 = $r5, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sllw $r1 = $r1, $r9
; CV1-NEXT:    srad $r2 = $r2, 32
; CV1-NEXT:    srad $r6 = $r6, 32
; CV1-NEXT:    srad $r9 = $r7, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sllw $r2 = $r2, $r6
; CV1-NEXT:    sllw $r3 = $r3, $r7
; CV1-NEXT:    srad $r6 = $r3, 32
; CV1-NEXT:    insf $r8 = $r0, 63, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    copyd $r0 = $r8
; CV1-NEXT:    insf $r4 = $r1, 63, 32
; CV1-NEXT:    insf $r5 = $r2, 63, 32
; CV1-NEXT:    sllw $r6 = $r6, $r9
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    copyd $r1 = $r4
; CV1-NEXT:    copyd $r2 = $r5
; CV1-NEXT:    insf $r3 = $r6, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 5)
;
; CV2-LABEL: shlwo:
; CV2:       # %bb.0:
; CV2-NEXT:    srad $r0 = $r0, 32
; CV2-NEXT:    sllw $r4 = $r1, $r5
; CV2-NEXT:    sllw $r8 = $r0, $r4
; CV2-NEXT:    srad $r9 = $r4, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sllw $r0 = $r0, $r9
; CV2-NEXT:    srad $r1 = $r1, 32
; CV2-NEXT:    srad $r5 = $r5, 32
; CV2-NEXT:    srad $r9 = $r2, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sllw $r1 = $r1, $r5
; CV2-NEXT:    srad $r5 = $r6, 32
; CV2-NEXT:    srad $r10 = $r3, 32
; CV2-NEXT:    srad $r11 = $r7, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sllw $r2 = $r2, $r6
; CV2-NEXT:    sllw $r3 = $r3, $r7
; CV2-NEXT:    sllw $r5 = $r9, $r5
; CV2-NEXT:    sllw $r6 = $r10, $r11
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    insf $r2 = $r5, 63, 32
; CV2-NEXT:    insf $r3 = $r6, 63, 32
; CV2-NEXT:    insf $r4 = $r1, 63, 32
; CV2-NEXT:    insf $r8 = $r0, 63, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    copyd $r0 = $r8
; CV2-NEXT:    copyd $r1 = $r4
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 5)
  %3 = extractelement <8 x i32> %0, i64 0
  %4 = extractelement <8 x i32> %1, i64 0
  %5 = tail call i32 @llvm.kvx.shl.i32(i32 %3, i32 %4, i32 0)
  %6 = extractelement <8 x i32> %0, i64 1
  %7 = extractelement <8 x i32> %1, i64 1
  %8 = tail call i32 @llvm.kvx.shl.i32(i32 %6, i32 %7, i32 0)
  %9 = extractelement <8 x i32> %0, i64 2
  %10 = extractelement <8 x i32> %1, i64 2
  %11 = tail call i32 @llvm.kvx.shl.i32(i32 %9, i32 %10, i32 0)
  %12 = extractelement <8 x i32> %0, i64 3
  %13 = extractelement <8 x i32> %1, i64 3
  %14 = tail call i32 @llvm.kvx.shl.i32(i32 %12, i32 %13, i32 0)
  %15 = extractelement <8 x i32> %0, i64 4
  %16 = extractelement <8 x i32> %1, i64 4
  %17 = tail call i32 @llvm.kvx.shl.i32(i32 %15, i32 %16, i32 0)
  %18 = extractelement <8 x i32> %0, i64 5
  %19 = extractelement <8 x i32> %1, i64 5
  %20 = tail call i32 @llvm.kvx.shl.i32(i32 %18, i32 %19, i32 0)
  %21 = extractelement <8 x i32> %0, i64 6
  %22 = extractelement <8 x i32> %1, i64 6
  %23 = tail call i32 @llvm.kvx.shl.i32(i32 %21, i32 %22, i32 0)
  %24 = extractelement <8 x i32> %0, i64 7
  %25 = extractelement <8 x i32> %1, i64 7
  %26 = tail call i32 @llvm.kvx.shl.i32(i32 %24, i32 %25, i32 0)
  %27 = insertelement <8 x i32> undef, i32 %5, i32 0
  %28 = insertelement <8 x i32> %27, i32 %8, i32 1
  %29 = insertelement <8 x i32> %28, i32 %11, i32 2
  %30 = insertelement <8 x i32> %29, i32 %14, i32 3
  %31 = insertelement <8 x i32> %30, i32 %17, i32 4
  %32 = insertelement <8 x i32> %31, i32 %20, i32 5
  %33 = insertelement <8 x i32> %32, i32 %23, i32 6
  %34 = insertelement <8 x i32> %33, i32 %26, i32 7
  ret <8 x i32> %34
}

define <8 x i32> @shlwo_s(<8 x i32> %0, <8 x i32> %1) {
; CV1-LABEL: shlwo_s:
; CV1:       # %bb.0:
; CV1-NEXT:    srad $r0 = $r0, 32
; CV1-NEXT:    slsw $r4 = $r1, $r5
; CV1-NEXT:    slsw $r8 = $r0, $r4
; CV1-NEXT:    srad $r9 = $r4, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    slsw $r0 = $r0, $r9
; CV1-NEXT:    srad $r1 = $r1, 32
; CV1-NEXT:    srad $r5 = $r5, 32
; CV1-NEXT:    slsw $r9 = $r2, $r6
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    slsw $r1 = $r1, $r5
; CV1-NEXT:    srad $r2 = $r2, 32
; CV1-NEXT:    srad $r5 = $r3, 32
; CV1-NEXT:    srad $r6 = $r6, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    slsw $r2 = $r2, $r6
; CV1-NEXT:    slsw $r3 = $r3, $r7
; CV1-NEXT:    srad $r6 = $r7, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    slsw $r5 = $r5, $r6
; CV1-NEXT:    insf $r8 = $r0, 63, 32
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    copyd $r0 = $r8
; CV1-NEXT:    insf $r4 = $r1, 63, 32
; CV1-NEXT:    insf $r9 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    copyd $r1 = $r4
; CV1-NEXT:    copyd $r2 = $r9
; CV1-NEXT:    insf $r3 = $r5, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: shlwo_s:
; CV2:       # %bb.0:
; CV2-NEXT:    srad $r0 = $r0, 32
; CV2-NEXT:    slsw $r4 = $r1, $r5
; CV2-NEXT:    slsw $r8 = $r0, $r4
; CV2-NEXT:    srad $r9 = $r4, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    slsw $r0 = $r0, $r9
; CV2-NEXT:    srad $r1 = $r1, 32
; CV2-NEXT:    slsw $r5 = $r2, $r6
; CV2-NEXT:    srad $r9 = $r5, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    slsw $r1 = $r1, $r9
; CV2-NEXT:    srad $r2 = $r2, 32
; CV2-NEXT:    srad $r6 = $r6, 32
; CV2-NEXT:    srad $r9 = $r7, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    slsw $r2 = $r2, $r6
; CV2-NEXT:    slsw $r3 = $r3, $r7
; CV2-NEXT:    srad $r6 = $r3, 32
; CV2-NEXT:    insf $r8 = $r0, 63, 32
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    copyd $r0 = $r8
; CV2-NEXT:    insf $r4 = $r1, 63, 32
; CV2-NEXT:    insf $r5 = $r2, 63, 32
; CV2-NEXT:    slsw $r6 = $r6, $r9
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    copyd $r1 = $r4
; CV2-NEXT:    copyd $r2 = $r5
; CV2-NEXT:    insf $r3 = $r6, 63, 32
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 5)
  %3 = extractelement <8 x i32> %0, i64 0
  %4 = extractelement <8 x i32> %1, i64 0
  %5 = tail call i32 @llvm.kvx.shl.i32(i32 %3, i32 %4, i32 1)
  %6 = extractelement <8 x i32> %0, i64 1
  %7 = extractelement <8 x i32> %1, i64 1
  %8 = tail call i32 @llvm.kvx.shl.i32(i32 %6, i32 %7, i32 1)
  %9 = extractelement <8 x i32> %0, i64 2
  %10 = extractelement <8 x i32> %1, i64 2
  %11 = tail call i32 @llvm.kvx.shl.i32(i32 %9, i32 %10, i32 1)
  %12 = extractelement <8 x i32> %0, i64 3
  %13 = extractelement <8 x i32> %1, i64 3
  %14 = tail call i32 @llvm.kvx.shl.i32(i32 %12, i32 %13, i32 1)
  %15 = extractelement <8 x i32> %0, i64 4
  %16 = extractelement <8 x i32> %1, i64 4
  %17 = tail call i32 @llvm.kvx.shl.i32(i32 %15, i32 %16, i32 1)
  %18 = extractelement <8 x i32> %0, i64 5
  %19 = extractelement <8 x i32> %1, i64 5
  %20 = tail call i32 @llvm.kvx.shl.i32(i32 %18, i32 %19, i32 1)
  %21 = extractelement <8 x i32> %0, i64 6
  %22 = extractelement <8 x i32> %1, i64 6
  %23 = tail call i32 @llvm.kvx.shl.i32(i32 %21, i32 %22, i32 1)
  %24 = extractelement <8 x i32> %0, i64 7
  %25 = extractelement <8 x i32> %1, i64 7
  %26 = tail call i32 @llvm.kvx.shl.i32(i32 %24, i32 %25, i32 1)
  %27 = insertelement <8 x i32> undef, i32 %5, i32 0
  %28 = insertelement <8 x i32> %27, i32 %8, i32 1
  %29 = insertelement <8 x i32> %28, i32 %11, i32 2
  %30 = insertelement <8 x i32> %29, i32 %14, i32 3
  %31 = insertelement <8 x i32> %30, i32 %17, i32 4
  %32 = insertelement <8 x i32> %31, i32 %20, i32 5
  %33 = insertelement <8 x i32> %32, i32 %23, i32 6
  %34 = insertelement <8 x i32> %33, i32 %26, i32 7
  ret <8 x i32> %34
}

define <8 x i32> @shlwo_us(<8 x i32> %0, <8 x i32> %1) {
; CV1-LABEL: shlwo_us:
; CV1:       # %bb.0:
; CV1-NEXT:    srad $r0 = $r0, 32
; CV1-NEXT:    zxwd $r8 = $r0
; CV1-NEXT:    srad $r9 = $r4, 32
; CV1-NEXT:    zxwd $r10 = $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    slld $r4 = $r8, $r4
; CV1-NEXT:    zxwd $r8 = $r0
; CV1-NEXT:    slld $r10 = $r10, $r5
; CV1-NEXT:    srad $r11 = $r1, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    minud $r0 = $r4, 0xffffffff
; CV1-NEXT:    slld $r4 = $r8, $r9
; CV1-NEXT:    srad $r5 = $r5, 32
; CV1-NEXT:    zxwd $r8 = $r11
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    minud $r1 = $r10, 0xffffffff
; CV1-NEXT:    srad $r2 = $r2, 32
; CV1-NEXT:    slld $r5 = $r8, $r5
; CV1-NEXT:    zxwd $r9 = $r2
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srad $r3 = $r3, 32
; CV1-NEXT:    srad $r6 = $r6, 32
; CV1-NEXT:    slld $r8 = $r9, $r6
; CV1-NEXT:    zxwd $r9 = $r3
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    zxwd $r2 = $r2
; CV1-NEXT:    zxwd $r3 = $r3
; CV1-NEXT:    srad $r7 = $r7, 32
; CV1-NEXT:    slld $r9 = $r9, $r7
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    minud $r4 = $r4, 0xffffffff
; CV1-NEXT:    minud $r5 = $r5, 0xffffffff
; CV1-NEXT:    slld $r6 = $r2, $r6
; CV1-NEXT:    slld $r7 = $r3, $r7
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    minud $r2 = $r8, 0xffffffff
; CV1-NEXT:    minud $r3 = $r9, 0xffffffff
; CV1-NEXT:    minud $r6 = $r6, 0xffffffff
; CV1-NEXT:    minud $r7 = $r7, 0xffffffff
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r0 = $r4, 63, 32
; CV1-NEXT:    insf $r1 = $r5, 63, 32
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r2 = $r6, 63, 32
; CV1-NEXT:    insf $r3 = $r7, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 9)
;
; CV2-LABEL: shlwo_us:
; CV2:       # %bb.0:
; CV2-NEXT:    srad $r0 = $r0, 32
; CV2-NEXT:    slusw $r4 = $r1, $r5
; CV2-NEXT:    slusw $r8 = $r0, $r4
; CV2-NEXT:    srad $r9 = $r4, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    slusw $r0 = $r0, $r9
; CV2-NEXT:    srad $r1 = $r1, 32
; CV2-NEXT:    slusw $r5 = $r2, $r6
; CV2-NEXT:    srad $r9 = $r5, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    slusw $r1 = $r1, $r9
; CV2-NEXT:    srad $r2 = $r2, 32
; CV2-NEXT:    srad $r6 = $r6, 32
; CV2-NEXT:    srad $r9 = $r7, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    slusw $r2 = $r2, $r6
; CV2-NEXT:    slusw $r3 = $r3, $r7
; CV2-NEXT:    srad $r6 = $r3, 32
; CV2-NEXT:    insf $r8 = $r0, 63, 32
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    copyd $r0 = $r8
; CV2-NEXT:    insf $r4 = $r1, 63, 32
; CV2-NEXT:    insf $r5 = $r2, 63, 32
; CV2-NEXT:    slusw $r6 = $r6, $r9
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    copyd $r1 = $r4
; CV2-NEXT:    copyd $r2 = $r5
; CV2-NEXT:    insf $r3 = $r6, 63, 32
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 5)
  %3 = extractelement <8 x i32> %0, i64 0
  %4 = extractelement <8 x i32> %1, i64 0
  %5 = tail call i32 @llvm.kvx.shl.i32(i32 %3, i32 %4, i32 2)
  %6 = extractelement <8 x i32> %0, i64 1
  %7 = extractelement <8 x i32> %1, i64 1
  %8 = tail call i32 @llvm.kvx.shl.i32(i32 %6, i32 %7, i32 2)
  %9 = extractelement <8 x i32> %0, i64 2
  %10 = extractelement <8 x i32> %1, i64 2
  %11 = tail call i32 @llvm.kvx.shl.i32(i32 %9, i32 %10, i32 2)
  %12 = extractelement <8 x i32> %0, i64 3
  %13 = extractelement <8 x i32> %1, i64 3
  %14 = tail call i32 @llvm.kvx.shl.i32(i32 %12, i32 %13, i32 2)
  %15 = extractelement <8 x i32> %0, i64 4
  %16 = extractelement <8 x i32> %1, i64 4
  %17 = tail call i32 @llvm.kvx.shl.i32(i32 %15, i32 %16, i32 2)
  %18 = extractelement <8 x i32> %0, i64 5
  %19 = extractelement <8 x i32> %1, i64 5
  %20 = tail call i32 @llvm.kvx.shl.i32(i32 %18, i32 %19, i32 2)
  %21 = extractelement <8 x i32> %0, i64 6
  %22 = extractelement <8 x i32> %1, i64 6
  %23 = tail call i32 @llvm.kvx.shl.i32(i32 %21, i32 %22, i32 2)
  %24 = extractelement <8 x i32> %0, i64 7
  %25 = extractelement <8 x i32> %1, i64 7
  %26 = tail call i32 @llvm.kvx.shl.i32(i32 %24, i32 %25, i32 2)
  %27 = insertelement <8 x i32> undef, i32 %5, i32 0
  %28 = insertelement <8 x i32> %27, i32 %8, i32 1
  %29 = insertelement <8 x i32> %28, i32 %11, i32 2
  %30 = insertelement <8 x i32> %29, i32 %14, i32 3
  %31 = insertelement <8 x i32> %30, i32 %17, i32 4
  %32 = insertelement <8 x i32> %31, i32 %20, i32 5
  %33 = insertelement <8 x i32> %32, i32 %23, i32 6
  %34 = insertelement <8 x i32> %33, i32 %26, i32 7
  ret <8 x i32> %34
}

define <8 x i32> @shlwo_r(<8 x i32> %0, <8 x i32> %1) {
; CV1-LABEL: shlwo_r:
; CV1:       # %bb.0:
; CV1-NEXT:    srad $r0 = $r0, 32
; CV1-NEXT:    rolw $r4 = $r1, $r5
; CV1-NEXT:    rolw $r8 = $r0, $r4
; CV1-NEXT:    srad $r9 = $r4, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    rolw $r0 = $r0, $r9
; CV1-NEXT:    srad $r1 = $r1, 32
; CV1-NEXT:    srad $r5 = $r5, 32
; CV1-NEXT:    rolw $r9 = $r2, $r6
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    rolw $r1 = $r1, $r5
; CV1-NEXT:    srad $r2 = $r2, 32
; CV1-NEXT:    srad $r5 = $r3, 32
; CV1-NEXT:    srad $r6 = $r6, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    rolw $r2 = $r2, $r6
; CV1-NEXT:    rolw $r3 = $r3, $r7
; CV1-NEXT:    srad $r6 = $r7, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    rolw $r5 = $r5, $r6
; CV1-NEXT:    insf $r8 = $r0, 63, 32
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    copyd $r0 = $r8
; CV1-NEXT:    insf $r4 = $r1, 63, 32
; CV1-NEXT:    insf $r9 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    copyd $r1 = $r4
; CV1-NEXT:    copyd $r2 = $r9
; CV1-NEXT:    insf $r3 = $r5, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: shlwo_r:
; CV2:       # %bb.0:
; CV2-NEXT:    srad $r0 = $r0, 32
; CV2-NEXT:    rolw $r4 = $r1, $r5
; CV2-NEXT:    rolw $r8 = $r0, $r4
; CV2-NEXT:    srad $r9 = $r4, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    rolw $r0 = $r0, $r9
; CV2-NEXT:    srad $r1 = $r1, 32
; CV2-NEXT:    rolw $r5 = $r2, $r6
; CV2-NEXT:    srad $r9 = $r5, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    rolw $r1 = $r1, $r9
; CV2-NEXT:    srad $r2 = $r2, 32
; CV2-NEXT:    srad $r6 = $r6, 32
; CV2-NEXT:    srad $r9 = $r7, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    rolw $r2 = $r2, $r6
; CV2-NEXT:    rolw $r3 = $r3, $r7
; CV2-NEXT:    srad $r6 = $r3, 32
; CV2-NEXT:    insf $r8 = $r0, 63, 32
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    copyd $r0 = $r8
; CV2-NEXT:    insf $r4 = $r1, 63, 32
; CV2-NEXT:    insf $r5 = $r2, 63, 32
; CV2-NEXT:    rolw $r6 = $r6, $r9
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    copyd $r1 = $r4
; CV2-NEXT:    copyd $r2 = $r5
; CV2-NEXT:    insf $r3 = $r6, 63, 32
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 5)
  %3 = extractelement <8 x i32> %0, i64 0
  %4 = extractelement <8 x i32> %1, i64 0
  %5 = tail call i32 @llvm.kvx.shl.i32(i32 %3, i32 %4, i32 3)
  %6 = extractelement <8 x i32> %0, i64 1
  %7 = extractelement <8 x i32> %1, i64 1
  %8 = tail call i32 @llvm.kvx.shl.i32(i32 %6, i32 %7, i32 3)
  %9 = extractelement <8 x i32> %0, i64 2
  %10 = extractelement <8 x i32> %1, i64 2
  %11 = tail call i32 @llvm.kvx.shl.i32(i32 %9, i32 %10, i32 3)
  %12 = extractelement <8 x i32> %0, i64 3
  %13 = extractelement <8 x i32> %1, i64 3
  %14 = tail call i32 @llvm.kvx.shl.i32(i32 %12, i32 %13, i32 3)
  %15 = extractelement <8 x i32> %0, i64 4
  %16 = extractelement <8 x i32> %1, i64 4
  %17 = tail call i32 @llvm.kvx.shl.i32(i32 %15, i32 %16, i32 3)
  %18 = extractelement <8 x i32> %0, i64 5
  %19 = extractelement <8 x i32> %1, i64 5
  %20 = tail call i32 @llvm.kvx.shl.i32(i32 %18, i32 %19, i32 3)
  %21 = extractelement <8 x i32> %0, i64 6
  %22 = extractelement <8 x i32> %1, i64 6
  %23 = tail call i32 @llvm.kvx.shl.i32(i32 %21, i32 %22, i32 3)
  %24 = extractelement <8 x i32> %0, i64 7
  %25 = extractelement <8 x i32> %1, i64 7
  %26 = tail call i32 @llvm.kvx.shl.i32(i32 %24, i32 %25, i32 3)
  %27 = insertelement <8 x i32> undef, i32 %5, i32 0
  %28 = insertelement <8 x i32> %27, i32 %8, i32 1
  %29 = insertelement <8 x i32> %28, i32 %11, i32 2
  %30 = insertelement <8 x i32> %29, i32 %14, i32 3
  %31 = insertelement <8 x i32> %30, i32 %17, i32 4
  %32 = insertelement <8 x i32> %31, i32 %20, i32 5
  %33 = insertelement <8 x i32> %32, i32 %23, i32 6
  %34 = insertelement <8 x i32> %33, i32 %26, i32 7
  ret <8 x i32> %34
}

define <8 x i32> @shlwos(<8 x i32> %0, i32 %1) {
; CV1-LABEL: shlwos:
; CV1:       # %bb.0:
; CV1-NEXT:    sllwps $r0 = $r0, $r4
; CV1-NEXT:    sllwps $r1 = $r1, $r4
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sllwps $r2 = $r2, $r4
; CV1-NEXT:    sllwps $r3 = $r3, $r4
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 1)
;
; CV2-LABEL: shlwos:
; CV2:       # %bb.0:
; CV2-NEXT:    sllwps $r0 = $r0, $r4
; CV2-NEXT:    sllwps $r1 = $r1, $r4
; CV2-NEXT:    sllwps $r2 = $r2, $r4
; CV2-NEXT:    sllwps $r3 = $r3, $r4
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %3 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 0, i32 1>
  %4 = tail call <2 x i32> @llvm.kvx.shl.v2i32(<2 x i32> %3, i32 %1, i32 0)
  %5 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 2, i32 3>
  %6 = tail call <2 x i32> @llvm.kvx.shl.v2i32(<2 x i32> %5, i32 %1, i32 0)
  %7 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 4, i32 5>
  %8 = tail call <2 x i32> @llvm.kvx.shl.v2i32(<2 x i32> %7, i32 %1, i32 0)
  %9 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 6, i32 7>
  %10 = tail call <2 x i32> @llvm.kvx.shl.v2i32(<2 x i32> %9, i32 %1, i32 0)
  %11 = shufflevector <2 x i32> %4, <2 x i32> %6, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %12 = shufflevector <2 x i32> %8, <2 x i32> %10, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %13 = shufflevector <4 x i32> %11, <4 x i32> %12, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i32> %13
}

declare <2 x i32> @llvm.kvx.shl.v2i32(<2 x i32>, i32, i32)

define <8 x i32> @shlwos_s(<8 x i32> %0, i32 %1) {
; ALL-LABEL: shlwos_s:
; ALL:       # %bb.0:
; ALL-NEXT:    slswps $r0 = $r0, $r4
; ALL-NEXT:    slswps $r1 = $r1, $r4
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    slswps $r2 = $r2, $r4
; ALL-NEXT:    slswps $r3 = $r3, $r4
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 1)
  %3 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 0, i32 1>
  %4 = tail call <2 x i32> @llvm.kvx.shl.v2i32(<2 x i32> %3, i32 %1, i32 1)
  %5 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 2, i32 3>
  %6 = tail call <2 x i32> @llvm.kvx.shl.v2i32(<2 x i32> %5, i32 %1, i32 1)
  %7 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 4, i32 5>
  %8 = tail call <2 x i32> @llvm.kvx.shl.v2i32(<2 x i32> %7, i32 %1, i32 1)
  %9 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 6, i32 7>
  %10 = tail call <2 x i32> @llvm.kvx.shl.v2i32(<2 x i32> %9, i32 %1, i32 1)
  %11 = shufflevector <2 x i32> %4, <2 x i32> %6, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %12 = shufflevector <2 x i32> %8, <2 x i32> %10, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %13 = shufflevector <4 x i32> %11, <4 x i32> %12, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i32> %13
}

define <8 x i32> @shlwos_us(<8 x i32> %0, i32 %1) {
; CV1-LABEL: shlwos_us:
; CV1:       # %bb.0:
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    zxwd $r1 = $r1
; CV1-NEXT:    srad $r5 = $r0, 32
; CV1-NEXT:    srad $r6 = $r1, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    slld $r0 = $r0, $r4
; CV1-NEXT:    slld $r1 = $r1, $r4
; CV1-NEXT:    zxwd $r5 = $r5
; CV1-NEXT:    zxwd $r6 = $r6
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    minud $r0 = $r0, 0xffffffff
; CV1-NEXT:    minud $r1 = $r1, 0xffffffff
; CV1-NEXT:    slld $r5 = $r5, $r4
; CV1-NEXT:    slld $r6 = $r6, $r4
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    zxwd $r2 = $r2
; CV1-NEXT:    minud $r5 = $r5, 0xffffffff
; CV1-NEXT:    minud $r6 = $r6, 0xffffffff
; CV1-NEXT:    srad $r7 = $r2, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    insf $r0 = $r5, 63, 32
; CV1-NEXT:    insf $r1 = $r6, 63, 32
; CV1-NEXT:    srad $r5 = $r3, 32
; CV1-NEXT:    zxwd $r6 = $r7
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    slld $r2 = $r2, $r4
; CV1-NEXT:    zxwd $r3 = $r3
; CV1-NEXT:    zxwd $r5 = $r5
; CV1-NEXT:    slld $r6 = $r6, $r4
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    minud $r2 = $r2, 0xffffffff
; CV1-NEXT:    slld $r3 = $r3, $r4
; CV1-NEXT:    minud $r4 = $r6, 0xffffffff
; CV1-NEXT:    slld $r5 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r2 = $r4, 63, 32
; CV1-NEXT:    minud $r3 = $r3, 0xffffffff
; CV1-NEXT:    minud $r5 = $r5, 0xffffffff
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r3 = $r5, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 8)
;
; CV2-LABEL: shlwos_us:
; CV2:       # %bb.0:
; CV2-NEXT:    sluswps $r0 = $r0, $r4
; CV2-NEXT:    sluswps $r1 = $r1, $r4
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sluswps $r2 = $r2, $r4
; CV2-NEXT:    sluswps $r3 = $r3, $r4
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 1)
  %3 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 0, i32 1>
  %4 = tail call <2 x i32> @llvm.kvx.shl.v2i32(<2 x i32> %3, i32 %1, i32 2)
  %5 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 2, i32 3>
  %6 = tail call <2 x i32> @llvm.kvx.shl.v2i32(<2 x i32> %5, i32 %1, i32 2)
  %7 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 4, i32 5>
  %8 = tail call <2 x i32> @llvm.kvx.shl.v2i32(<2 x i32> %7, i32 %1, i32 2)
  %9 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 6, i32 7>
  %10 = tail call <2 x i32> @llvm.kvx.shl.v2i32(<2 x i32> %9, i32 %1, i32 2)
  %11 = shufflevector <2 x i32> %4, <2 x i32> %6, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %12 = shufflevector <2 x i32> %8, <2 x i32> %10, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %13 = shufflevector <4 x i32> %11, <4 x i32> %12, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i32> %13
}

define <8 x i32> @shlwos_r(<8 x i32> %0, i32 %1) {
; ALL-LABEL: shlwos_r:
; ALL:       # %bb.0:
; ALL-NEXT:    rolwps $r0 = $r0, $r4
; ALL-NEXT:    rolwps $r1 = $r1, $r4
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    rolwps $r2 = $r2, $r4
; ALL-NEXT:    rolwps $r3 = $r3, $r4
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 1)
  %3 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 0, i32 1>
  %4 = tail call <2 x i32> @llvm.kvx.shl.v2i32(<2 x i32> %3, i32 %1, i32 3)
  %5 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 2, i32 3>
  %6 = tail call <2 x i32> @llvm.kvx.shl.v2i32(<2 x i32> %5, i32 %1, i32 3)
  %7 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 4, i32 5>
  %8 = tail call <2 x i32> @llvm.kvx.shl.v2i32(<2 x i32> %7, i32 %1, i32 3)
  %9 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 6, i32 7>
  %10 = tail call <2 x i32> @llvm.kvx.shl.v2i32(<2 x i32> %9, i32 %1, i32 3)
  %11 = shufflevector <2 x i32> %4, <2 x i32> %6, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %12 = shufflevector <2 x i32> %8, <2 x i32> %10, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %13 = shufflevector <4 x i32> %11, <4 x i32> %12, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i32> %13
}

define <2 x i32> @shlwp(<2 x i32> %0, <2 x i32> %1) {
; ALL-LABEL: shlwp:
; ALL:       # %bb.0:
; ALL-NEXT:    sllw $r0 = $r0, $r1
; ALL-NEXT:    srad $r2 = $r0, 32
; ALL-NEXT:    srad $r3 = $r1, 32
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sllw $r1 = $r2, $r3
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    insf $r0 = $r1, 63, 32
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 2)
  %3 = extractelement <2 x i32> %0, i64 0
  %4 = extractelement <2 x i32> %1, i64 0
  %5 = tail call i32 @llvm.kvx.shl.i32(i32 %3, i32 %4, i32 0)
  %6 = extractelement <2 x i32> %0, i64 1
  %7 = extractelement <2 x i32> %1, i64 1
  %8 = tail call i32 @llvm.kvx.shl.i32(i32 %6, i32 %7, i32 0)
  %9 = insertelement <2 x i32> undef, i32 %5, i32 0
  %10 = insertelement <2 x i32> %9, i32 %8, i32 1
  ret <2 x i32> %10
}

define <2 x i32> @shlwp_s(<2 x i32> %0, <2 x i32> %1) {
; ALL-LABEL: shlwp_s:
; ALL:       # %bb.0:
; ALL-NEXT:    slsw $r0 = $r0, $r1
; ALL-NEXT:    srad $r2 = $r0, 32
; ALL-NEXT:    srad $r3 = $r1, 32
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    slsw $r1 = $r2, $r3
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    insf $r0 = $r1, 63, 32
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 2)
  %3 = extractelement <2 x i32> %0, i64 0
  %4 = extractelement <2 x i32> %1, i64 0
  %5 = tail call i32 @llvm.kvx.shl.i32(i32 %3, i32 %4, i32 1)
  %6 = extractelement <2 x i32> %0, i64 1
  %7 = extractelement <2 x i32> %1, i64 1
  %8 = tail call i32 @llvm.kvx.shl.i32(i32 %6, i32 %7, i32 1)
  %9 = insertelement <2 x i32> undef, i32 %5, i32 0
  %10 = insertelement <2 x i32> %9, i32 %8, i32 1
  ret <2 x i32> %10
}

define <2 x i32> @shlwp_us(<2 x i32> %0, <2 x i32> %1) {
; CV1-LABEL: shlwp_us:
; CV1:       # %bb.0:
; CV1-NEXT:    srad $r0 = $r0, 32
; CV1-NEXT:    zxwd $r2 = $r0
; CV1-NEXT:    srad $r3 = $r1, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    slld $r1 = $r2, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    minud $r0 = $r1, 0xffffffff
; CV1-NEXT:    slld $r2 = $r0, $r3
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    minud $r1 = $r2, 0xffffffff
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    insf $r0 = $r1, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 4)
;
; CV2-LABEL: shlwp_us:
; CV2:       # %bb.0:
; CV2-NEXT:    slusw $r0 = $r0, $r1
; CV2-NEXT:    srad $r2 = $r0, 32
; CV2-NEXT:    srad $r3 = $r1, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    slusw $r1 = $r2, $r3
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r0 = $r1, 63, 32
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 2)
  %3 = extractelement <2 x i32> %0, i64 0
  %4 = extractelement <2 x i32> %1, i64 0
  %5 = tail call i32 @llvm.kvx.shl.i32(i32 %3, i32 %4, i32 2)
  %6 = extractelement <2 x i32> %0, i64 1
  %7 = extractelement <2 x i32> %1, i64 1
  %8 = tail call i32 @llvm.kvx.shl.i32(i32 %6, i32 %7, i32 2)
  %9 = insertelement <2 x i32> undef, i32 %5, i32 0
  %10 = insertelement <2 x i32> %9, i32 %8, i32 1
  ret <2 x i32> %10
}

define <2 x i32> @shlwp_r(<2 x i32> %0, <2 x i32> %1) {
; ALL-LABEL: shlwp_r:
; ALL:       # %bb.0:
; ALL-NEXT:    rolw $r0 = $r0, $r1
; ALL-NEXT:    srad $r2 = $r0, 32
; ALL-NEXT:    srad $r3 = $r1, 32
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    rolw $r1 = $r2, $r3
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    insf $r0 = $r1, 63, 32
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 2)
  %3 = extractelement <2 x i32> %0, i64 0
  %4 = extractelement <2 x i32> %1, i64 0
  %5 = tail call i32 @llvm.kvx.shl.i32(i32 %3, i32 %4, i32 3)
  %6 = extractelement <2 x i32> %0, i64 1
  %7 = extractelement <2 x i32> %1, i64 1
  %8 = tail call i32 @llvm.kvx.shl.i32(i32 %6, i32 %7, i32 3)
  %9 = insertelement <2 x i32> undef, i32 %5, i32 0
  %10 = insertelement <2 x i32> %9, i32 %8, i32 1
  ret <2 x i32> %10
}

define <2 x i32> @shlwps(<2 x i32> %0, i32 %1) {
; ALL-LABEL: shlwps:
; ALL:       # %bb.0:
; ALL-NEXT:    sllwps $r0 = $r0, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %3 = tail call <2 x i32> @llvm.kvx.shl.v2i32(<2 x i32> %0, i32 %1, i32 0)
  ret <2 x i32> %3
}

define <2 x i32> @shlwps_s(<2 x i32> %0, i32 %1) {
; ALL-LABEL: shlwps_s:
; ALL:       # %bb.0:
; ALL-NEXT:    slswps $r0 = $r0, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %3 = tail call <2 x i32> @llvm.kvx.shl.v2i32(<2 x i32> %0, i32 %1, i32 1)
  ret <2 x i32> %3
}

define <2 x i32> @shlwps_us(<2 x i32> %0, i32 %1) {
; CV1-LABEL: shlwps_us:
; CV1:       # %bb.0:
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    srad $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    slld $r0 = $r0, $r1
; CV1-NEXT:    zxwd $r2 = $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    minud $r0 = $r0, 0xffffffff
; CV1-NEXT:    slld $r2 = $r2, $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    minud $r1 = $r2, 0xffffffff
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    insf $r0 = $r1, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 4)
;
; CV2-LABEL: shlwps_us:
; CV2:       # %bb.0:
; CV2-NEXT:    sluswps $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %3 = tail call <2 x i32> @llvm.kvx.shl.v2i32(<2 x i32> %0, i32 %1, i32 2)
  ret <2 x i32> %3
}

define <2 x i32> @shlwps_r(<2 x i32> %0, i32 %1) {
; ALL-LABEL: shlwps_r:
; ALL:       # %bb.0:
; ALL-NEXT:    rolwps $r0 = $r0, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %3 = tail call <2 x i32> @llvm.kvx.shl.v2i32(<2 x i32> %0, i32 %1, i32 3)
  ret <2 x i32> %3
}

define <4 x i32> @shlwq(<4 x i32> %0, <4 x i32> %1) {
; ALL-LABEL: shlwq:
; ALL:       # %bb.0:
; ALL-NEXT:    sllw $r0 = $r0, $r2
; ALL-NEXT:    srad $r2 = $r2, 32
; ALL-NEXT:    srad $r4 = $r0, 32
; ALL-NEXT:    srad $r5 = $r1, 32
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sllw $r1 = $r1, $r3
; ALL-NEXT:    sllw $r2 = $r4, $r2
; ALL-NEXT:    srad $r6 = $r3, 32
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    insf $r0 = $r2, 63, 32
; ALL-NEXT:    sllw $r3 = $r5, $r6
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    insf $r1 = $r3, 63, 32
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 3)
  %3 = extractelement <4 x i32> %0, i64 0
  %4 = extractelement <4 x i32> %1, i64 0
  %5 = tail call i32 @llvm.kvx.shl.i32(i32 %3, i32 %4, i32 0)
  %6 = extractelement <4 x i32> %0, i64 1
  %7 = extractelement <4 x i32> %1, i64 1
  %8 = tail call i32 @llvm.kvx.shl.i32(i32 %6, i32 %7, i32 0)
  %9 = extractelement <4 x i32> %0, i64 2
  %10 = extractelement <4 x i32> %1, i64 2
  %11 = tail call i32 @llvm.kvx.shl.i32(i32 %9, i32 %10, i32 0)
  %12 = extractelement <4 x i32> %0, i64 3
  %13 = extractelement <4 x i32> %1, i64 3
  %14 = tail call i32 @llvm.kvx.shl.i32(i32 %12, i32 %13, i32 0)
  %15 = insertelement <4 x i32> undef, i32 %5, i32 0
  %16 = insertelement <4 x i32> %15, i32 %8, i32 1
  %17 = insertelement <4 x i32> %16, i32 %11, i32 2
  %18 = insertelement <4 x i32> %17, i32 %14, i32 3
  ret <4 x i32> %18
}

define <4 x i32> @shlwq_s(<4 x i32> %0, <4 x i32> %1) {
; ALL-LABEL: shlwq_s:
; ALL:       # %bb.0:
; ALL-NEXT:    slsw $r0 = $r0, $r2
; ALL-NEXT:    srad $r2 = $r2, 32
; ALL-NEXT:    srad $r4 = $r0, 32
; ALL-NEXT:    srad $r5 = $r1, 32
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    slsw $r1 = $r1, $r3
; ALL-NEXT:    slsw $r2 = $r4, $r2
; ALL-NEXT:    srad $r4 = $r3, 32
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    insf $r0 = $r2, 63, 32
; ALL-NEXT:    slsw $r3 = $r5, $r4
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    insf $r1 = $r3, 63, 32
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 3)
  %3 = extractelement <4 x i32> %0, i64 0
  %4 = extractelement <4 x i32> %1, i64 0
  %5 = tail call i32 @llvm.kvx.shl.i32(i32 %3, i32 %4, i32 1)
  %6 = extractelement <4 x i32> %0, i64 1
  %7 = extractelement <4 x i32> %1, i64 1
  %8 = tail call i32 @llvm.kvx.shl.i32(i32 %6, i32 %7, i32 1)
  %9 = extractelement <4 x i32> %0, i64 2
  %10 = extractelement <4 x i32> %1, i64 2
  %11 = tail call i32 @llvm.kvx.shl.i32(i32 %9, i32 %10, i32 1)
  %12 = extractelement <4 x i32> %0, i64 3
  %13 = extractelement <4 x i32> %1, i64 3
  %14 = tail call i32 @llvm.kvx.shl.i32(i32 %12, i32 %13, i32 1)
  %15 = insertelement <4 x i32> undef, i32 %5, i32 0
  %16 = insertelement <4 x i32> %15, i32 %8, i32 1
  %17 = insertelement <4 x i32> %16, i32 %11, i32 2
  %18 = insertelement <4 x i32> %17, i32 %14, i32 3
  ret <4 x i32> %18
}

define <4 x i32> @shlwq_us(<4 x i32> %0, <4 x i32> %1) {
; CV1-LABEL: shlwq_us:
; CV1:       # %bb.0:
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    zxwd $r1 = $r1
; CV1-NEXT:    srad $r4 = $r0, 32
; CV1-NEXT:    srad $r5 = $r1, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    zxwd $r4 = $r4
; CV1-NEXT:    zxwd $r5 = $r5
; CV1-NEXT:    srad $r6 = $r2, 32
; CV1-NEXT:    srad $r7 = $r3, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    slld $r0 = $r0, $r2
; CV1-NEXT:    slld $r1 = $r1, $r3
; CV1-NEXT:    slld $r2 = $r4, $r6
; CV1-NEXT:    slld $r3 = $r5, $r7
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    minud $r0 = $r0, 0xffffffff
; CV1-NEXT:    minud $r1 = $r1, 0xffffffff
; CV1-NEXT:    minud $r2 = $r2, 0xffffffff
; CV1-NEXT:    minud $r3 = $r3, 0xffffffff
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    insf $r0 = $r2, 63, 32
; CV1-NEXT:    insf $r1 = $r3, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 4)
;
; CV2-LABEL: shlwq_us:
; CV2:       # %bb.0:
; CV2-NEXT:    slusw $r0 = $r0, $r2
; CV2-NEXT:    srad $r2 = $r2, 32
; CV2-NEXT:    srad $r4 = $r0, 32
; CV2-NEXT:    srad $r5 = $r1, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    slusw $r1 = $r1, $r3
; CV2-NEXT:    slusw $r2 = $r4, $r2
; CV2-NEXT:    srad $r4 = $r3, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r0 = $r2, 63, 32
; CV2-NEXT:    slusw $r3 = $r5, $r4
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r1 = $r3, 63, 32
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 3)
  %3 = extractelement <4 x i32> %0, i64 0
  %4 = extractelement <4 x i32> %1, i64 0
  %5 = tail call i32 @llvm.kvx.shl.i32(i32 %3, i32 %4, i32 2)
  %6 = extractelement <4 x i32> %0, i64 1
  %7 = extractelement <4 x i32> %1, i64 1
  %8 = tail call i32 @llvm.kvx.shl.i32(i32 %6, i32 %7, i32 2)
  %9 = extractelement <4 x i32> %0, i64 2
  %10 = extractelement <4 x i32> %1, i64 2
  %11 = tail call i32 @llvm.kvx.shl.i32(i32 %9, i32 %10, i32 2)
  %12 = extractelement <4 x i32> %0, i64 3
  %13 = extractelement <4 x i32> %1, i64 3
  %14 = tail call i32 @llvm.kvx.shl.i32(i32 %12, i32 %13, i32 2)
  %15 = insertelement <4 x i32> undef, i32 %5, i32 0
  %16 = insertelement <4 x i32> %15, i32 %8, i32 1
  %17 = insertelement <4 x i32> %16, i32 %11, i32 2
  %18 = insertelement <4 x i32> %17, i32 %14, i32 3
  ret <4 x i32> %18
}

define <4 x i32> @shlwq_r(<4 x i32> %0, <4 x i32> %1) {
; ALL-LABEL: shlwq_r:
; ALL:       # %bb.0:
; ALL-NEXT:    rolw $r0 = $r0, $r2
; ALL-NEXT:    srad $r2 = $r2, 32
; ALL-NEXT:    srad $r4 = $r0, 32
; ALL-NEXT:    srad $r5 = $r1, 32
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    rolw $r1 = $r1, $r3
; ALL-NEXT:    rolw $r2 = $r4, $r2
; ALL-NEXT:    srad $r4 = $r3, 32
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    insf $r0 = $r2, 63, 32
; ALL-NEXT:    rolw $r3 = $r5, $r4
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    insf $r1 = $r3, 63, 32
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 3)
  %3 = extractelement <4 x i32> %0, i64 0
  %4 = extractelement <4 x i32> %1, i64 0
  %5 = tail call i32 @llvm.kvx.shl.i32(i32 %3, i32 %4, i32 3)
  %6 = extractelement <4 x i32> %0, i64 1
  %7 = extractelement <4 x i32> %1, i64 1
  %8 = tail call i32 @llvm.kvx.shl.i32(i32 %6, i32 %7, i32 3)
  %9 = extractelement <4 x i32> %0, i64 2
  %10 = extractelement <4 x i32> %1, i64 2
  %11 = tail call i32 @llvm.kvx.shl.i32(i32 %9, i32 %10, i32 3)
  %12 = extractelement <4 x i32> %0, i64 3
  %13 = extractelement <4 x i32> %1, i64 3
  %14 = tail call i32 @llvm.kvx.shl.i32(i32 %12, i32 %13, i32 3)
  %15 = insertelement <4 x i32> undef, i32 %5, i32 0
  %16 = insertelement <4 x i32> %15, i32 %8, i32 1
  %17 = insertelement <4 x i32> %16, i32 %11, i32 2
  %18 = insertelement <4 x i32> %17, i32 %14, i32 3
  ret <4 x i32> %18
}

define <4 x i32> @shlwqs(<4 x i32> %0, i32 %1) {
; ALL-LABEL: shlwqs:
; ALL:       # %bb.0:
; ALL-NEXT:    sllwps $r0 = $r0, $r2
; ALL-NEXT:    sllwps $r1 = $r1, $r2
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %3 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 0, i32 1>
  %4 = tail call <2 x i32> @llvm.kvx.shl.v2i32(<2 x i32> %3, i32 %1, i32 0)
  %5 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 2, i32 3>
  %6 = tail call <2 x i32> @llvm.kvx.shl.v2i32(<2 x i32> %5, i32 %1, i32 0)
  %7 = shufflevector <2 x i32> %4, <2 x i32> %6, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  ret <4 x i32> %7
}

define <4 x i32> @shlwqs_s(<4 x i32> %0, i32 %1) {
; ALL-LABEL: shlwqs_s:
; ALL:       # %bb.0:
; ALL-NEXT:    slswps $r0 = $r0, $r2
; ALL-NEXT:    slswps $r1 = $r1, $r2
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %3 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 0, i32 1>
  %4 = tail call <2 x i32> @llvm.kvx.shl.v2i32(<2 x i32> %3, i32 %1, i32 1)
  %5 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 2, i32 3>
  %6 = tail call <2 x i32> @llvm.kvx.shl.v2i32(<2 x i32> %5, i32 %1, i32 1)
  %7 = shufflevector <2 x i32> %4, <2 x i32> %6, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  ret <4 x i32> %7
}

define <4 x i32> @shlwqs_us(<4 x i32> %0, i32 %1) {
; CV1-LABEL: shlwqs_us:
; CV1:       # %bb.0:
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    zxwd $r1 = $r1
; CV1-NEXT:    srad $r3 = $r0, 32
; CV1-NEXT:    srad $r4 = $r1, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    slld $r0 = $r0, $r2
; CV1-NEXT:    slld $r1 = $r1, $r2
; CV1-NEXT:    zxwd $r3 = $r3
; CV1-NEXT:    zxwd $r4 = $r4
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    minud $r0 = $r0, 0xffffffff
; CV1-NEXT:    minud $r1 = $r1, 0xffffffff
; CV1-NEXT:    slld $r3 = $r3, $r2
; CV1-NEXT:    slld $r4 = $r4, $r2
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    minud $r2 = $r3, 0xffffffff
; CV1-NEXT:    minud $r3 = $r4, 0xffffffff
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    insf $r0 = $r2, 63, 32
; CV1-NEXT:    insf $r1 = $r3, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 4)
;
; CV2-LABEL: shlwqs_us:
; CV2:       # %bb.0:
; CV2-NEXT:    sluswps $r0 = $r0, $r2
; CV2-NEXT:    sluswps $r1 = $r1, $r2
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %3 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 0, i32 1>
  %4 = tail call <2 x i32> @llvm.kvx.shl.v2i32(<2 x i32> %3, i32 %1, i32 2)
  %5 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 2, i32 3>
  %6 = tail call <2 x i32> @llvm.kvx.shl.v2i32(<2 x i32> %5, i32 %1, i32 2)
  %7 = shufflevector <2 x i32> %4, <2 x i32> %6, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  ret <4 x i32> %7
}

define <4 x i32> @shlwqs_r(<4 x i32> %0, i32 %1) {
; ALL-LABEL: shlwqs_r:
; ALL:       # %bb.0:
; ALL-NEXT:    rolwps $r0 = $r0, $r2
; ALL-NEXT:    rolwps $r1 = $r1, $r2
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 0)
  %3 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 0, i32 1>
  %4 = tail call <2 x i32> @llvm.kvx.shl.v2i32(<2 x i32> %3, i32 %1, i32 3)
  %5 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 2, i32 3>
  %6 = tail call <2 x i32> @llvm.kvx.shl.v2i32(<2 x i32> %5, i32 %1, i32 3)
  %7 = shufflevector <2 x i32> %4, <2 x i32> %6, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  ret <4 x i32> %7
}

define <8 x i8> @shlbos(<8 x i8> %0, i32 %1) {
; CV1-LABEL: shlbos:
; CV1:       # %bb.0:
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r2 = $r0, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    slld $r0 = $r0, $r1
; CV1-NEXT:    slld $r2 = $r2, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r1 = $r2, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    ord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: shlbos:
; CV2:       # %bb.0:
; CV2-NEXT:    sllbos $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %3 = tail call <8 x i8> @llvm.kvx.shl.v8i8(<8 x i8> %0, i32 %1, i32 0)
  ret <8 x i8> %3
}

declare <8 x i8> @llvm.kvx.shl.v8i8(<8 x i8>, i32, i32)

define <8 x i8> @shlbos_s(<8 x i8> %0, i32 %1) {
; CV1-LABEL: shlbos_s:
; CV1:       # %bb.0:
; CV1-NEXT:    zxbd $r1 = $r1
; CV1-NEXT:    extfz $r2 = $r0, 55, 48
; CV1-NEXT:    srld $r3 = $r0, 56
; CV1-NEXT:    extfz $r4 = $r0, 47, 40
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sllw $r2 = $r2, 24
; CV1-NEXT:    sllw $r3 = $r3, 24
; CV1-NEXT:    sllw $r4 = $r4, 24
; CV1-NEXT:    extfz $r5 = $r0, 39, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    slsw $r2 = $r2, $r1
; CV1-NEXT:    slsw $r3 = $r3, $r1
; CV1-NEXT:    sllw $r5 = $r5, 24
; CV1-NEXT:    srlw $r6 = $r0, 24
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sraw $r2 = $r2, 24
; CV1-NEXT:    sraw $r3 = $r3, 24
; CV1-NEXT:    slsw $r4 = $r4, $r1
; CV1-NEXT:    slsw $r5 = $r5, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    insf $r2 = $r3, 15, 8
; CV1-NEXT:    sraw $r3 = $r5, 24
; CV1-NEXT:    sraw $r4 = $r4, 24
; CV1-NEXT:    extfz $r5 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    insf $r3 = $r4, 15, 8
; CV1-NEXT:    sllw $r4 = $r5, 24
; CV1-NEXT:    extfz $r5 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sllw $r0 = $r0, 24
; CV1-NEXT:    slsw $r4 = $r4, $r1
; CV1-NEXT:    sllw $r5 = $r5, 24
; CV1-NEXT:    sllw $r6 = $r6, 24
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    sraw $r4 = $r4, 24
; CV1-NEXT:    slsw $r5 = $r5, $r1
; CV1-NEXT:    slsw $r6 = $r6, $r1
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    slsw $r0 = $r0, $r1
; CV1-NEXT:    sraw $r1 = $r6, 24
; CV1-NEXT:    insf $r3 = $r2, 31, 16
; CV1-NEXT:    sraw $r5 = $r5, 24
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    sraw $r0 = $r0, 24
; CV1-NEXT:    insf $r4 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r0 = $r5, 15, 8
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    insf $r0 = $r4, 31, 16
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    insf $r0 = $r3, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 12)
;
; CV2-LABEL: shlbos_s:
; CV2:       # %bb.0:
; CV2-NEXT:    slsbos $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %3 = tail call <8 x i8> @llvm.kvx.shl.v8i8(<8 x i8> %0, i32 %1, i32 1)
  ret <8 x i8> %3
}

define <8 x i8> @shlbos_us(<8 x i8> %0, i32 %1) {
; CV1-LABEL: shlbos_us:
; CV1:       # %bb.0:
; CV1-NEXT:    zxbd $r1 = $r1
; CV1-NEXT:    srld $r2 = $r0, 56
; CV1-NEXT:    extfz $r3 = $r0, 55, 48
; CV1-NEXT:    extfz $r4 = $r0, 47, 40
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sllw $r2 = $r2, 24
; CV1-NEXT:    sllw $r3 = $r3, 24
; CV1-NEXT:    sllw $r4 = $r4, 24
; CV1-NEXT:    extfz $r6 = $r0, 39, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    zxwd $r2 = $r2
; CV1-NEXT:    zxwd $r3 = $r3
; CV1-NEXT:    zxwd $r4 = $r4
; CV1-NEXT:    sllw $r6 = $r6, 24
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    slld $r2 = $r2, $r1
; CV1-NEXT:    slld $r3 = $r3, $r1
; CV1-NEXT:    slld $r4 = $r4, $r1
; CV1-NEXT:    zxwd $r6 = $r6
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    minud $r2 = $r2, 0xffffffff
; CV1-NEXT:    minud $r3 = $r3, 0xffffffff
; CV1-NEXT:    srlw $r5 = $r0, 24
; CV1-NEXT:    slld $r6 = $r6, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    srlw $r2 = $r2, 24
; CV1-NEXT:    srlw $r3 = $r3, 24
; CV1-NEXT:    minud $r4 = $r4, 0xffffffff
; CV1-NEXT:    extfz $r7 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    minud $r2 = $r6, 0xffffffff
; CV1-NEXT:    insf $r3 = $r2, 15, 8
; CV1-NEXT:    extfz $r6 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    srlw $r2 = $r2, 24
; CV1-NEXT:    srlw $r4 = $r4, 24
; CV1-NEXT:    sllw $r5 = $r5, 24
; CV1-NEXT:    sllw $r7 = $r7, 24
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    sllw $r0 = $r0, 24
; CV1-NEXT:    insf $r2 = $r4, 15, 8
; CV1-NEXT:    zxwd $r5 = $r5
; CV1-NEXT:    sllw $r6 = $r6, 24
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    zxwd $r4 = $r7
; CV1-NEXT:    slld $r5 = $r5, $r1
; CV1-NEXT:    zxwd $r6 = $r6
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    slld $r0 = $r0, $r1
; CV1-NEXT:    minud $r1 = $r5, 0xffffffff
; CV1-NEXT:    slld $r4 = $r4, $r1
; CV1-NEXT:    slld $r6 = $r6, $r1
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    minud $r0 = $r0, 0xffffffff
; CV1-NEXT:    srlw $r1 = $r1, 24
; CV1-NEXT:    minud $r4 = $r4, 0xffffffff
; CV1-NEXT:    minud $r5 = $r6, 0xffffffff
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    srlw $r0 = $r0, 24
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    srlw $r4 = $r4, 24
; CV1-NEXT:    srlw $r5 = $r5, 24
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    insf $r0 = $r5, 15, 8
; CV1-NEXT:    insf $r4 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    insf $r0 = $r4, 31, 16
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    insf $r0 = $r2, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 15)
;
; CV2-LABEL: shlbos_us:
; CV2:       # %bb.0:
; CV2-NEXT:    slusbos $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %3 = tail call <8 x i8> @llvm.kvx.shl.v8i8(<8 x i8> %0, i32 %1, i32 2)
  ret <8 x i8> %3
}

define <8 x i8> @shlbos_r(<8 x i8> %0, i32 %1) {
; ALL-LABEL: shlbos_r:
; ALL:       # %bb.0:
; ALL-NEXT:    sbmm8 $r0 = $r0, 0x8080202008080202
; ALL-NEXT:    sbmm8 $r2 = $r0, 0x4040101004040101
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    sllhqs $r0 = $r0, $r1
; ALL-NEXT:    sllhqs $r2 = $r2, $r1
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; ALL-NEXT:    srlhqs $r1 = $r2, 8
; ALL-NEXT:    ;; # (end cycle 2)
; ALL-NEXT:    ord $r0 = $r0, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 3)
  %3 = tail call <8 x i8> @llvm.kvx.shl.v8i8(<8 x i8> %0, i32 %1, i32 3)
  ret <8 x i8> %3
}

define <2 x i8> @shlbps(<2 x i8> %0, i32 %1) {
; CV1-LABEL: shlbps:
; CV1:       # %bb.0:
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    extfz $r2 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sllw $r0 = $r0, $r1
; CV1-NEXT:    sllw $r2 = $r2, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r0 = $r2, 15, 8
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 2)
;
; CV2-LABEL: shlbps:
; CV2:       # %bb.0:
; CV2-NEXT:    sllbos $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %3 = tail call <2 x i8> @llvm.kvx.shl.v2i8(<2 x i8> %0, i32 %1, i32 0)
  ret <2 x i8> %3
}

declare <2 x i8> @llvm.kvx.shl.v2i8(<2 x i8>, i32, i32)

define <2 x i8> @shlbps_s(<2 x i8> %0, i32 %1) {
; CV1-LABEL: shlbps_s:
; CV1:       # %bb.0:
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    zxbd $r1 = $r1
; CV1-NEXT:    extfz $r2 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sllw $r0 = $r0, 24
; CV1-NEXT:    sllw $r2 = $r2, 24
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    slsw $r0 = $r0, $r1
; CV1-NEXT:    slsw $r2 = $r2, $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sraw $r0 = $r0, 24
; CV1-NEXT:    sraw $r1 = $r2, 24
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    insf $r0 = $r1, 15, 8
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 4)
;
; CV2-LABEL: shlbps_s:
; CV2:       # %bb.0:
; CV2-NEXT:    slsbos $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %3 = tail call <2 x i8> @llvm.kvx.shl.v2i8(<2 x i8> %0, i32 %1, i32 1)
  ret <2 x i8> %3
}

define <2 x i8> @shlbps_us(<2 x i8> %0, i32 %1) {
; CV1-LABEL: shlbps_us:
; CV1:       # %bb.0:
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    zxbd $r1 = $r1
; CV1-NEXT:    extfz $r2 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sllw $r0 = $r0, 24
; CV1-NEXT:    sllw $r2 = $r2, 24
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    zxwd $r2 = $r2
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    slld $r0 = $r0, $r1
; CV1-NEXT:    slld $r2 = $r2, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    minud $r0 = $r0, 0xffffffff
; CV1-NEXT:    minud $r1 = $r2, 0xffffffff
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    srlw $r0 = $r0, 24
; CV1-NEXT:    srlw $r1 = $r1, 24
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    insf $r0 = $r1, 15, 8
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: shlbps_us:
; CV2:       # %bb.0:
; CV2-NEXT:    slusbos $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %3 = tail call <2 x i8> @llvm.kvx.shl.v2i8(<2 x i8> %0, i32 %1, i32 2)
  ret <2 x i8> %3
}

define <2 x i8> @shlbps_r(<2 x i8> %0, i32 %1) {
; CV1-LABEL: shlbps_r:
; CV1:       # %bb.0:
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    negw $r1 = $r1
; CV1-NEXT:    andw $r2 = $r1, 7
; CV1-NEXT:    extfz $r3 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andw $r1 = $r1, 7
; CV1-NEXT:    sllw $r3 = $r3, $r2
; CV1-NEXT:    zxbd $r4 = $r3
; CV1-NEXT:    zxbd $r5 = $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sllw $r0 = $r0, $r2
; CV1-NEXT:    srlw $r1 = $r5, $r1
; CV1-NEXT:    srlw $r2 = $r4, $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    orw $r0 = $r0, $r1
; CV1-NEXT:    orw $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    insf $r0 = $r2, 15, 8
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 4)
;
; CV2-LABEL: shlbps_r:
; CV2:       # %bb.0:
; CV2-NEXT:    insf $r1 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    negbo $r1 = $r1
; CV2-NEXT:    andw $r2 = $r1, 0x707
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    andw $r1 = $r1, 0x707
; CV2-NEXT:    extfz $r2 = $r2, 10, 8
; CV2-NEXT:    sllbos $r3 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    srlbos $r1 = $r0, $r1
; CV2-NEXT:    sllbos $r2 = $r0, $r2
; CV2-NEXT:    extfz $r4 = $r1, 10, 8
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlbos $r0 = $r0, $r4
; CV2-NEXT:    insf $r2 = $r3, 7, 0
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r0 = $r1, 7, 0
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    orw $r0 = $r2, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 6)
  %3 = tail call <2 x i8> @llvm.kvx.shl.v2i8(<2 x i8> %0, i32 %1, i32 3)
  ret <2 x i8> %3
}

define <4 x i8> @shlbqs(<4 x i8> %0, i32 %1) {
; CV1-LABEL: shlbqs:
; CV1:       # %bb.0:
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sllhqs $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 2)
;
; CV2-LABEL: shlbqs:
; CV2:       # %bb.0:
; CV2-NEXT:    sllbos $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %3 = tail call <4 x i8> @llvm.kvx.shl.v4i8(<4 x i8> %0, i32 %1, i32 0)
  ret <4 x i8> %3
}

declare <4 x i8> @llvm.kvx.shl.v4i8(<4 x i8>, i32, i32)

define <4 x i8> @shlbqs_s(<4 x i8> %0, i32 %1) {
; CV1-LABEL: shlbqs_s:
; CV1:       # %bb.0:
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    srlw $r2 = $r0, 24
; CV1-NEXT:    extfz $r3 = $r0, 23, 16
; CV1-NEXT:    extfz $r4 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    zxbd $r1 = $r1
; CV1-NEXT:    sllw $r2 = $r2, 24
; CV1-NEXT:    sllw $r3 = $r3, 24
; CV1-NEXT:    sllw $r4 = $r4, 24
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sllw $r0 = $r0, 24
; CV1-NEXT:    slsw $r2 = $r2, $r1
; CV1-NEXT:    slsw $r3 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    slsw $r0 = $r0, $r1
; CV1-NEXT:    sraw $r1 = $r2, 24
; CV1-NEXT:    sraw $r2 = $r3, 24
; CV1-NEXT:    slsw $r4 = $r4, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sraw $r0 = $r0, 24
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    sraw $r3 = $r4, 24
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    insf $r0 = $r3, 15, 8
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    insf $r0 = $r2, 31, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: shlbqs_s:
; CV2:       # %bb.0:
; CV2-NEXT:    slsbos $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %3 = tail call <4 x i8> @llvm.kvx.shl.v4i8(<4 x i8> %0, i32 %1, i32 1)
  ret <4 x i8> %3
}

define <4 x i8> @shlbqs_us(<4 x i8> %0, i32 %1) {
; CV1-LABEL: shlbqs_us:
; CV1:       # %bb.0:
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    srlw $r2 = $r0, 24
; CV1-NEXT:    extfz $r3 = $r0, 23, 16
; CV1-NEXT:    extfz $r4 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sllw $r0 = $r0, 24
; CV1-NEXT:    sllw $r2 = $r2, 24
; CV1-NEXT:    sllw $r3 = $r3, 24
; CV1-NEXT:    sllw $r4 = $r4, 24
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    zxbd $r1 = $r1
; CV1-NEXT:    zxwd $r2 = $r2
; CV1-NEXT:    zxwd $r3 = $r3
; CV1-NEXT:    zxwd $r4 = $r4
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    slld $r2 = $r2, $r1
; CV1-NEXT:    slld $r3 = $r3, $r1
; CV1-NEXT:    slld $r4 = $r4, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    slld $r0 = $r0, $r1
; CV1-NEXT:    minud $r1 = $r2, 0xffffffff
; CV1-NEXT:    minud $r2 = $r3, 0xffffffff
; CV1-NEXT:    minud $r3 = $r4, 0xffffffff
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    minud $r0 = $r0, 0xffffffff
; CV1-NEXT:    srlw $r1 = $r1, 24
; CV1-NEXT:    srlw $r2 = $r2, 24
; CV1-NEXT:    srlw $r3 = $r3, 24
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r0 = $r0, 24
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r0 = $r3, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r0 = $r2, 31, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 8)
;
; CV2-LABEL: shlbqs_us:
; CV2:       # %bb.0:
; CV2-NEXT:    slusbos $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %3 = tail call <4 x i8> @llvm.kvx.shl.v4i8(<4 x i8> %0, i32 %1, i32 2)
  ret <4 x i8> %3
}

define <4 x i8> @shlbqs_r(<4 x i8> %0, i32 %1) {
; CV1-LABEL: shlbqs_r:
; CV1:       # %bb.0:
; CV1-NEXT:    negw $r1 = $r1
; CV1-NEXT:    andw $r2 = $r1, 7
; CV1-NEXT:    srlw $r3 = $r0, 24
; CV1-NEXT:    extfz $r4 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    andw $r1 = $r1, 7
; CV1-NEXT:    extfz $r5 = $r0, 15, 8
; CV1-NEXT:    zxbd $r6 = $r3
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sllw $r3 = $r3, $r2
; CV1-NEXT:    sllw $r4 = $r4, $r2
; CV1-NEXT:    zxbd $r7 = $r4
; CV1-NEXT:    sllw $r8 = $r5, $r2
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    sllw $r2 = $r0, $r2
; CV1-NEXT:    zxbd $r5 = $r5
; CV1-NEXT:    srlw $r6 = $r6, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srlw $r0 = $r0, $r1
; CV1-NEXT:    orw $r1 = $r3, $r6
; CV1-NEXT:    srlw $r5 = $r5, $r1
; CV1-NEXT:    srlw $r7 = $r7, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    orw $r0 = $r2, $r0
; CV1-NEXT:    orw $r3 = $r4, $r7
; CV1-NEXT:    orw $r4 = $r8, $r5
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    insf $r0 = $r4, 15, 8
; CV1-NEXT:    insf $r3 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r0 = $r3, 31, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: shlbqs_r:
; CV2:       # %bb.0:
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    negbo $r1 = $r1
; CV2-NEXT:    andw $r2 = $r1, 0x7070707
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    andw $r1 = $r1, 0x7070707
; CV2-NEXT:    sllbos $r3 = $r0, $r2
; CV2-NEXT:    extfz $r4 = $r2, 10, 8
; CV2-NEXT:    extfz $r5 = $r2, 18, 16
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sllbos $r4 = $r0, $r4
; CV2-NEXT:    sllbos $r5 = $r0, $r5
; CV2-NEXT:    srlbos $r6 = $r0, $r1
; CV2-NEXT:    extfz $r7 = $r1, 10, 8
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    extfz $r2 = $r2, 26, 24
; CV2-NEXT:    srlbos $r3 = $r0, $r7
; CV2-NEXT:    insf $r4 = $r3, 7, 0
; CV2-NEXT:    extfz $r7 = $r1, 18, 16
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    extfz $r1 = $r1, 26, 24
; CV2-NEXT:    insf $r3 = $r6, 7, 0
; CV2-NEXT:    insf $r5 = $r4, 15, 0
; CV2-NEXT:    srlbos $r6 = $r0, $r7
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    srlbos $r0 = $r0, $r1
; CV2-NEXT:    sllbos $r2 = $r0, $r2
; CV2-NEXT:    insf $r6 = $r3, 15, 0
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r0 = $r6, 23, 0
; CV2-NEXT:    insf $r2 = $r5, 23, 0
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    orw $r0 = $r2, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
  %3 = tail call <4 x i8> @llvm.kvx.shl.v4i8(<4 x i8> %0, i32 %1, i32 3)
  ret <4 x i8> %3
}

define <32 x i8> @shlbvs(<32 x i8> %0, i32 %1) {
; CV1-LABEL: shlbvs:
; CV1:       # %bb.0:
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    andd $r5 = $r0, 0xff00ff.@
; CV1-NEXT:    andd $r6 = $r1, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    andd $r3 = $r3, 0xff00ff00.@
; CV1-NEXT:    andd $r7 = $r2, 0xff00ff.@
; CV1-NEXT:    andd $r8 = $r3, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    slld $r0 = $r0, $r4
; CV1-NEXT:    slld $r1 = $r1, $r4
; CV1-NEXT:    slld $r5 = $r5, $r4
; CV1-NEXT:    slld $r6 = $r6, $r4
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    slld $r2 = $r2, $r4
; CV1-NEXT:    slld $r3 = $r3, $r4
; CV1-NEXT:    slld $r7 = $r7, $r4
; CV1-NEXT:    slld $r8 = $r8, $r4
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    andd $r4 = $r6, 0xff00ff.@
; CV1-NEXT:    andd $r5 = $r5, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    andd $r3 = $r3, 0xff00ff00.@
; CV1-NEXT:    andd $r6 = $r7, 0xff00ff.@
; CV1-NEXT:    andd $r7 = $r8, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    ord $r0 = $r0, $r5
; CV1-NEXT:    ord $r1 = $r1, $r4
; CV1-NEXT:    ord $r2 = $r2, $r6
; CV1-NEXT:    ord $r3 = $r3, $r7
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: shlbvs:
; CV2:       # %bb.0:
; CV2-NEXT:    sllbos $r0 = $r0, $r4
; CV2-NEXT:    sllbos $r1 = $r1, $r4
; CV2-NEXT:    sllbos $r2 = $r2, $r4
; CV2-NEXT:    sllbos $r3 = $r3, $r4
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %3 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %4 = tail call <8 x i8> @llvm.kvx.shl.v8i8(<8 x i8> %3, i32 %1, i32 0)
  %5 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %6 = tail call <8 x i8> @llvm.kvx.shl.v8i8(<8 x i8> %5, i32 %1, i32 0)
  %7 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %8 = tail call <8 x i8> @llvm.kvx.shl.v8i8(<8 x i8> %7, i32 %1, i32 0)
  %9 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  %10 = tail call <8 x i8> @llvm.kvx.shl.v8i8(<8 x i8> %9, i32 %1, i32 0)
  %11 = shufflevector <8 x i8> %4, <8 x i8> %6, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %12 = shufflevector <8 x i8> %8, <8 x i8> %10, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %13 = shufflevector <16 x i8> %11, <16 x i8> %12, <32 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  ret <32 x i8> %13
}

define <32 x i8> @shlbvs_s(<32 x i8> %0, i32 %1) {
; CV1-LABEL: shlbvs_s:
; CV1:       # %bb.0:
; CV1-NEXT:    extfz $r4 = $r0, 55, 48
; CV1-NEXT:    zxbd $r5 = $r4
; CV1-NEXT:    srld $r6 = $r0, 56
; CV1-NEXT:    extfz $r7 = $r0, 47, 40
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sllw $r4 = $r4, 24
; CV1-NEXT:    sllw $r6 = $r6, 24
; CV1-NEXT:    sllw $r7 = $r7, 24
; CV1-NEXT:    extfz $r8 = $r0, 39, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    slsw $r4 = $r4, $r5
; CV1-NEXT:    slsw $r6 = $r6, $r5
; CV1-NEXT:    sllw $r8 = $r8, 24
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sraw $r4 = $r4, 24
; CV1-NEXT:    sraw $r6 = $r6, 24
; CV1-NEXT:    slsw $r7 = $r7, $r5
; CV1-NEXT:    slsw $r8 = $r8, $r5
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    insf $r4 = $r6, 15, 8
; CV1-NEXT:    sraw $r6 = $r7, 24
; CV1-NEXT:    extfz $r7 = $r0, 23, 16
; CV1-NEXT:    sraw $r8 = $r8, 24
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    srlw $r6 = $r0, 24
; CV1-NEXT:    sllw $r7 = $r7, 24
; CV1-NEXT:    insf $r8 = $r6, 15, 8
; CV1-NEXT:    extfz $r9 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    sllw $r4 = $r6, 24
; CV1-NEXT:    slsw $r6 = $r7, $r5
; CV1-NEXT:    insf $r8 = $r4, 31, 16
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    sllw $r0 = $r0, 24
; CV1-NEXT:    slsw $r4 = $r4, $r5
; CV1-NEXT:    sraw $r6 = $r6, 24
; CV1-NEXT:    sllw $r7 = $r9, 24
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    sraw $r4 = $r4, 24
; CV1-NEXT:    slsw $r7 = $r7, $r5
; CV1-NEXT:    extfz $r9 = $r1, 55, 48
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    slsw $r0 = $r0, $r5
; CV1-NEXT:    srld $r4 = $r1, 56
; CV1-NEXT:    insf $r6 = $r4, 15, 8
; CV1-NEXT:    sraw $r7 = $r7, 24
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    sraw $r0 = $r0, 24
; CV1-NEXT:    sllw $r4 = $r4, 24
; CV1-NEXT:    sllw $r9 = $r9, 24
; CV1-NEXT:    extfz $r10 = $r1, 47, 40
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    insf $r0 = $r7, 15, 8
; CV1-NEXT:    slsw $r9 = $r9, $r5
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    insf $r0 = $r6, 31, 16
; CV1-NEXT:    slsw $r4 = $r4, $r5
; CV1-NEXT:    sraw $r7 = $r9, 24
; CV1-NEXT:    sllw $r9 = $r10, 24
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    insf $r0 = $r8, 63, 32
; CV1-NEXT:    sraw $r4 = $r4, 24
; CV1-NEXT:    slsw $r6 = $r9, $r5
; CV1-NEXT:    srlw $r9 = $r1, 24
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    sllw $r4 = $r9, 24
; CV1-NEXT:    insf $r7 = $r4, 15, 8
; CV1-NEXT:    extfz $r8 = $r1, 39, 32
; CV1-NEXT:    zxbd $r9 = $r1
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    slsw $r4 = $r4, $r5
; CV1-NEXT:    sraw $r6 = $r6, 24
; CV1-NEXT:    sllw $r8 = $r8, 24
; CV1-NEXT:    srld $r10 = $r2, 56
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    sllw $r4 = $r9, 24
; CV1-NEXT:    slsw $r8 = $r8, $r5
; CV1-NEXT:    sllw $r9 = $r10, 24
; CV1-NEXT:    sraw $r11 = $r4, 24
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    extfz $r1 = $r1, 15, 8
; CV1-NEXT:    sraw $r8 = $r8, 24
; CV1-NEXT:    extfz $r10 = $r1, 23, 16
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    sllw $r1 = $r1, 24
; CV1-NEXT:    slsw $r4 = $r4, $r5
; CV1-NEXT:    sllw $r6 = $r10, 24
; CV1-NEXT:    insf $r8 = $r6, 15, 8
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    slsw $r1 = $r1, $r5
; CV1-NEXT:    sraw $r4 = $r4, 24
; CV1-NEXT:    slsw $r6 = $r6, $r5
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    sraw $r1 = $r1, 24
; CV1-NEXT:    sraw $r6 = $r6, 24
; CV1-NEXT:    extfz $r7 = $r2, 47, 40
; CV1-NEXT:    insf $r8 = $r7, 31, 16
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    insf $r4 = $r1, 15, 8
; CV1-NEXT:    insf $r6 = $r11, 15, 8
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    extfz $r1 = $r2, 55, 48
; CV1-NEXT:    extfz $r10 = $r2, 39, 32
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    sllw $r1 = $r1, 24
; CV1-NEXT:    insf $r4 = $r6, 31, 16
; CV1-NEXT:    sllw $r6 = $r7, 24
; CV1-NEXT:    sllw $r7 = $r10, 24
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    slsw $r1 = $r1, $r5
; CV1-NEXT:    slsw $r9 = $r9, $r5
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    sraw $r1 = $r1, 24
; CV1-NEXT:    slsw $r6 = $r6, $r5
; CV1-NEXT:    slsw $r7 = $r7, $r5
; CV1-NEXT:    sraw $r9 = $r9, 24
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    insf $r1 = $r9, 15, 8
; CV1-NEXT:    insf $r4 = $r8, 63, 32
; CV1-NEXT:    sraw $r6 = $r6, 24
; CV1-NEXT:    sraw $r7 = $r7, 24
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    srlw $r6 = $r2, 24
; CV1-NEXT:    insf $r7 = $r6, 15, 8
; CV1-NEXT:    extfz $r8 = $r2, 23, 16
; CV1-NEXT:    ;; # (end cycle 27)
; CV1-NEXT:    zxbd $r2 = $r2
; CV1-NEXT:    sllw $r6 = $r6, 24
; CV1-NEXT:    sllw $r8 = $r8, 24
; CV1-NEXT:    extfz $r9 = $r2, 15, 8
; CV1-NEXT:    ;; # (end cycle 28)
; CV1-NEXT:    sllw $r2 = $r2, 24
; CV1-NEXT:    slsw $r6 = $r6, $r5
; CV1-NEXT:    slsw $r8 = $r8, $r5
; CV1-NEXT:    sllw $r9 = $r9, 24
; CV1-NEXT:    ;; # (end cycle 29)
; CV1-NEXT:    slsw $r2 = $r2, $r5
; CV1-NEXT:    sraw $r6 = $r6, 24
; CV1-NEXT:    sraw $r8 = $r8, 24
; CV1-NEXT:    slsw $r9 = $r9, $r5
; CV1-NEXT:    ;; # (end cycle 30)
; CV1-NEXT:    sraw $r1 = $r9, 24
; CV1-NEXT:    sraw $r2 = $r2, 24
; CV1-NEXT:    insf $r7 = $r1, 31, 16
; CV1-NEXT:    insf $r8 = $r6, 15, 8
; CV1-NEXT:    ;; # (end cycle 31)
; CV1-NEXT:    srld $r1 = $r3, 56
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    extfz $r6 = $r3, 55, 48
; CV1-NEXT:    ;; # (end cycle 32)
; CV1-NEXT:    sllw $r1 = $r1, 24
; CV1-NEXT:    sllw $r6 = $r6, 24
; CV1-NEXT:    extfz $r9 = $r3, 47, 40
; CV1-NEXT:    extfz $r10 = $r3, 39, 32
; CV1-NEXT:    ;; # (end cycle 33)
; CV1-NEXT:    slsw $r1 = $r1, $r5
; CV1-NEXT:    slsw $r6 = $r6, $r5
; CV1-NEXT:    sllw $r9 = $r9, 24
; CV1-NEXT:    sllw $r10 = $r10, 24
; CV1-NEXT:    ;; # (end cycle 34)
; CV1-NEXT:    sraw $r1 = $r1, 24
; CV1-NEXT:    sraw $r6 = $r6, 24
; CV1-NEXT:    slsw $r9 = $r9, $r5
; CV1-NEXT:    slsw $r10 = $r10, $r5
; CV1-NEXT:    ;; # (end cycle 35)
; CV1-NEXT:    sraw $r1 = $r9, 24
; CV1-NEXT:    insf $r2 = $r8, 31, 16
; CV1-NEXT:    insf $r6 = $r1, 15, 8
; CV1-NEXT:    sraw $r8 = $r10, 24
; CV1-NEXT:    ;; # (end cycle 36)
; CV1-NEXT:    srlw $r1 = $r3, 24
; CV1-NEXT:    insf $r8 = $r1, 15, 8
; CV1-NEXT:    extfz $r9 = $r3, 23, 16
; CV1-NEXT:    ;; # (end cycle 37)
; CV1-NEXT:    sllw $r1 = $r1, 24
; CV1-NEXT:    zxbd $r3 = $r3
; CV1-NEXT:    sllw $r9 = $r9, 24
; CV1-NEXT:    extfz $r10 = $r3, 15, 8
; CV1-NEXT:    ;; # (end cycle 38)
; CV1-NEXT:    slsw $r1 = $r1, $r5
; CV1-NEXT:    sllw $r3 = $r3, 24
; CV1-NEXT:    slsw $r9 = $r9, $r5
; CV1-NEXT:    sllw $r10 = $r10, 24
; CV1-NEXT:    ;; # (end cycle 39)
; CV1-NEXT:    sraw $r1 = $r1, 24
; CV1-NEXT:    slsw $r3 = $r3, $r5
; CV1-NEXT:    sraw $r5 = $r9, 24
; CV1-NEXT:    slsw $r10 = $r10, $r5
; CV1-NEXT:    ;; # (end cycle 40)
; CV1-NEXT:    sraw $r3 = $r3, 24
; CV1-NEXT:    insf $r5 = $r1, 15, 8
; CV1-NEXT:    insf $r8 = $r6, 31, 16
; CV1-NEXT:    sraw $r9 = $r10, 24
; CV1-NEXT:    ;; # (end cycle 41)
; CV1-NEXT:    copyd $r1 = $r4
; CV1-NEXT:    insf $r2 = $r7, 63, 32
; CV1-NEXT:    insf $r3 = $r9, 15, 8
; CV1-NEXT:    ;; # (end cycle 42)
; CV1-NEXT:    insf $r3 = $r5, 31, 16
; CV1-NEXT:    ;; # (end cycle 43)
; CV1-NEXT:    insf $r3 = $r8, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 44)
;
; CV2-LABEL: shlbvs_s:
; CV2:       # %bb.0:
; CV2-NEXT:    slsbos $r0 = $r0, $r4
; CV2-NEXT:    slsbos $r1 = $r1, $r4
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    slsbos $r2 = $r2, $r4
; CV2-NEXT:    slsbos $r3 = $r3, $r4
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 1)
  %3 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %4 = tail call <8 x i8> @llvm.kvx.shl.v8i8(<8 x i8> %3, i32 %1, i32 1)
  %5 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %6 = tail call <8 x i8> @llvm.kvx.shl.v8i8(<8 x i8> %5, i32 %1, i32 1)
  %7 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %8 = tail call <8 x i8> @llvm.kvx.shl.v8i8(<8 x i8> %7, i32 %1, i32 1)
  %9 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  %10 = tail call <8 x i8> @llvm.kvx.shl.v8i8(<8 x i8> %9, i32 %1, i32 1)
  %11 = shufflevector <8 x i8> %4, <8 x i8> %6, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %12 = shufflevector <8 x i8> %8, <8 x i8> %10, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %13 = shufflevector <16 x i8> %11, <16 x i8> %12, <32 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  ret <32 x i8> %13
}

define <32 x i8> @shlbvs_us(<32 x i8> %0, i32 %1) {
; CV1-LABEL: shlbvs_us:
; CV1:       # %bb.0:
; CV1-NEXT:    zxbd $r4 = $r4
; CV1-NEXT:    srld $r5 = $r0, 56
; CV1-NEXT:    extfz $r6 = $r0, 55, 48
; CV1-NEXT:    extfz $r7 = $r0, 47, 40
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sllw $r5 = $r5, 24
; CV1-NEXT:    sllw $r6 = $r6, 24
; CV1-NEXT:    sllw $r7 = $r7, 24
; CV1-NEXT:    extfz $r8 = $r0, 39, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    zxwd $r5 = $r5
; CV1-NEXT:    zxwd $r6 = $r6
; CV1-NEXT:    zxwd $r7 = $r7
; CV1-NEXT:    sllw $r8 = $r8, 24
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    slld $r5 = $r5, $r4
; CV1-NEXT:    slld $r6 = $r6, $r4
; CV1-NEXT:    slld $r7 = $r7, $r4
; CV1-NEXT:    zxwd $r8 = $r8
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    minud $r5 = $r5, 0xffffffff
; CV1-NEXT:    minud $r6 = $r6, 0xffffffff
; CV1-NEXT:    minud $r7 = $r7, 0xffffffff
; CV1-NEXT:    slld $r8 = $r8, $r4
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    srlw $r5 = $r5, 24
; CV1-NEXT:    srlw $r6 = $r6, 24
; CV1-NEXT:    minud $r8 = $r8, 0xffffffff
; CV1-NEXT:    zxbd $r9 = $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r5 = $r7, 24
; CV1-NEXT:    insf $r6 = $r5, 15, 8
; CV1-NEXT:    srlw $r7 = $r8, 24
; CV1-NEXT:    srlw $r8 = $r0, 24
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    sllw $r5 = $r8, 24
; CV1-NEXT:    insf $r7 = $r5, 15, 8
; CV1-NEXT:    extfz $r8 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    extfz $r0 = $r0, 15, 8
; CV1-NEXT:    zxwd $r5 = $r5
; CV1-NEXT:    sllw $r6 = $r8, 24
; CV1-NEXT:    insf $r7 = $r6, 31, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    sllw $r0 = $r0, 24
; CV1-NEXT:    slld $r5 = $r5, $r4
; CV1-NEXT:    zxwd $r6 = $r6
; CV1-NEXT:    sllw $r8 = $r9, 24
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    minud $r5 = $r5, 0xffffffff
; CV1-NEXT:    slld $r6 = $r6, $r4
; CV1-NEXT:    zxwd $r8 = $r8
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    slld $r0 = $r0, $r4
; CV1-NEXT:    srlw $r5 = $r5, 24
; CV1-NEXT:    minud $r6 = $r6, 0xffffffff
; CV1-NEXT:    slld $r8 = $r8, $r4
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    minud $r0 = $r0, 0xffffffff
; CV1-NEXT:    srlw $r6 = $r6, 24
; CV1-NEXT:    minud $r8 = $r8, 0xffffffff
; CV1-NEXT:    srld $r9 = $r1, 56
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    srlw $r0 = $r8, 24
; CV1-NEXT:    srlw $r5 = $r0, 24
; CV1-NEXT:    insf $r6 = $r5, 15, 8
; CV1-NEXT:    sllw $r8 = $r9, 24
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    insf $r0 = $r5, 15, 8
; CV1-NEXT:    zxwd $r5 = $r8
; CV1-NEXT:    extfz $r8 = $r1, 55, 48
; CV1-NEXT:    srlw $r9 = $r1, 24
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    insf $r0 = $r6, 31, 16
; CV1-NEXT:    slld $r5 = $r5, $r4
; CV1-NEXT:    sllw $r6 = $r8, 24
; CV1-NEXT:    extfz $r8 = $r1, 47, 40
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    insf $r0 = $r7, 63, 32
; CV1-NEXT:    zxwd $r6 = $r6
; CV1-NEXT:    sllw $r7 = $r8, 24
; CV1-NEXT:    extfz $r8 = $r1, 39, 32
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    minud $r5 = $r5, 0xffffffff
; CV1-NEXT:    slld $r6 = $r6, $r4
; CV1-NEXT:    zxwd $r7 = $r7
; CV1-NEXT:    sllw $r8 = $r8, 24
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    srlw $r5 = $r5, 24
; CV1-NEXT:    minud $r6 = $r6, 0xffffffff
; CV1-NEXT:    slld $r7 = $r7, $r4
; CV1-NEXT:    sllw $r9 = $r9, 24
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    srlw $r6 = $r6, 24
; CV1-NEXT:    minud $r7 = $r7, 0xffffffff
; CV1-NEXT:    zxwd $r8 = $r8
; CV1-NEXT:    zxwd $r9 = $r9
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    srlw $r5 = $r7, 24
; CV1-NEXT:    insf $r6 = $r5, 15, 8
; CV1-NEXT:    slld $r7 = $r8, $r4
; CV1-NEXT:    slld $r8 = $r9, $r4
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    zxbd $r1 = $r1
; CV1-NEXT:    minud $r7 = $r7, 0xffffffff
; CV1-NEXT:    extfz $r9 = $r1, 23, 16
; CV1-NEXT:    extfz $r10 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    sllw $r1 = $r1, 24
; CV1-NEXT:    srlw $r7 = $r7, 24
; CV1-NEXT:    sllw $r9 = $r9, 24
; CV1-NEXT:    sllw $r10 = $r10, 24
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    zxwd $r1 = $r1
; CV1-NEXT:    zxwd $r5 = $r9
; CV1-NEXT:    insf $r7 = $r5, 15, 8
; CV1-NEXT:    zxwd $r9 = $r10
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    slld $r1 = $r1, $r4
; CV1-NEXT:    slld $r5 = $r5, $r4
; CV1-NEXT:    slld $r6 = $r9, $r4
; CV1-NEXT:    insf $r7 = $r6, 31, 16
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    minud $r1 = $r1, 0xffffffff
; CV1-NEXT:    minud $r5 = $r5, 0xffffffff
; CV1-NEXT:    minud $r6 = $r6, 0xffffffff
; CV1-NEXT:    minud $r8 = $r8, 0xffffffff
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    srlw $r1 = $r1, 24
; CV1-NEXT:    srlw $r5 = $r5, 24
; CV1-NEXT:    srlw $r6 = $r6, 24
; CV1-NEXT:    srlw $r8 = $r8, 24
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    insf $r1 = $r6, 15, 8
; CV1-NEXT:    insf $r5 = $r8, 15, 8
; CV1-NEXT:    srld $r9 = $r2, 56
; CV1-NEXT:    ;; # (end cycle 27)
; CV1-NEXT:    insf $r1 = $r5, 31, 16
; CV1-NEXT:    extfz $r5 = $r2, 55, 48
; CV1-NEXT:    sllw $r9 = $r9, 24
; CV1-NEXT:    ;; # (end cycle 28)
; CV1-NEXT:    sllw $r5 = $r5, 24
; CV1-NEXT:    extfz $r6 = $r2, 47, 40
; CV1-NEXT:    extfz $r8 = $r2, 39, 32
; CV1-NEXT:    zxwd $r9 = $r9
; CV1-NEXT:    ;; # (end cycle 29)
; CV1-NEXT:    zxwd $r5 = $r5
; CV1-NEXT:    sllw $r6 = $r6, 24
; CV1-NEXT:    sllw $r8 = $r8, 24
; CV1-NEXT:    slld $r9 = $r9, $r4
; CV1-NEXT:    ;; # (end cycle 30)
; CV1-NEXT:    slld $r5 = $r5, $r4
; CV1-NEXT:    zxwd $r6 = $r6
; CV1-NEXT:    zxwd $r8 = $r8
; CV1-NEXT:    minud $r9 = $r9, 0xffffffff
; CV1-NEXT:    ;; # (end cycle 31)
; CV1-NEXT:    minud $r5 = $r5, 0xffffffff
; CV1-NEXT:    slld $r6 = $r6, $r4
; CV1-NEXT:    slld $r8 = $r8, $r4
; CV1-NEXT:    srlw $r9 = $r9, 24
; CV1-NEXT:    ;; # (end cycle 32)
; CV1-NEXT:    insf $r1 = $r7, 63, 32
; CV1-NEXT:    srlw $r5 = $r5, 24
; CV1-NEXT:    minud $r6 = $r6, 0xffffffff
; CV1-NEXT:    minud $r8 = $r8, 0xffffffff
; CV1-NEXT:    ;; # (end cycle 33)
; CV1-NEXT:    insf $r5 = $r9, 15, 8
; CV1-NEXT:    srlw $r6 = $r6, 24
; CV1-NEXT:    extfz $r7 = $r2, 15, 8
; CV1-NEXT:    srlw $r8 = $r8, 24
; CV1-NEXT:    ;; # (end cycle 34)
; CV1-NEXT:    extfz $r6 = $r2, 23, 16
; CV1-NEXT:    sllw $r7 = $r7, 24
; CV1-NEXT:    insf $r8 = $r6, 15, 8
; CV1-NEXT:    ;; # (end cycle 35)
; CV1-NEXT:    zxbd $r2 = $r2
; CV1-NEXT:    srlw $r5 = $r2, 24
; CV1-NEXT:    sllw $r6 = $r6, 24
; CV1-NEXT:    insf $r8 = $r5, 31, 16
; CV1-NEXT:    ;; # (end cycle 36)
; CV1-NEXT:    sllw $r2 = $r2, 24
; CV1-NEXT:    sllw $r5 = $r5, 24
; CV1-NEXT:    zxwd $r6 = $r6
; CV1-NEXT:    zxwd $r7 = $r7
; CV1-NEXT:    ;; # (end cycle 37)
; CV1-NEXT:    zxwd $r2 = $r2
; CV1-NEXT:    zxwd $r5 = $r5
; CV1-NEXT:    slld $r6 = $r6, $r4
; CV1-NEXT:    slld $r7 = $r7, $r4
; CV1-NEXT:    ;; # (end cycle 38)
; CV1-NEXT:    slld $r2 = $r2, $r4
; CV1-NEXT:    slld $r5 = $r5, $r4
; CV1-NEXT:    minud $r6 = $r6, 0xffffffff
; CV1-NEXT:    minud $r7 = $r7, 0xffffffff
; CV1-NEXT:    ;; # (end cycle 39)
; CV1-NEXT:    minud $r2 = $r2, 0xffffffff
; CV1-NEXT:    minud $r5 = $r5, 0xffffffff
; CV1-NEXT:    srlw $r6 = $r6, 24
; CV1-NEXT:    srlw $r7 = $r7, 24
; CV1-NEXT:    ;; # (end cycle 40)
; CV1-NEXT:    srlw $r2 = $r2, 24
; CV1-NEXT:    srlw $r5 = $r5, 24
; CV1-NEXT:    extfz $r9 = $r3, 39, 32
; CV1-NEXT:    extfz $r10 = $r3, 15, 8
; CV1-NEXT:    ;; # (end cycle 41)
; CV1-NEXT:    insf $r2 = $r7, 15, 8
; CV1-NEXT:    insf $r6 = $r5, 15, 8
; CV1-NEXT:    sllw $r9 = $r9, 24
; CV1-NEXT:    sllw $r10 = $r10, 24
; CV1-NEXT:    ;; # (end cycle 42)
; CV1-NEXT:    insf $r2 = $r6, 31, 16
; CV1-NEXT:    extfz $r5 = $r3, 55, 48
; CV1-NEXT:    srld $r6 = $r3, 56
; CV1-NEXT:    zxwd $r9 = $r9
; CV1-NEXT:    ;; # (end cycle 43)
; CV1-NEXT:    sllw $r5 = $r5, 24
; CV1-NEXT:    sllw $r6 = $r6, 24
; CV1-NEXT:    extfz $r7 = $r3, 47, 40
; CV1-NEXT:    slld $r9 = $r9, $r4
; CV1-NEXT:    ;; # (end cycle 44)
; CV1-NEXT:    zxwd $r5 = $r5
; CV1-NEXT:    zxwd $r6 = $r6
; CV1-NEXT:    sllw $r7 = $r7, 24
; CV1-NEXT:    minud $r9 = $r9, 0xffffffff
; CV1-NEXT:    ;; # (end cycle 45)
; CV1-NEXT:    slld $r5 = $r5, $r4
; CV1-NEXT:    slld $r6 = $r6, $r4
; CV1-NEXT:    zxwd $r7 = $r7
; CV1-NEXT:    srlw $r9 = $r9, 24
; CV1-NEXT:    ;; # (end cycle 46)
; CV1-NEXT:    minud $r5 = $r5, 0xffffffff
; CV1-NEXT:    minud $r6 = $r6, 0xffffffff
; CV1-NEXT:    slld $r7 = $r7, $r4
; CV1-NEXT:    zxwd $r10 = $r10
; CV1-NEXT:    ;; # (end cycle 47)
; CV1-NEXT:    srlw $r5 = $r5, 24
; CV1-NEXT:    srlw $r6 = $r6, 24
; CV1-NEXT:    minud $r7 = $r7, 0xffffffff
; CV1-NEXT:    slld $r10 = $r10, $r4
; CV1-NEXT:    ;; # (end cycle 48)
; CV1-NEXT:    insf $r2 = $r8, 63, 32
; CV1-NEXT:    insf $r5 = $r6, 15, 8
; CV1-NEXT:    srlw $r6 = $r3, 24
; CV1-NEXT:    srlw $r7 = $r7, 24
; CV1-NEXT:    ;; # (end cycle 49)
; CV1-NEXT:    zxbd $r3 = $r3
; CV1-NEXT:    sllw $r6 = $r6, 24
; CV1-NEXT:    extfz $r7 = $r3, 23, 16
; CV1-NEXT:    insf $r9 = $r7, 15, 8
; CV1-NEXT:    ;; # (end cycle 50)
; CV1-NEXT:    sllw $r3 = $r3, 24
; CV1-NEXT:    zxwd $r6 = $r6
; CV1-NEXT:    sllw $r7 = $r7, 24
; CV1-NEXT:    insf $r9 = $r5, 31, 16
; CV1-NEXT:    ;; # (end cycle 51)
; CV1-NEXT:    zxwd $r3 = $r3
; CV1-NEXT:    slld $r6 = $r6, $r4
; CV1-NEXT:    zxwd $r7 = $r7
; CV1-NEXT:    ;; # (end cycle 52)
; CV1-NEXT:    slld $r3 = $r3, $r4
; CV1-NEXT:    minud $r4 = $r6, 0xffffffff
; CV1-NEXT:    slld $r7 = $r7, $r4
; CV1-NEXT:    ;; # (end cycle 53)
; CV1-NEXT:    minud $r3 = $r3, 0xffffffff
; CV1-NEXT:    srlw $r4 = $r4, 24
; CV1-NEXT:    minud $r6 = $r7, 0xffffffff
; CV1-NEXT:    minud $r7 = $r10, 0xffffffff
; CV1-NEXT:    ;; # (end cycle 54)
; CV1-NEXT:    srlw $r3 = $r3, 24
; CV1-NEXT:    srlw $r6 = $r6, 24
; CV1-NEXT:    srlw $r7 = $r7, 24
; CV1-NEXT:    ;; # (end cycle 55)
; CV1-NEXT:    insf $r3 = $r7, 15, 8
; CV1-NEXT:    insf $r6 = $r4, 15, 8
; CV1-NEXT:    ;; # (end cycle 56)
; CV1-NEXT:    insf $r3 = $r6, 31, 16
; CV1-NEXT:    ;; # (end cycle 57)
; CV1-NEXT:    insf $r3 = $r9, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 58)
;
; CV2-LABEL: shlbvs_us:
; CV2:       # %bb.0:
; CV2-NEXT:    slusbos $r0 = $r0, $r4
; CV2-NEXT:    slusbos $r1 = $r1, $r4
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    slusbos $r2 = $r2, $r4
; CV2-NEXT:    slusbos $r3 = $r3, $r4
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 1)
  %3 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %4 = tail call <8 x i8> @llvm.kvx.shl.v8i8(<8 x i8> %3, i32 %1, i32 2)
  %5 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %6 = tail call <8 x i8> @llvm.kvx.shl.v8i8(<8 x i8> %5, i32 %1, i32 2)
  %7 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %8 = tail call <8 x i8> @llvm.kvx.shl.v8i8(<8 x i8> %7, i32 %1, i32 2)
  %9 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  %10 = tail call <8 x i8> @llvm.kvx.shl.v8i8(<8 x i8> %9, i32 %1, i32 2)
  %11 = shufflevector <8 x i8> %4, <8 x i8> %6, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %12 = shufflevector <8 x i8> %8, <8 x i8> %10, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %13 = shufflevector <16 x i8> %11, <16 x i8> %12, <32 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  ret <32 x i8> %13
}

define <32 x i8> @shlbvs_r(<32 x i8> %0, i32 %1) {
; CV1-LABEL: shlbvs_r:
; CV1:       # %bb.0:
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x8080202008080202
; CV1-NEXT:    sbmm8 $r5 = $r0, 0x4040101004040101
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x8080202008080202
; CV1-NEXT:    sbmm8 $r6 = $r1, 0x4040101004040101
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r2 = $r2, 0x8080202008080202
; CV1-NEXT:    sbmm8 $r7 = $r2, 0x4040101004040101
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sbmm8 $r3 = $r3, 0x8080202008080202
; CV1-NEXT:    sbmm8 $r8 = $r3, 0x4040101004040101
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sllhqs $r0 = $r0, $r4
; CV1-NEXT:    sllhqs $r5 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r1 = $r1, $r4
; CV1-NEXT:    sllhqs $r6 = $r6, $r4
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r2 = $r2, $r4
; CV1-NEXT:    sllhqs $r7 = $r7, $r4
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r3, $r4
; CV1-NEXT:    sllhqs $r8 = $r8, $r4
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    andd $r3 = $r3, 0xff00ff00.@
; CV1-NEXT:    srlhqs $r4 = $r6, 8
; CV1-NEXT:    srlhqs $r5 = $r5, 8
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    ord $r0 = $r0, $r5
; CV1-NEXT:    ord $r1 = $r1, $r4
; CV1-NEXT:    srlhqs $r6 = $r7, 8
; CV1-NEXT:    srlhqs $r7 = $r8, 8
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    ord $r2 = $r2, $r6
; CV1-NEXT:    ord $r3 = $r3, $r7
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 10)
;
; CV2-LABEL: shlbvs_r:
; CV2:       # %bb.0:
; CV2-NEXT:    sbmm8 $r0 = $r0, 0x8080202008080202
; CV2-NEXT:    sbmm8 $r5 = $r0, 0x4040101004040101
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sllhqs $r0 = $r0, $r4
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x8080202008080202
; CV2-NEXT:    sllhqs $r5 = $r5, $r4
; CV2-NEXT:    sbmm8 $r6 = $r1, 0x4040101004040101
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sllhqs $r1 = $r1, $r4
; CV2-NEXT:    sbmm8 $r2 = $r2, 0x8080202008080202
; CV2-NEXT:    sllhqs $r6 = $r6, $r4
; CV2-NEXT:    sbmm8 $r7 = $r2, 0x4040101004040101
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sllhqs $r2 = $r2, $r4
; CV2-NEXT:    sbmm8 $r3 = $r3, 0x8080202008080202
; CV2-NEXT:    sllhqs $r7 = $r7, $r4
; CV2-NEXT:    sbmm8 $r8 = $r3, 0x4040101004040101
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV2-NEXT:    sllhqs $r3 = $r3, $r4
; CV2-NEXT:    srlhqs $r5 = $r5, 8
; CV2-NEXT:    sllhqs $r8 = $r8, $r4
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV2-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV2-NEXT:    srlhqs $r4 = $r6, 8
; CV2-NEXT:    srlhqs $r6 = $r7, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    ord $r0 = $r0, $r5
; CV2-NEXT:    ord $r1 = $r1, $r4
; CV2-NEXT:    andd $r3 = $r3, 0xff00ff00.@
; CV2-NEXT:    srlhqs $r7 = $r8, 8
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    ord $r2 = $r2, $r6
; CV2-NEXT:    ord $r3 = $r3, $r7
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %3 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %4 = tail call <8 x i8> @llvm.kvx.shl.v8i8(<8 x i8> %3, i32 %1, i32 3)
  %5 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %6 = tail call <8 x i8> @llvm.kvx.shl.v8i8(<8 x i8> %5, i32 %1, i32 3)
  %7 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %8 = tail call <8 x i8> @llvm.kvx.shl.v8i8(<8 x i8> %7, i32 %1, i32 3)
  %9 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  %10 = tail call <8 x i8> @llvm.kvx.shl.v8i8(<8 x i8> %9, i32 %1, i32 3)
  %11 = shufflevector <8 x i8> %4, <8 x i8> %6, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %12 = shufflevector <8 x i8> %8, <8 x i8> %10, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %13 = shufflevector <16 x i8> %11, <16 x i8> %12, <32 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  ret <32 x i8> %13
}

define <16 x i8> @shlbxs(<16 x i8> %0, i32 %1) {
; CV1-LABEL: shlbxs:
; CV1:       # %bb.0:
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    andd $r3 = $r0, 0xff00ff.@
; CV1-NEXT:    andd $r4 = $r1, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    slld $r0 = $r0, $r2
; CV1-NEXT:    slld $r1 = $r1, $r2
; CV1-NEXT:    slld $r3 = $r3, $r2
; CV1-NEXT:    slld $r4 = $r4, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    andd $r2 = $r3, 0xff00ff.@
; CV1-NEXT:    andd $r3 = $r4, 0xff00ff.@
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    ord $r0 = $r0, $r2
; CV1-NEXT:    ord $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: shlbxs:
; CV2:       # %bb.0:
; CV2-NEXT:    sllbos $r0 = $r0, $r2
; CV2-NEXT:    sllbos $r1 = $r1, $r2
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %3 = shufflevector <16 x i8> %0, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %4 = tail call <8 x i8> @llvm.kvx.shl.v8i8(<8 x i8> %3, i32 %1, i32 0)
  %5 = shufflevector <16 x i8> %0, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %6 = tail call <8 x i8> @llvm.kvx.shl.v8i8(<8 x i8> %5, i32 %1, i32 0)
  %7 = shufflevector <8 x i8> %4, <8 x i8> %6, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  ret <16 x i8> %7
}

define <16 x i8> @shlbxs_s(<16 x i8> %0, i32 %1) {
; CV1-LABEL: shlbxs_s:
; CV1:       # %bb.0:
; CV1-NEXT:    zxbd $r2 = $r2
; CV1-NEXT:    extfz $r3 = $r0, 55, 48
; CV1-NEXT:    srld $r4 = $r0, 56
; CV1-NEXT:    extfz $r5 = $r0, 47, 40
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sllw $r3 = $r3, 24
; CV1-NEXT:    sllw $r4 = $r4, 24
; CV1-NEXT:    sllw $r5 = $r5, 24
; CV1-NEXT:    srlw $r6 = $r0, 24
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    slsw $r3 = $r3, $r2
; CV1-NEXT:    slsw $r4 = $r4, $r2
; CV1-NEXT:    sllw $r6 = $r6, 24
; CV1-NEXT:    zxbd $r7 = $r0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sraw $r3 = $r3, 24
; CV1-NEXT:    sraw $r4 = $r4, 24
; CV1-NEXT:    slsw $r5 = $r5, $r2
; CV1-NEXT:    extfz $r9 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    insf $r3 = $r4, 15, 8
; CV1-NEXT:    extfz $r4 = $r0, 39, 32
; CV1-NEXT:    sraw $r5 = $r5, 24
; CV1-NEXT:    sllw $r7 = $r7, 24
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    extfz $r0 = $r0, 15, 8
; CV1-NEXT:    sllw $r4 = $r4, 24
; CV1-NEXT:    slsw $r6 = $r6, $r2
; CV1-NEXT:    sllw $r9 = $r9, 24
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sllw $r0 = $r0, 24
; CV1-NEXT:    slsw $r4 = $r4, $r2
; CV1-NEXT:    sraw $r6 = $r6, 24
; CV1-NEXT:    slsw $r9 = $r9, $r2
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    sraw $r4 = $r4, 24
; CV1-NEXT:    slsw $r7 = $r7, $r2
; CV1-NEXT:    srld $r8 = $r1, 56
; CV1-NEXT:    sraw $r9 = $r9, 24
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    sraw $r0 = $r7, 24
; CV1-NEXT:    insf $r4 = $r5, 15, 8
; CV1-NEXT:    slsw $r5 = $r0, $r2
; CV1-NEXT:    sllw $r8 = $r8, 24
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    sraw $r3 = $r5, 24
; CV1-NEXT:    insf $r4 = $r3, 31, 16
; CV1-NEXT:    insf $r9 = $r6, 15, 8
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r0 = $r3, 15, 8
; CV1-NEXT:    extfz $r3 = $r1, 55, 48
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sllw $r3 = $r3, 24
; CV1-NEXT:    extfz $r5 = $r1, 47, 40
; CV1-NEXT:    extfz $r6 = $r1, 39, 32
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    slsw $r3 = $r3, $r2
; CV1-NEXT:    sllw $r5 = $r5, 24
; CV1-NEXT:    sllw $r6 = $r6, 24
; CV1-NEXT:    slsw $r7 = $r8, $r2
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    sraw $r3 = $r3, 24
; CV1-NEXT:    slsw $r5 = $r5, $r2
; CV1-NEXT:    slsw $r6 = $r6, $r2
; CV1-NEXT:    sraw $r7 = $r7, 24
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    insf $r3 = $r7, 15, 8
; CV1-NEXT:    sraw $r5 = $r5, 24
; CV1-NEXT:    sraw $r6 = $r6, 24
; CV1-NEXT:    extfz $r7 = $r1, 23, 16
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    zxbd $r1 = $r1
; CV1-NEXT:    srlw $r5 = $r1, 24
; CV1-NEXT:    insf $r6 = $r5, 15, 8
; CV1-NEXT:    extfz $r8 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    sllw $r1 = $r1, 24
; CV1-NEXT:    sllw $r5 = $r5, 24
; CV1-NEXT:    sllw $r7 = $r7, 24
; CV1-NEXT:    sllw $r8 = $r8, 24
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    slsw $r5 = $r5, $r2
; CV1-NEXT:    slsw $r7 = $r7, $r2
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    slsw $r1 = $r1, $r2
; CV1-NEXT:    sraw $r2 = $r5, 24
; CV1-NEXT:    sraw $r5 = $r7, 24
; CV1-NEXT:    slsw $r8 = $r8, $r2
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    insf $r0 = $r9, 31, 16
; CV1-NEXT:    sraw $r1 = $r1, 24
; CV1-NEXT:    insf $r5 = $r2, 15, 8
; CV1-NEXT:    sraw $r7 = $r8, 24
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    insf $r1 = $r7, 15, 8
; CV1-NEXT:    insf $r6 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    insf $r0 = $r4, 63, 32
; CV1-NEXT:    insf $r1 = $r5, 31, 16
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    insf $r1 = $r6, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 22)
;
; CV2-LABEL: shlbxs_s:
; CV2:       # %bb.0:
; CV2-NEXT:    slsbos $r0 = $r0, $r2
; CV2-NEXT:    slsbos $r1 = $r1, $r2
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %3 = shufflevector <16 x i8> %0, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %4 = tail call <8 x i8> @llvm.kvx.shl.v8i8(<8 x i8> %3, i32 %1, i32 1)
  %5 = shufflevector <16 x i8> %0, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %6 = tail call <8 x i8> @llvm.kvx.shl.v8i8(<8 x i8> %5, i32 %1, i32 1)
  %7 = shufflevector <8 x i8> %4, <8 x i8> %6, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  ret <16 x i8> %7
}

define <16 x i8> @shlbxs_us(<16 x i8> %0, i32 %1) {
; CV1-LABEL: shlbxs_us:
; CV1:       # %bb.0:
; CV1-NEXT:    zxbd $r2 = $r2
; CV1-NEXT:    srld $r3 = $r0, 56
; CV1-NEXT:    extfz $r4 = $r0, 55, 48
; CV1-NEXT:    extfz $r5 = $r0, 47, 40
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sllw $r3 = $r3, 24
; CV1-NEXT:    sllw $r4 = $r4, 24
; CV1-NEXT:    sllw $r5 = $r5, 24
; CV1-NEXT:    zxbd $r7 = $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    zxwd $r3 = $r3
; CV1-NEXT:    zxwd $r4 = $r4
; CV1-NEXT:    zxwd $r5 = $r5
; CV1-NEXT:    srlw $r6 = $r0, 24
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    slld $r3 = $r3, $r2
; CV1-NEXT:    slld $r4 = $r4, $r2
; CV1-NEXT:    sllw $r7 = $r7, 24
; CV1-NEXT:    extfz $r9 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    minud $r3 = $r3, 0xffffffff
; CV1-NEXT:    minud $r4 = $r4, 0xffffffff
; CV1-NEXT:    slld $r5 = $r5, $r2
; CV1-NEXT:    sllw $r6 = $r6, 24
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    srlw $r3 = $r3, 24
; CV1-NEXT:    srlw $r4 = $r4, 24
; CV1-NEXT:    zxwd $r7 = $r7
; CV1-NEXT:    sllw $r9 = $r9, 24
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    extfz $r3 = $r0, 39, 32
; CV1-NEXT:    insf $r4 = $r3, 15, 8
; CV1-NEXT:    minud $r5 = $r5, 0xffffffff
; CV1-NEXT:    zxwd $r6 = $r6
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    extfz $r0 = $r0, 15, 8
; CV1-NEXT:    sllw $r3 = $r3, 24
; CV1-NEXT:    slld $r7 = $r7, $r2
; CV1-NEXT:    zxwd $r9 = $r9
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    sllw $r0 = $r0, 24
; CV1-NEXT:    zxwd $r3 = $r3
; CV1-NEXT:    srlw $r5 = $r5, 24
; CV1-NEXT:    slld $r6 = $r6, $r2
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    slld $r3 = $r3, $r2
; CV1-NEXT:    minud $r7 = $r7, 0xffffffff
; CV1-NEXT:    slld $r9 = $r9, $r2
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    minud $r3 = $r3, 0xffffffff
; CV1-NEXT:    minud $r6 = $r6, 0xffffffff
; CV1-NEXT:    minud $r9 = $r9, 0xffffffff
; CV1-NEXT:    slld $r10 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    srlw $r0 = $r7, 24
; CV1-NEXT:    srlw $r3 = $r3, 24
; CV1-NEXT:    srlw $r6 = $r6, 24
; CV1-NEXT:    minud $r7 = $r10, 0xffffffff
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    insf $r3 = $r5, 15, 8
; CV1-NEXT:    srlw $r5 = $r9, 24
; CV1-NEXT:    srld $r8 = $r1, 56
; CV1-NEXT:    extfz $r9 = $r1, 55, 48
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    insf $r3 = $r4, 31, 16
; CV1-NEXT:    srlw $r4 = $r7, 24
; CV1-NEXT:    insf $r5 = $r6, 15, 8
; CV1-NEXT:    sllw $r8 = $r8, 24
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    insf $r0 = $r4, 15, 8
; CV1-NEXT:    extfz $r4 = $r1, 47, 40
; CV1-NEXT:    zxwd $r7 = $r8
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    insf $r0 = $r5, 31, 16
; CV1-NEXT:    sllw $r4 = $r4, 24
; CV1-NEXT:    sllw $r5 = $r9, 24
; CV1-NEXT:    extfz $r6 = $r1, 39, 32
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    zxwd $r4 = $r4
; CV1-NEXT:    zxwd $r5 = $r5
; CV1-NEXT:    sllw $r6 = $r6, 24
; CV1-NEXT:    slld $r7 = $r7, $r2
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    slld $r4 = $r4, $r2
; CV1-NEXT:    slld $r5 = $r5, $r2
; CV1-NEXT:    zxwd $r6 = $r6
; CV1-NEXT:    minud $r7 = $r7, 0xffffffff
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    minud $r4 = $r4, 0xffffffff
; CV1-NEXT:    minud $r5 = $r5, 0xffffffff
; CV1-NEXT:    slld $r6 = $r6, $r2
; CV1-NEXT:    srlw $r7 = $r7, 24
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    srlw $r4 = $r4, 24
; CV1-NEXT:    srlw $r5 = $r5, 24
; CV1-NEXT:    minud $r6 = $r6, 0xffffffff
; CV1-NEXT:    extfz $r8 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    insf $r5 = $r7, 15, 8
; CV1-NEXT:    srlw $r6 = $r6, 24
; CV1-NEXT:    extfz $r7 = $r1, 23, 16
; CV1-NEXT:    sllw $r8 = $r8, 24
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    zxbd $r1 = $r1
; CV1-NEXT:    srlw $r4 = $r1, 24
; CV1-NEXT:    insf $r6 = $r4, 15, 8
; CV1-NEXT:    sllw $r7 = $r7, 24
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    sllw $r1 = $r1, 24
; CV1-NEXT:    sllw $r4 = $r4, 24
; CV1-NEXT:    zxwd $r7 = $r7
; CV1-NEXT:    zxwd $r8 = $r8
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    zxwd $r1 = $r1
; CV1-NEXT:    zxwd $r4 = $r4
; CV1-NEXT:    slld $r7 = $r7, $r2
; CV1-NEXT:    slld $r8 = $r8, $r2
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    insf $r0 = $r3, 63, 32
; CV1-NEXT:    slld $r1 = $r1, $r2
; CV1-NEXT:    slld $r4 = $r4, $r2
; CV1-NEXT:    insf $r6 = $r5, 31, 16
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    minud $r1 = $r1, 0xffffffff
; CV1-NEXT:    minud $r2 = $r4, 0xffffffff
; CV1-NEXT:    minud $r4 = $r7, 0xffffffff
; CV1-NEXT:    minud $r7 = $r8, 0xffffffff
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    srlw $r1 = $r1, 24
; CV1-NEXT:    srlw $r2 = $r2, 24
; CV1-NEXT:    srlw $r4 = $r4, 24
; CV1-NEXT:    srlw $r7 = $r7, 24
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    insf $r1 = $r7, 15, 8
; CV1-NEXT:    insf $r4 = $r2, 15, 8
; CV1-NEXT:    ;; # (end cycle 27)
; CV1-NEXT:    insf $r1 = $r4, 31, 16
; CV1-NEXT:    ;; # (end cycle 28)
; CV1-NEXT:    insf $r1 = $r6, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 29)
;
; CV2-LABEL: shlbxs_us:
; CV2:       # %bb.0:
; CV2-NEXT:    slusbos $r0 = $r0, $r2
; CV2-NEXT:    slusbos $r1 = $r1, $r2
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %3 = shufflevector <16 x i8> %0, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %4 = tail call <8 x i8> @llvm.kvx.shl.v8i8(<8 x i8> %3, i32 %1, i32 2)
  %5 = shufflevector <16 x i8> %0, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %6 = tail call <8 x i8> @llvm.kvx.shl.v8i8(<8 x i8> %5, i32 %1, i32 2)
  %7 = shufflevector <8 x i8> %4, <8 x i8> %6, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  ret <16 x i8> %7
}

define <16 x i8> @shlbxs_r(<16 x i8> %0, i32 %1) {
; CV1-LABEL: shlbxs_r:
; CV1:       # %bb.0:
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x8080202008080202
; CV1-NEXT:    sbmm8 $r3 = $r0, 0x4040101004040101
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x8080202008080202
; CV1-NEXT:    sbmm8 $r4 = $r1, 0x4040101004040101
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sllhqs $r0 = $r0, $r2
; CV1-NEXT:    sllhqs $r3 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r1 = $r1, $r2
; CV1-NEXT:    sllhqs $r4 = $r4, $r2
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    srlhqs $r2 = $r3, 8
; CV1-NEXT:    srlhqs $r3 = $r4, 8
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    ord $r0 = $r0, $r2
; CV1-NEXT:    ord $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 5)
;
; CV2-LABEL: shlbxs_r:
; CV2:       # %bb.0:
; CV2-NEXT:    sbmm8 $r0 = $r0, 0x8080202008080202
; CV2-NEXT:    sbmm8 $r3 = $r0, 0x4040101004040101
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sllhqs $r0 = $r0, $r2
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x8080202008080202
; CV2-NEXT:    sllhqs $r3 = $r3, $r2
; CV2-NEXT:    sbmm8 $r4 = $r1, 0x4040101004040101
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV2-NEXT:    sllhqs $r1 = $r1, $r2
; CV2-NEXT:    srlhqs $r2 = $r3, 8
; CV2-NEXT:    sllhqs $r4 = $r4, $r2
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    ord $r0 = $r0, $r2
; CV2-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV2-NEXT:    srlhqs $r3 = $r4, 8
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    ord $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 4)
  %3 = shufflevector <16 x i8> %0, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %4 = tail call <8 x i8> @llvm.kvx.shl.v8i8(<8 x i8> %3, i32 %1, i32 3)
  %5 = shufflevector <16 x i8> %0, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %6 = tail call <8 x i8> @llvm.kvx.shl.v8i8(<8 x i8> %5, i32 %1, i32 3)
  %7 = shufflevector <8 x i8> %4, <8 x i8> %6, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  ret <16 x i8> %7
}

