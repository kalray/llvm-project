; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -O3 -o - %s -mcpu=kv3-1 | FileCheck %s --check-prefixes=CV1
; RUN: llc -O3 -o - %s -mcpu=kv3-2 | FileCheck %s --check-prefixes=CV2
; RUN: clang -O3 -march=kv3-1 -c -o /dev/null %s
; RUN: clang -O3 -march=kv3-2 -c -o /dev/null %s
; Check the correct order of the swapvo
; TODO: Remove unecessary sv/lv

target triple = "kvx-kalray-cos"

@.str = private constant [44 x i8] c"{ %ld, %ld, %ld, %ld } == { 0, 1, 2, 3 } ?\0A\00"
@.str.1 = private constant [44 x i8] c"{ %ld, %ld, %ld, %ld } == { 1, 2, 3, 4 } ?\0A\00"

define i32 @main() {
; CV1-LABEL: main:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    make $r5 = 1
; CV1-NEXT:    addd $r12 = $r12, -64
; CV1-NEXT:    get $r16 = $ra
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sd 56[$r12] = $r16
; CV1-NEXT:    make $r6 = 2
; CV1-NEXT:    make $r7 = 3
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    copyd $r0 = $r5
; CV1-NEXT:    copyd $r1 = $r6
; CV1-NEXT:    copyd $r2 = $r7
; CV1-NEXT:    make $r3 = 4
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    xmovetq $a0.lo = $r0, $r1
; CV1-NEXT:    xmovetq $a0.hi = $r2, $r3
; CV1-NEXT:    xmovefo $r0r1r2r3 = $a0
; CV1-NEXT:    make $r4 = 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    make $r0 = .L.str
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    xso 0[$r12] = $a0
; CV1-NEXT:    copyd $r1 = $r4
; CV1-NEXT:    copyd $r2 = $r5
; CV1-NEXT:    copyd $r3 = $r6
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    copyd $r4 = $r7
; CV1-NEXT:    call printf
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    xlo.u $a0 = 0[$r12]
; CV1-NEXT:    make $r0 = .L.str.1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    xmovefo $r4r5r6r7 = $a0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    copyd $r1 = $r4
; CV1-NEXT:    copyd $r2 = $r5
; CV1-NEXT:    copyd $r3 = $r6
; CV1-NEXT:    copyd $r4 = $r7
; CV1-NEXT:    call printf
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    make $r0 = 0
; CV1-NEXT:    ld $r16 = 56[$r12]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    set $ra = $r16
; CV1-NEXT:    addd $r12 = $r12, 64
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: main:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    make $r5 = 1
; CV2-NEXT:    addd $r12 = $r12, -64
; CV2-NEXT:    get $r16 = $ra
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sd 56[$r12] = $r16
; CV2-NEXT:    make $r6 = 2
; CV2-NEXT:    make $r7 = 3
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    copyd $r0 = $r5
; CV2-NEXT:    copyd $r1 = $r6
; CV2-NEXT:    copyd $r2 = $r7
; CV2-NEXT:    make $r3 = 4
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    xmovetq $a0.lo = $r0, $r1
; CV2-NEXT:    xmovetq $a0.hi = $r2, $r3
; CV2-NEXT:    xmovefo $r0r1r2r3 = $a0
; CV2-NEXT:    make $r4 = 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    make $r0 = .L.str
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    xso 0[$r12] = $a0
; CV2-NEXT:    copyd $r1 = $r4
; CV2-NEXT:    copyd $r2 = $r5
; CV2-NEXT:    copyd $r3 = $r6
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    copyd $r4 = $r7
; CV2-NEXT:    call printf
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    xlo $a0 = 0[$r12]
; CV2-NEXT:    make $r0 = .L.str.1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    xmovefo $r4r5r6r7 = $a0
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    copyd $r1 = $r4
; CV2-NEXT:    copyd $r2 = $r5
; CV2-NEXT:    copyd $r3 = $r6
; CV2-NEXT:    copyd $r4 = $r7
; CV2-NEXT:    call printf
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    make $r0 = 0
; CV2-NEXT:    ld $r16 = 56[$r12]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    set $ra = $r16
; CV2-NEXT:    addd $r12 = $r12, 64
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
entry:
  %0 = tail call <256 x i1> @llvm.kvx.xmoveto256(<4 x i64> <i64 0, i64 1, i64 2, i64 3>)
  %1 = tail call { <4 x i64>, <256 x i1> } @llvm.kvx.xswapo256(<4 x i64> <i64 1, i64 2, i64 3, i64 4>, <256 x i1> undef)
  %2 = extractvalue { <4 x i64>, <256 x i1> } %1, 1
  %3 = tail call <4 x i64> @llvm.kvx.xmovefo256(<256 x i1> %0)
  %vecext = extractelement <4 x i64> %3, i32 0
  %vecext1 = extractelement <4 x i64> %3, i32 1
  %vecext2 = extractelement <4 x i64> %3, i32 2
  %vecext3 = extractelement <4 x i64> %3, i32 3
  %call = tail call i32 (i8*, ...) @printf(i8* nonnull dereferenceable(1) getelementptr inbounds ([44 x i8], [44 x i8]* @.str, i64 0, i64 0), i64 %vecext, i64 %vecext1, i64 %vecext2, i64 %vecext3)
  %4 = tail call <4 x i64> @llvm.kvx.xmovefo256(<256 x i1> %2)
  %vecext4 = extractelement <4 x i64> %4, i32 0
  %vecext5 = extractelement <4 x i64> %4, i32 1
  %vecext6 = extractelement <4 x i64> %4, i32 2
  %vecext7 = extractelement <4 x i64> %4, i32 3
  %call8 = tail call i32 (i8*, ...) @printf(i8* nonnull dereferenceable(1) getelementptr inbounds ([44 x i8], [44 x i8]* @.str.1, i64 0, i64 0), i64 %vecext4, i64 %vecext5, i64 %vecext6, i64 %vecext7)
  ret i32 0
}
declare <256 x i1> @llvm.kvx.xmoveto256(<4 x i64>)
declare { <4 x i64>, <256 x i1> } @llvm.kvx.xswapo256(<4 x i64>, <256 x i1>)
declare <4 x i64> @llvm.kvx.xmovefo256(<256 x i1>)
declare i32 @printf(i8* nocapture readonly, ...)
