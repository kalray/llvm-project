; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -O3 -o - %s | FileCheck %s
; RUN: clang -O3 -c -o /dev/null %s

target triple = "kvx-kalray-cos"

declare <256 x i1> @llvm.kvx.xmovetq(<256 x i1>, i64, i64, i32)

define void @test_movetohi(i64 %a, i64 %b, ptr %p0, ptr %p1) {
; CHECK-LABEL: test_movetohi:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xlo.u $a0 = 0[$r2]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo.u $a1 = 0[$r3]
; CHECK-NEXT:    xmovetq $a0.hi = $r1, $r0
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xmovetq $a1.hi = $r0, $r1
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    xso 0[$r2] = $a0
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xso 0[$r3] = $a1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 6)
entry:
  %v0 = load <256 x i1>, ptr %p0, align 32
  %v1 = load <256 x i1>, ptr %p1, align 32
  %v2 = tail call <256 x i1> @llvm.kvx.xmovetq(<256 x i1> %v1, i64 %a, i64 %b, i32 1)
  %v3 = tail call <256 x i1> @llvm.kvx.xmovetq(<256 x i1> %v0, i64 %b, i64 %a, i32 1)
  store <256 x i1> %v3, ptr %p0, align 32
  store <256 x i1> %v2, ptr %p1, align 32
  ret void
}

define void @test_movetolo(i64 %a, i64 %b, ptr %p0, ptr %p1) {
; CHECK-LABEL: test_movetolo:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xlo.u $a0 = 0[$r2]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo.u $a1 = 0[$r3]
; CHECK-NEXT:    xmovetq $a0.lo = $r1, $r0
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xmovetq $a1.lo = $r0, $r1
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    xso 0[$r2] = $a0
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xso 0[$r3] = $a1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 6)
entry:
  %v0 = load <256 x i1>, ptr %p0, align 32
  %v1 = load <256 x i1>, ptr %p1, align 32
  %v2 = tail call <256 x i1> @llvm.kvx.xmovetq(<256 x i1> %v1, i64 %a, i64 %b, i32 0)
  %v3 = tail call <256 x i1> @llvm.kvx.xmovetq(<256 x i1> %v0, i64 %b, i64 %a, i32 0)
  store <256 x i1> %v3, ptr %p0, align 32
  store <256 x i1> %v2, ptr %p1, align 32
  ret void
}

define void @test_movetohilo(i64 %a, i64 %b, i64 %c, i64 %d, ptr %p0) {
; CHECK-LABEL: test_movetohilo:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xmovetq $a0.lo = $r0, $r1
; CHECK-NEXT:    xmovetq $a0.hi = $r2, $r3
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xso 0[$r4] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 4)
  %v1 = tail call <256 x i1> @llvm.kvx.xmovetq(<256 x i1> undef, i64 %a, i64 %b, i32 0)
  %v2 = tail call <256 x i1> @llvm.kvx.xmovetq(<256 x i1> %v1, i64 %c, i64 %d, i32 1)
  store <256 x i1> %v2, ptr %p0, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.xmoveto(i64, i64, i64, i64)

define void @test_moveto(i64 %a, i64 %b, i64 %c, i64 %d, ptr %p0, ptr %p1) {
; CHECK-LABEL: test_moveto:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xmovetq $a0.lo = $r2, $r3
; CHECK-NEXT:    xmovetq $a0.hi = $r0, $r1
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xmovetq $a1.lo = $r1, $r0
; CHECK-NEXT:    xmovetq $a1.hi = $r3, $r2
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xso 0[$r4] = $a0
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 0[$r5] = $a1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 5)
  %v1 = tail call <256 x i1> @llvm.kvx.xmoveto(i64 %a, i64 %b, i64 %c, i64 %d)
  %v2 = tail call <256 x i1> @llvm.kvx.xmoveto(i64 %d, i64 %c, i64 %b, i64 %a)
  store <256 x i1> %v1, ptr %p0, align 32
  store <256 x i1> %v2, ptr %p1, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.xmoveto256(<4 x i64>)

define void @test_moveoto(<4 x i64> %r, ptr %p0) {
; CHECK-LABEL: test_moveoto:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xmovetq $a0.lo = $r0, $r1
; CHECK-NEXT:    xmovetq $a0.hi = $r2, $r3
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xso 0[$r4] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 4)
  %v0 = tail call <256 x i1> @llvm.kvx.xmoveto256(<4 x i64> %r)
  store <256 x i1> %v0, ptr %p0, align 32
  ret void
}

declare <4 x i64> @llvm.kvx.xmovefo256(<256 x i1>)

define <4 x i64> @test_xmovefo256(ptr %p0) {
; CHECK-LABEL: test_xmovefo256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo.u $a0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xmovefo $r0r1r2r3 = $a0
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %v0 = load <256 x i1>, ptr %p0, align 32
  %v1 = tail call <4 x i64> @llvm.kvx.xmovefo256(<256 x i1> %v0)
  ret <4 x i64> %v1
}

declare <4 x i64> @llvm.kvx.xaccesso.v512i1(<512 x i1>, i64)

define <4 x i64> @test_alignovi(ptr %p0) {
; CHECK-LABEL: test_alignovi:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo.u $a1 = 32[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo.u $a0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    aligno $r0r1r2r3 = $a0, $a1, 16
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %v0 = load <512 x i1>, ptr %p0, align 32
  %v1 = tail call <4 x i64> @llvm.kvx.xaccesso.v512i1(<512 x i1> %v0, i64 16)
  ret <4 x i64> %v1
}

define <4 x i64> @test_alignovr(ptr %p0, i64 %s) {
; CHECK-LABEL: test_alignovr:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo.u $a1 = 32[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo.u $a0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    aligno $r0r1r2r3 = $a0, $a1, $r1
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %v0 = load <512 x i1>, ptr %p0, align 32
  %v1 = tail call <4 x i64> @llvm.kvx.xaccesso.v512i1(<512 x i1> %v0, i64 %s)
  ret <4 x i64> %v1
}

declare <256 x i1> @llvm.kvx.xaligno.v512i1(<512 x i1>, i64)

define void @test_alignvi(ptr %p0) {
; CHECK-LABEL: test_alignvi:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo.u $a1 = 32[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo.u $a0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    alignv $a0 = $a0, $a1, 16
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 8)
  %v0 = load <512 x i1>, ptr %p0, align 32
  %v1 = tail call <256 x i1> @llvm.kvx.xaligno.v512i1(<512 x i1> %v0, i64 16)
  store <256 x i1> %v1, ptr %p0, align 32
  ret void
}

define void @test_alignvr(ptr %p0, i64 %s) {
; CHECK-LABEL: test_alignvr:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo.u $a1 = 32[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo.u $a0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    alignv $a0 = $a0, $a1, $r1
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 8)
  %v0 = load <512 x i1>, ptr %p0, align 32
  %v1 = tail call <256 x i1> @llvm.kvx.xaligno.v512i1(<512 x i1> %v0, i64 %s)
  store <256 x i1> %v1, ptr %p0, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.xconvdhv0(<256 x i1>, <1024 x i1>, i32, i32)

define void @test_convdhv0(ptr %p0, ptr %p1) {
; CHECK-LABEL: test_convdhv0:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo.u $a3 = 96[$r1]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo.u $a2 = 64[$r1]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xlo.u $a1 = 32[$r1]
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    xlo.u $a0 = 0[$r1]
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    convdhv0.rhu.satu $a0.lo = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 10)
  %m0 = load <1024 x i1>, ptr %p1, align 32
  %v1 = tail call <256 x i1> @llvm.kvx.xconvdhv0(<256 x i1> undef, <1024 x i1> %m0, i32 4, i32 1)
  store <256 x i1> %v1, ptr %p0, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.xconvdhv1(<256 x i1>, <1024 x i1>, i32, i32)

define void @test_convdhv1(ptr %p0, ptr %p1) {
; CHECK-LABEL: test_convdhv1:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo.u $a3 = 96[$r1]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo.u $a2 = 64[$r1]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xlo.u $a1 = 32[$r1]
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    xlo.u $a0 = 0[$r1]
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    convdhv1.rz.satu $a0.hi = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 10)
  %m0 = load <1024 x i1>, ptr %p1, align 32
  %v1 = tail call <256 x i1> @llvm.kvx.xconvdhv1(<256 x i1> undef, <1024 x i1> %m0, i32 3, i32 1)
  store <256 x i1> %v1, ptr %p0, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.xconvdhv(<1024 x i1>, i32, i32)

define void @test_convdhv(ptr %p0, ptr %p1) {
; CHECK-LABEL: test_convdhv:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo.u $a3 = 96[$r1]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo.u $a2 = 64[$r1]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xlo.u $a1 = 32[$r1]
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    xlo.u $a0 = 0[$r1]
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    convdhv0.rd.sat $a4.lo = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    convdhv1.rd.sat $a4.hi = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 7)
; CHECK-NEXT:    xso 0[$r0] = $a4
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 11)
  %m0 = load <1024 x i1>, ptr %p1, align 32
  %v1 = tail call <256 x i1> @llvm.kvx.xconvdhv(<1024 x i1> %m0, i32 2, i32 0)
  store <256 x i1> %v1, ptr %p0, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.xconvwbv0(<256 x i1>, <1024 x i1>, i32, i32)

define void @test_convwbv0(ptr %p0) {
; CHECK-LABEL: test_convwbv0:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo.u $a0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    convwbv0.rn.sat $a0.x = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 7)
  %v0 = load <256 x i1>, ptr %p0, align 32
  %v1 = tail call <256 x i1> @llvm.kvx.xconvwbv0(<256 x i1> %v0, <1024 x i1> undef, i32 0, i32 0)
  store <256 x i1> %v1, ptr %p0, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.xconvwbv1(<256 x i1>, <1024 x i1>, i32, i32)

define void @test_convwbv1(ptr %p0) {
; CHECK-LABEL: test_convwbv1:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo.u $a0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    convwbv1.ru.satu $a0.y = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 7)
  %v0 = load <256 x i1>, ptr %p0, align 32
  %v1 = tail call <256 x i1> @llvm.kvx.xconvwbv1(<256 x i1> %v0, <1024 x i1> undef, i32 1, i32 1)
  store <256 x i1> %v1, ptr %p0, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.xconvwbv2(<256 x i1>, <1024 x i1>, i32, i32)

define void @test_convwbv2(ptr %p0) {
; CHECK-LABEL: test_convwbv2:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo.u $a0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    convwbv2.rd.sat $a0.z = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 7)
  %v0 = load <256 x i1>, ptr %p0, align 32
  %v1 = tail call <256 x i1> @llvm.kvx.xconvwbv2(<256 x i1> %v0, <1024 x i1> undef, i32 2, i32 0)
  store <256 x i1> %v1, ptr %p0, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.xconvwbv3(<256 x i1>, <1024 x i1>, i32, i32)

define void @test_convwbv3(ptr %p0) {
; CHECK-LABEL: test_convwbv3:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo.u $a0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    convwbv3.rz.sat $a0.t = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 7)
  %v0 = load <256 x i1>, ptr %p0, align 32
  %v1 = tail call <256 x i1> @llvm.kvx.xconvwbv3(<256 x i1> %v0, <1024 x i1> undef, i32 3, i32 0)
  store <256 x i1> %v1, ptr %p0, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.xconvwbv(<1024 x i1>, i32, i32)

define void @test_convwbv(ptr %p0) {
; CHECK-LABEL: test_convwbv:
; CHECK:       # %bb.0:
; CHECK-NEXT:    convwbv0.rhu.sat $a0.x = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    convwbv1.rhu.sat $a0.y = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    convwbv2.rhu.sat $a0.z = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 8)
; CHECK-NEXT:    convwbv3.rhu.sat $a0.t = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 12)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 16)
  %v1 = tail call <256 x i1> @llvm.kvx.xconvwbv(<1024 x i1> undef, i32 4, i32 0)
  store <256 x i1> %v1, ptr %p0, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.xfmma242hw0(<256 x i1>, <512 x i1>, <256 x i1>, <256 x i1>)

define void @test_fmma242hw0(ptr %p0) {
; CHECK-LABEL: test_fmma242hw0:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo.u $a0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    fmma242hw0 $a0.lo = $a0a1, $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 9)
  %v0 = load <256 x i1>, ptr %p0, align 32
  %v1 = tail call <256 x i1> @llvm.kvx.xfmma242hw0(<256 x i1> %v0, <512 x i1> undef, <256 x i1> undef, <256 x i1> undef)
  store <256 x i1> %v1, ptr %p0, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.xfmma242hw1(<256 x i1>, <512 x i1>, <256 x i1>, <256 x i1>)

define void @test_fmma242hw1(ptr %p0) {
; CHECK-LABEL: test_fmma242hw1:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo.u $a0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    fmma242hw1 $a0.hi = $a0a1, $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 9)
  %v0 = load <256 x i1>, ptr %p0, align 32
  %v1 = tail call <256 x i1> @llvm.kvx.xfmma242hw1(<256 x i1> %v0, <512 x i1> undef, <256 x i1> undef, <256 x i1> undef)
  store <256 x i1> %v1, ptr %p0, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.xfmma242hw2(<256 x i1>, <512 x i1>, <256 x i1>, <256 x i1>)

define void @test_fmma242hw2(ptr %p0) {
; CHECK-LABEL: test_fmma242hw2:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo.u $a1 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    fmma242hw2 $a1.lo = $a0a1, $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    xso 0[$r0] = $a1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 9)
  %v0 = load <256 x i1>, ptr %p0, align 32
  %v1 = tail call <256 x i1> @llvm.kvx.xfmma242hw2(<256 x i1> %v0, <512 x i1> undef, <256 x i1> undef, <256 x i1> undef)
  store <256 x i1> %v1, ptr %p0, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.xfmma242hw3(<256 x i1>, <512 x i1>, <256 x i1>, <256 x i1>)

define void @test_fmma242hw3(ptr %p0) {
; CHECK-LABEL: test_fmma242hw3:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo.u $a1 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    fmma242hw3 $a1.hi = $a0a1, $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    xso 0[$r0] = $a1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 9)
  %v0 = load <256 x i1>, ptr %p0, align 32
  %v1 = tail call <256 x i1> @llvm.kvx.xfmma242hw3(<256 x i1> %v0, <512 x i1> undef, <256 x i1> undef, <256 x i1> undef)
  store <256 x i1> %v1, ptr %p0, align 32
  ret void
}

declare <512 x i1> @llvm.kvx.xfmma444hw(<256 x i1>, <256 x i1>, <512 x i1>)

define void @test_fmma444hw(ptr %p0) {
; CHECK-LABEL: test_fmma444hw:
; CHECK:       # %bb.0:
; CHECK-NEXT:    fmma242hw0 $a0.lo = $a0a1, $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    fmma242hw1 $a0.hi = $a0a1, $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    fmma242hw2 $a1.lo = $a0a1, $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 12)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    fmma242hw3 $a1.hi = $a0a1, $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 18)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 24)
  %v1 = tail call <512 x i1> @llvm.kvx.xfmma444hw(<256 x i1> undef, <256 x i1> undef, <512 x i1> undef)
  store <512 x i1> %v1, ptr %p0, align 32
  ret void
}

define void @test_mma444hbd0(ptr %p0) {
; CHECK-LABEL: test_mma444hbd0:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mma444hbd0 $a0a1a2a3 = $a0a1a2a3, $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xso 96[$r0] = $a3
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    xso 64[$r0] = $a2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 7)
  %m1 = tail call <1024 x i1> @llvm.kvx.xmma444hbd0(<256 x i1> undef, <256 x i1> undef, <1024 x i1> undef, i32 0)
  store <1024 x i1> %m1, ptr %p0, align 32
  ret void
}

define void @test_mma444hbd1(ptr %p0) {
; CHECK-LABEL: test_mma444hbd1:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mma444hbd1 $a0a1a2a3 = $a0a1a2a3, $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xso 96[$r0] = $a3
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    xso 64[$r0] = $a2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 7)
  %m1 = tail call <1024 x i1> @llvm.kvx.xmma444hbd1(<256 x i1> undef, <256 x i1> undef, <1024 x i1> undef, i32 0)
  store <1024 x i1> %m1, ptr %p0, align 32
  ret void
}

define void @test_mma444hd(ptr %p0) {
; CHECK-LABEL: test_mma444hd:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mma444hd $a0a1a2a3 = $a0a1a2a3, $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xso 96[$r0] = $a3
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    xso 64[$r0] = $a2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 7)
  %m1 = tail call <1024 x i1> @llvm.kvx.xmma444hd(<256 x i1> undef, <256 x i1> undef, <1024 x i1> undef, i32 0)
  store <1024 x i1> %m1, ptr %p0, align 32
  ret void
}

define void @test_mma444suhbd0(ptr %p0) {
; CHECK-LABEL: test_mma444suhbd0:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mma444suhbd0 $a0a1a2a3 = $a0a1a2a3, $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xso 96[$r0] = $a3
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    xso 64[$r0] = $a2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 7)
  %m1 = tail call <1024 x i1> @llvm.kvx.xmma444hbd0(<256 x i1> undef, <256 x i1> undef, <1024 x i1> undef, i32 1)
  store <1024 x i1> %m1, ptr %p0, align 32
  ret void
}

define void @test_mma444suhbd1(ptr %p0) {
; CHECK-LABEL: test_mma444suhbd1:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mma444suhbd1 $a0a1a2a3 = $a0a1a2a3, $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xso 96[$r0] = $a3
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    xso 64[$r0] = $a2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 7)
  %m1 = tail call <1024 x i1> @llvm.kvx.xmma444hbd1(<256 x i1> undef, <256 x i1> undef, <1024 x i1> undef, i32 1)
  store <1024 x i1> %m1, ptr %p0, align 32
  ret void
}

define void @test_mma444suhd(ptr %p0) {
; CHECK-LABEL: test_mma444suhd:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mma444suhd $a0a1a2a3 = $a0a1a2a3, $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xso 96[$r0] = $a3
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    xso 64[$r0] = $a2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 7)
  %m1 = tail call <1024 x i1> @llvm.kvx.xmma444hd(<256 x i1> undef, <256 x i1> undef, <1024 x i1> undef, i32 1)
  store <1024 x i1> %m1, ptr %p0, align 32
  ret void
}

define void @test_mma444uhbd0(ptr %p0) {
; CHECK-LABEL: test_mma444uhbd0:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mma444uhbd0 $a0a1a2a3 = $a0a1a2a3, $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xso 96[$r0] = $a3
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    xso 64[$r0] = $a2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 7)
  %m1 = tail call <1024 x i1> @llvm.kvx.xmma444hbd0(<256 x i1> undef, <256 x i1> undef, <1024 x i1> undef, i32 2)
  store <1024 x i1> %m1, ptr %p0, align 32
  ret void
}

define void @test_mma444uhbd1(ptr %p0) {
; CHECK-LABEL: test_mma444uhbd1:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mma444uhbd1 $a0a1a2a3 = $a0a1a2a3, $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xso 96[$r0] = $a3
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    xso 64[$r0] = $a2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 7)
  %m1 = tail call <1024 x i1> @llvm.kvx.xmma444hbd1(<256 x i1> undef, <256 x i1> undef, <1024 x i1> undef, i32 2)
  store <1024 x i1> %m1, ptr %p0, align 32
  ret void
}

define void @test_mma444uhd(ptr %p0) {
; CHECK-LABEL: test_mma444uhd:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mma444uhd $a0a1a2a3 = $a0a1a2a3, $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xso 96[$r0] = $a3
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    xso 64[$r0] = $a2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 7)
  %m1 = tail call <1024 x i1> @llvm.kvx.xmma444hd(<256 x i1> undef, <256 x i1> undef, <1024 x i1> undef, i32 2)
  store <1024 x i1> %m1, ptr %p0, align 32
  ret void
}

define void @test_mma444ushbd0(ptr %p0) {
; CHECK-LABEL: test_mma444ushbd0:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mma444ushbd0 $a0a1a2a3 = $a0a1a2a3, $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xso 96[$r0] = $a3
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    xso 64[$r0] = $a2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 7)
  %m1 = tail call <1024 x i1> @llvm.kvx.xmma444hbd0(<256 x i1> undef, <256 x i1> undef, <1024 x i1> undef, i32 3)
  store <1024 x i1> %m1, ptr %p0, align 32
  ret void
}

define void @test_mma444ushbd1(ptr %p0) {
; CHECK-LABEL: test_mma444ushbd1:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mma444ushbd1 $a0a1a2a3 = $a0a1a2a3, $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xso 96[$r0] = $a3
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    xso 64[$r0] = $a2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 7)
  %m1 = tail call <1024 x i1> @llvm.kvx.xmma444hbd1(<256 x i1> undef, <256 x i1> undef, <1024 x i1> undef, i32 3)
  store <1024 x i1> %m1, ptr %p0, align 32
  ret void
}

define void @test_mma444ushd(ptr %p0) {
; CHECK-LABEL: test_mma444ushd:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mma444ushd $a0a1a2a3 = $a0a1a2a3, $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xso 96[$r0] = $a3
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    xso 64[$r0] = $a2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 7)
  %m1 = tail call <1024 x i1> @llvm.kvx.xmma444hd(<256 x i1> undef, <256 x i1> undef, <1024 x i1> undef, i32 3)
  store <1024 x i1> %m1, ptr %p0, align 32
  ret void
}

define void @test_xmma484bw(ptr %p0) {
; CHECK-LABEL: test_xmma484bw:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xmma484bw $a0a1 = $a0a1, $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 5)
  %m1 = tail call <512 x i1> @llvm.kvx.xmma484bw(<256 x i1> undef, <256 x i1> undef, <512 x i1> undef, i32 0)
  store <512 x i1> %m1, ptr %p0, align 32
  ret void
}

define void @test_xmma484subw(ptr %p0) {
; CHECK-LABEL: test_xmma484subw:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xmma484subw $a0a1 = $a0a1, $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 5)
  %m1 = tail call <512 x i1> @llvm.kvx.xmma484bw(<256 x i1> undef, <256 x i1> undef, <512 x i1> undef, i32 1)
  store <512 x i1> %m1, ptr %p0, align 32
  ret void
}

define void @test_xmma484ubw(ptr %p0) {
; CHECK-LABEL: test_xmma484ubw:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xmma484ubw $a0a1 = $a0a1, $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 5)
  %m1 = tail call <512 x i1> @llvm.kvx.xmma484bw(<256 x i1> undef, <256 x i1> undef, <512 x i1> undef, i32 2)
  store <512 x i1> %m1, ptr %p0, align 32
  ret void
}

define void @test_xmma484usbw(ptr %p0) {
; CHECK-LABEL: test_xmma484usbw:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xmma484usbw $a0a1 = $a0a1, $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 5)
  %m1 = tail call <512 x i1> @llvm.kvx.xmma484bw(<256 x i1> undef, <256 x i1> undef, <512 x i1> undef, i32 3)
  store <512 x i1> %m1, ptr %p0, align 32
  ret void
}

declare <1024 x i1> @llvm.kvx.xmt44d(<1024 x i1>)

define void @test_xmt44d(ptr %p0) {
; CHECK-LABEL: test_xmt44d:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xmt44d $a0a1a2a3 = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xso 96[$r0] = $a3
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    xso 64[$r0] = $a2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 7)
  %m1 = tail call <1024 x i1> @llvm.kvx.xmt44d(<1024 x i1> undef)
  store <1024 x i1> %m1, ptr %p0, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.xfscalewv(<256 x i1>, i32, i32, i32)

define void @test_fscalewv(ptr %p0) {
; CHECK-LABEL: test_fscalewv:
; CHECK:       # %bb.0:
; CHECK-NEXT:    fscalewv.rn $a0 = $a0
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 4)
  %m1 = tail call <256 x i1> @llvm.kvx.xfscalewv(<256 x i1> undef, i32 0, i32 0, i32 0)
  store <256 x i1> %m1, ptr %p0, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.xfnarrow44wh(<512 x i1>, i32, i32)

define void @test_xfnarrow44wh(ptr %p0) {
; CHECK-LABEL: test_xfnarrow44wh:
; CHECK:       # %bb.0:
; CHECK-NEXT:    fnarrow44wh.rn $a0 = $a0a1
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 4)
  %m1 = tail call <256 x i1> @llvm.kvx.xfnarrow44wh(<512 x i1> undef, i32 0, i32 0)
  store <256 x i1> %m1, ptr %p0, align 32
  ret void
}

declare <1024 x i1> @llvm.kvx.lvc(<1024 x i1>, ptr, i32, i32)

declare <1024 x i1> @llvm.kvx.lvc.cond(<1024 x i1>, ptr, i64, i32, i32, i32)

declare { <4 x i64>, <256 x i1> } @llvm.kvx.xswapo256(<4 x i64>, <256 x i1>)

declare void @llvm.kvx.sv.cond(ptr, <256 x i1>, i64, i32)

define <4 x i64> @test_tca_builtins(i64 %0, i64 %1, i64 %2, i64 %3, ptr %4, ptr %5, ptr %6) {
; CHECK-LABEL: test_tca_builtins:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo.u $a0 = 0[$r4]
; CHECK-NEXT:    make $r32 = 0
; CHECK-NEXT:    make $r33 = 1
; CHECK-NEXT:    make $r34 = 2
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xmovetq $a0.hi = $r32, $r33
; CHECK-NEXT:    xmovetq $a4.lo = $r32, $r33
; CHECK-NEXT:    make $r1 = 4
; CHECK-NEXT:    make $r35 = 3
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xmovetq $a1.lo = $r35, $r1
; CHECK-NEXT:    xmovetq $a1.hi = $r33, $r34
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    xmovetq $a4.hi = $r34, $r35
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    xso 0[$r4] = $a0
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xlo.u $a0 = 0[$r4]
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    xmovetq $a0.lo = $r35, $r34
; CHECK-NEXT:    ;; # (end cycle 7)
; CHECK-NEXT:    xso 0[$r4] = $a0
; CHECK-NEXT:    ;; # (end cycle 11)
; CHECK-NEXT:    xso 32[$r4] = $a1
; CHECK-NEXT:    ;; # (end cycle 12)
; CHECK-NEXT:    xlo.u $a5 = 0[$r4]
; CHECK-NEXT:    ;; # (end cycle 13)
; CHECK-NEXT:    xlo.u $a3 = 96[$r6]
; CHECK-NEXT:    ;; # (end cycle 14)
; CHECK-NEXT:    xlo.u $a2 = 64[$r6]
; CHECK-NEXT:    ;; # (end cycle 15)
; CHECK-NEXT:    xlo.u $a1 = 32[$r6]
; CHECK-NEXT:    alignv $a5 = $a4, $a5, 16
; CHECK-NEXT:    ;; # (end cycle 16)
; CHECK-NEXT:    xlo.u $a0 = 0[$r6]
; CHECK-NEXT:    ;; # (end cycle 17)
; CHECK-NEXT:    convdhv0.rn.sat $a4.lo = $a0a1a2a3
; CHECK-NEXT:    aligno $r8r9r10r11 = $a4, $a5, 1
; CHECK-NEXT:    ;; # (end cycle 20)
; CHECK-NEXT:    xlo.u $a7 = 32[$r5]
; CHECK-NEXT:    convdhv1.ru.satu $a4.hi = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 21)
; CHECK-NEXT:    xlo.u $a6 = 0[$r5]
; CHECK-NEXT:    convwbv0.ru.sat $a5.x = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 22)
; CHECK-NEXT:    convwbv1.ru.sat $a5.y = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 23)
; CHECK-NEXT:    convwbv2.ru.sat $a5.z = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 24)
; CHECK-NEXT:    xcopyo $a8 = $a4
; CHECK-NEXT:    convwbv3.ru.sat $a5.t = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 25)
; CHECK-NEXT:    fmma242hw0 $a8.lo = $a6a7, $a8, $a5
; CHECK-NEXT:    ;; # (end cycle 29)
; CHECK-NEXT:    xcopyo $a10 = $a8
; CHECK-NEXT:    ;; # (end cycle 35)
; CHECK-NEXT:    fmma242hw1 $a10.hi = $a6a7, $a4, $a10
; CHECK-NEXT:    ;; # (end cycle 39)
; CHECK-NEXT:    xcopyo $a5 = $a10
; CHECK-NEXT:    ;; # (end cycle 45)
; CHECK-NEXT:    fmma242hw2 $a5.lo = $a6a7, $a10, $a8
; CHECK-NEXT:    ;; # (end cycle 46)
; CHECK-NEXT:    xcopyo $a9 = $a5
; CHECK-NEXT:    ;; # (end cycle 52)
; CHECK-NEXT:    fmma242hw3 $a9.hi = $a6a7, $a10, $a9
; CHECK-NEXT:    ;; # (end cycle 56)
; CHECK-NEXT:    fmma242hw0 $a10.lo = $a6a7, $a5, $a9
; CHECK-NEXT:    ;; # (end cycle 62)
; CHECK-NEXT:    fmma242hw1 $a10.hi = $a6a7, $a5, $a9
; CHECK-NEXT:    ;; # (end cycle 63)
; CHECK-NEXT:    fmma242hw2 $a11.lo = $a6a7, $a5, $a9
; CHECK-NEXT:    ;; # (end cycle 64)
; CHECK-NEXT:    fmma242hw3 $a11.hi = $a6a7, $a5, $a9
; CHECK-NEXT:    ;; # (end cycle 65)
; CHECK-NEXT:    mma444hbd0 $a0a1a2a3 = $a0a1a2a3, $a9, $a9
; CHECK-NEXT:    ;; # (end cycle 66)
; CHECK-NEXT:    mma444hbd1 $a0a1a2a3 = $a0a1a2a3, $a9, $a9
; CHECK-NEXT:    ;; # (end cycle 70)
; CHECK-NEXT:    mma444hbd0 $a0a1a2a3 = $a0a1a2a3, $a10, $a9
; CHECK-NEXT:    ;; # (end cycle 74)
; CHECK-NEXT:    mma444hbd1 $a0a1a2a3 = $a0a1a2a3, $a11, $a9
; CHECK-NEXT:    ;; # (end cycle 78)
; CHECK-NEXT:    mma444hd $a0a1a2a3 = $a0a1a2a3, $a9, $a9
; CHECK-NEXT:    ;; # (end cycle 82)
; CHECK-NEXT:    mma444suhbd0 $a0a1a2a3 = $a0a1a2a3, $a9, $a9
; CHECK-NEXT:    ;; # (end cycle 86)
; CHECK-NEXT:    mma444suhbd1 $a0a1a2a3 = $a0a1a2a3, $a9, $a9
; CHECK-NEXT:    ;; # (end cycle 90)
; CHECK-NEXT:    mma444suhbd0 $a0a1a2a3 = $a0a1a2a3, $a10, $a9
; CHECK-NEXT:    ;; # (end cycle 94)
; CHECK-NEXT:    mma444suhbd1 $a0a1a2a3 = $a0a1a2a3, $a11, $a9
; CHECK-NEXT:    ;; # (end cycle 98)
; CHECK-NEXT:    mma444suhd $a0a1a2a3 = $a0a1a2a3, $a9, $a9
; CHECK-NEXT:    ;; # (end cycle 102)
; CHECK-NEXT:    mma444uhbd0 $a0a1a2a3 = $a0a1a2a3, $a9, $a9
; CHECK-NEXT:    ;; # (end cycle 106)
; CHECK-NEXT:    mma444uhbd1 $a0a1a2a3 = $a0a1a2a3, $a9, $a9
; CHECK-NEXT:    ;; # (end cycle 110)
; CHECK-NEXT:    mma444uhbd0 $a0a1a2a3 = $a0a1a2a3, $a10, $a9
; CHECK-NEXT:    ;; # (end cycle 114)
; CHECK-NEXT:    mma444uhbd1 $a0a1a2a3 = $a0a1a2a3, $a11, $a9
; CHECK-NEXT:    ;; # (end cycle 118)
; CHECK-NEXT:    mma444uhd $a0a1a2a3 = $a0a1a2a3, $a9, $a9
; CHECK-NEXT:    ;; # (end cycle 122)
; CHECK-NEXT:    mma444ushbd0 $a0a1a2a3 = $a0a1a2a3, $a9, $a9
; CHECK-NEXT:    ;; # (end cycle 126)
; CHECK-NEXT:    mma444ushbd1 $a0a1a2a3 = $a0a1a2a3, $a9, $a9
; CHECK-NEXT:    ;; # (end cycle 130)
; CHECK-NEXT:    mma444ushbd0 $a0a1a2a3 = $a0a1a2a3, $a10, $a9
; CHECK-NEXT:    ;; # (end cycle 134)
; CHECK-NEXT:    mma444ushbd1 $a0a1a2a3 = $a0a1a2a3, $a11, $a9
; CHECK-NEXT:    ;; # (end cycle 138)
; CHECK-NEXT:    mma444ushd $a0a1a2a3 = $a0a1a2a3, $a9, $a9
; CHECK-NEXT:    ;; # (end cycle 142)
; CHECK-NEXT:    xmma484bw $a6a7 = $a10a11, $a9, $a9
; CHECK-NEXT:    ;; # (end cycle 143)
; CHECK-NEXT:    xmma484subw $a6a7 = $a6a7, $a9, $a9
; CHECK-NEXT:    ;; # (end cycle 147)
; CHECK-NEXT:    xmma484ubw $a6a7 = $a6a7, $a9, $a9
; CHECK-NEXT:    ;; # (end cycle 151)
; CHECK-NEXT:    xmma484usbw $a6a7 = $a6a7, $a9, $a9
; CHECK-NEXT:    ;; # (end cycle 155)
; CHECK-NEXT:    xmt44d $a0a1a2a3 = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 156)
; CHECK-NEXT:    fscalewv $a4 = $a5
; CHECK-NEXT:    ;; # (end cycle 157)
; CHECK-NEXT:    fnarrow44wh.rn.s $a5 = $a6a7
; CHECK-NEXT:    ;; # (end cycle 159)
; CHECK-NEXT:    fscalewv.rn.relu $a4 = $a4
; CHECK-NEXT:    ;; # (end cycle 161)
; CHECK-NEXT:    xmovetq $a4.lo = $r8, $r9
; CHECK-NEXT:    xmovetq $a4.hi = $r10, $r11
; CHECK-NEXT:    xmovefo $r8r9r10r11 = $a4
; CHECK-NEXT:    ;; # (end cycle 165)
; CHECK-NEXT:    fscalewv.relu $a4 = $a4
; CHECK-NEXT:    ;; # (end cycle 168)
; CHECK-NEXT:    xlo.us $a4 = 0[$r4]
; CHECK-NEXT:    ;; # (end cycle 169)
; CHECK-NEXT:    xlo.us.q0 $a0a1a2a3 = 128[$r4]
; CHECK-NEXT:    ;; # (end cycle 170)
; CHECK-NEXT:    xlo.u.odd.q0 $r0 ? $a0a1a2a3 = 160[$r4]
; CHECK-NEXT:    ;; # (end cycle 173)
; CHECK-NEXT:    xso 0[$r4] = $a4
; CHECK-NEXT:    ;; # (end cycle 174)
; CHECK-NEXT:    xso.even $r33 ? 32[$r4] = $a4
; CHECK-NEXT:    ;; # (end cycle 175)
; CHECK-NEXT:    xso 32[$r5] = $a7
; CHECK-NEXT:    ;; # (end cycle 176)
; CHECK-NEXT:    xso 0[$r5] = $a6
; CHECK-NEXT:    ;; # (end cycle 177)
; CHECK-NEXT:    xso 32[$r6] = $a1
; CHECK-NEXT:    ;; # (end cycle 178)
; CHECK-NEXT:    xso 0[$r6] = $a0
; CHECK-NEXT:    ;; # (end cycle 179)
; CHECK-NEXT:    xso 96[$r6] = $a3
; CHECK-NEXT:    ;; # (end cycle 180)
; CHECK-NEXT:    xso 64[$r6] = $a2
; CHECK-NEXT:    copyd $r0 = $r8
; CHECK-NEXT:    copyd $r1 = $r9
; CHECK-NEXT:    copyd $r2 = $r10
; CHECK-NEXT:    ;; # (end cycle 181)
; CHECK-NEXT:    copyd $r3 = $r11
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 182)
  %8 = load volatile <256 x i1>, ptr %4, align 32
  %9 = tail call <256 x i1> @llvm.kvx.xmovetq(<256 x i1> %8, i64 0, i64 1, i32 1)
  store volatile <256 x i1> %9, ptr %4, align 32
  %10 = load volatile <256 x i1>, ptr %4, align 32
  %11 = tail call <256 x i1> @llvm.kvx.xmovetq(<256 x i1> %10, i64 3, i64 2, i32 0)
  store volatile <256 x i1> %11, ptr %4, align 32
  %12 = tail call <256 x i1> @llvm.kvx.xmoveto(i64 1, i64 2, i64 3, i64 4)
  %13 = getelementptr inbounds i8, ptr %4, i64 32
  store volatile <256 x i1> %12, ptr %13, align 32
  %14 = tail call <256 x i1> @llvm.kvx.xmoveto256(<4 x i64> <i64 0, i64 1, i64 2, i64 3>)
  %15 = load volatile <256 x i1>, ptr %4, align 32
  %16 = tail call <512 x i1> @llvm.kvx.cat.v512i1(<256 x i1> %14, <256 x i1> %15)
  %17 = tail call <256 x i1> @llvm.kvx.xaligno.v512i1(<512 x i1> %16, i64 16)
  %18 = tail call <512 x i1> @llvm.kvx.cat.v512i1(<256 x i1> %14, <256 x i1> %17)
  %19 = tail call <4 x i64> @llvm.kvx.xaccesso.v512i1(<512 x i1> %18, i64 1)
  %20 = load volatile <1024 x i1>, ptr %6, align 32
  %21 = tail call <256 x i1> @llvm.kvx.xconvdhv0(<256 x i1> %14, <1024 x i1> %20, i32 0, i32 0)
  %22 = tail call <256 x i1> @llvm.kvx.xconvdhv1(<256 x i1> %21, <1024 x i1> %20, i32 1, i32 1)
  %23 = load volatile <512 x i1>, ptr %5, align 32
  %24 = tail call <256 x i1> @llvm.kvx.xconvwbv(<1024 x i1> %20, i32 1, i32 0)
  %25 = tail call <256 x i1> @llvm.kvx.xfmma242hw0(<256 x i1> %22, <512 x i1> %23, <256 x i1> %22, <256 x i1> %24)
  %26 = tail call <256 x i1> @llvm.kvx.xfmma242hw1(<256 x i1> %25, <512 x i1> %23, <256 x i1> %22, <256 x i1> %25)
  %27 = tail call <256 x i1> @llvm.kvx.xfmma242hw2(<256 x i1> %26, <512 x i1> %23, <256 x i1> %26, <256 x i1> %25)
  %28 = tail call <256 x i1> @llvm.kvx.xfmma242hw3(<256 x i1> %27, <512 x i1> %23, <256 x i1> %26, <256 x i1> %27)
  %29 = tail call <512 x i1> @llvm.kvx.xfmma444hw(<256 x i1> %27, <256 x i1> %28, <512 x i1> %23)
  %30 = tail call <1024 x i1> @llvm.kvx.xmma444hbd0(<256 x i1> %28, <256 x i1> %28, <1024 x i1> %20, i32 0)
  %31 = tail call <1024 x i1> @llvm.kvx.xmma444hbd1(<256 x i1> %28, <256 x i1> %28, <1024 x i1> %30, i32 0)
  %32 = tail call <1024 x i1> @llvm.kvx.xmma484hbd(<512 x i1> %29, <256 x i1> %28, <1024 x i1> %31, i32 0)
  %33 = tail call <1024 x i1> @llvm.kvx.xmma444hd(<256 x i1> %28, <256 x i1> %28, <1024 x i1> %32, i32 0)
  %34 = tail call <1024 x i1> @llvm.kvx.xmma444hbd0(<256 x i1> %28, <256 x i1> %28, <1024 x i1> %33, i32 1)
  %35 = tail call <1024 x i1> @llvm.kvx.xmma444hbd1(<256 x i1> %28, <256 x i1> %28, <1024 x i1> %34, i32 1)
  %36 = tail call <1024 x i1> @llvm.kvx.xmma484hbd(<512 x i1> %29, <256 x i1> %28, <1024 x i1> %35, i32 1)
  %37 = tail call <1024 x i1> @llvm.kvx.xmma444hd(<256 x i1> %28, <256 x i1> %28, <1024 x i1> %36, i32 1)
  %38 = tail call <1024 x i1> @llvm.kvx.xmma444hbd0(<256 x i1> %28, <256 x i1> %28, <1024 x i1> %37, i32 2)
  %39 = tail call <1024 x i1> @llvm.kvx.xmma444hbd1(<256 x i1> %28, <256 x i1> %28, <1024 x i1> %38, i32 2)
  %40 = tail call <1024 x i1> @llvm.kvx.xmma484hbd(<512 x i1> %29, <256 x i1> %28, <1024 x i1> %39, i32 2)
  %41 = tail call <1024 x i1> @llvm.kvx.xmma444hd(<256 x i1> %28, <256 x i1> %28, <1024 x i1> %40, i32 2)
  %42 = tail call <1024 x i1> @llvm.kvx.xmma444hbd0(<256 x i1> %28, <256 x i1> %28, <1024 x i1> %41, i32 3)
  %43 = tail call <1024 x i1> @llvm.kvx.xmma444hbd1(<256 x i1> %28, <256 x i1> %28, <1024 x i1> %42, i32 3)
  %44 = tail call <1024 x i1> @llvm.kvx.xmma484hbd(<512 x i1> %29, <256 x i1> %28, <1024 x i1> %43, i32 3)
  %45 = tail call <1024 x i1> @llvm.kvx.xmma444hd(<256 x i1> %28, <256 x i1> %28, <1024 x i1> %44, i32 3)
  %46 = tail call <512 x i1> @llvm.kvx.xmma484bw(<256 x i1> %28, <256 x i1> %28, <512 x i1> %29, i32 0)
  %47 = tail call <512 x i1> @llvm.kvx.xmma484bw(<256 x i1> %28, <256 x i1> %28, <512 x i1> %46, i32 1)
  %48 = tail call <512 x i1> @llvm.kvx.xmma484bw(<256 x i1> %28, <256 x i1> %28, <512 x i1> %47, i32 2)
  %49 = tail call <512 x i1> @llvm.kvx.xmma484bw(<256 x i1> %28, <256 x i1> %28, <512 x i1> %48, i32 3)
  %50 = tail call <1024 x i1> @llvm.kvx.xmt44d(<1024 x i1> %45)
  %51 = tail call <256 x i1> @llvm.kvx.xfscalewv(<256 x i1> %27, i32 7, i32 0, i32 0)
  %52 = tail call <256 x i1> @llvm.kvx.xfnarrow44wh(<512 x i1> %49, i32 0, i32 1)
  %53 = tail call <256 x i1> @llvm.kvx.xfscalewv(<256 x i1> %51, i32 0, i32 0, i32 1)
  %54 = tail call { <4 x i64>, <256 x i1> } @llvm.kvx.xswapo256(<4 x i64> %19, <256 x i1> %53)
  %55 = extractvalue { <4 x i64>, <256 x i1> } %54, 1
  %56 = extractvalue { <4 x i64>, <256 x i1> } %54, 0
  %57 = tail call <256 x i1> @llvm.kvx.xfscalewv(<256 x i1> %55, i32 7, i32 0, i32 1)
  %58 = tail call <256 x i1> @llvm.kvx.lv(ptr nonnull %4, i32 1)
  %59 = getelementptr inbounds i8, ptr %4, i64 128
  %60 = tail call <1024 x i1> @llvm.kvx.lvc(<1024 x i1> %50, ptr nonnull %59, i32 1, i32 0)
  %61 = getelementptr inbounds i8, ptr %4, i64 160
  %62 = tail call <1024 x i1> @llvm.kvx.lvc.cond(<1024 x i1> %60, ptr nonnull %61, i64 %0, i32 0, i32 0, i32 6)
  tail call void @llvm.kvx.sv(ptr nonnull %4, <256 x i1> %58)
  tail call void @llvm.kvx.sv.cond(ptr nonnull %13, <256 x i1> %58, i64 1, i32 7)
  store volatile <512 x i1> %49, ptr %5, align 32
  store volatile <1024 x i1> %62, ptr %6, align 32
  ret <4 x i64> %56
}

define void @convdhv(ptr %v, ptr %m) {
; CHECK-LABEL: convdhv:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xlo.u $a3 = 96[$r1]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo.u $a2 = 64[$r1]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xlo.u $a1 = 32[$r1]
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    xlo.u $a0 = 0[$r1]
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    xlo.u $a4 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    convdhv1.rn.sat $a4.hi = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    convdhv0.rn.satu $a4.lo = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 7)
; CHECK-NEXT:    xso 32[$r0] = $a4
; CHECK-NEXT:    ;; # (end cycle 11)
; CHECK-NEXT:    xlo.u $a3 = 224[$r1]
; CHECK-NEXT:    ;; # (end cycle 12)
; CHECK-NEXT:    xlo.u $a2 = 192[$r1]
; CHECK-NEXT:    ;; # (end cycle 13)
; CHECK-NEXT:    xlo.u $a1 = 160[$r1]
; CHECK-NEXT:    ;; # (end cycle 14)
; CHECK-NEXT:    xlo.u $a0 = 128[$r1]
; CHECK-NEXT:    ;; # (end cycle 15)
; CHECK-NEXT:    convdhv0.rz.sat $a4.lo = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 18)
; CHECK-NEXT:    convdhv1.rz.sat $a4.hi = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 19)
; CHECK-NEXT:    xso 64[$r0] = $a4
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 23)
entry:
  %0 = load <256 x i1>, ptr %v, align 32
  %1 = load <1024 x i1>, ptr %m, align 128
  %2 = tail call <256 x i1> @llvm.kvx.xconvdhv1(<256 x i1> %0, <1024 x i1> %1, i32 0, i32 0)
  %3 = tail call <256 x i1> @llvm.kvx.xconvdhv0(<256 x i1> %2, <1024 x i1> %1, i32 0, i32 1)
  %arrayidx3 = getelementptr inbounds i8, ptr %v, i64 32
  store <256 x i1> %3, ptr %arrayidx3, align 32
  %arrayidx4 = getelementptr inbounds i8, ptr %m, i64 128
  %4 = load <1024 x i1>, ptr %arrayidx4, align 128
  %5 = tail call <256 x i1> @llvm.kvx.xconvdhv(<1024 x i1> %4, i32 3, i32 0)
  %arrayidx5 = getelementptr inbounds i8, ptr %v, i64 64
  store <256 x i1> %5, ptr %arrayidx5, align 32
  ret void
}

define void @convwbv(ptr %v, ptr %m) {
; CHECK-LABEL: convwbv:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xlo.u $a3 = 96[$r1]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo.u $a2 = 64[$r1]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xlo.u $a1 = 32[$r1]
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    xlo.u $a0 = 0[$r1]
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    xlo.u $a4 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    convwbv1.rn.sat $a4.y = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    convwbv0.rn.satu $a4.x = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 7)
; CHECK-NEXT:    convwbv2.rd.sat $a4.z = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 8)
; CHECK-NEXT:    xcopyo $a5 = $a4
; CHECK-NEXT:    ;; # (end cycle 12)
; CHECK-NEXT:    convwbv3.rhu.satu $a5.t = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 13)
; CHECK-NEXT:    xso 32[$r0] = $a5
; CHECK-NEXT:    convwbv3.rn.sat $a4.t = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 17)
; CHECK-NEXT:    xso 0[$r0] = $a4
; CHECK-NEXT:    ;; # (end cycle 21)
; CHECK-NEXT:    xlo.u $a3 = 224[$r1]
; CHECK-NEXT:    ;; # (end cycle 22)
; CHECK-NEXT:    xlo.u $a2 = 192[$r1]
; CHECK-NEXT:    ;; # (end cycle 23)
; CHECK-NEXT:    xlo.u $a1 = 160[$r1]
; CHECK-NEXT:    ;; # (end cycle 24)
; CHECK-NEXT:    xlo.u $a0 = 128[$r1]
; CHECK-NEXT:    ;; # (end cycle 25)
; CHECK-NEXT:    convwbv0.rz.satu $a4.x = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 28)
; CHECK-NEXT:    convwbv1.rz.satu $a4.y = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 29)
; CHECK-NEXT:    convwbv2.rz.satu $a4.z = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 30)
; CHECK-NEXT:    convwbv3.rz.satu $a4.t = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 31)
; CHECK-NEXT:    xso 64[$r0] = $a4
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 35)
entry:
  %0 = load <256 x i1>, ptr %v, align 32
  %1 = load <1024 x i1>, ptr %m, align 128
  %2 = tail call <256 x i1> @llvm.kvx.xconvwbv1(<256 x i1> %0, <1024 x i1> %1, i32 0, i32 0)
  %3 = tail call <256 x i1> @llvm.kvx.xconvwbv0(<256 x i1> %2, <1024 x i1> %1, i32 0, i32 1)
  %4 = tail call <256 x i1> @llvm.kvx.xconvwbv2(<256 x i1> %3, <1024 x i1> %1, i32 2, i32 0)
  %5 = tail call <256 x i1> @llvm.kvx.xconvwbv3(<256 x i1> %4, <1024 x i1> %1, i32 4, i32 1)
  %arrayidx5 = getelementptr inbounds i8, ptr %v, i64 32
  store <256 x i1> %5, ptr %arrayidx5, align 32
  %6 = tail call <256 x i1> @llvm.kvx.xconvwbv3(<256 x i1> %4, <1024 x i1> %1, i32 0, i32 0)
  store <256 x i1> %6, ptr %v, align 32
  %arrayidx8 = getelementptr inbounds i8, ptr %m, i64 128
  %7 = load <1024 x i1>, ptr %arrayidx8, align 128
  %8 = tail call <256 x i1> @llvm.kvx.xconvwbv(<1024 x i1> %7, i32 3, i32 1)
  %arrayidx9 = getelementptr inbounds i8, ptr %v, i64 64
  store <256 x i1> %8, ptr %arrayidx9, align 32
  ret void
}

define void @fmma444hw(ptr %v, ptr %w) {
; CHECK-LABEL: fmma444hw:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xlo.u $a3 = 32[$r1]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo.u $a2 = 0[$r1]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xlo.u $a4 = 32[$r0]
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    xlo.u $a6 = 64[$r0]
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    xlo.u $a0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    fmma242hw0 $a0.lo = $a2a3, $a4, $a6
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    fmma242hw1 $a0.hi = $a2a3, $a4, $a6
; CHECK-NEXT:    ;; # (end cycle 7)
; CHECK-NEXT:    xcopyo $a1 = $a0
; CHECK-NEXT:    ;; # (end cycle 13)
; CHECK-NEXT:    xcopyo $a5 = $a1
; CHECK-NEXT:    ;; # (end cycle 17)
; CHECK-NEXT:    fmma242hw2 $a5.lo = $a2a3, $a4, $a6
; CHECK-NEXT:    ;; # (end cycle 18)
; CHECK-NEXT:    xso 32[$r0] = $a5
; CHECK-NEXT:    fmma242hw3 $a1.hi = $a2a3, $a5, $a6
; CHECK-NEXT:    ;; # (end cycle 24)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 25)
; CHECK-NEXT:    xso 64[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 30)
; CHECK-NEXT:    xlo.u $a3 = 96[$r1]
; CHECK-NEXT:    ;; # (end cycle 31)
; CHECK-NEXT:    xlo.u $a2 = 64[$r1]
; CHECK-NEXT:    ;; # (end cycle 32)
; CHECK-NEXT:    fmma242hw0 $a0.lo = $a2a3, $a1, $a5
; CHECK-NEXT:    ;; # (end cycle 35)
; CHECK-NEXT:    fmma242hw1 $a0.hi = $a2a3, $a1, $a5
; CHECK-NEXT:    ;; # (end cycle 36)
; CHECK-NEXT:    fmma242hw2 $a7.lo = $a2a3, $a1, $a5
; CHECK-NEXT:    ;; # (end cycle 37)
; CHECK-NEXT:    fmma242hw3 $a7.hi = $a2a3, $a1, $a5
; CHECK-NEXT:    ;; # (end cycle 38)
; CHECK-NEXT:    xso 192[$r1] = $a0
; CHECK-NEXT:    ;; # (end cycle 42)
; CHECK-NEXT:    xso 224[$r1] = $a7
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 44)
entry:
  %0 = load <256 x i1>, ptr %v, align 32
  %1 = load <512 x i1>, ptr %w, align 32
  %arrayidx2 = getelementptr inbounds i8, ptr %v, i64 32
  %2 = load <256 x i1>, ptr %arrayidx2, align 32
  %arrayidx3 = getelementptr inbounds i8, ptr %v, i64 64
  %3 = load <256 x i1>, ptr %arrayidx3, align 32
  %4 = tail call <256 x i1> @llvm.kvx.xfmma242hw0(<256 x i1> %0, <512 x i1> %1, <256 x i1> %2, <256 x i1> %3)
  %5 = tail call <256 x i1> @llvm.kvx.xfmma242hw1(<256 x i1> %4, <512 x i1> %1, <256 x i1> %2, <256 x i1> %3)
  %6 = tail call <256 x i1> @llvm.kvx.xfmma242hw2(<256 x i1> %5, <512 x i1> %1, <256 x i1> %2, <256 x i1> %3)
  store <256 x i1> %6, ptr %arrayidx2, align 32
  %7 = tail call <256 x i1> @llvm.kvx.xfmma242hw3(<256 x i1> %5, <512 x i1> %1, <256 x i1> %6, <256 x i1> %3)
  store <256 x i1> %7, ptr %arrayidx3, align 32
  store <256 x i1> %5, ptr %v, align 32
  %arrayidx16 = getelementptr inbounds i8, ptr %w, i64 64
  %8 = load <512 x i1>, ptr %arrayidx16, align 32
  %9 = tail call <512 x i1> @llvm.kvx.xfmma444hw(<256 x i1> %7, <256 x i1> %6, <512 x i1> %8)
  %arrayidx19 = getelementptr inbounds i8, ptr %w, i64 192
  store <512 x i1> %9, ptr %arrayidx19, align 32
  ret void
}

define void @test(ptr %v) {
; CHECK-LABEL: test:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xlo.u $a0 = 0[$r0]
; CHECK-NEXT:    make $r1 = 0
; CHECK-NEXT:    make $r2 = 1
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xmovetq $a0.lo = $r2, $r1
; CHECK-NEXT:    xmovetq $a0.hi = $r2, $r1
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 5)
entry:
  %0 = load <256 x i1>, ptr %v, align 32
  %1 = tail call <256 x i1> @llvm.kvx.xmovetq(<256 x i1> %0, i64 1, i64 0, i32 1)
  %2 = tail call <256 x i1> @llvm.kvx.xmovetq(<256 x i1> %1, i64 1, i64 0, i32 0)
  store <256 x i1> %2, ptr %v, align 32
  ret void
}

define void @insertwm(ptr %a0, ptr %a1) {
; CHECK-LABEL: insertwm:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    # implicit-def: $x1
; CHECK-NEXT:    xlo.u $a0 = 96[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo.u $a0 = 64[$r0]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xlo.u $a1 = 32[$r1]
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    xlo.u $a0 = 0[$r1]
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    xlo.u $a2 = 32[$r0]
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xlo.u $a2 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 7)
; CHECK-NEXT:    xcopyo $a2 = $a0
; CHECK-NEXT:    ;; # (end cycle 8)
; CHECK-NEXT:    # implicit-def: $w0
; CHECK-NEXT:    # implicit-def: $w0
; CHECK-NEXT:    xcopyo $a3 = $a1
; CHECK-NEXT:    ;; # (end cycle 9)
; CHECK-NEXT:    xso 64[$r0] = $a2
; CHECK-NEXT:    ;; # (end cycle 12)
; CHECK-NEXT:    xso 96[$r0] = $a3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 13)
entry:
  %0 = load <1024 x i1>, ptr %a0, align 128
  %1 = load <512 x i1>, ptr %a1, align 32
  %2 = tail call <1024 x i1> @llvm.kvx.xinsertwm(<1024 x i1> %0, <512 x i1> %1, i32 0)
  %3 = tail call <1024 x i1> @llvm.kvx.xinsertwm(<1024 x i1> %2, <512 x i1> %1, i32 1)
  store <1024 x i1> %3, ptr %a0, align 128
  ret void
}

declare <1024 x i1> @llvm.kvx.xinsertwm(<1024 x i1>, <512 x i1>, i32)

define void @insertvm(ptr %a0, ptr %a1) {
; CHECK-LABEL: insertvm:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    # implicit-def: $x1
; CHECK-NEXT:    xlo.u $a0 = 96[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo.u $a0 = 0[$r1]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xlo.u $a1 = 64[$r0]
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    xlo.u $a1 = 32[$r0]
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    xlo.u $a1 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xcopyo $a1 = $a0
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 10)
; CHECK-NEXT:    xcopyo $a2 = $a0
; CHECK-NEXT:    ;; # (end cycle 11)
; CHECK-NEXT:    xso 64[$r0] = $a2
; CHECK-NEXT:    ;; # (end cycle 15)
; CHECK-NEXT:    # implicit-def: $w0
; CHECK-NEXT:    # implicit-def: $w0
; CHECK-NEXT:    xcopyo $a3 = $a0
; CHECK-NEXT:    ;; # (end cycle 16)
; CHECK-NEXT:    xso 96[$r0] = $a3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 20)
entry:
  %0 = load <1024 x i1>, ptr %a0, align 128
  %1 = load <256 x i1>, ptr %a1, align 32
  %2 = tail call <1024 x i1> @llvm.kvx.xinsertvm(<1024 x i1> %0, <256 x i1> %1, i32 0)
  %3 = tail call <1024 x i1> @llvm.kvx.xinsertvm(<1024 x i1> %2, <256 x i1> %1, i32 1)
  %4 = tail call <1024 x i1> @llvm.kvx.xinsertvm(<1024 x i1> %3, <256 x i1> %1, i32 2)
  %5 = tail call <1024 x i1> @llvm.kvx.xinsertvm(<1024 x i1> %4, <256 x i1> %1, i32 3)
  store <1024 x i1> %5, ptr %a0, align 128
  ret void
}

declare <1024 x i1> @llvm.kvx.xinsertvm(<1024 x i1>, <256 x i1>, i32)

define void @insertvw(ptr %a0, ptr %a1) {
; CHECK-LABEL: insertvw:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    # implicit-def: $w1
; CHECK-NEXT:    xlo.u $a0 = 0[$r1]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo.u $a1 = 32[$r0]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xlo.u $a1 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    xcopyo $a1 = $a0
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 8)
entry:
  %0 = load <512 x i1>, ptr %a0, align 32
  %1 = load <256 x i1>, ptr %a1, align 32
  %2 = tail call <512 x i1> @llvm.kvx.xinsertvw(<512 x i1> %0, <256 x i1> %1, i32 0)
  %3 = tail call <512 x i1> @llvm.kvx.xinsertvw(<512 x i1> %2, <256 x i1> %1, i32 1)
  store <512 x i1> %3, ptr %a0, align 32
  ret void
}

declare <512 x i1> @llvm.kvx.xinsertvw(<512 x i1>, <256 x i1>, i32)

define void @movefmw(ptr %o, ptr %a0) {
; CHECK-LABEL: movefmw:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xlo.u $a3 = 96[$r1]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo.u $a1 = 32[$r1]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xlo.u $a0 = 0[$r1]
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    xlo.u $a2 = 64[$r1]
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xso 96[$r0] = $a3
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    xso 64[$r0] = $a2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 7)
entry:
  %0 = load <1024 x i1>, ptr %a0, align 128
  %1 = tail call <512 x i1> @llvm.kvx.xmovefmw(<1024 x i1> %0, i32 0)
  store <512 x i1> %1, ptr %o, align 32
  %2 = tail call <512 x i1> @llvm.kvx.xmovefmw(<1024 x i1> %0, i32 1)
  %arrayidx3 = getelementptr inbounds i8, ptr %o, i64 64
  store <512 x i1> %2, ptr %arrayidx3, align 32
  ret void
}

declare <512 x i1> @llvm.kvx.xmovefmw(<1024 x i1>, i32)

define void @movefmv(ptr %o, ptr %a0) {
; CHECK-LABEL: movefmv:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xlo.u $a3 = 96[$r1]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo.u $a0 = 0[$r1]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xlo.u $a1 = 32[$r1]
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    xlo.u $a2 = 64[$r1]
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xso 64[$r0] = $a2
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    xso 96[$r0] = $a3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 7)
entry:
  %0 = load <1024 x i1>, ptr %a0, align 128
  %1 = tail call <256 x i1> @llvm.kvx.xmovefmv(<1024 x i1> %0, i32 0)
  store <256 x i1> %1, ptr %o, align 32
  %2 = tail call <256 x i1> @llvm.kvx.xmovefmv(<1024 x i1> %0, i32 1)
  %arrayidx3 = getelementptr inbounds i8, ptr %o, i64 32
  store <256 x i1> %2, ptr %arrayidx3, align 32
  %3 = tail call <256 x i1> @llvm.kvx.xmovefmv(<1024 x i1> %0, i32 2)
  %arrayidx5 = getelementptr inbounds i8, ptr %o, i64 64
  store <256 x i1> %3, ptr %arrayidx5, align 32
  %4 = tail call <256 x i1> @llvm.kvx.xmovefmv(<1024 x i1> %0, i32 3)
  %arrayidx7 = getelementptr inbounds i8, ptr %o, i64 96
  store <256 x i1> %4, ptr %arrayidx7, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.xmovefmv(<1024 x i1>, i32)

define void @movefwv(ptr %o, ptr %a0) {
; CHECK-LABEL: movefwv:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xlo.u $a0 = 0[$r1]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo.u $a1 = 32[$r1]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 4)
entry:
  %0 = load <512 x i1>, ptr %a0, align 32
  %1 = tail call <256 x i1> @llvm.kvx.xmovefwv(<512 x i1> %0, i32 0)
  store <256 x i1> %1, ptr %o, align 32
  %2 = tail call <256 x i1> @llvm.kvx.xmovefwv(<512 x i1> %0, i32 1)
  %arrayidx3 = getelementptr inbounds i8, ptr %o, i64 32
  store <256 x i1> %2, ptr %arrayidx3, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.xmovefwv(<512 x i1>, i32)

define void @buildfvm(ptr %a, ptr %M) {
; CHECK-LABEL: buildfvm:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xlo.u $a1 = 64[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo.u $a0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xlo.u $a3 = 32[$r0]
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    xso 32[$r1] = $a1
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    xso 0[$r1] = $a0
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 96[$r1] = $a3
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xcopyo $a2 = $a0
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    xso 64[$r1] = $a2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 10)
entry:
  %0 = load <256 x i1>, ptr %a, align 32
  %arrayidx1 = getelementptr inbounds i8, ptr %a, i64 64
  %1 = load <256 x i1>, ptr %arrayidx1, align 32
  %arrayidx3 = getelementptr inbounds i8, ptr %a, i64 32
  %2 = load <256 x i1>, ptr %arrayidx3, align 32
  %3 = tail call <1024 x i1> @llvm.kvx.xbuild1024(<256 x i1> %0, <256 x i1> %1, <256 x i1> %0, <256 x i1> %2)
  store <1024 x i1> %3, ptr %M, align 128
  ret void
}

declare <1024 x i1> @llvm.kvx.xbuild1024(<256 x i1>, <256 x i1>, <256 x i1>, <256 x i1>)

define void @buildfwm(ptr %a, ptr %M) {
; CHECK-LABEL: buildfwm:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xlo.u $a1 = 160[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo.u $a0 = 128[$r0]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xso 160[$r1] = $a1
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    xso 128[$r1] = $a0
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xcopyo $a2 = $a0
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xcopyo $a3 = $a1
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    xso 192[$r1] = $a2
; CHECK-NEXT:    ;; # (end cycle 9)
; CHECK-NEXT:    xso 224[$r1] = $a3
; CHECK-NEXT:    ;; # (end cycle 10)
; CHECK-NEXT:    xlo.u $a3 = 96[$r0]
; CHECK-NEXT:    ;; # (end cycle 11)
; CHECK-NEXT:    xlo.u $a2 = 64[$r0]
; CHECK-NEXT:    ;; # (end cycle 12)
; CHECK-NEXT:    xso 32[$r1] = $a1
; CHECK-NEXT:    ;; # (end cycle 13)
; CHECK-NEXT:    xso 0[$r1] = $a0
; CHECK-NEXT:    ;; # (end cycle 14)
; CHECK-NEXT:    xso 96[$r1] = $a3
; CHECK-NEXT:    ;; # (end cycle 15)
; CHECK-NEXT:    xso 64[$r1] = $a2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 16)
entry:
  %arrayidx = getelementptr inbounds i8, ptr %a, i64 128
  %0 = load <512 x i1>, ptr %arrayidx, align 32
  %1 = tail call <1024 x i1> @llvm.kvx.cat.v1024i1(<512 x i1> %0, <512 x i1> %0)
  %arrayidx2 = getelementptr inbounds i8, ptr %M, i64 128
  store <1024 x i1> %1, ptr %arrayidx2, align 128
  %arrayidx4 = getelementptr inbounds i8, ptr %a, i64 64
  %2 = load <512 x i1>, ptr %arrayidx4, align 32
  %3 = tail call <1024 x i1> @llvm.kvx.cat.v1024i1(<512 x i1> %0, <512 x i1> %2)
  store <1024 x i1> %3, ptr %M, align 128
  ret void
}

declare <1024 x i1> @llvm.kvx.cat.v1024i1(<512 x i1>, <512 x i1>)

define void @buildfvw(ptr %a, ptr %W) {
; CHECK-LABEL: buildfvw:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xlo.u $a1 = 64[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo.u $a0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xso 32[$r1] = $a1
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    xso 0[$r1] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 4)
entry:
  %0 = load <256 x i1>, ptr %a, align 32
  %arrayidx1 = getelementptr inbounds i8, ptr %a, i64 64
  %1 = load <256 x i1>, ptr %arrayidx1, align 32
  %2 = tail call <512 x i1> @llvm.kvx.cat.v512i1(<256 x i1> %0, <256 x i1> %1)
  store <512 x i1> %2, ptr %W, align 32
  ret void
}

declare <512 x i1> @llvm.kvx.cat.v512i1(<256 x i1>, <256 x i1>)

declare <1024 x i1> @llvm.kvx.xmma444hbd0(<256 x i1>, <256 x i1>, <1024 x i1>, i32)

declare <1024 x i1> @llvm.kvx.xmma444hbd1(<256 x i1>, <256 x i1>, <1024 x i1>, i32)

declare <1024 x i1> @llvm.kvx.xmma484hbd(<512 x i1>, <256 x i1>, <1024 x i1>, i32)

declare <1024 x i1> @llvm.kvx.xmma444hd(<256 x i1>, <256 x i1>, <1024 x i1>, i32)

declare <512 x i1> @llvm.kvx.xmma484bw(<256 x i1>, <256 x i1>, <512 x i1>, i32)

declare <256 x i1> @llvm.kvx.lv(ptr, i32)

declare void @llvm.kvx.sv(ptr, <256 x i1>)

