; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mcpu=kv3-2 -o - %s -O2 | FileCheck %s
; RUN: clang -O2 -march=kv3-2 -c -o /dev/null %s

target triple = "kvx-kalray-cos"

define void @xfscalewo_test(ptr %0, i32 %1) {
; CHECK-LABEL: xfscalewo_test:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo $a0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xfscalewo $a0 = $a0, $r1
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xfscalewo.s $a0 = $a0, $r1
; CHECK-NEXT:    ;; # (end cycle 8)
; CHECK-NEXT:    xfscalewo.rn $a0 = $a0, $r1
; CHECK-NEXT:    ;; # (end cycle 12)
; CHECK-NEXT:    xfscalewo.rn.s $a0 = $a0, $r1
; CHECK-NEXT:    ;; # (end cycle 16)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 20)
  %3 = load <256 x i1>, ptr %0
  %4 = tail call <256 x i1> @llvm.kvx.xfscalewo(<256 x i1> %3, i32 %1, i32 7, i32 0)
  %5 = tail call <256 x i1> @llvm.kvx.xfscalewo(<256 x i1> %4, i32 %1, i32 7, i32 1)
  %6 = tail call <256 x i1> @llvm.kvx.xfscalewo(<256 x i1> %5, i32 %1, i32 0, i32 0)
  %7 = tail call <256 x i1> @llvm.kvx.xfscalewo(<256 x i1> %6, i32 %1, i32 0, i32 1)
  store <256 x i1> %7, ptr %0
  ret void
}

declare <256 x i1> @llvm.kvx.xfscalewo(<256 x i1>, i32, i32, i32)

define void @xclampwo_test(ptr %0) {
; CHECK-LABEL: xclampwo_test:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo $a0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xclampwo $a0 = $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 8)
  %2 = load <256 x i1>, ptr %0
  %3 = tail call <256 x i1> @llvm.kvx.xclampwo(<256 x i1> %2, <256 x i1> %2, <256 x i1> %2)
  store <256 x i1> %3, ptr %0
  ret void
}

declare <256 x i1> @llvm.kvx.xclampwo(<256 x i1>, <256 x i1>, <256 x i1>)

define void @xffma44hw_test(ptr %0, ptr %1) {
; CHECK-LABEL: xffma44hw_test:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo $a2 = 0[$r1]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo $a1 = 32[$r0]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xlo $a0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    xffma44hw $a0a1 = $a2, $a2
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    xffma44hw.s $a0a1 = $a2, $a2
; CHECK-NEXT:    ;; # (end cycle 11)
; CHECK-NEXT:    xffma44hw.ru $a0a1 = $a2, $a2
; CHECK-NEXT:    ;; # (end cycle 16)
; CHECK-NEXT:    xffma44hw.rz.s $a0a1 = $a2, $a2
; CHECK-NEXT:    ;; # (end cycle 21)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 26)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 27)
  %3 = load <256 x i1>, ptr %1
  %4 = load <512 x i1>, ptr %0
  %5 = tail call <512 x i1> @llvm.kvx.xffma44hw(<256 x i1> %3, <256 x i1> %3, <512 x i1> %4, i32 7, i32 0)
  %6 = tail call <512 x i1> @llvm.kvx.xffma44hw(<256 x i1> %3, <256 x i1> %3, <512 x i1> %5, i32 7, i32 1)
  %7 = tail call <512 x i1> @llvm.kvx.xffma44hw(<256 x i1> %3, <256 x i1> %3, <512 x i1> %6, i32 1, i32 0)
  %8 = tail call <512 x i1> @llvm.kvx.xffma44hw(<256 x i1> %3, <256 x i1> %3, <512 x i1> %7, i32 3, i32 1)
  store <512 x i1> %8, ptr %0
  ret void
}

declare <512 x i1> @llvm.kvx.xffma44hw(<256 x i1>, <256 x i1>, <512 x i1>, i32, i32)

define void @xfmma484hw_test(ptr %0) {
; CHECK-LABEL: xfmma484hw_test:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo $a1 = 32[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo $a0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xfmma484hw $a0a1 = $a0a1, $a0a1
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xfmma484hw.s $a0a1 = $a0a1, $a0a1
; CHECK-NEXT:    ;; # (end cycle 10)
; CHECK-NEXT:    xfmma484hw.rn $a0a1 = $a0a1, $a0a1
; CHECK-NEXT:    ;; # (end cycle 15)
; CHECK-NEXT:    xfmma484hw.rz.s $a0a1 = $a0a1, $a0a1
; CHECK-NEXT:    ;; # (end cycle 20)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 25)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 26)
  %2 = load <512 x i1>, ptr %0
  %3 = tail call <512 x i1> @llvm.kvx.xfmma484hw(<512 x i1> %2, <512 x i1> %2, <512 x i1> %2, i32 7, i32 0)
  %4 = tail call <512 x i1> @llvm.kvx.xfmma484hw(<512 x i1> %3, <512 x i1> %3, <512 x i1> %3, i32 7, i32 1)
  %5 = tail call <512 x i1> @llvm.kvx.xfmma484hw(<512 x i1> %4, <512 x i1> %4, <512 x i1> %4, i32 0, i32 0)
  %6 = tail call <512 x i1> @llvm.kvx.xfmma484hw(<512 x i1> %5, <512 x i1> %5, <512 x i1> %5, i32 3, i32 1)
  store <512 x i1> %6, ptr %0
  ret void
}

declare <512 x i1> @llvm.kvx.xfmma484hw(<512 x i1>, <512 x i1>, <512 x i1>, i32, i32)

define void @xfnarrow44wh_test(ptr writeonly %0, ptr %1) {
; CHECK-LABEL: xfnarrow44wh_test:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo $a1 = 32[$r1]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo $a0 = 0[$r1]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xfnarrow44wh $a0 = $a0a1
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 9)
; CHECK-NEXT:    xlo $a1 = 32[$r1]
; CHECK-NEXT:    ;; # (end cycle 10)
; CHECK-NEXT:    xlo $a0 = 0[$r1]
; CHECK-NEXT:    ;; # (end cycle 11)
; CHECK-NEXT:    xfnarrow44wh.s $a0 = $a0a1
; CHECK-NEXT:    ;; # (end cycle 15)
; CHECK-NEXT:    xso 32[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 19)
; CHECK-NEXT:    xlo $a1 = 32[$r1]
; CHECK-NEXT:    ;; # (end cycle 20)
; CHECK-NEXT:    xlo $a0 = 0[$r1]
; CHECK-NEXT:    ;; # (end cycle 21)
; CHECK-NEXT:    xfnarrow44wh.rz $a0 = $a0a1
; CHECK-NEXT:    ;; # (end cycle 25)
; CHECK-NEXT:    xso 64[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 29)
; CHECK-NEXT:    xlo $a1 = 32[$r1]
; CHECK-NEXT:    ;; # (end cycle 30)
; CHECK-NEXT:    xlo $a0 = 0[$r1]
; CHECK-NEXT:    ;; # (end cycle 31)
; CHECK-NEXT:    xfnarrow44wh.ru.s $a0 = $a0a1
; CHECK-NEXT:    ;; # (end cycle 35)
; CHECK-NEXT:    xso 96[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 39)
  %3 = load <512 x i1>, ptr %1
  %4 = tail call <256 x i1> @llvm.kvx.xfnarrow44wh(<512 x i1> %3, i32 7, i32 0)
  store <256 x i1> %4, ptr %0
  %5 = load <512 x i1>, ptr %1
  %6 = tail call <256 x i1> @llvm.kvx.xfnarrow44wh(<512 x i1> %5, i32 7, i32 1)
  %7 = getelementptr inbounds <256 x i1>, ptr %0, i64 1
  store <256 x i1> %6, ptr %7
  %8 = load <512 x i1>, ptr %1
  %9 = tail call <256 x i1> @llvm.kvx.xfnarrow44wh(<512 x i1> %8, i32 3, i32 0)
  %10 = getelementptr inbounds <256 x i1>, ptr %0, i64 2
  store <256 x i1> %9, ptr %10
  %11 = load <512 x i1>, ptr %1
  %12 = tail call <256 x i1> @llvm.kvx.xfnarrow44wh(<512 x i1> %11, i32 1, i32 1)
  %13 = getelementptr inbounds <256 x i1>, ptr %0, i64 3
  store <256 x i1> %12, ptr %13
  ret void
}

declare <256 x i1> @llvm.kvx.xfnarrow44wh(<512 x i1>, i32, i32)

define void @xmadd44bw_test(ptr %0, ptr %1) {
; CHECK-LABEL: xmadd44bw_test:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo $a4 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo $a1 = 32[$r1]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xlo $a0 = 0[$r1]
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    xmadd44bw0 $a0a1 = $a4, $a4
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    xcopyx $a2a3 = $a0a1
; CHECK-NEXT:    ;; # (end cycle 10)
; CHECK-NEXT:    xmadd44bw1 $a2a3 = $a4, $a4
; CHECK-NEXT:    ;; # (end cycle 14)
; CHECK-NEXT:    xso 0[$r1] = $a2
; CHECK-NEXT:    ;; # (end cycle 18)
; CHECK-NEXT:    xso 32[$r1] = $a3
; CHECK-NEXT:    xcopyx $a2a3 = $a0a1
; CHECK-NEXT:    ;; # (end cycle 19)
; CHECK-NEXT:    xmadd44bw0 $a2a3 = $a4, $a4
; CHECK-NEXT:    ;; # (end cycle 23)
; CHECK-NEXT:    xso 32[$r1] = $a3
; CHECK-NEXT:    ;; # (end cycle 27)
; CHECK-NEXT:    xso 0[$r1] = $a2
; CHECK-NEXT:    xcopyx $a2a3 = $a0a1
; CHECK-NEXT:    ;; # (end cycle 28)
; CHECK-NEXT:    xmaddsu44bw1 $a2a3 = $a4, $a4
; CHECK-NEXT:    ;; # (end cycle 32)
; CHECK-NEXT:    xso 32[$r1] = $a3
; CHECK-NEXT:    ;; # (end cycle 36)
; CHECK-NEXT:    xso 0[$r1] = $a2
; CHECK-NEXT:    xcopyx $a2a3 = $a0a1
; CHECK-NEXT:    ;; # (end cycle 37)
; CHECK-NEXT:    xmaddu44bw0 $a2a3 = $a4, $a4
; CHECK-NEXT:    ;; # (end cycle 41)
; CHECK-NEXT:    xso 32[$r1] = $a3
; CHECK-NEXT:    ;; # (end cycle 45)
; CHECK-NEXT:    xso 0[$r1] = $a2
; CHECK-NEXT:    ;; # (end cycle 46)
; CHECK-NEXT:    xmaddsu44bw1 $a0a1 = $a4, $a4
; CHECK-NEXT:    ;; # (end cycle 47)
; CHECK-NEXT:    xso 32[$r1] = $a1
; CHECK-NEXT:    ;; # (end cycle 51)
; CHECK-NEXT:    xso 0[$r1] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 52)
  %3 = load <256 x i1>, ptr %0
  %4 = load <512 x i1>, ptr %1
  %5 = tail call <512 x i1> @llvm.kvx.xmadd44bw0(<256 x i1> %3, <256 x i1> %3, <512 x i1> %4, i32 0)
  %6 = tail call <512 x i1> @llvm.kvx.xmadd44bw1(<256 x i1> %3, <256 x i1> %3, <512 x i1> %5, i32 0)
  store <512 x i1> %6, ptr %1
  %7 = tail call <512 x i1> @llvm.kvx.xmadd44bw0(<256 x i1> %3, <256 x i1> %3, <512 x i1> %5, i32 0)
  store <512 x i1> %7, ptr %1
  %8 = tail call <512 x i1> @llvm.kvx.xmadd44bw1(<256 x i1> %3, <256 x i1> %3, <512 x i1> %5, i32 1)
  store <512 x i1> %8, ptr %1
  %9 = tail call <512 x i1> @llvm.kvx.xmadd44bw0(<256 x i1> %3, <256 x i1> %3, <512 x i1> %5, i32 2)
  store <512 x i1> %9, ptr %1
  %10 = tail call <512 x i1> @llvm.kvx.xmadd44bw1(<256 x i1> %3, <256 x i1> %3, <512 x i1> %5, i32 1)
  store <512 x i1> %10, ptr %1
  ret void
}

declare <512 x i1> @llvm.kvx.xmadd44bw0(<256 x i1>, <256 x i1>, <512 x i1>, i32)

declare <512 x i1> @llvm.kvx.xmadd44bw1(<256 x i1>, <256 x i1>, <512 x i1>, i32)

define void @xmaddifwo_test(ptr %0) {
; CHECK-LABEL: xmaddifwo_test:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo $a0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xmaddifwo $a0 = $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xmaddifwo.s $a0 = $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 8)
; CHECK-NEXT:    xmaddifwo.rn $a0 = $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 12)
; CHECK-NEXT:    xmaddifwo.rz.s $a0 = $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 16)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 20)
  %2 = load <256 x i1>, ptr %0
  %3 = tail call <256 x i1> @llvm.kvx.xmaddifwo(<256 x i1> %2, <256 x i1> %2, <256 x i1> %2, i32 7, i32 0)
  %4 = tail call <256 x i1> @llvm.kvx.xmaddifwo(<256 x i1> %3, <256 x i1> %3, <256 x i1> %3, i32 7, i32 1)
  %5 = tail call <256 x i1> @llvm.kvx.xmaddifwo(<256 x i1> %4, <256 x i1> %4, <256 x i1> %4, i32 0, i32 0)
  %6 = tail call <256 x i1> @llvm.kvx.xmaddifwo(<256 x i1> %5, <256 x i1> %5, <256 x i1> %5, i32 3, i32 1)
  store <256 x i1> %6, ptr %0
  ret void
}

declare <256 x i1> @llvm.kvx.xmaddifwo(<256 x i1>, <256 x i1>, <256 x i1>, i32, i32)

define void @xmma4164bw_test(ptr %0) {
; CHECK-LABEL: xmma4164bw_test:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo $a1 = 32[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo $a0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xmma4164bw $a0a1 = $a0a1, $a0a1
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xmma4164bw $a0a1 = $a0a1, $a0a1
; CHECK-NEXT:    ;; # (end cycle 9)
; CHECK-NEXT:    xmmasu4164bw $a0a1 = $a0a1, $a0a1
; CHECK-NEXT:    ;; # (end cycle 13)
; CHECK-NEXT:    xmmau4164bw $a0a1 = $a0a1, $a0a1
; CHECK-NEXT:    ;; # (end cycle 17)
; CHECK-NEXT:    xmmaus4164bw $a0a1 = $a0a1, $a0a1
; CHECK-NEXT:    ;; # (end cycle 21)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 25)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 26)
  %2 = load <512 x i1>, ptr %0
  %3 = tail call <512 x i1> @llvm.kvx.xmma4164bw(<512 x i1> %2, <512 x i1> %2, <512 x i1> %2, i32 0)
  %4 = tail call <512 x i1> @llvm.kvx.xmma4164bw(<512 x i1> %3, <512 x i1> %3, <512 x i1> %3, i32 0)
  %5 = tail call <512 x i1> @llvm.kvx.xmma4164bw(<512 x i1> %4, <512 x i1> %4, <512 x i1> %4, i32 1)
  %6 = tail call <512 x i1> @llvm.kvx.xmma4164bw(<512 x i1> %5, <512 x i1> %5, <512 x i1> %5, i32 2)
  %7 = tail call <512 x i1> @llvm.kvx.xmma4164bw(<512 x i1> %6, <512 x i1> %6, <512 x i1> %6, i32 3)
  store <512 x i1> %7, ptr %0
  ret void
}

declare <512 x i1> @llvm.kvx.xmma4164bw(<512 x i1>, <512 x i1>, <512 x i1>, i32)

define void @xmma484bw_test(ptr %0, ptr %1) {
; CHECK-LABEL: xmma484bw_test:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo $a2 = 0[$r1]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo $a1 = 32[$r0]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xlo $a0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    xmma484bw $a0a1 = $a2, $a2
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    xmmasu484bw $a0a1 = $a2, $a2
; CHECK-NEXT:    ;; # (end cycle 10)
; CHECK-NEXT:    xmmau484bw $a0a1 = $a2, $a2
; CHECK-NEXT:    ;; # (end cycle 14)
; CHECK-NEXT:    xmmaus484bw $a0a1 = $a2, $a2
; CHECK-NEXT:    ;; # (end cycle 18)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 22)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 23)
  %3 = load <256 x i1>, ptr %1
  %4 = load <512 x i1>, ptr %0
  %5 = tail call <512 x i1> @llvm.kvx.xmma484bw(<256 x i1> %3, <256 x i1> %3, <512 x i1> %4, i32 0)
  %6 = tail call <512 x i1> @llvm.kvx.xmma484bw(<256 x i1> %3, <256 x i1> %3, <512 x i1> %5, i32 1)
  %7 = tail call <512 x i1> @llvm.kvx.xmma484bw(<256 x i1> %3, <256 x i1> %3, <512 x i1> %6, i32 2)
  %8 = tail call <512 x i1> @llvm.kvx.xmma484bw(<256 x i1> %3, <256 x i1> %3, <512 x i1> %7, i32 3)
  store <512 x i1> %8, ptr %0
  ret void
}

declare <512 x i1> @llvm.kvx.xmma484bw(<256 x i1>, <256 x i1>, <512 x i1>, i32)

define void @xmsbfifwo_test(ptr %0) {
; CHECK-LABEL: xmsbfifwo_test:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo $a0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xmsbfifwo $a0 = $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xmsbfifwo.s $a0 = $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 8)
; CHECK-NEXT:    xmsbfifwo.rn $a0 = $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 12)
; CHECK-NEXT:    xmsbfifwo.rz.s $a0 = $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 16)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 20)
  %2 = load <256 x i1>, ptr %0
  %3 = tail call <256 x i1> @llvm.kvx.xmsbfifwo(<256 x i1> %2, <256 x i1> %2, <256 x i1> %2, i32 7, i32 0)
  %4 = tail call <256 x i1> @llvm.kvx.xmsbfifwo(<256 x i1> %3, <256 x i1> %3, <256 x i1> %3, i32 7, i32 1)
  %5 = tail call <256 x i1> @llvm.kvx.xmsbfifwo(<256 x i1> %4, <256 x i1> %4, <256 x i1> %4, i32 0, i32 0)
  %6 = tail call <256 x i1> @llvm.kvx.xmsbfifwo(<256 x i1> %5, <256 x i1> %5, <256 x i1> %5, i32 3, i32 1)
  store <256 x i1> %6, ptr %0
  ret void
}

declare <256 x i1> @llvm.kvx.xmsbfifwo(<256 x i1>, <256 x i1>, <256 x i1>, i32, i32)

define void @xsx48bw_test(ptr writeonly %0, ptr %1) {
; CHECK-LABEL: xsx48bw_test:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo $a0 = 0[$r1]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xsx48bw $a0a1a2a3 = $a0
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 8)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 9)
; CHECK-NEXT:    xso 96[$r0] = $a3
; CHECK-NEXT:    ;; # (end cycle 10)
; CHECK-NEXT:    xso 64[$r0] = $a2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 11)
  %3 = load <256 x i1>, ptr %1
  %4 = tail call <1024 x i1> @llvm.kvx.xsx48bw(<256 x i1> %3)
  store <1024 x i1> %4, ptr %0
  ret void
}

declare <1024 x i1> @llvm.kvx.xsx48bw(<256 x i1>)

define void @xzx48bw_test(ptr writeonly %0, ptr %1) {
; CHECK-LABEL: xzx48bw_test:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo $a0 = 0[$r1]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xzx48bw $a0a1a2a3 = $a0
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 8)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 9)
; CHECK-NEXT:    xso 96[$r0] = $a3
; CHECK-NEXT:    ;; # (end cycle 10)
; CHECK-NEXT:    xso 64[$r0] = $a2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 11)
  %3 = load <256 x i1>, ptr %1
  %4 = tail call <1024 x i1> @llvm.kvx.xzx48bw(<256 x i1> %3)
  store <1024 x i1> %4, ptr %0
  ret void
}

declare <1024 x i1> @llvm.kvx.xzx48bw(<256 x i1>)

define void @xtrunc48wb_test(ptr %0, ptr writeonly %1) {
; CHECK-LABEL: xtrunc48wb_test:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo $a3 = 96[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo $a2 = 64[$r0]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xlo $a1 = 32[$r0]
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    xlo $a0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    xtrunc48wb $a0 = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 7)
; CHECK-NEXT:    xso 0[$r1] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 11)
  %3 = load <1024 x i1>, ptr %0
  %4 = tail call <256 x i1> @llvm.kvx.xtrunc48wb(<1024 x i1> %3)
  store <256 x i1> %4, ptr %1
  ret void
}

declare <256 x i1> @llvm.kvx.xtrunc48wb(<1024 x i1>)

define void @xmt44d_test(ptr %0) {
; CHECK-LABEL: xmt44d_test:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo $a3 = 96[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo $a2 = 64[$r0]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xlo $a1 = 32[$r0]
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    xlo $a0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    xmt44d $a0a1a2a3 = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 7)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 11)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 12)
; CHECK-NEXT:    xso 64[$r0] = $a2
; CHECK-NEXT:    ;; # (end cycle 13)
; CHECK-NEXT:    xso 96[$r0] = $a3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 14)
  %2 = load <1024 x i1>, ptr %0
  %3 = tail call <1024 x i1> @llvm.kvx.xmt44d(<1024 x i1> %2)
  store <1024 x i1> %3, ptr %0
  ret void
}

declare <1024 x i1> @llvm.kvx.xmt44d(<1024 x i1>)

define void @xload256(ptr %0) {
; CHECK-LABEL: xload256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo $a0 = 32[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xlo.s $a0 = 96[$r0]
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xso 64[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 9)
; CHECK-NEXT:    xlo.u $a0 = 96[$r0]
; CHECK-NEXT:    ;; # (end cycle 10)
; CHECK-NEXT:    xso 64[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 14)
; CHECK-NEXT:    xlo.us $a0 = 64[$r0]
; CHECK-NEXT:    ;; # (end cycle 15)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 19)
  %2 = getelementptr inbounds <256 x i1>, ptr %0, i64 1
  %3 = tail call <256 x i1> @llvm.kvx.xload256(ptr nonnull %2, i32 0)
  store <256 x i1> %3, ptr %0
  %4 = getelementptr inbounds <256 x i1>, ptr %0, i64 3
  %5 = tail call <256 x i1> @llvm.kvx.xload256(ptr nonnull %4, i32 1)
  %6 = getelementptr inbounds <256 x i1>, ptr %0, i64 2
  store <256 x i1> %5, ptr %6
  %7 = tail call <256 x i1> @llvm.kvx.xload256(ptr nonnull %4, i32 2)
  store <256 x i1> %7, ptr %6
  %8 = tail call <256 x i1> @llvm.kvx.xload256(ptr nonnull %6, i32 3)
  store <256 x i1> %8, ptr %0
  ret void
}

declare <256 x i1> @llvm.kvx.xload256(ptr, i32)

define void @xloadc256(ptr %0, i64 %1) {
; CHECK-LABEL: xloadc256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo.mt $r1 ? $a0 = 32[$r0]
; CHECK-NEXT:    compd.eq $r1 = $r1, 0
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo.s.mf $r1 ? $a0 = 96[$r0]
; CHECK-NEXT:    make $r1 = 0
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xlo.u.mtc $r1 ? $a0 = 96[$r0]
; CHECK-NEXT:    make $r1 = 1
; CHECK-NEXT:    ;; # (end cycle 8)
; CHECK-NEXT:    xlo.us.mfc $r1 ? $a0 = 64[$r0]
; CHECK-NEXT:    ;; # (end cycle 12)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 16)
  %3 = getelementptr inbounds <256 x i1>, ptr %0, i64 1
  %4 = tail call <256 x i1> @llvm.kvx.xloadc256(<256 x i1> undef, ptr nonnull %3, i64 %1, i32 0, i32 4)
  %5 = getelementptr inbounds <256 x i1>, ptr %0, i64 3
  %6 = icmp eq i64 %1, 0
  %7 = zext i1 %6 to i64
  %8 = tail call <256 x i1> @llvm.kvx.xloadc256(<256 x i1> %4, ptr nonnull %5, i64 %7, i32 1, i32 5)
  %9 = tail call <256 x i1> @llvm.kvx.xloadc256(<256 x i1> %8, ptr nonnull %5, i64 0, i32 2, i32 6)
  %10 = getelementptr inbounds <256 x i1>, ptr %0, i64 2
  %11 = tail call <256 x i1> @llvm.kvx.xloadc256(<256 x i1> %9, ptr nonnull %10, i64 1, i32 3, i32 7)
  store <256 x i1> %11, ptr %0
  ret void
}

declare <256 x i1> @llvm.kvx.xloadc256(<256 x i1>, ptr, i64, i32, i32)

define void @xload1024q(ptr %0) {
; CHECK-LABEL: xload1024q:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo.q0 $a0a1a2a3 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo.q1 $a0a1a2a3 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xlo.s.q2 $a0a1a2a3 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 8)
; CHECK-NEXT:    xlo.us.q3 $a0a1a2a3 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 12)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 16)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 17)
; CHECK-NEXT:    xso 96[$r0] = $a3
; CHECK-NEXT:    ;; # (end cycle 18)
; CHECK-NEXT:    xso 64[$r0] = $a2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 19)
  %2 = tail call <1024 x i1> @llvm.kvx.xload1024q0(<1024 x i1> undef, ptr %0, i32 0)
  %3 = tail call <1024 x i1> @llvm.kvx.xload1024q1(<1024 x i1> %2, ptr %0, i32 0)
  %4 = tail call <1024 x i1> @llvm.kvx.xload1024q2(<1024 x i1> %3, ptr %0, i32 1)
  %5 = tail call <1024 x i1> @llvm.kvx.xload1024q3(<1024 x i1> %4, ptr %0, i32 3)
  store <1024 x i1> %5, ptr %0
  ret void
}

declare <1024 x i1> @llvm.kvx.xload1024q0(<1024 x i1>, ptr, i32)
declare <1024 x i1> @llvm.kvx.xload1024q1(<1024 x i1>, ptr, i32)
declare <1024 x i1> @llvm.kvx.xload1024q2(<1024 x i1>, ptr, i32)
declare <1024 x i1> @llvm.kvx.xload1024q3(<1024 x i1>, ptr, i32)

define void @xloadc1024q(ptr %0, i64 %1) {
; CHECK-LABEL: xloadc1024q:
; CHECK:       # %bb.0:
; CHECK-NEXT:    compd.eq $r1 = $r1, 0
; CHECK-NEXT:    xlo.mt.q0 $r1 ? $a0a1a2a3 = [$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    make $r1 = 0
; CHECK-NEXT:    xlo.s.mf.q1 $r1 ? $a0a1a2a3 = [$r0]
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    make $r1 = 1
; CHECK-NEXT:    xlo.u.mtc.q2 $r1 ? $a0a1a2a3 = [$r0]
; CHECK-NEXT:    ;; # (end cycle 8)
; CHECK-NEXT:    xlo.us.mfc.q3 $r1 ? $a0a1a2a3 = [$r0]
; CHECK-NEXT:    ;; # (end cycle 12)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 16)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 17)
; CHECK-NEXT:    xso 96[$r0] = $a3
; CHECK-NEXT:    ;; # (end cycle 18)
; CHECK-NEXT:    xso 64[$r0] = $a2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 19)
  %3 = tail call <1024 x i1> @llvm.kvx.xloadc1024q0(<1024 x i1> undef, ptr %0, i64 %1, i32 0, i32 4)
  %4 = icmp eq i64 %1, 0
  %5 = zext i1 %4 to i64
  %6 = tail call <1024 x i1> @llvm.kvx.xloadc1024q1(<1024 x i1> %3, ptr %0, i64 %5, i32 1, i32 5)
  %7 = tail call <1024 x i1> @llvm.kvx.xloadc1024q2(<1024 x i1> %6, ptr %0, i64 0, i32 2, i32 6)
  %8 = tail call <1024 x i1> @llvm.kvx.xloadc1024q3(<1024 x i1> %7, ptr %0, i64 1, i32 3, i32 7)
  store <1024 x i1> %8, ptr %0
  ret void
}

declare <1024 x i1> @llvm.kvx.xloadc1024q0(<1024 x i1>, ptr, i64, i32, i32)
declare <1024 x i1> @llvm.kvx.xloadc1024q1(<1024 x i1>, ptr, i64, i32, i32)
declare <1024 x i1> @llvm.kvx.xloadc1024q2(<1024 x i1>, ptr, i64, i32, i32)
declare <1024 x i1> @llvm.kvx.xloadc1024q3(<1024 x i1>, ptr, i64, i32, i32)

define void @xload512(ptr %0, i64 %1) {
; CHECK-LABEL: xload512:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo $a0 = 0[$r0]
; CHECK-NEXT:    addx64d $r1 = $r1, $r0
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo $a1 = 32[$r0]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xso 64[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 96[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xlo $a0 = 0[$r1]
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    xlo $a1 = 32[$r1]
; CHECK-NEXT:    ;; # (end cycle 7)
; CHECK-NEXT:    xso 128[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 10)
; CHECK-NEXT:    xso 160[$r0] = $a1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 11)
  %3 = tail call <512 x i1> @llvm.kvx.xload512(ptr %0, i32 0)
  %4 = getelementptr inbounds <512 x i1>, ptr %0, i64 1
  store <512 x i1> %3, ptr %4
  %5 = getelementptr inbounds <512 x i1>, ptr %0, i64 %1
  %6 = tail call <512 x i1> @llvm.kvx.xload512(ptr %5, i32 0)
  %7 = getelementptr inbounds <512 x i1>, ptr %0, i64 2
  store <512 x i1> %6, ptr %7
  ret void
}

declare <512 x i1> @llvm.kvx.xload512(ptr, i32)

define void @xload1024(ptr %0, i64 %1) {
; CHECK-LABEL: xload1024:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo $a0 = 0[$r0]
; CHECK-NEXT:    slld $r1 = $r1, 7
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo $a1 = 32[$r0]
; CHECK-NEXT:    addd $r1 = $r0, $r1
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xlo $a2 = 64[$r0]
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    xlo $a3 = 96[$r0]
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    xso 128[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 160[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xso 192[$r0] = $a2
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    xso 224[$r0] = $a3
; CHECK-NEXT:    ;; # (end cycle 7)
; CHECK-NEXT:    xlo.s $a0 = 0[$r1]
; CHECK-NEXT:    ;; # (end cycle 8)
; CHECK-NEXT:    xlo.s $a1 = 32[$r1]
; CHECK-NEXT:    ;; # (end cycle 9)
; CHECK-NEXT:    xlo.s $a2 = 64[$r1]
; CHECK-NEXT:    ;; # (end cycle 10)
; CHECK-NEXT:    xlo.s $a3 = 96[$r1]
; CHECK-NEXT:    ;; # (end cycle 11)
; CHECK-NEXT:    xso 256[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 12)
; CHECK-NEXT:    xso 288[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 13)
; CHECK-NEXT:    xso 320[$r0] = $a2
; CHECK-NEXT:    ;; # (end cycle 14)
; CHECK-NEXT:    xso 352[$r0] = $a3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 15)
  %3 = tail call <1024 x i1> @llvm.kvx.xload1024(ptr %0, i32 0)
  %4 = getelementptr inbounds <1024 x i1>, ptr %0, i64 1
  store <1024 x i1> %3, ptr %4
  %5 = getelementptr inbounds <1024 x i1>, ptr %0, i64 %1
  %6 = tail call <1024 x i1> @llvm.kvx.xload1024(ptr %5, i32 1)
  %7 = getelementptr inbounds <1024 x i1>, ptr %0, i64 2
  store <1024 x i1> %6, ptr %7
  ret void
}

declare <1024 x i1> @llvm.kvx.xload1024(ptr, i32)

define void @xloadc512(ptr %0, i64 %1) {
; CHECK-LABEL: xloadc512:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addd $r2 = $r0, 64
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo.dnez $r1 ? $a0 = [$r2]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xlo.dnez $r1 ? $a1 = 32[$r2]
; CHECK-NEXT:    compd.eq $r1 = $r1, 0
; CHECK-NEXT:    addd $r2 = $r0, 192
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    xlo.s.weqz $r1 ? $a0 = [$r2]
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xlo.s.weqz $r1 ? $a1 = 32[$r2]
; CHECK-NEXT:    make $r1 = 0
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    xlo.u.mtc $r1 ? $a0 = [$r2]
; CHECK-NEXT:    srld $r1 = $r1, 32
; CHECK-NEXT:    ;; # (end cycle 9)
; CHECK-NEXT:    xlo.u.mtc $r1 ? $a1 = 32[$r2]
; CHECK-NEXT:    addd $r1 = $r0, 128
; CHECK-NEXT:    make $r2 = 1
; CHECK-NEXT:    ;; # (end cycle 10)
; CHECK-NEXT:    xlo.us.mfc $r2 ? $a0 = [$r1]
; CHECK-NEXT:    srld $r2 = $r2, 32
; CHECK-NEXT:    ;; # (end cycle 13)
; CHECK-NEXT:    xlo.us.mfc $r2 ? $a1 = 32[$r1]
; CHECK-NEXT:    ;; # (end cycle 14)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 17)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 18)
  %3 = getelementptr inbounds <512 x i1>, ptr %0, i64 1
  %4 = tail call <512 x i1> @llvm.kvx.xloadc512(<512 x i1> undef, ptr nonnull %3, i64 %1, i32 0, i32 0)
  %5 = getelementptr inbounds <512 x i1>, ptr %0, i64 3
  %6 = icmp eq i64 %1, 0
  %7 = zext i1 %6 to i64
  %8 = tail call <512 x i1> @llvm.kvx.xloadc512(<512 x i1> %4, ptr nonnull %5, i64 %7, i32 1, i32 3)
  %9 = tail call <512 x i1> @llvm.kvx.xloadc512(<512 x i1> %8, ptr nonnull %5, i64 0, i32 2, i32 6)
  %10 = getelementptr inbounds <512 x i1>, ptr %0, i64 2
  %11 = tail call <512 x i1> @llvm.kvx.xloadc512(<512 x i1> %9, ptr nonnull %10, i64 1, i32 3, i32 7)
  store <512 x i1> %11, ptr %0
  ret void
}

declare <512 x i1> @llvm.kvx.xloadc512(<512 x i1>, ptr, i64, i32, i32)

define void @xloadc1024(ptr %0, <4 x i32> %1) {
; CHECK-LABEL: xloadc1024:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addd $r1 = $r0, 128
; CHECK-NEXT:    copyd $r2 = $r1
; CHECK-NEXT:    copyd $r3 = $r2
; CHECK-NEXT:    addd $r4 = $r0, 384
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo.dnez $r2 ? $a0 = [$r1]
; CHECK-NEXT:    srld $r5 = $r3, 32
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xlo.dnez $r2 ? $a1 = 32[$r1]
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    xlo.dnez $r2 ? $a2 = 64[$r1]
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    xcopyo $a3 = $a1
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    xlo.dnez $r2 ? $a2 = 96[$r1]
; CHECK-NEXT:    srld $r1 = $r2, 32
; CHECK-NEXT:    ;; # (end cycle 7)
; CHECK-NEXT:    xlo.s.weqz $r2 ? $a0 = [$r4]
; CHECK-NEXT:    xcopyo $a2 = $a1
; CHECK-NEXT:    ;; # (end cycle 8)
; CHECK-NEXT:    xlo.s.weqz $r2 ? $a2 = 32[$r4]
; CHECK-NEXT:    ;; # (end cycle 12)
; CHECK-NEXT:    xlo.s.weqz $r2 ? $a3 = 64[$r4]
; CHECK-NEXT:    ;; # (end cycle 13)
; CHECK-NEXT:    xlo.s.weqz $r2 ? $a1 = 96[$r4]
; CHECK-NEXT:    ;; # (end cycle 14)
; CHECK-NEXT:    xlo.u.mtc $r2 ? $a0 = [$r4]
; CHECK-NEXT:    ;; # (end cycle 15)
; CHECK-NEXT:    xcopyo $a1 = $a2
; CHECK-NEXT:    xlo.u.mtc $r1 ? $a2 = 32[$r4]
; CHECK-NEXT:    ;; # (end cycle 16)
; CHECK-NEXT:    xcopyo $a3 = $a1
; CHECK-NEXT:    ;; # (end cycle 20)
; CHECK-NEXT:    xlo.u.mtc $r3 ? $a3 = 64[$r4]
; CHECK-NEXT:    ;; # (end cycle 24)
; CHECK-NEXT:    xlo.u.mtc $r5 ? $a1 = 96[$r4]
; CHECK-NEXT:    addd $r4 = $r0, 256
; CHECK-NEXT:    ;; # (end cycle 25)
; CHECK-NEXT:    xlo.us.mfc $r2 ? $a0 = [$r4]
; CHECK-NEXT:    xcopyo $a1 = $a2
; CHECK-NEXT:    ;; # (end cycle 26)
; CHECK-NEXT:    xcopyo $a5 = $a1
; CHECK-NEXT:    ;; # (end cycle 30)
; CHECK-NEXT:    xlo.us.mfc $r1 ? $a5 = 32[$r4]
; CHECK-NEXT:    ;; # (end cycle 34)
; CHECK-NEXT:    xlo.us.mfc $r3 ? $a2 = 64[$r4]
; CHECK-NEXT:    ;; # (end cycle 35)
; CHECK-NEXT:    xlo.us.mfc $r5 ? $a1 = 96[$r4]
; CHECK-NEXT:    ;; # (end cycle 36)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 37)
; CHECK-NEXT:    xso 32[$r0] = $a5
; CHECK-NEXT:    ;; # (end cycle 38)
; CHECK-NEXT:    xso 64[$r0] = $a2
; CHECK-NEXT:    ;; # (end cycle 39)
; CHECK-NEXT:    xcopyo $a3 = $a1
; CHECK-NEXT:    ;; # (end cycle 40)
; CHECK-NEXT:    xso 96[$r0] = $a3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 44)
  %3 = getelementptr inbounds <1024 x i1>, ptr %0, i64 1
  %4 = tail call <1024 x i1> @llvm.kvx.xloadc1024(<1024 x i1> undef, ptr nonnull %3, <4 x i32> %1, i32 0, i32 0)
  %5 = getelementptr inbounds <1024 x i1>, ptr %0, i64 3
  %6 = tail call <1024 x i1> @llvm.kvx.xloadc1024(<1024 x i1> %4, ptr nonnull %5, <4 x i32> %1, i32 1, i32 3)
  %7 = tail call <1024 x i1> @llvm.kvx.xloadc1024(<1024 x i1> %6, ptr nonnull %5, <4 x i32> %1, i32 2, i32 6)
  %8 = getelementptr inbounds <1024 x i1>, ptr %0, i64 2
  %9 = tail call <1024 x i1> @llvm.kvx.xloadc1024(<1024 x i1> %7, ptr nonnull %8, <4 x i32> %1, i32 3, i32 7)
  store <1024 x i1> %9, ptr %0
  ret void
}

declare <1024 x i1> @llvm.kvx.xloadc1024(<1024 x i1>, ptr, <4 x i32>, i32, i32)

define void @xstore256(ptr %0, ptr %1) {
; CHECK-LABEL: xstore256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo $a0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xso 0[$r1] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 4)
  %3 = load <256 x i1>, ptr %0
  tail call void @llvm.kvx.xstore256(<256 x i1> %3, ptr %1)
  ret void
}

declare void @llvm.kvx.xstore256(<256 x i1>, ptr)

define void @xstorec256(ptr %0, ptr %1, i64 %2) {
; CHECK-LABEL: xstorec256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo $a0 = 0[$r0]
; CHECK-NEXT:    make $r0 = 1
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xso.mt $r0 ? [$r1] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 4)
  %4 = load <256 x i1>, ptr %0
  tail call void @llvm.kvx.xstorec256(<256 x i1> %4, ptr %1, i64 1, i32 4)
  ret void
}

declare void @llvm.kvx.xstorec256(<256 x i1>, ptr, i64, i32)

define void @xloadStore256(ptr addrspace(257) %0) {
; CHECK-LABEL: xloadStore256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo.us $a0 = 32[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 4)
  %2 = getelementptr inbounds <256 x i1>, ptr addrspace(257) %0, i64 1
  %3 = load volatile <256 x i1>, ptr addrspace(257) %2
  store volatile <256 x i1> %3, ptr addrspace(257) %0
  ret void
}

define void @xloadStore512(ptr addrspace(258) %0) {
; CHECK-LABEL: xloadStore512:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo.s $a0 = 96[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo.s $a1 = 64[$r0]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xso 0[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xso 32[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 6)
  %2 = getelementptr inbounds <512 x i1>, ptr addrspace(258) %0, i64 1
  %3 = load volatile <512 x i1>, ptr addrspace(258) %2
  store volatile <512 x i1> %3, ptr addrspace(258) %0
  ret void
}

define void @xloadStore1024(ptr addrspace(256) %0) {
; CHECK-LABEL: xloadStore1024:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo.u $a0 = 224[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo.u $a1 = 192[$r0]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xlo.u $a2 = 160[$r0]
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    xlo.u $a3 = 128[$r0]
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    xso 0[$r0] = $a3
; CHECK-NEXT:    ;; # (end cycle 7)
; CHECK-NEXT:    xso 32[$r0] = $a2
; CHECK-NEXT:    ;; # (end cycle 8)
; CHECK-NEXT:    xso 64[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 9)
; CHECK-NEXT:    xso 96[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 10)
  %2 = getelementptr inbounds <1024 x i1>, ptr addrspace(256) %0, i64 1
  %3 = load volatile <1024 x i1>, ptr addrspace(256) %2
  store volatile <1024 x i1> %3, ptr addrspace(256) %0
  ret void
}

define void @xsendo(ptr %0) {
; CHECK-LABEL: xsendo:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo $a0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xsendo.b $a0
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xlo $a0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xsendo.f $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 9)
  %2 = load <256 x i1>, ptr %0
  tail call void @llvm.kvx.xsendo(<256 x i1> %2, i32 1)
  %3 = load <256 x i1>, ptr %0
  tail call void @llvm.kvx.xsendo(<256 x i1> %3, i32 0)
  ret void
}

declare void @llvm.kvx.xsendo(<256 x i1>, i32)

define void @xrecvo(ptr writeonly %0) {
; CHECK-LABEL: xrecvo:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xrecvo.b $a0
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xrecvo.f $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
  %2 = tail call <256 x i1> @llvm.kvx.xrecvo(i32 1)
  %3 = tail call <256 x i1> @llvm.kvx.xrecvo(i32 0)
  ret void
}

declare <256 x i1> @llvm.kvx.xrecvo(i32)

define void @xsendrecvo(ptr %0) {
; CHECK-LABEL: xsendrecvo:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo $a0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xsendrecvo.b.b $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xlo $a1 = 32[$r0]
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xso 64[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 8)
; CHECK-NEXT:    xsendrecvo.f.b $a1, $a0
; CHECK-NEXT:    ;; # (end cycle 9)
; CHECK-NEXT:    xlo $a1 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 10)
; CHECK-NEXT:    xso 128[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 13)
; CHECK-NEXT:    xsendrecvo.b.f $a1, $a0
; CHECK-NEXT:    ;; # (end cycle 14)
; CHECK-NEXT:    xlo $a1 = 32[$r0]
; CHECK-NEXT:    ;; # (end cycle 15)
; CHECK-NEXT:    xso 96[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 18)
; CHECK-NEXT:    xsendrecvo.f.f $a1, $a0
; CHECK-NEXT:    ;; # (end cycle 19)
; CHECK-NEXT:    xso 160[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 23)
  %2 = load <256 x i1>, ptr %0
  %3 = tail call <256 x i1> @llvm.kvx.xsendrecvo(<256 x i1> %2, i32 1, i32 1)
  %4 = getelementptr inbounds <256 x i1>, ptr %0, i64 2
  store <256 x i1> %3, ptr %4
  %5 = getelementptr inbounds <256 x i1>, ptr %0, i64 1
  %6 = load <256 x i1>, ptr %5
  %7 = tail call <256 x i1> @llvm.kvx.xsendrecvo(<256 x i1> %6, i32 0, i32 1)
  %8 = getelementptr inbounds <256 x i1>, ptr %0, i64 4
  store <256 x i1> %7, ptr %8
  %9 = load <256 x i1>, ptr %0
  %10 = tail call <256 x i1> @llvm.kvx.xsendrecvo(<256 x i1> %9, i32 1, i32 0)
  %11 = getelementptr inbounds <256 x i1>, ptr %0, i64 3
  store <256 x i1> %10, ptr %11
  %12 = load <256 x i1>, ptr %5
  %13 = tail call <256 x i1> @llvm.kvx.xsendrecvo(<256 x i1> %12, i32 0, i32 0)
  %14 = getelementptr inbounds <256 x i1>, ptr %0, i64 5
  store <256 x i1> %13, ptr %14
  ret void
}

declare <256 x i1> @llvm.kvx.xsendrecvo(<256 x i1>, i32, i32)

define void @xcopyv(ptr %0) {
; CHECK-LABEL: xcopyv:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo $a3 = 96[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo $a2 = 64[$r0]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xlo $a1 = 32[$r0]
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    xlo $a0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    xso 480[$r0] = $a3
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 448[$r0] = $a2
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xso 416[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    xso 384[$r0] = $a0
; CHECK-NEXT:    xcopyv.td $a4a5a6a7 = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 7)
; CHECK-NEXT:    xso 160[$r0] = $a5
; CHECK-NEXT:    ;; # (end cycle 11)
; CHECK-NEXT:    xso 128[$r0] = $a4
; CHECK-NEXT:    ;; # (end cycle 12)
; CHECK-NEXT:    xso 224[$r0] = $a7
; CHECK-NEXT:    ;; # (end cycle 13)
; CHECK-NEXT:    xso 192[$r0] = $a6
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 14)
  %2 = load <1024 x i1>, ptr %0
  %3 = tail call <1024 x i1> @llvm.kvx.xcopyv(<1024 x i1> %2, i32 0)
  %4 = getelementptr inbounds <1024 x i1>, ptr %0, i64 3
  store <1024 x i1> %3, ptr %4
  %5 = tail call <1024 x i1> @llvm.kvx.xcopyv(<1024 x i1> %2, i32 1)
  %6 = getelementptr inbounds <1024 x i1>, ptr %0, i64 1
  store <1024 x i1> %5, ptr %6
  ret void
}

declare <1024 x i1> @llvm.kvx.xcopyv(<1024 x i1>, i32)

define void @xcopyx(ptr %0) {
; CHECK-LABEL: xcopyx:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo $a1 = 32[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo $a0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xso 96[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 64[$r0] = $a0
; CHECK-NEXT:    xcopyx.tq $a2a3 = $a0a1
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xcopyx.zd $a4a5 = $a0a1
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    xcopyx.ud $a6a7 = $a0a1
; CHECK-NEXT:    ;; # (end cycle 7)
; CHECK-NEXT:    xcopyx.tw $a2a3 = $a2a3
; CHECK-NEXT:    ;; # (end cycle 9)
; CHECK-NEXT:    xso 160[$r0] = $a5
; CHECK-NEXT:    ;; # (end cycle 10)
; CHECK-NEXT:    xso 128[$r0] = $a4
; CHECK-NEXT:    ;; # (end cycle 11)
; CHECK-NEXT:    xso 224[$r0] = $a7
; CHECK-NEXT:    ;; # (end cycle 12)
; CHECK-NEXT:    xso 192[$r0] = $a6
; CHECK-NEXT:    xcopyx.zw $a2a3 = $a2a3
; CHECK-NEXT:    ;; # (end cycle 13)
; CHECK-NEXT:    xcopyx.uw $a0a1 = $a2a3
; CHECK-NEXT:    ;; # (end cycle 17)
; CHECK-NEXT:    xso 288[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 21)
; CHECK-NEXT:    xso 256[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 22)
  %2 = load <512 x i1>, ptr %0
  %3 = tail call <512 x i1> @llvm.kvx.xcopyx(<512 x i1> %2, i32 0)
  %4 = getelementptr inbounds <512 x i1>, ptr %0, i64 1
  store <512 x i1> %3, ptr %4
  %5 = tail call <512 x i1> @llvm.kvx.xcopyx(<512 x i1> %2, i32 1)
  %6 = getelementptr inbounds <512 x i1>, ptr %0, i64 2
  store <512 x i1> %5, ptr %6
  %7 = tail call <512 x i1> @llvm.kvx.xcopyx(<512 x i1> %2, i32 2)
  %8 = getelementptr inbounds <512 x i1>, ptr %0, i64 3
  store <512 x i1> %7, ptr %8
  %9 = tail call <512 x i1> @llvm.kvx.xcopyx(<512 x i1> %2, i32 3)
  %.9 = tail call <512 x i1> @llvm.kvx.xcopyx(<512 x i1> %9, i32 4)
  %..9 = tail call <512 x i1> @llvm.kvx.xcopyx(<512 x i1> %.9, i32 5)
  %...9 = tail call <512 x i1> @llvm.kvx.xcopyx(<512 x i1> %..9, i32 6)
  %10 = getelementptr inbounds <512 x i1>, ptr %0, i64 4
  store <512 x i1> %...9, ptr %10
  ret void
}

declare <512 x i1> @llvm.kvx.xcopyx(<512 x i1>, i32)

define void @xfminmaxhx(ptr %0) {
; CHECK-LABEL: xfminmaxhx:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo $a0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo $a1 = 32[$r0]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xfmaxhx $a1 = $a0, $a1
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xfminhx $a0 = $a1, $a0
; CHECK-NEXT:    ;; # (end cycle 10)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 15)
  %2 = load <256 x i1>, ptr %0
  %3 = getelementptr inbounds <256 x i1>, ptr %0, i64 1
  %4 = load <256 x i1>, ptr %3
  %5 = tail call <256 x i1> @llvm.kvx.xfmaxhx(<256 x i1> %2, <256 x i1> %4)
  %6 = tail call <256 x i1> @llvm.kvx.xfminhx(<256 x i1> %5, <256 x i1> %2)
  store <256 x i1> %6, ptr %0
  ret void
}

declare <256 x i1> @llvm.kvx.xfminhx(<256 x i1>, <256 x i1>)

declare <256 x i1> @llvm.kvx.xfmaxhx(<256 x i1>, <256 x i1>)

define void @xsplatov(ptr writeonly %0, ptr %1) {
; CHECK-LABEL: xsplatov:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo $a4 = 0[$r1]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xsplatov $a0a1a2a3 = $a4
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xsplatov.td $a4a5a6a7 = $a4
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 8)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 9)
; CHECK-NEXT:    xso 96[$r0] = $a3
; CHECK-NEXT:    ;; # (end cycle 10)
; CHECK-NEXT:    xso 64[$r0] = $a2
; CHECK-NEXT:    ;; # (end cycle 11)
; CHECK-NEXT:    xso 160[$r0] = $a5
; CHECK-NEXT:    ;; # (end cycle 12)
; CHECK-NEXT:    xso 128[$r0] = $a4
; CHECK-NEXT:    ;; # (end cycle 13)
; CHECK-NEXT:    xso 224[$r0] = $a7
; CHECK-NEXT:    ;; # (end cycle 14)
; CHECK-NEXT:    xso 192[$r0] = $a6
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 15)
  %3 = load <256 x i1>, ptr %1
  %4 = tail call <1024 x i1> @llvm.kvx.xsplatov(<256 x i1> %3, i32 0)
  store <1024 x i1> %4, ptr %0
  %5 = tail call <1024 x i1> @llvm.kvx.xsplatov(<256 x i1> %3, i32 1)
  %6 = getelementptr inbounds <1024 x i1>, ptr %0, i64 1
  store <1024 x i1> %5, ptr %6
  ret void
}

declare <1024 x i1> @llvm.kvx.xsplatov(<256 x i1>, i32)

define void @xsplatox(ptr writeonly %0, ptr %1) {
; CHECK-LABEL: xsplatox:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo $a6 = 0[$r1]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xsplatox $a0a1 = $a6
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xsplatox.zd $a2a3 = $a6
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xsplatox.ud $a4a5 = $a6
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 8)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    xsplatox.tq $a0a1 = $a6
; CHECK-NEXT:    ;; # (end cycle 9)
; CHECK-NEXT:    xso 96[$r0] = $a3
; CHECK-NEXT:    ;; # (end cycle 10)
; CHECK-NEXT:    xso 64[$r0] = $a2
; CHECK-NEXT:    ;; # (end cycle 11)
; CHECK-NEXT:    xso 160[$r0] = $a5
; CHECK-NEXT:    ;; # (end cycle 12)
; CHECK-NEXT:    xso 128[$r0] = $a4
; CHECK-NEXT:    ;; # (end cycle 13)
; CHECK-NEXT:    xso 224[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 14)
; CHECK-NEXT:    xso 192[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 15)
  %3 = load <256 x i1>, ptr %1
  %4 = tail call <512 x i1> @llvm.kvx.xsplatox(<256 x i1> %3, i32 0)
  store <512 x i1> %4, ptr %0
  %5 = tail call <512 x i1> @llvm.kvx.xsplatox(<256 x i1> %3, i32 1)
  %6 = getelementptr inbounds <512 x i1>, ptr %0, i64 1
  store <512 x i1> %5, ptr %6
  %7 = tail call <512 x i1> @llvm.kvx.xsplatox(<256 x i1> %3, i32 2)
  %8 = getelementptr inbounds <512 x i1>, ptr %0, i64 2
  store <512 x i1> %7, ptr %8
  %9 = tail call <512 x i1> @llvm.kvx.xsplatox(<256 x i1> %3, i32 3)
  %10 = getelementptr inbounds <512 x i1>, ptr %0, i64 3
  store <512 x i1> %9, ptr %10
  ret void
}

declare <512 x i1> @llvm.kvx.xsplatox(<256 x i1>, i32)

define void @xsplatdo(ptr writeonly %0) {
; CHECK-LABEL: xsplatdo:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xsplatdo $a0 = 511
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xsplatdo $a1 = 0x1fffffffff
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xsplatdo $a2 = 0x18ffffffff9c
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xso 64[$r0] = $a2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 6)
  %2 = tail call <256 x i1> @llvm.kvx.xsplatdo(i64 511)
  store <256 x i1> %2, ptr %0
  %3 = tail call <256 x i1> @llvm.kvx.xsplatdo(i64 137438953471)
  %4 = getelementptr inbounds <256 x i1>, ptr %0, i64 1
  store <256 x i1> %3, ptr %4
  %5 = tail call <256 x i1> @llvm.kvx.xsplatdo(i64 27487790694300)
  %6 = getelementptr inbounds <256 x i1>, ptr %0, i64 2
  store <256 x i1> %5, ptr %6
  ret void
}

declare <256 x i1> @llvm.kvx.xsplatdo(i64)

define void @xaligno512(ptr writeonly %0, ptr %1, i64 %2) {
; CHECK-LABEL: xaligno512:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo $a1 = 32[$r1]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo $a0 = 0[$r1]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xaligno $a0 = $a0..a1, $r2
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 9)
  %4 = load <512 x i1>, ptr %1
  %5 = tail call <256 x i1> @llvm.kvx.xaligno.v512i1(<512 x i1> %4, i64 %2)
  store <256 x i1> %5, ptr %0
  ret void
}

declare <256 x i1> @llvm.kvx.xaligno.v512i1(<512 x i1>, i64)

define void @xaligno1024(ptr writeonly %0, ptr %1, i64 %2) {
; CHECK-LABEL: xaligno1024:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo $a3 = 96[$r1]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo $a2 = 64[$r1]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xlo $a1 = 32[$r1]
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    xlo $a0 = 0[$r1]
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    xaligno $a0 = $a0..a3, $r2
; CHECK-NEXT:    ;; # (end cycle 7)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 11)
  %4 = load <1024 x i1>, ptr %1
  %5 = tail call <256 x i1> @llvm.kvx.xaligno.v1024i1(<1024 x i1> %4, i64 %2)
  store <256 x i1> %5, ptr %0
  ret void
}

declare <256 x i1> @llvm.kvx.xaligno.v1024i1(<1024 x i1>, i64)

define void @xaligno2048(ptr writeonly %0, i64 %1) {
; CHECK-LABEL: xaligno2048:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xsplatdo $a0 = 0
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xsplatov $a0a1a2a3 = $a0
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xcopyv $a4a5a6a7 = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 8)
; CHECK-NEXT:    xaligno $a0 = $a0..a7, $r1
; CHECK-NEXT:    ;; # (end cycle 12)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 16)
  %3 = tail call <256 x i1> @llvm.kvx.xaligno.v2048i1(<2048 x i1> zeroinitializer, i64 %1)
  store <256 x i1> %3, ptr %0
  ret void
}

declare <256 x i1> @llvm.kvx.xaligno.v2048i1(<2048 x i1>, i64)

define void @xaligno4096(ptr writeonly %0, i64 %1) {
; CHECK-LABEL: xaligno4096:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xsplatdo $a0 = 0
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xsplatov $a0a1a2a3 = $a0
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xcopyv $a4a5a6a7 = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 8)
; CHECK-NEXT:    xcopyv $a8a9a10a11 = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 9)
; CHECK-NEXT:    xcopyv $a12a13a14a15 = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 10)
; CHECK-NEXT:    xaligno $a0 = $a0..a15, $r1
; CHECK-NEXT:    ;; # (end cycle 14)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 18)
  %3 = tail call <256 x i1> @llvm.kvx.xaligno.v4096i1(<4096 x i1> zeroinitializer, i64 %1)
  store <256 x i1> %3, ptr %0
  ret void
}

declare <256 x i1> @llvm.kvx.xaligno.v4096i1(<4096 x i1>, i64)

define void @xpreload512(ptr %0, i64 %1) {
; CHECK-LABEL: xpreload512:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo $a0..a1, $r1 = [$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 5)
  %3 = tail call <512 x i1> @llvm.kvx.xpreload.v512i1(<512 x i1> undef, ptr %0, i64 %1, i32 0, i32 0)
  store <512 x i1> %3, ptr %0
  ret void
}

declare <512 x i1> @llvm.kvx.xpreload.v512i1(<512 x i1>, ptr, i64, i32, i32)

define void @xpreload1024(ptr %0, i64 %1) {
; CHECK-LABEL: xpreload1024:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo.us.q $a0..a3, $r1 = [$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xso 96[$r0] = $a3
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    xso 64[$r0] = $a2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 7)
  %3 = tail call <1024 x i1> @llvm.kvx.xpreload.v1024i1(<1024 x i1> undef, ptr %0, i64 %1, i32 3, i32 1)
  store <1024 x i1> %3, ptr %0
  ret void
}

declare <1024 x i1> @llvm.kvx.xpreload.v1024i1(<1024 x i1>, ptr, i64, i32, i32)

define void @xpreload2048(ptr %0, i64 %1) {
; CHECK-LABEL: xpreload2048:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo.s.w $a0..a7, $r1 = [$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xso 96[$r0] = $a3
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    xso 64[$r0] = $a2
; CHECK-NEXT:    ;; # (end cycle 7)
; CHECK-NEXT:    xso 160[$r0] = $a5
; CHECK-NEXT:    ;; # (end cycle 8)
; CHECK-NEXT:    xso 128[$r0] = $a4
; CHECK-NEXT:    ;; # (end cycle 9)
; CHECK-NEXT:    xso 224[$r0] = $a7
; CHECK-NEXT:    ;; # (end cycle 10)
; CHECK-NEXT:    xso 192[$r0] = $a6
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 11)
  %3 = tail call <2048 x i1> @llvm.kvx.xpreload.v2048i1(<2048 x i1> undef, ptr %0, i64 %1, i32 1, i32 3)
  store <2048 x i1> %3, ptr %0
  ret void
}

declare <2048 x i1> @llvm.kvx.xpreload.v2048i1(<2048 x i1>, ptr, i64, i32, i32)

define void @xpreload4096(ptr %0, i64 %1) {
; CHECK-LABEL: xpreload4096:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo.u.b $a0..a15, $r1 = [$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xso 96[$r0] = $a3
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    xso 64[$r0] = $a2
; CHECK-NEXT:    ;; # (end cycle 7)
; CHECK-NEXT:    xso 160[$r0] = $a5
; CHECK-NEXT:    ;; # (end cycle 8)
; CHECK-NEXT:    xso 128[$r0] = $a4
; CHECK-NEXT:    ;; # (end cycle 9)
; CHECK-NEXT:    xso 224[$r0] = $a7
; CHECK-NEXT:    ;; # (end cycle 10)
; CHECK-NEXT:    xso 192[$r0] = $a6
; CHECK-NEXT:    ;; # (end cycle 11)
; CHECK-NEXT:    xso 288[$r0] = $a9
; CHECK-NEXT:    ;; # (end cycle 12)
; CHECK-NEXT:    xso 256[$r0] = $a8
; CHECK-NEXT:    ;; # (end cycle 13)
; CHECK-NEXT:    xso 352[$r0] = $a11
; CHECK-NEXT:    ;; # (end cycle 14)
; CHECK-NEXT:    xso 320[$r0] = $a10
; CHECK-NEXT:    ;; # (end cycle 15)
; CHECK-NEXT:    xso 416[$r0] = $a13
; CHECK-NEXT:    ;; # (end cycle 16)
; CHECK-NEXT:    xso 384[$r0] = $a12
; CHECK-NEXT:    ;; # (end cycle 17)
; CHECK-NEXT:    xso 480[$r0] = $a15
; CHECK-NEXT:    ;; # (end cycle 18)
; CHECK-NEXT:    xso 448[$r0] = $a14
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 19)
  %3 = tail call <4096 x i1> @llvm.kvx.xpreload.v4096i1(<4096 x i1> undef, ptr %0, i64 %1, i32 2, i32 5)
  store <4096 x i1> %3, ptr %0
  ret void
}

declare <4096 x i1> @llvm.kvx.xpreload.v4096i1(<4096 x i1>, ptr, i64, i32, i32)

define <4 x i64> @xaccesso512(ptr %0, i64 %1) {
; CHECK-LABEL: xaccesso512:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo $a1 = 32[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo $a0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xaccesso $r0r1r2r3 = $a0..a1, $r1
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <512 x i1>, ptr %0
  %4 = tail call <4 x i64> @llvm.kvx.xaccesso.v512i1(<512 x i1> %3, i64 %1)
  ret <4 x i64> %4
}

declare <4 x i64> @llvm.kvx.xaccesso.v512i1(<512 x i1>, i64)

define <4 x i64> @xaccesso1024(ptr %0, i64 %1) {
; CHECK-LABEL: xaccesso1024:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xlo $a3 = 96[$r0]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xlo $a2 = 64[$r0]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xlo $a1 = 32[$r0]
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    xlo $a0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    xaccesso $r0r1r2r3 = $a0..a3, $r1
; CHECK-NEXT:    ;; # (end cycle 7)
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = load <1024 x i1>, ptr %0
  %4 = tail call <4 x i64> @llvm.kvx.xaccesso.v1024i1(<1024 x i1> %3, i64 %1)
  ret <4 x i64> %4
}

declare <4 x i64> @llvm.kvx.xaccesso.v1024i1(<1024 x i1>, i64)

define <4 x i64> @xaccesso2048(i64 %0) {
; CHECK-LABEL: xaccesso2048:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xsplatdo $a0 = 0
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xsplatov $a0a1a2a3 = $a0
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xcopyv $a4a5a6a7 = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 8)
; CHECK-NEXT:    xaccesso $r0r1r2r3 = $a0..a7, $r0
; CHECK-NEXT:    ;; # (end cycle 12)
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %2 = tail call <4 x i64> @llvm.kvx.xaccesso.v2048i1(<2048 x i1> zeroinitializer, i64 %0)
  ret <4 x i64> %2
}

declare <4 x i64> @llvm.kvx.xaccesso.v2048i1(<2048 x i1>, i64)

define <4 x i64> @xaccesso4096(i64 %0) {
; CHECK-LABEL: xaccesso4096:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xsplatdo $a0 = 0
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xsplatov $a0a1a2a3 = $a0
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xcopyv $a4a5a6a7 = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 8)
; CHECK-NEXT:    xcopyv $a8a9a10a11 = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 9)
; CHECK-NEXT:    xcopyv $a12a13a14a15 = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 10)
; CHECK-NEXT:    xaccesso $r0r1r2r3 = $a0..a15, $r0
; CHECK-NEXT:    ;; # (end cycle 14)
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %2 = tail call <4 x i64> @llvm.kvx.xaccesso.v4096i1(<4096 x i1> zeroinitializer, i64 %0)
  ret <4 x i64> %2
}

declare <4 x i64> @llvm.kvx.xaccesso.v4096i1(<4096 x i1>, i64)

define void @binOps(ptr writeonly %0) {
; CHECK-LABEL: binOps:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xsplatdo $a0 = 734
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xandno $a0 = $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xando $a0 = $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 8)
; CHECK-NEXT:    xnando $a0 = $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 12)
; CHECK-NEXT:    xnioro $a0 = $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 16)
; CHECK-NEXT:    xneoro $a0 = $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 20)
; CHECK-NEXT:    xiorno $a0 = $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 24)
; CHECK-NEXT:    xioro $a0 = $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 28)
; CHECK-NEXT:    xeoro $a0 = $a0, $a0
; CHECK-NEXT:    ;; # (end cycle 32)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 36)
  %2 = tail call <256 x i1> @llvm.kvx.xsplat.v256i1(i64 734)
  %3 = tail call <256 x i1> @llvm.kvx.xandno(<256 x i1> %2, <256 x i1> %2)
  %4 = tail call <256 x i1> @llvm.kvx.xando(<256 x i1> %3, <256 x i1> %3)
  %5 = tail call <256 x i1> @llvm.kvx.xnando(<256 x i1> %4, <256 x i1> %4)
  %6 = tail call <256 x i1> @llvm.kvx.xnioro(<256 x i1> %5, <256 x i1> %5)
  %7 = tail call <256 x i1> @llvm.kvx.xneoro(<256 x i1> %6, <256 x i1> %6)
  %8 = tail call <256 x i1> @llvm.kvx.xiorno(<256 x i1> %7, <256 x i1> %7)
  %9 = tail call <256 x i1> @llvm.kvx.xioro(<256 x i1> %8, <256 x i1> %8)
  %10 = tail call <256 x i1> @llvm.kvx.xeoro(<256 x i1> %9, <256 x i1> %9)
  store <256 x i1> %10, ptr %0
  ret void
}

declare <256 x i1> @llvm.kvx.xsplat.v256i1(i64)

declare <256 x i1> @llvm.kvx.xandno(<256 x i1>, <256 x i1>)

declare <256 x i1> @llvm.kvx.xando(<256 x i1>, <256 x i1>)

declare <256 x i1> @llvm.kvx.xnando(<256 x i1>, <256 x i1>)

declare <256 x i1> @llvm.kvx.xnioro(<256 x i1>, <256 x i1>)

declare <256 x i1> @llvm.kvx.xneoro(<256 x i1>, <256 x i1>)

declare <256 x i1> @llvm.kvx.xiorno(<256 x i1>, <256 x i1>)

declare <256 x i1> @llvm.kvx.xioro(<256 x i1>, <256 x i1>)

declare <256 x i1> @llvm.kvx.xeoro(<256 x i1>, <256 x i1>)

define void @xsbmm8(ptr writeonly %0) {
; CHECK-LABEL: xsbmm8:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xsplatdo $a0 = 734
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xsplatdo $a1 = 15
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    xsbmm8dq $a0 = $a0, $a1
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    xsbmmt8dq $a0 = $a1, $a0
; CHECK-NEXT:    ;; # (end cycle 9)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 13)
  %2 = tail call <256 x i1> @llvm.kvx.xsplat.v256i1(i64 734)
  %3 = tail call <256 x i1> @llvm.kvx.xsplat.v256i1(i64 15)
  %4 = tail call <256 x i1> @llvm.kvx.xsbmm8dq(<256 x i1> %2, <256 x i1> %3)
  %5 = tail call <256 x i1> @llvm.kvx.xsbmmt8dq(<256 x i1> %3, <256 x i1> %4)
  store <256 x i1> %5, ptr %0
  ret void
}

declare <256 x i1> @llvm.kvx.xsbmm8dq(<256 x i1>, <256 x i1>)

declare <256 x i1> @llvm.kvx.xsbmmt8dq(<256 x i1>, <256 x i1>)

define void @xcat2048(ptr writeonly %0) {
; CHECK-LABEL: xcat2048:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xsplatdo $a0 = 0
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xsplatov $a0a1a2a3 = $a0
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    xcopyv $a4a5a6a7 = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 8)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;; # (end cycle 9)
; CHECK-NEXT:    xso 96[$r0] = $a3
; CHECK-NEXT:    ;; # (end cycle 10)
; CHECK-NEXT:    xso 64[$r0] = $a2
; CHECK-NEXT:    ;; # (end cycle 11)
; CHECK-NEXT:    xso 160[$r0] = $a5
; CHECK-NEXT:    ;; # (end cycle 12)
; CHECK-NEXT:    xso 128[$r0] = $a4
; CHECK-NEXT:    ;; # (end cycle 13)
; CHECK-NEXT:    xso 224[$r0] = $a7
; CHECK-NEXT:    ;; # (end cycle 14)
; CHECK-NEXT:    xso 192[$r0] = $a6
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 15)
  %2 = tail call <2048 x i1> @llvm.kvx.cat.v2048i1(<1024 x i1> zeroinitializer, <1024 x i1> zeroinitializer)
  store <2048 x i1> %2, ptr %0
  ret void
}

declare <2048 x i1> @llvm.kvx.cat.v2048i1(<1024 x i1>, <1024 x i1>)

define void @xcat4096(ptr writeonly %0) {
; CHECK-LABEL: xcat4096:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xsplatdo $a0 = 0
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    xsplatov $a0a1a2a3 = $a0
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    xso 32[$r0] = $a1
; CHECK-NEXT:    xcopyv $a4a5a6a7 = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 8)
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    xcopyv $a8a9a10a11 = $a0a1a2a3
; CHECK-NEXT:    ;; # (end cycle 9)
; CHECK-NEXT:    xso 96[$r0] = $a3
; CHECK-NEXT:    ;; # (end cycle 10)
; CHECK-NEXT:    xso 64[$r0] = $a2
; CHECK-NEXT:    ;; # (end cycle 11)
; CHECK-NEXT:    xso 160[$r0] = $a5
; CHECK-NEXT:    xcopyv $a12a13a14a15 = $a4a5a6a7
; CHECK-NEXT:    ;; # (end cycle 12)
; CHECK-NEXT:    xso 128[$r0] = $a4
; CHECK-NEXT:    ;; # (end cycle 13)
; CHECK-NEXT:    xso 224[$r0] = $a7
; CHECK-NEXT:    ;; # (end cycle 14)
; CHECK-NEXT:    xso 192[$r0] = $a6
; CHECK-NEXT:    ;; # (end cycle 15)
; CHECK-NEXT:    xso 288[$r0] = $a9
; CHECK-NEXT:    ;; # (end cycle 16)
; CHECK-NEXT:    xso 256[$r0] = $a8
; CHECK-NEXT:    ;; # (end cycle 17)
; CHECK-NEXT:    xso 352[$r0] = $a11
; CHECK-NEXT:    ;; # (end cycle 18)
; CHECK-NEXT:    xso 320[$r0] = $a10
; CHECK-NEXT:    ;; # (end cycle 19)
; CHECK-NEXT:    xso 416[$r0] = $a13
; CHECK-NEXT:    ;; # (end cycle 20)
; CHECK-NEXT:    xso 384[$r0] = $a12
; CHECK-NEXT:    ;; # (end cycle 21)
; CHECK-NEXT:    xso 480[$r0] = $a15
; CHECK-NEXT:    ;; # (end cycle 22)
; CHECK-NEXT:    xso 448[$r0] = $a14
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 23)
  %2 = tail call <4096 x i1> @llvm.kvx.cat.v4096i1(<2048 x i1> zeroinitializer, <2048 x i1> zeroinitializer)
  store <4096 x i1> %2, ptr %0
  ret void
}

declare <4096 x i1> @llvm.kvx.cat.v4096i1(<2048 x i1>, <2048 x i1>)

