; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mcpu=kv3-1 -O2 -o - %s | FileCheck %s --check-prefixes=CV1
; RUN: llc -mcpu=kv3-2 -O2 -o - %s | FileCheck %s --check-prefixes=CV2
; RUN: clang -O2 -march=kv3-1 -c -o /dev/null %s
; RUN: clang -O2 -march=kv3-2 -c -o /dev/null %s

target triple = "kvx-kalray-cos"

define <4 x i32> @maddwq(<4 x i32> %0, <4 x i32> %1, <4 x i32> %2) {
; CV1-LABEL: maddwq:
; CV1:       # %bb.0:
; CV1-NEXT:    maddwp $r0 = $r2, $r4
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    maddwp $r1 = $r3, $r5
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 1)
;
; CV2-LABEL: maddwq:
; CV2:       # %bb.0:
; CV2-NEXT:    maddwq $r0r1 = $r2r3, $r4r5
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %4 = mul <4 x i32> %1, %2
  %5 = add <4 x i32> %4, %0
  ret <4 x i32> %5
}

define <4 x i32> @msbfwq(<4 x i32> %0, <4 x i32> %1, <4 x i32> %2) {
; CV1-LABEL: msbfwq:
; CV1:       # %bb.0:
; CV1-NEXT:    msbfwp $r0 = $r2, $r4
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    msbfwp $r1 = $r3, $r5
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 1)
;
; CV2-LABEL: msbfwq:
; CV2:       # %bb.0:
; CV2-NEXT:    msbfwq $r0r1 = $r2r3, $r4r5
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %4 = mul <4 x i32> %1, %2
  %5 = sub <4 x i32> %0, %4
  ret <4 x i32> %5
}

define <4 x i32> @maddmwq(<4 x i32> %0, <4 x i32> %1, <4 x i32> %2) {
; CV1-LABEL: maddmwq:
; CV1:       # %bb.0:
; CV1-NEXT:    mulwdp $r6r7 = $r3, $r5
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    mulwdp $r4r5 = $r2, $r4
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srld $r2 = $r6, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    srld $r3 = $r4, 32
; CV1-NEXT:    insf $r7 = $r2, 31, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    addwp $r1 = $r7, $r1
; CV1-NEXT:    insf $r5 = $r3, 31, 0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    copyd $r2 = $r5
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    addwp $r0 = $r2, $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: maddmwq:
; CV2:       # %bb.0:
; CV2-NEXT:    maddmwq $r0r1 = $r2r3, $r4r5
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %4 = sext <4 x i32> %2 to <4 x i64>
  %5 = sext <4 x i32> %1 to <4 x i64>
  %6 = mul nsw <4 x i64> %5, %4
  %7 = lshr <4 x i64> %6, <i64 32, i64 32, i64 32, i64 32>
  %8 = trunc <4 x i64> %7 to <4 x i32>
  %9 = add <4 x i32> %8, %0
  ret <4 x i32> %9
}

define <4 x i32> @msbfmwq(<4 x i32> %0, <4 x i32> %1, <4 x i32> %2) {
; CV1-LABEL: msbfmwq:
; CV1:       # %bb.0:
; CV1-NEXT:    mulwdp $r6r7 = $r3, $r5
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    mulwdp $r4r5 = $r2, $r4
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srld $r2 = $r6, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    srld $r3 = $r4, 32
; CV1-NEXT:    insf $r7 = $r2, 31, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbfwp $r1 = $r7, $r1
; CV1-NEXT:    insf $r5 = $r3, 31, 0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    copyd $r2 = $r5
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbfwp $r0 = $r2, $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: msbfmwq:
; CV2:       # %bb.0:
; CV2-NEXT:    msbfmwq $r0r1 = $r2r3, $r4r5
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %4 = sext <4 x i32> %2 to <4 x i64>
  %5 = sext <4 x i32> %1 to <4 x i64>
  %6 = mul nsw <4 x i64> %5, %4
  %7 = lshr <4 x i64> %6, <i64 32, i64 32, i64 32, i64 32>
  %8 = trunc <4 x i64> %7 to <4 x i32>
  %9 = sub <4 x i32> %0, %8
  ret <4 x i32> %9
}

define <4 x i32> @maddumwq(<4 x i32> %0, <4 x i32> %1, <4 x i32> %2) {
; CV1-LABEL: maddumwq:
; CV1:       # %bb.0:
; CV1-NEXT:    muluwdp $r6r7 = $r3, $r5
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    muluwdp $r4r5 = $r2, $r4
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srld $r2 = $r6, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    srld $r3 = $r4, 32
; CV1-NEXT:    insf $r7 = $r2, 31, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    addwp $r1 = $r7, $r1
; CV1-NEXT:    insf $r5 = $r3, 31, 0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    copyd $r2 = $r5
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    addwp $r0 = $r2, $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: maddumwq:
; CV2:       # %bb.0:
; CV2-NEXT:    maddumwq $r0r1 = $r2r3, $r4r5
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %4 = zext <4 x i32> %2 to <4 x i64>
  %5 = zext <4 x i32> %1 to <4 x i64>
  %6 = mul nsw <4 x i64> %5, %4
  %7 = lshr <4 x i64> %6, <i64 32, i64 32, i64 32, i64 32>
  %8 = trunc <4 x i64> %7 to <4 x i32>
  %9 = add <4 x i32> %8, %0
  ret <4 x i32> %9
}

define <4 x i32> @msbfumwq(<4 x i32> %0, <4 x i32> %1, <4 x i32> %2) {
; CV1-LABEL: msbfumwq:
; CV1:       # %bb.0:
; CV1-NEXT:    muluwdp $r6r7 = $r3, $r5
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    muluwdp $r4r5 = $r2, $r4
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srld $r2 = $r6, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    srld $r3 = $r4, 32
; CV1-NEXT:    insf $r7 = $r2, 31, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbfwp $r1 = $r7, $r1
; CV1-NEXT:    insf $r5 = $r3, 31, 0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    copyd $r2 = $r5
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbfwp $r0 = $r2, $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: msbfumwq:
; CV2:       # %bb.0:
; CV2-NEXT:    msbfumwq $r0r1 = $r2r3, $r4r5
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %4 = zext <4 x i32> %2 to <4 x i64>
  %5 = zext <4 x i32> %1 to <4 x i64>
  %6 = mul nsw <4 x i64> %5, %4
  %7 = lshr <4 x i64> %6, <i64 32, i64 32, i64 32, i64 32>
  %8 = trunc <4 x i64> %7 to <4 x i32>
  %9 = sub <4 x i32> %0, %8
  ret <4 x i32> %9
}

define <4 x i32> @mulmwq(<4 x i32> %0, <4 x i32> %1) {
; CV1-LABEL: mulmwq:
; CV1:       # %bb.0:
; CV1-NEXT:    mulwdp $r6r7 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    mulwdp $r0r1 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srld $r2 = $r6, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    srld $r0 = $r0, 32
; CV1-NEXT:    insf $r7 = $r2, 31, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    insf $r1 = $r0, 31, 0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    copyd $r0 = $r1
; CV1-NEXT:    copyd $r1 = $r7
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 5)
;
; CV2-LABEL: mulmwq:
; CV2:       # %bb.0:
; CV2-NEXT:    mulmwq $r0r1 = $r2r3, $r0r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %3 = sext <4 x i32> %0 to <4 x i64>
  %4 = sext <4 x i32> %1 to <4 x i64>
  %5 = mul nuw <4 x i64> %4, %3
  %6 = lshr <4 x i64> %5, <i64 32, i64 32, i64 32, i64 32>
  %7 = trunc <4 x i64> %6 to <4 x i32>
  ret <4 x i32> %7
}

define <4 x i32> @mulumwq(<4 x i32> %0, <4 x i32> %1) {
; CV1-LABEL: mulumwq:
; CV1:       # %bb.0:
; CV1-NEXT:    muluwdp $r6r7 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    muluwdp $r0r1 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srld $r2 = $r6, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    srld $r0 = $r0, 32
; CV1-NEXT:    insf $r7 = $r2, 31, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    insf $r1 = $r0, 31, 0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    copyd $r0 = $r1
; CV1-NEXT:    copyd $r1 = $r7
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 5)
;
; CV2-LABEL: mulumwq:
; CV2:       # %bb.0:
; CV2-NEXT:    mulumwq $r0r1 = $r2r3, $r0r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %3 = zext <4 x i32> %0 to <4 x i64>
  %4 = zext <4 x i32> %1 to <4 x i64>
  %5 = mul nuw <4 x i64> %4, %3
  %6 = lshr <4 x i64> %5, <i64 32, i64 32, i64 32, i64 32>
  %7 = trunc <4 x i64> %6 to <4 x i32>
  ret <4 x i32> %7
}

define <4 x i32> @maddsumwq(<4 x i32> %0, <4 x i32> %1, <4 x i32> %2) {
; CV1-LABEL: maddsumwq:
; CV1:       # %bb.0:
; CV1-NEXT:    mulsuwdp $r6r7 = $r3, $r5
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    mulsuwdp $r4r5 = $r2, $r4
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srld $r2 = $r6, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    srld $r3 = $r4, 32
; CV1-NEXT:    insf $r7 = $r2, 31, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    addwp $r1 = $r7, $r1
; CV1-NEXT:    insf $r5 = $r3, 31, 0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    copyd $r2 = $r5
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    addwp $r0 = $r2, $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: maddsumwq:
; CV2:       # %bb.0:
; CV2-NEXT:    maddsumwq $r0r1 = $r2r3, $r4r5
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %4 = sext <4 x i32> %1 to <4 x i64>
  %5 = zext <4 x i32> %2 to <4 x i64>
  %6 = mul nsw <4 x i64> %5, %4
  %7 = lshr <4 x i64> %6, <i64 32, i64 32, i64 32, i64 32>
  %8 = trunc <4 x i64> %7 to <4 x i32>
  %9 = add <4 x i32> %8, %0
  ret <4 x i32> %9
}

define <4 x i32> @msbfsumwq(<4 x i32> %0, <4 x i32> %1, <4 x i32> %2) {
; CV1-LABEL: msbfsumwq:
; CV1:       # %bb.0:
; CV1-NEXT:    mulsuwdp $r6r7 = $r3, $r5
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    mulsuwdp $r4r5 = $r2, $r4
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srld $r2 = $r6, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    srld $r3 = $r4, 32
; CV1-NEXT:    insf $r7 = $r2, 31, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbfwp $r1 = $r7, $r1
; CV1-NEXT:    insf $r5 = $r3, 31, 0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    copyd $r2 = $r5
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbfwp $r0 = $r2, $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: msbfsumwq:
; CV2:       # %bb.0:
; CV2-NEXT:    msbfsumwq $r0r1 = $r2r3, $r4r5
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %4 = sext <4 x i32> %1 to <4 x i64>
  %5 = zext <4 x i32> %2 to <4 x i64>
  %6 = mul nsw <4 x i64> %5, %4
  %7 = lshr <4 x i64> %6, <i64 32, i64 32, i64 32, i64 32>
  %8 = trunc <4 x i64> %7 to <4 x i32>
  %9 = sub <4 x i32> %0, %8
  ret <4 x i32> %9
}

define <4 x i32> @mulsumwq(<4 x i32> %0, <4 x i32> %1) {
; CV1-LABEL: mulsumwq:
; CV1:       # %bb.0:
; CV1-NEXT:    mulsuwdp $r6r7 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    mulsuwdp $r0r1 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srld $r2 = $r6, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    srld $r0 = $r0, 32
; CV1-NEXT:    insf $r7 = $r2, 31, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    insf $r1 = $r0, 31, 0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    copyd $r0 = $r1
; CV1-NEXT:    copyd $r1 = $r7
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 5)
;
; CV2-LABEL: mulsumwq:
; CV2:       # %bb.0:
; CV2-NEXT:    mulsumwq $r0r1 = $r0r1, $r2r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %3 = sext <4 x i32> %0 to <4 x i64>
  %4 = zext <4 x i32> %1 to <4 x i64>
  %5 = mul nsw <4 x i64> %4, %3
  %6 = lshr <4 x i64> %5, <i64 32, i64 32, i64 32, i64 32>
  %7 = trunc <4 x i64> %6 to <4 x i32>
  ret <4 x i32> %7
}


define <3 x i32> @maddsumwq_v3(<3 x i32> %0, <3 x i32> %1, <3 x i32> %2) {
; CV1-LABEL: maddsumwq_v3:
; CV1:       # %bb.0:
; CV1-NEXT:    mulsuwdp $r6r7 = $r3, $r5
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    mulsuwdp $r4r5 = $r2, $r4
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srld $r2 = $r7, $r0
; CV1-NEXT:    srld $r3 = $r6, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r3 = $r2, 63, 32
; CV1-NEXT:    srld $r4 = $r4, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    addwp $r1 = $r3, $r1
; CV1-NEXT:    insf $r5 = $r4, 31, 0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    copyd $r2 = $r5
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    addwp $r0 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r1 = $r0, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: maddsumwq_v3:
; CV2:       # %bb.0:
; CV2-NEXT:    maddsumwq $r0r1 = $r2r3, $r4r5
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 63, 32
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 2)
  %4 = sext <3 x i32> %1 to <3 x i64>
  %5 = zext <3 x i32> %2 to <3 x i64>
  %6 = mul nsw <3 x i64> %5, %4
  %7 = lshr <3 x i64> %6, <i64 32, i64 32, i64 32>
  %8 = trunc <3 x i64> %7 to <3 x i32>
  %9 = add <3 x i32> %8, %0
  ret <3 x i32> %9
}
