; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mcpu=kv3-1 -O2 -o - %s | FileCheck %s --check-prefixes=ALL,CV1
; RUN: llc -mcpu=kv3-2 -O2 -o - %s | FileCheck %s --check-prefixes=ALL,CV2
; RUN: clang -O2 -c -o /dev/null %s
; RUN: clang -O2 -march=kv3-2 -c -o /dev/null %s

target triple = "kvx-kalray-cos"

define <8 x i8> @avgbo(<8 x i8> %0, <8 x i8> %1) {
; CV1-LABEL: avgbo:
; CV1:       # %bb.0:
; CV1-NEXT:    xord $r0 = $r0, $r1
; CV1-NEXT:    andd.@ $r2 = $r1, 0x7f7f7f7f
; CV1-NEXT:    andd.@ $r3 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfz $r1 = $r0, 55, 48
; CV1-NEXT:    srld $r2 = $r0, 56
; CV1-NEXT:    extfz $r3 = $r0, 47, 40
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    extfz $r4 = $r0, 39, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    sraw $r2 = $r2, 1
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r1 = $r1, 1
; CV1-NEXT:    sraw $r3 = $r3, 1
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    extfz $r5 = $r0, 23, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r2, 15, 8
; CV1-NEXT:    srlw $r2 = $r0, 24
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    extfz $r3 = $r0, 15, 8
; CV1-NEXT:    insf $r4 = $r3, 15, 8
; CV1-NEXT:    sraw $r5 = $r5, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sraw $r2 = $r2, 1
; CV1-NEXT:    sraw $r3 = $r3, 1
; CV1-NEXT:    insf $r4 = $r1, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r0 = $r0, 1
; CV1-NEXT:    insf $r5 = $r2, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r3, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r5, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r4, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgbo:
; CV2:       # %bb.0:
; CV2-NEXT:    avgbo $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %0, <8 x i8> %1, i32 0)
  ret <8 x i8> %3
}

declare <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8>, <8 x i8>, i32)

define <2 x i8> @avgbp(<2 x i8> %0, <2 x i8> %1) {
; CV1-LABEL: avgbp:
; CV1:       # %bb.0:
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x401
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r0 = $r0, 1
; CV1-NEXT:    sraw $r1 = $r1, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r1, 15, 8
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgbp:
; CV2:       # %bb.0:
; CV2-NEXT:    avgbo $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = tail call <2 x i8> @llvm.kvx.avg.v2i8(<2 x i8> %0, <2 x i8> %1, i32 0)
  ret <2 x i8> %3
}

declare <2 x i8> @llvm.kvx.avg.v2i8(<2 x i8>, <2 x i8>, i32)

define <4 x i8> @avgbq(<4 x i8> %0, <4 x i8> %1) {
; CV1-LABEL: avgbq:
; CV1:       # %bb.0:
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    extfz $r3 = $r0, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sraw $r1 = $r1, 1
; CV1-NEXT:    sraw $r2 = $r2, 1
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r0 = $r0, 1
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    sraw $r3 = $r3, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r3, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r2, 31, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgbq:
; CV2:       # %bb.0:
; CV2-NEXT:    avgbo $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = tail call <4 x i8> @llvm.kvx.avg.v4i8(<4 x i8> %0, <4 x i8> %1, i32 0)
  ret <4 x i8> %3
}

declare <4 x i8> @llvm.kvx.avg.v4i8(<4 x i8>, <4 x i8>, i32)

define <16 x i8> @avgbx(<16 x i8> %0, <16 x i8> %1) {
; CV1-LABEL: avgbx:
; CV1:       # %bb.0:
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r4 = $r2, 0x7f7f7f7f
; CV1-NEXT:    andd.@ $r5 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r2 = $r5, $r4
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r2, $r0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfz $r2 = $r0, 55, 48
; CV1-NEXT:    srld $r4 = $r0, 56
; CV1-NEXT:    extfz $r5 = $r0, 47, 40
; CV1-NEXT:    srlw $r7 = $r0, 24
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    extfz $r6 = $r0, 39, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r2 = $r2, 1
; CV1-NEXT:    sraw $r5 = $r5, 1
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    sxbd $r7 = $r7
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r4, 15, 8
; CV1-NEXT:    extfz $r4 = $r0, 23, 16
; CV1-NEXT:    sraw $r6 = $r6, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    sraw $r5 = $r7, 1
; CV1-NEXT:    insf $r6 = $r5, 15, 8
; CV1-NEXT:    extfz $r7 = $r0, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    insf $r6 = $r2, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sxbd $r2 = $r7
; CV1-NEXT:    xord $r3 = $r1, $r3
; CV1-NEXT:    andd.@ $r7 = $r3, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r0 = $r0, 1
; CV1-NEXT:    andd.@ $r1 = $r1, 0x7f7f7f7f
; CV1-NEXT:    sraw $r2 = $r2, 1
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r2, 15, 8
; CV1-NEXT:    addd $r1 = $r1, $r7
; CV1-NEXT:    andd.@ $r3 = $r3, 0x80808080
; CV1-NEXT:    insf $r4 = $r5, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r4, 31, 16
; CV1-NEXT:    xord $r1 = $r1, $r3
; CV1-NEXT:    ;;
; CV1-NEXT:    srld $r2 = $r1, 56
; CV1-NEXT:    extfz $r3 = $r1, 55, 48
; CV1-NEXT:    extfz $r4 = $r1, 47, 40
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    extfz $r5 = $r1, 39, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r2 = $r2, 1
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r3 = $r3, 1
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    extfz $r7 = $r1, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    srlw $r2 = $r1, 24
; CV1-NEXT:    insf $r3 = $r2, 15, 8
; CV1-NEXT:    sraw $r5 = $r5, 1
; CV1-NEXT:    sxbd $r7 = $r7
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r1 = $r1
; CV1-NEXT:    extfz $r4 = $r1, 23, 16
; CV1-NEXT:    insf $r5 = $r4, 15, 8
; CV1-NEXT:    sraw $r7 = $r7, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    sraw $r2 = $r2, 1
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    insf $r5 = $r3, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r6, 63, 32
; CV1-NEXT:    sraw $r1 = $r1, 1
; CV1-NEXT:    insf $r4 = $r2, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r7, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r4, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r5, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgbx:
; CV2:       # %bb.0:
; CV2-NEXT:    avgbo $r0 = $r2, $r0
; CV2-NEXT:    avgbo $r1 = $r3, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = shufflevector <16 x i8> %0, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %4 = shufflevector <16 x i8> %1, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %3, <8 x i8> %4, i32 0)
  %6 = shufflevector <16 x i8> %0, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %7 = shufflevector <16 x i8> %1, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %8 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %6, <8 x i8> %7, i32 0)
  %9 = shufflevector <8 x i8> %5, <8 x i8> %8, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  ret <16 x i8> %9
}

define <32 x i8> @avgbv(<32 x i8> %0, <32 x i8> %1) {
; CV1-LABEL: avgbv:
; CV1:       # %bb.0:
; CV1-NEXT:    xord $r0 = $r0, $r4
; CV1-NEXT:    andd.@ $r8 = $r4, 0x7f7f7f7f
; CV1-NEXT:    andd.@ $r9 = $r0, 0x7f7f7f7f
; CV1-NEXT:    andd.@ $r11 = $r1, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    xord $r1 = $r1, $r5
; CV1-NEXT:    addd $r4 = $r9, $r8
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r4, $r0
; CV1-NEXT:    andd.@ $r1 = $r1, 0x80808080
; CV1-NEXT:    ;;
; CV1-NEXT:    extfz $r4 = $r0, 55, 48
; CV1-NEXT:    srld $r8 = $r0, 56
; CV1-NEXT:    extfz $r9 = $r0, 47, 40
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r8 = $r8
; CV1-NEXT:    extfz $r10 = $r0, 39, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    sraw $r8 = $r8, 1
; CV1-NEXT:    sxbd $r9 = $r9
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    sraw $r9 = $r9, 1
; CV1-NEXT:    sxbd $r10 = $r10
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r4 = $r8, 15, 8
; CV1-NEXT:    srlw $r8 = $r0, 24
; CV1-NEXT:    sraw $r10 = $r10, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    extfz $r9 = $r0, 23, 16
; CV1-NEXT:    insf $r10 = $r9, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r4 = $r5, 0x7f7f7f7f
; CV1-NEXT:    sxbd $r8 = $r8
; CV1-NEXT:    insf $r10 = $r4, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    addd $r4 = $r11, $r4
; CV1-NEXT:    sxbd $r9 = $r9
; CV1-NEXT:    extfz $r11 = $r0, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sraw $r5 = $r8, 1
; CV1-NEXT:    sxbd $r8 = $r11
; CV1-NEXT:    sraw $r9 = $r9, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r0 = $r0, 1
; CV1-NEXT:    xord $r1 = $r4, $r1
; CV1-NEXT:    sraw $r8 = $r8, 1
; CV1-NEXT:    insf $r9 = $r5, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r8, 15, 8
; CV1-NEXT:    srld $r4 = $r1, 56
; CV1-NEXT:    extfz $r5 = $r1, 55, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r9, 31, 16
; CV1-NEXT:    extfz $r8 = $r1, 47, 40
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    extfz $r9 = $r1, 39, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    sxbd $r8 = $r8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r10, 63, 32
; CV1-NEXT:    sraw $r5 = $r5, 1
; CV1-NEXT:    sraw $r8 = $r8, 1
; CV1-NEXT:    sxbd $r9 = $r9
; CV1-NEXT:    ;;
; CV1-NEXT:    srlw $r4 = $r1, 24
; CV1-NEXT:    insf $r5 = $r4, 15, 8
; CV1-NEXT:    sraw $r9 = $r9, 1
; CV1-NEXT:    andd.@ $r10 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r2 = $r2, $r6
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    andd.@ $r8 = $r6, 0x7f7f7f7f
; CV1-NEXT:    insf $r9 = $r8, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    extfz $r5 = $r1, 23, 16
; CV1-NEXT:    addd $r8 = $r10, $r8
; CV1-NEXT:    insf $r9 = $r5, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r1 = $r1
; CV1-NEXT:    andd.@ $r2 = $r2, 0x80808080
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    extfz $r10 = $r1, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    xord $r2 = $r8, $r2
; CV1-NEXT:    sraw $r5 = $r5, 1
; CV1-NEXT:    sxbd $r6 = $r10
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r1 = $r1, 1
; CV1-NEXT:    srld $r4 = $r2, 56
; CV1-NEXT:    insf $r5 = $r4, 15, 8
; CV1-NEXT:    sraw $r6 = $r6, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r6, 15, 8
; CV1-NEXT:    extfz $r6 = $r2, 47, 40
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r5, 31, 16
; CV1-NEXT:    extfz $r5 = $r2, 55, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    extfz $r8 = $r2, 39, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r9, 63, 32
; CV1-NEXT:    sraw $r5 = $r5, 1
; CV1-NEXT:    sraw $r6 = $r6, 1
; CV1-NEXT:    sxbd $r8 = $r8
; CV1-NEXT:    ;;
; CV1-NEXT:    srlw $r4 = $r2, 24
; CV1-NEXT:    insf $r5 = $r4, 15, 8
; CV1-NEXT:    sraw $r8 = $r8, 1
; CV1-NEXT:    andd.@ $r9 = $r7, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r3 = $r3, 0x7f7f7f7f
; CV1-NEXT:    extfz $r6 = $r2, 15, 8
; CV1-NEXT:    xord $r7 = $r3, $r7
; CV1-NEXT:    insf $r8 = $r6, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r2 = $r2
; CV1-NEXT:    addd $r3 = $r3, $r9
; CV1-NEXT:    extfz $r5 = $r2, 23, 16
; CV1-NEXT:    insf $r8 = $r5, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    andd.@ $r7 = $r7, 0x80808080
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    sraw $r5 = $r5, 1
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r2 = $r2, 1
; CV1-NEXT:    xord $r3 = $r3, $r7
; CV1-NEXT:    insf $r5 = $r4, 15, 8
; CV1-NEXT:    sraw $r6 = $r6, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r6, 15, 8
; CV1-NEXT:    srld $r4 = $r3, 56
; CV1-NEXT:    extfz $r6 = $r3, 47, 40
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r5, 31, 16
; CV1-NEXT:    extfz $r5 = $r3, 55, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    extfz $r7 = $r3, 39, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r5 = $r5, 1
; CV1-NEXT:    sraw $r6 = $r6, 1
; CV1-NEXT:    sxbd $r7 = $r7
; CV1-NEXT:    extfz $r9 = $r3, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    srlw $r4 = $r3, 24
; CV1-NEXT:    insf $r5 = $r4, 15, 8
; CV1-NEXT:    sraw $r7 = $r7, 1
; CV1-NEXT:    sxbd $r9 = $r9
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r3 = $r3
; CV1-NEXT:    extfz $r6 = $r3, 23, 16
; CV1-NEXT:    insf $r7 = $r6, 15, 8
; CV1-NEXT:    sraw $r9 = $r9, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    sraw $r6 = $r6, 1
; CV1-NEXT:    insf $r7 = $r5, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r8, 63, 32
; CV1-NEXT:    sraw $r3 = $r3, 1
; CV1-NEXT:    insf $r6 = $r4, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r3 = $r9, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r3 = $r6, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r3 = $r7, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgbv:
; CV2:       # %bb.0:
; CV2-NEXT:    avgbo $r0 = $r4, $r0
; CV2-NEXT:    avgbo $r1 = $r5, $r1
; CV2-NEXT:    avgbo $r2 = $r6, $r2
; CV2-NEXT:    avgbo $r3 = $r7, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %4 = shufflevector <32 x i8> %1, <32 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %3, <8 x i8> %4, i32 0)
  %6 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %7 = shufflevector <32 x i8> %1, <32 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %8 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %6, <8 x i8> %7, i32 0)
  %9 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %10 = shufflevector <32 x i8> %1, <32 x i8> undef, <8 x i32> <i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %11 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %9, <8 x i8> %10, i32 0)
  %12 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  %13 = shufflevector <32 x i8> %1, <32 x i8> undef, <8 x i32> <i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  %14 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %12, <8 x i8> %13, i32 0)
  %15 = shufflevector <8 x i8> %5, <8 x i8> %8, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %16 = shufflevector <8 x i8> %11, <8 x i8> %14, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %17 = shufflevector <16 x i8> %15, <16 x i8> %16, <32 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  ret <32 x i8> %17
}

define <8 x i8> @avgrbo(<8 x i8> %0, <8 x i8> %1) {
; CV1-LABEL: avgrbo:
; CV1:       # %bb.0:
; CV1-NEXT:    xord $r0 = $r0, $r1
; CV1-NEXT:    andd.@ $r2 = $r1, 0x7f7f7f7f
; CV1-NEXT:    andd.@ $r3 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    make $r2 = 0x101010101010101
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    andd.@ $r1 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r3 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r3, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfz $r1 = $r0, 55, 48
; CV1-NEXT:    srld $r2 = $r0, 56
; CV1-NEXT:    extfz $r3 = $r0, 47, 40
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    extfz $r4 = $r0, 39, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    sraw $r2 = $r2, 1
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r1 = $r1, 1
; CV1-NEXT:    sraw $r3 = $r3, 1
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    extfz $r5 = $r0, 23, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r2, 15, 8
; CV1-NEXT:    srlw $r2 = $r0, 24
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    extfz $r3 = $r0, 15, 8
; CV1-NEXT:    insf $r4 = $r3, 15, 8
; CV1-NEXT:    sraw $r5 = $r5, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sraw $r2 = $r2, 1
; CV1-NEXT:    sraw $r3 = $r3, 1
; CV1-NEXT:    insf $r4 = $r1, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r0 = $r0, 1
; CV1-NEXT:    insf $r5 = $r2, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r3, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r5, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r4, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgrbo:
; CV2:       # %bb.0:
; CV2-NEXT:    avgrbo $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %0, <8 x i8> %1, i32 1)
  ret <8 x i8> %3
}

define <2 x i8> @avgrbp(<2 x i8> %0, <2 x i8> %1) {
; CV1-LABEL: avgrbp:
; CV1:       # %bb.0:
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    addhq $r0 = $r0, 0x10001
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x401
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r0 = $r0, 1
; CV1-NEXT:    sraw $r1 = $r1, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r1, 15, 8
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgrbp:
; CV2:       # %bb.0:
; CV2-NEXT:    avgrbo $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = tail call <2 x i8> @llvm.kvx.avg.v2i8(<2 x i8> %0, <2 x i8> %1, i32 1)
  ret <2 x i8> %3
}

define <4 x i8> @avgrbq(<4 x i8> %0, <4 x i8> %1) {
; CV1-LABEL: avgrbq:
; CV1:       # %bb.0:
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    addhq.@ $r0 = $r0, 0x10001
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    extfz $r3 = $r0, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sraw $r1 = $r1, 1
; CV1-NEXT:    sraw $r2 = $r2, 1
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r0 = $r0, 1
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    sraw $r3 = $r3, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r3, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r2, 31, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgrbq:
; CV2:       # %bb.0:
; CV2-NEXT:    avgrbo $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = tail call <4 x i8> @llvm.kvx.avg.v4i8(<4 x i8> %0, <4 x i8> %1, i32 1)
  ret <4 x i8> %3
}

define <16 x i8> @avgrbx(<16 x i8> %0, <16 x i8> %1) {
; CV1-LABEL: avgrbx:
; CV1:       # %bb.0:
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r4 = $r2, 0x7f7f7f7f
; CV1-NEXT:    andd.@ $r5 = $r0, 0x7f7f7f7f
; CV1-NEXT:    andd.@ $r6 = $r3, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r2 = $r5, $r4
; CV1-NEXT:    make $r4 = 0x101010101010101
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r2, $r0
; CV1-NEXT:    andd.@ $r2 = $r4, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r0, $r4
; CV1-NEXT:    andd.@ $r5 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r5 = $r5, $r2
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r5, $r0
; CV1-NEXT:    xord $r1 = $r1, $r3
; CV1-NEXT:    andd.@ $r5 = $r1, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    addd $r3 = $r5, $r6
; CV1-NEXT:    srld $r7 = $r0, 56
; CV1-NEXT:    extfz $r8 = $r0, 55, 48
; CV1-NEXT:    extfz $r9 = $r0, 47, 40
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r1 = $r1, 0x80808080
; CV1-NEXT:    sxbd $r5 = $r7
; CV1-NEXT:    extfz $r7 = $r0, 39, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r5 = $r5, 1
; CV1-NEXT:    sxbd $r6 = $r8
; CV1-NEXT:    sxbd $r8 = $r9
; CV1-NEXT:    srlw $r9 = $r0, 24
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r1 = $r3, $r1
; CV1-NEXT:    sraw $r6 = $r6, 1
; CV1-NEXT:    sxbd $r7 = $r7
; CV1-NEXT:    sraw $r8 = $r8, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r4 = $r1, $r4
; CV1-NEXT:    extfz $r5 = $r0, 23, 16
; CV1-NEXT:    insf $r6 = $r5, 15, 8
; CV1-NEXT:    sraw $r7 = $r7, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r1 = $r1, 0x7f7f7f7f
; CV1-NEXT:    insf $r7 = $r8, 15, 8
; CV1-NEXT:    sxbd $r9 = $r9
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    sraw $r3 = $r9, 1
; CV1-NEXT:    extfz $r6 = $r0, 15, 8
; CV1-NEXT:    insf $r7 = $r6, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    addd $r1 = $r1, $r2
; CV1-NEXT:    andd.@ $r2 = $r4, 0x80808080
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    xord $r1 = $r1, $r2
; CV1-NEXT:    sraw $r5 = $r5, 1
; CV1-NEXT:    sraw $r6 = $r6, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r0 = $r0, 1
; CV1-NEXT:    srld $r2 = $r1, 56
; CV1-NEXT:    extfz $r3 = $r1, 55, 48
; CV1-NEXT:    insf $r5 = $r3, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r6, 15, 8
; CV1-NEXT:    extfz $r4 = $r1, 47, 40
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r5, 31, 16
; CV1-NEXT:    extfz $r5 = $r1, 39, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r2 = $r2, 1
; CV1-NEXT:    sraw $r3 = $r3, 1
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    ;;
; CV1-NEXT:    srlw $r2 = $r1, 24
; CV1-NEXT:    insf $r3 = $r2, 15, 8
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    sraw $r5 = $r5, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    extfz $r4 = $r1, 23, 16
; CV1-NEXT:    insf $r5 = $r4, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r1 = $r1
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    extfz $r6 = $r1, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r2 = $r2, 1
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    insf $r5 = $r3, 31, 16
; CV1-NEXT:    sraw $r6 = $r6, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r7, 63, 32
; CV1-NEXT:    sraw $r1 = $r1, 1
; CV1-NEXT:    insf $r4 = $r2, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r6, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r4, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r5, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgrbx:
; CV2:       # %bb.0:
; CV2-NEXT:    avgrbo $r0 = $r2, $r0
; CV2-NEXT:    avgrbo $r1 = $r3, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = shufflevector <16 x i8> %0, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %4 = shufflevector <16 x i8> %1, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %3, <8 x i8> %4, i32 1)
  %6 = shufflevector <16 x i8> %0, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %7 = shufflevector <16 x i8> %1, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %8 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %6, <8 x i8> %7, i32 1)
  %9 = shufflevector <8 x i8> %5, <8 x i8> %8, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  ret <16 x i8> %9
}

define <32 x i8> @avgrbv(<32 x i8> %0, <32 x i8> %1) {
; CV1-LABEL: avgrbv:
; CV1:       # %bb.0:
; CV1-NEXT:    xord $r0 = $r0, $r4
; CV1-NEXT:    andd.@ $r8 = $r4, 0x7f7f7f7f
; CV1-NEXT:    andd.@ $r9 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r4 = $r9, $r8
; CV1-NEXT:    make $r8 = 0x101010101010101
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r4, $r0
; CV1-NEXT:    andd.@ $r4 = $r8, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r0, $r8
; CV1-NEXT:    andd.@ $r9 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r9 = $r9, $r4
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r9, $r0
; CV1-NEXT:    ;;
; CV1-NEXT:    srld $r9 = $r0, 56
; CV1-NEXT:    extfz $r11 = $r0, 47, 40
; CV1-NEXT:    extfz $r15 = $r0, 39, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    extfz $r10 = $r0, 55, 48
; CV1-NEXT:    sxbd $r11 = $r11
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r9 = $r9
; CV1-NEXT:    sraw $r11 = $r11, 1
; CV1-NEXT:    sxbd $r15 = $r15
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r9 = $r9, 1
; CV1-NEXT:    sxbd $r10 = $r10
; CV1-NEXT:    sraw $r15 = $r15, 1
; CV1-NEXT:    extfz $r16 = $r0, 23, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r5 = $r1, $r5
; CV1-NEXT:    sraw $r10 = $r10, 1
; CV1-NEXT:    andd.@ $r11 = $r5, 0x7f7f7f7f
; CV1-NEXT:    insf $r15 = $r11, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r1 = $r1, 0x7f7f7f7f
; CV1-NEXT:    andd.@ $r5 = $r5, 0x80808080
; CV1-NEXT:    srlw $r9 = $r0, 24
; CV1-NEXT:    insf $r10 = $r9, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    addd $r1 = $r1, $r11
; CV1-NEXT:    sxbd $r9 = $r9
; CV1-NEXT:    extfz $r11 = $r0, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    xord $r1 = $r1, $r5
; CV1-NEXT:    sraw $r9 = $r9, 1
; CV1-NEXT:    sxbd $r11 = $r11
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r0 = $r0, 1
; CV1-NEXT:    xord $r1 = $r1, $r8
; CV1-NEXT:    sraw $r5 = $r11, 1
; CV1-NEXT:    andd.@ $r11 = $r1, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r5, 15, 8
; CV1-NEXT:    andd.@ $r1 = $r1, 0x80808080
; CV1-NEXT:    addd $r5 = $r11, $r4
; CV1-NEXT:    sxbd $r16 = $r16
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r1 = $r5, $r1
; CV1-NEXT:    insf $r15 = $r10, 31, 16
; CV1-NEXT:    sraw $r16 = $r16, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    srld $r5 = $r1, 56
; CV1-NEXT:    extfz $r9 = $r1, 55, 48
; CV1-NEXT:    insf $r16 = $r9, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r16, 31, 16
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r5 = $r5, 1
; CV1-NEXT:    sxbd $r9 = $r9
; CV1-NEXT:    extfz $r10 = $r1, 47, 40
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r15, 63, 32
; CV1-NEXT:    sraw $r9 = $r9, 1
; CV1-NEXT:    extfz $r11 = $r1, 39, 32
; CV1-NEXT:    srlw $r15 = $r1, 24
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r10 = $r10
; CV1-NEXT:    sxbd $r11 = $r11
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r5 = $r10, 1
; CV1-NEXT:    insf $r9 = $r5, 15, 8
; CV1-NEXT:    sraw $r10 = $r11, 1
; CV1-NEXT:    sxbd $r11 = $r15
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r2 = $r2, 0x7f7f7f7f
; CV1-NEXT:    xord $r6 = $r2, $r6
; CV1-NEXT:    insf $r10 = $r5, 15, 8
; CV1-NEXT:    andd.@ $r15 = $r6, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    addd $r2 = $r2, $r15
; CV1-NEXT:    sraw $r5 = $r11, 1
; CV1-NEXT:    extfz $r11 = $r1, 23, 16
; CV1-NEXT:    extfz $r15 = $r1, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r1 = $r1
; CV1-NEXT:    andd.@ $r6 = $r6, 0x80808080
; CV1-NEXT:    sxbd $r11 = $r11
; CV1-NEXT:    sxbd $r15 = $r15
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    xord $r2 = $r2, $r6
; CV1-NEXT:    sraw $r6 = $r15, 1
; CV1-NEXT:    sraw $r11 = $r11, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r1 = $r1, 1
; CV1-NEXT:    xord $r2 = $r2, $r8
; CV1-NEXT:    insf $r11 = $r5, 15, 8
; CV1-NEXT:    andd.@ $r15 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r6, 15, 8
; CV1-NEXT:    andd.@ $r2 = $r2, 0x80808080
; CV1-NEXT:    addd $r5 = $r15, $r4
; CV1-NEXT:    insf $r10 = $r9, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r11, 31, 16
; CV1-NEXT:    xord $r2 = $r5, $r2
; CV1-NEXT:    xord $r3 = $r3, $r7
; CV1-NEXT:    andd.@ $r11 = $r3, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r10, 63, 32
; CV1-NEXT:    andd.@ $r3 = $r3, 0x80808080
; CV1-NEXT:    srld $r5 = $r2, 56
; CV1-NEXT:    extfz $r9 = $r2, 47, 40
; CV1-NEXT:    ;;
; CV1-NEXT:    extfz $r6 = $r2, 55, 48
; CV1-NEXT:    extfz $r10 = $r2, 39, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r9 = $r9
; CV1-NEXT:    sxbd $r10 = $r10
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    sraw $r9 = $r9, 1
; CV1-NEXT:    sraw $r10 = $r10, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r5 = $r5, 1
; CV1-NEXT:    sraw $r6 = $r6, 1
; CV1-NEXT:    andd.@ $r9 = $r7, 0x7f7f7f7f
; CV1-NEXT:    insf $r10 = $r9, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    srlw $r5 = $r2, 24
; CV1-NEXT:    insf $r6 = $r5, 15, 8
; CV1-NEXT:    addd $r7 = $r11, $r9
; CV1-NEXT:    extfz $r9 = $r2, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r2 = $r2
; CV1-NEXT:    xord $r3 = $r7, $r3
; CV1-NEXT:    extfz $r6 = $r2, 23, 16
; CV1-NEXT:    insf $r10 = $r6, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    xord $r3 = $r3, $r8
; CV1-NEXT:    sxbd $r7 = $r9
; CV1-NEXT:    andd.@ $r9 = $r3, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r2 = $r2, 1
; CV1-NEXT:    sraw $r5 = $r5, 1
; CV1-NEXT:    sraw $r6 = $r6, 1
; CV1-NEXT:    sraw $r7 = $r7, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r7, 15, 8
; CV1-NEXT:    andd.@ $r3 = $r3, 0x80808080
; CV1-NEXT:    addd $r4 = $r9, $r4
; CV1-NEXT:    insf $r6 = $r5, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r6, 31, 16
; CV1-NEXT:    xord $r3 = $r4, $r3
; CV1-NEXT:    ;;
; CV1-NEXT:    srld $r4 = $r3, 56
; CV1-NEXT:    extfz $r5 = $r3, 55, 48
; CV1-NEXT:    extfz $r6 = $r3, 47, 40
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    extfz $r7 = $r3, 39, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r5 = $r5, 1
; CV1-NEXT:    sraw $r6 = $r6, 1
; CV1-NEXT:    sxbd $r7 = $r7
; CV1-NEXT:    extfz $r8 = $r3, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    srlw $r4 = $r3, 24
; CV1-NEXT:    insf $r5 = $r4, 15, 8
; CV1-NEXT:    sraw $r7 = $r7, 1
; CV1-NEXT:    sxbd $r8 = $r8
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r3 = $r3
; CV1-NEXT:    extfz $r6 = $r3, 23, 16
; CV1-NEXT:    insf $r7 = $r6, 15, 8
; CV1-NEXT:    sraw $r8 = $r8, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    sraw $r6 = $r6, 1
; CV1-NEXT:    insf $r7 = $r5, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r10, 63, 32
; CV1-NEXT:    sraw $r3 = $r3, 1
; CV1-NEXT:    insf $r6 = $r4, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r3 = $r8, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r3 = $r6, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r3 = $r7, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgrbv:
; CV2:       # %bb.0:
; CV2-NEXT:    avgrbo $r0 = $r4, $r0
; CV2-NEXT:    avgrbo $r1 = $r5, $r1
; CV2-NEXT:    avgrbo $r2 = $r6, $r2
; CV2-NEXT:    avgrbo $r3 = $r7, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %4 = shufflevector <32 x i8> %1, <32 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %3, <8 x i8> %4, i32 1)
  %6 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %7 = shufflevector <32 x i8> %1, <32 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %8 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %6, <8 x i8> %7, i32 1)
  %9 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %10 = shufflevector <32 x i8> %1, <32 x i8> undef, <8 x i32> <i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %11 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %9, <8 x i8> %10, i32 1)
  %12 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  %13 = shufflevector <32 x i8> %1, <32 x i8> undef, <8 x i32> <i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  %14 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %12, <8 x i8> %13, i32 1)
  %15 = shufflevector <8 x i8> %5, <8 x i8> %8, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %16 = shufflevector <8 x i8> %11, <8 x i8> %14, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %17 = shufflevector <16 x i8> %15, <16 x i8> %16, <32 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  ret <32 x i8> %17
}

define <8 x i8> @avgubo(<8 x i8> %0, <8 x i8> %1) {
; CV1-LABEL: avgubo:
; CV1:       # %bb.0:
; CV1-NEXT:    xord $r0 = $r0, $r1
; CV1-NEXT:    andd.@ $r2 = $r1, 0x7f7f7f7f
; CV1-NEXT:    andd.@ $r3 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0xff00ff00
; CV1-NEXT:    andd.@ $r1 = $r0, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    srld $r0 = $r0, 1
; CV1-NEXT:    srld $r1 = $r1, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0xff00ff00
; CV1-NEXT:    andd.@ $r1 = $r1, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    ord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgubo:
; CV2:       # %bb.0:
; CV2-NEXT:    avgubo $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %0, <8 x i8> %1, i32 2)
  ret <8 x i8> %3
}

define <2 x i8> @avgubp(<2 x i8> %0, <2 x i8> %1) {
; CV1-LABEL: avgubp:
; CV1:       # %bb.0:
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x401
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    andw $r0 = $r0, 254
; CV1-NEXT:    andw $r1 = $r1, 254
; CV1-NEXT:    ;;
; CV1-NEXT:    srlw $r0 = $r0, 1
; CV1-NEXT:    srlw $r1 = $r1, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r1, 15, 8
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgubp:
; CV2:       # %bb.0:
; CV2-NEXT:    avgubo $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = tail call <2 x i8> @llvm.kvx.avg.v2i8(<2 x i8> %0, <2 x i8> %1, i32 2)
  ret <2 x i8> %3
}

define <4 x i8> @avgubq(<4 x i8> %0, <4 x i8> %1) {
; CV1-LABEL: avgubq:
; CV1:       # %bb.0:
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    extfz $r3 = $r0, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    andw $r0 = $r0, 254
; CV1-NEXT:    andw $r1 = $r1, 254
; CV1-NEXT:    andw $r2 = $r2, 254
; CV1-NEXT:    andw $r3 = $r3, 254
; CV1-NEXT:    ;;
; CV1-NEXT:    srlw $r0 = $r0, 1
; CV1-NEXT:    srlw $r1 = $r1, 1
; CV1-NEXT:    srlw $r2 = $r2, 1
; CV1-NEXT:    srlw $r3 = $r3, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r3, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r2, 31, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgubq:
; CV2:       # %bb.0:
; CV2-NEXT:    avgubo $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = tail call <4 x i8> @llvm.kvx.avg.v4i8(<4 x i8> %0, <4 x i8> %1, i32 2)
  ret <4 x i8> %3
}

define <16 x i8> @avgubx(<16 x i8> %0, <16 x i8> %1) {
; CV1-LABEL: avgubx:
; CV1:       # %bb.0:
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r2 = $r3, 0x7f7f7f7f
; CV1-NEXT:    andd.@ $r4 = $r2, 0x7f7f7f7f
; CV1-NEXT:    andd.@ $r5 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    andd.@ $r1 = $r1, 0x7f7f7f7f
; CV1-NEXT:    xord $r3 = $r1, $r3
; CV1-NEXT:    addd $r4 = $r5, $r4
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r4, $r0
; CV1-NEXT:    addd $r1 = $r1, $r2
; CV1-NEXT:    andd.@ $r2 = $r3, 0x80808080
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0xff00ff00
; CV1-NEXT:    xord $r1 = $r1, $r2
; CV1-NEXT:    andd.@ $r2 = $r0, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    srld $r0 = $r0, 1
; CV1-NEXT:    andd.@ $r1 = $r1, 0xff00ff00
; CV1-NEXT:    srld $r2 = $r2, 1
; CV1-NEXT:    andd.@ $r3 = $r1, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0xff00ff00
; CV1-NEXT:    srld $r1 = $r1, 1
; CV1-NEXT:    andd.@ $r2 = $r2, 0xff00ff
; CV1-NEXT:    srld $r3 = $r3, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    ord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r1 = $r1, 0xff00ff00
; CV1-NEXT:    andd.@ $r3 = $r3, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    ord $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgubx:
; CV2:       # %bb.0:
; CV2-NEXT:    avgubo $r0 = $r2, $r0
; CV2-NEXT:    avgubo $r1 = $r3, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = shufflevector <16 x i8> %0, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %4 = shufflevector <16 x i8> %1, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %3, <8 x i8> %4, i32 2)
  %6 = shufflevector <16 x i8> %0, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %7 = shufflevector <16 x i8> %1, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %8 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %6, <8 x i8> %7, i32 2)
  %9 = shufflevector <8 x i8> %5, <8 x i8> %8, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  ret <16 x i8> %9
}

define <32 x i8> @avgubv(<32 x i8> %0, <32 x i8> %1) {
; CV1-LABEL: avgubv:
; CV1:       # %bb.0:
; CV1-NEXT:    xord $r0 = $r0, $r4
; CV1-NEXT:    andd.@ $r8 = $r4, 0x7f7f7f7f
; CV1-NEXT:    andd.@ $r9 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    xord $r1 = $r1, $r5
; CV1-NEXT:    addd $r4 = $r9, $r8
; CV1-NEXT:    andd.@ $r8 = $r1, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r4, $r0
; CV1-NEXT:    xord $r2 = $r2, $r6
; CV1-NEXT:    andd.@ $r4 = $r5, 0x7f7f7f7f
; CV1-NEXT:    andd.@ $r9 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    addd $r4 = $r8, $r4
; CV1-NEXT:    andd.@ $r6 = $r7, 0x7f7f7f7f
; CV1-NEXT:    xord $r7 = $r3, $r7
; CV1-NEXT:    andd.@ $r8 = $r6, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r1 = $r1, 0x80808080
; CV1-NEXT:    andd.@ $r3 = $r3, 0x7f7f7f7f
; CV1-NEXT:    andd.@ $r5 = $r0, 0xff00ff
; CV1-NEXT:    addd $r8 = $r9, $r8
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r2 = $r2, 0x80808080
; CV1-NEXT:    addd $r3 = $r3, $r6
; CV1-NEXT:    srld $r5 = $r5, 1
; CV1-NEXT:    andd.@ $r6 = $r7, 0x80808080
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0xff00ff00
; CV1-NEXT:    xord $r1 = $r4, $r1
; CV1-NEXT:    xord $r2 = $r8, $r2
; CV1-NEXT:    xord $r3 = $r3, $r6
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r1 = $r1, 0xff00ff00
; CV1-NEXT:    andd.@ $r4 = $r5, 0xff00ff
; CV1-NEXT:    andd.@ $r5 = $r1, 0xff00ff
; CV1-NEXT:    andd.@ $r6 = $r2, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    srld $r0 = $r0, 1
; CV1-NEXT:    andd.@ $r2 = $r2, 0xff00ff00
; CV1-NEXT:    andd.@ $r3 = $r3, 0xff00ff00
; CV1-NEXT:    andd.@ $r7 = $r3, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    srld $r1 = $r1, 1
; CV1-NEXT:    srld $r2 = $r2, 1
; CV1-NEXT:    srld $r5 = $r5, 1
; CV1-NEXT:    srld $r6 = $r6, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0xff00ff00
; CV1-NEXT:    srld $r3 = $r3, 1
; CV1-NEXT:    andd.@ $r5 = $r5, 0xff00ff
; CV1-NEXT:    srld $r7 = $r7, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r1 = $r1, 0xff00ff00
; CV1-NEXT:    andd.@ $r2 = $r2, 0xff00ff00
; CV1-NEXT:    andd.@ $r6 = $r6, 0xff00ff
; CV1-NEXT:    andd.@ $r7 = $r7, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    ord $r0 = $r0, $r4
; CV1-NEXT:    ord $r1 = $r1, $r5
; CV1-NEXT:    ord $r2 = $r2, $r6
; CV1-NEXT:    andd.@ $r3 = $r3, 0xff00ff00
; CV1-NEXT:    ;;
; CV1-NEXT:    ord $r3 = $r3, $r7
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgubv:
; CV2:       # %bb.0:
; CV2-NEXT:    avgubo $r0 = $r4, $r0
; CV2-NEXT:    avgubo $r1 = $r5, $r1
; CV2-NEXT:    avgubo $r2 = $r6, $r2
; CV2-NEXT:    avgubo $r3 = $r7, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %4 = shufflevector <32 x i8> %1, <32 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %3, <8 x i8> %4, i32 2)
  %6 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %7 = shufflevector <32 x i8> %1, <32 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %8 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %6, <8 x i8> %7, i32 2)
  %9 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %10 = shufflevector <32 x i8> %1, <32 x i8> undef, <8 x i32> <i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %11 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %9, <8 x i8> %10, i32 2)
  %12 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  %13 = shufflevector <32 x i8> %1, <32 x i8> undef, <8 x i32> <i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  %14 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %12, <8 x i8> %13, i32 2)
  %15 = shufflevector <8 x i8> %5, <8 x i8> %8, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %16 = shufflevector <8 x i8> %11, <8 x i8> %14, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %17 = shufflevector <16 x i8> %15, <16 x i8> %16, <32 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  ret <32 x i8> %17
}

define <8 x i8> @avgrubo(<8 x i8> %0, <8 x i8> %1) {
; CV1-LABEL: avgrubo:
; CV1:       # %bb.0:
; CV1-NEXT:    xord $r0 = $r0, $r1
; CV1-NEXT:    andd.@ $r2 = $r1, 0x7f7f7f7f
; CV1-NEXT:    andd.@ $r3 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    make $r1 = 0x101010101010101
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r0, $r1
; CV1-NEXT:    andd.@ $r2 = $r0, 0x7f7f7f7f
; CV1-NEXT:    andd.@ $r3 = $r1, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r2, $r3
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0xff00ff00
; CV1-NEXT:    andd.@ $r1 = $r0, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    srld $r0 = $r0, 1
; CV1-NEXT:    srld $r1 = $r1, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0xff00ff00
; CV1-NEXT:    andd.@ $r1 = $r1, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    ord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgrubo:
; CV2:       # %bb.0:
; CV2-NEXT:    avgrubo $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %0, <8 x i8> %1, i32 3)
  ret <8 x i8> %3
}

define <2 x i8> @avgrubp(<2 x i8> %0, <2 x i8> %1) {
; CV1-LABEL: avgrubp:
; CV1:       # %bb.0:
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    addhq $r0 = $r0, 0x10001
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x401
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    andw $r0 = $r0, 254
; CV1-NEXT:    andw $r1 = $r1, 254
; CV1-NEXT:    ;;
; CV1-NEXT:    srlw $r0 = $r0, 1
; CV1-NEXT:    srlw $r1 = $r1, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r1, 15, 8
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgrubp:
; CV2:       # %bb.0:
; CV2-NEXT:    avgrubo $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = tail call <2 x i8> @llvm.kvx.avg.v2i8(<2 x i8> %0, <2 x i8> %1, i32 3)
  ret <2 x i8> %3
}

define <4 x i8> @avgrubq(<4 x i8> %0, <4 x i8> %1) {
; CV1-LABEL: avgrubq:
; CV1:       # %bb.0:
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    addhq.@ $r0 = $r0, 0x10001
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    extfz $r3 = $r0, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    andw $r0 = $r0, 254
; CV1-NEXT:    andw $r1 = $r1, 254
; CV1-NEXT:    andw $r2 = $r2, 254
; CV1-NEXT:    andw $r3 = $r3, 254
; CV1-NEXT:    ;;
; CV1-NEXT:    srlw $r0 = $r0, 1
; CV1-NEXT:    srlw $r1 = $r1, 1
; CV1-NEXT:    srlw $r2 = $r2, 1
; CV1-NEXT:    srlw $r3 = $r3, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r3, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r2, 31, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgrubq:
; CV2:       # %bb.0:
; CV2-NEXT:    avgrubo $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = tail call <4 x i8> @llvm.kvx.avg.v4i8(<4 x i8> %0, <4 x i8> %1, i32 3)
  ret <4 x i8> %3
}

define <16 x i8> @avgrubx(<16 x i8> %0, <16 x i8> %1) {
; CV1-LABEL: avgrubx:
; CV1:       # %bb.0:
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r4 = $r2, 0x7f7f7f7f
; CV1-NEXT:    andd.@ $r5 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r2 = $r5, $r4
; CV1-NEXT:    andd.@ $r4 = $r3, 0x7f7f7f7f
; CV1-NEXT:    andd.@ $r5 = $r1, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r2, $r0
; CV1-NEXT:    xord $r1 = $r1, $r3
; CV1-NEXT:    addd $r2 = $r5, $r4
; CV1-NEXT:    make $r4 = 0x101010101010101
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r0, $r4
; CV1-NEXT:    andd.@ $r1 = $r1, 0x80808080
; CV1-NEXT:    andd.@ $r3 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    xord $r1 = $r2, $r1
; CV1-NEXT:    andd.@ $r2 = $r4, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r1 = $r1, $r4
; CV1-NEXT:    addd $r3 = $r3, $r2
; CV1-NEXT:    andd.@ $r5 = $r1, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r3, $r0
; CV1-NEXT:    andd.@ $r1 = $r1, 0x80808080
; CV1-NEXT:    addd $r2 = $r5, $r2
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0xff00ff00
; CV1-NEXT:    xord $r1 = $r2, $r1
; CV1-NEXT:    andd.@ $r2 = $r0, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    srld $r0 = $r0, 1
; CV1-NEXT:    andd.@ $r1 = $r1, 0xff00ff00
; CV1-NEXT:    srld $r2 = $r2, 1
; CV1-NEXT:    andd.@ $r3 = $r1, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0xff00ff00
; CV1-NEXT:    srld $r1 = $r1, 1
; CV1-NEXT:    andd.@ $r2 = $r2, 0xff00ff
; CV1-NEXT:    srld $r3 = $r3, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    ord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r1 = $r1, 0xff00ff00
; CV1-NEXT:    andd.@ $r3 = $r3, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    ord $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgrubx:
; CV2:       # %bb.0:
; CV2-NEXT:    avgrubo $r0 = $r2, $r0
; CV2-NEXT:    avgrubo $r1 = $r3, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = shufflevector <16 x i8> %0, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %4 = shufflevector <16 x i8> %1, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %3, <8 x i8> %4, i32 3)
  %6 = shufflevector <16 x i8> %0, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %7 = shufflevector <16 x i8> %1, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %8 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %6, <8 x i8> %7, i32 3)
  %9 = shufflevector <8 x i8> %5, <8 x i8> %8, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  ret <16 x i8> %9
}

define <32 x i8> @avgrubv(<32 x i8> %0, <32 x i8> %1) {
; CV1-LABEL: avgrubv:
; CV1:       # %bb.0:
; CV1-NEXT:    xord $r0 = $r0, $r4
; CV1-NEXT:    andd.@ $r8 = $r4, 0x7f7f7f7f
; CV1-NEXT:    andd.@ $r9 = $r0, 0x7f7f7f7f
; CV1-NEXT:    andd.@ $r10 = $r5, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    andd.@ $r1 = $r1, 0x7f7f7f7f
; CV1-NEXT:    addd $r4 = $r9, $r8
; CV1-NEXT:    xord $r5 = $r1, $r5
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r4, $r0
; CV1-NEXT:    addd $r1 = $r1, $r10
; CV1-NEXT:    make $r4 = 0x101010101010101
; CV1-NEXT:    andd.@ $r5 = $r5, 0x80808080
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r0, $r4
; CV1-NEXT:    xord $r1 = $r1, $r5
; CV1-NEXT:    andd.@ $r8 = $r0, 0x7f7f7f7f
; CV1-NEXT:    andd.@ $r9 = $r4, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    xord $r2 = $r2, $r6
; CV1-NEXT:    addd $r8 = $r8, $r9
; CV1-NEXT:    andd.@ $r10 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r8, $r0
; CV1-NEXT:    xord $r1 = $r1, $r4
; CV1-NEXT:    andd.@ $r2 = $r2, 0x80808080
; CV1-NEXT:    andd.@ $r8 = $r1, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0xff00ff00
; CV1-NEXT:    andd.@ $r1 = $r1, 0x80808080
; CV1-NEXT:    andd.@ $r5 = $r0, 0xff00ff
; CV1-NEXT:    addd $r8 = $r8, $r9
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r1 = $r8, $r1
; CV1-NEXT:    andd.@ $r6 = $r7, 0x7f7f7f7f
; CV1-NEXT:    xord $r7 = $r3, $r7
; CV1-NEXT:    andd.@ $r8 = $r6, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r1 = $r1, 0xff00ff00
; CV1-NEXT:    andd.@ $r3 = $r3, 0x7f7f7f7f
; CV1-NEXT:    addd $r8 = $r10, $r8
; CV1-NEXT:    andd.@ $r10 = $r1, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r2 = $r8, $r2
; CV1-NEXT:    addd $r3 = $r3, $r6
; CV1-NEXT:    srld $r5 = $r5, 1
; CV1-NEXT:    andd.@ $r6 = $r7, 0x80808080
; CV1-NEXT:    ;;
; CV1-NEXT:    srld $r0 = $r0, 1
; CV1-NEXT:    xord $r2 = $r2, $r4
; CV1-NEXT:    xord $r3 = $r3, $r6
; CV1-NEXT:    andd.@ $r6 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r2 = $r2, 0x80808080
; CV1-NEXT:    xord $r3 = $r3, $r4
; CV1-NEXT:    addd $r4 = $r6, $r9
; CV1-NEXT:    andd.@ $r8 = $r3, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r2 = $r4, $r2
; CV1-NEXT:    andd.@ $r3 = $r3, 0x80808080
; CV1-NEXT:    addd $r6 = $r8, $r9
; CV1-NEXT:    srld $r7 = $r10, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    srld $r1 = $r1, 1
; CV1-NEXT:    andd.@ $r2 = $r2, 0xff00ff00
; CV1-NEXT:    xord $r3 = $r6, $r3
; CV1-NEXT:    andd.@ $r4 = $r2, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    srld $r2 = $r2, 1
; CV1-NEXT:    andd.@ $r3 = $r3, 0xff00ff00
; CV1-NEXT:    srld $r4 = $r4, 1
; CV1-NEXT:    andd.@ $r6 = $r3, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0xff00ff00
; CV1-NEXT:    srld $r3 = $r3, 1
; CV1-NEXT:    andd.@ $r5 = $r5, 0xff00ff
; CV1-NEXT:    srld $r6 = $r6, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r1 = $r1, 0xff00ff00
; CV1-NEXT:    andd.@ $r2 = $r2, 0xff00ff00
; CV1-NEXT:    andd.@ $r4 = $r4, 0xff00ff
; CV1-NEXT:    andd.@ $r7 = $r7, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    ord $r0 = $r0, $r5
; CV1-NEXT:    ord $r1 = $r1, $r7
; CV1-NEXT:    andd.@ $r3 = $r3, 0xff00ff00
; CV1-NEXT:    andd.@ $r6 = $r6, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    ord $r2 = $r2, $r4
; CV1-NEXT:    ord $r3 = $r3, $r6
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgrubv:
; CV2:       # %bb.0:
; CV2-NEXT:    avgrubo $r0 = $r4, $r0
; CV2-NEXT:    avgrubo $r1 = $r5, $r1
; CV2-NEXT:    ;;
; CV2-NEXT:    avgrubo $r2 = $r6, $r2
; CV2-NEXT:    avgrubo $r3 = $r7, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %4 = shufflevector <32 x i8> %1, <32 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %3, <8 x i8> %4, i32 3)
  %6 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %7 = shufflevector <32 x i8> %1, <32 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %8 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %6, <8 x i8> %7, i32 3)
  %9 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %10 = shufflevector <32 x i8> %1, <32 x i8> undef, <8 x i32> <i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %11 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %9, <8 x i8> %10, i32 3)
  %12 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  %13 = shufflevector <32 x i8> %1, <32 x i8> undef, <8 x i32> <i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  %14 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %12, <8 x i8> %13, i32 3)
  %15 = shufflevector <8 x i8> %5, <8 x i8> %8, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %16 = shufflevector <8 x i8> %11, <8 x i8> %14, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %17 = shufflevector <16 x i8> %15, <16 x i8> %16, <32 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  ret <32 x i8> %17
}

define <8 x i16> @avgho(<8 x i16> %0, <8 x i16> %1) {
; ALL-LABEL: avgho:
; ALL:       # %bb.0:
; ALL-NEXT:    avghq $r0 = $r2, $r0
; ALL-NEXT:    avghq $r1 = $r3, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %3 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4 = shufflevector <8 x i16> %1, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %3, <4 x i16> %4, i32 0)
  %6 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7 = shufflevector <8 x i16> %1, <8 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %6, <4 x i16> %7, i32 0)
  %9 = shufflevector <4 x i16> %5, <4 x i16> %8, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i16> %9
}

declare <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16>, <4 x i16>, i32)

define <2 x i16> @avghp(<2 x i16> %0, <2 x i16> %1) {
; ALL-LABEL: avghp:
; ALL:       # %bb.0:
; ALL-NEXT:    avghq $r0 = $r1, $r0
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %3 = tail call <2 x i16> @llvm.kvx.avg.v2i16(<2 x i16> %0, <2 x i16> %1, i32 0)
  ret <2 x i16> %3
}

declare <2 x i16> @llvm.kvx.avg.v2i16(<2 x i16>, <2 x i16>, i32)

define <4 x i16> @avghq(<4 x i16> %0, <4 x i16> %1) {
; ALL-LABEL: avghq:
; ALL:       # %bb.0:
; ALL-NEXT:    avghq $r0 = $r1, $r0
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %3 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %0, <4 x i16> %1, i32 0)
  ret <4 x i16> %3
}

define <16 x i16> @avghx(<16 x i16> %0, <16 x i16> %1) {
; CV1-LABEL: avghx:
; CV1:       # %bb.0:
; CV1-NEXT:    avghq $r0 = $r4, $r0
; CV1-NEXT:    avghq $r1 = $r5, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    avghq $r2 = $r6, $r2
; CV1-NEXT:    avghq $r3 = $r7, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avghx:
; CV2:       # %bb.0:
; CV2-NEXT:    avghq $r0 = $r4, $r0
; CV2-NEXT:    avghq $r1 = $r5, $r1
; CV2-NEXT:    avghq $r2 = $r6, $r2
; CV2-NEXT:    avghq $r3 = $r7, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4 = shufflevector <16 x i16> %1, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %3, <4 x i16> %4, i32 0)
  %6 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7 = shufflevector <16 x i16> %1, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %6, <4 x i16> %7, i32 0)
  %9 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %10 = shufflevector <16 x i16> %1, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %11 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %9, <4 x i16> %10, i32 0)
  %12 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %13 = shufflevector <16 x i16> %1, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %14 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %12, <4 x i16> %13, i32 0)
  %15 = shufflevector <4 x i16> %5, <4 x i16> %8, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16 = shufflevector <4 x i16> %11, <4 x i16> %14, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %17 = shufflevector <8 x i16> %15, <8 x i16> %16, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  ret <16 x i16> %17
}

define <8 x i16> @avgrho(<8 x i16> %0, <8 x i16> %1) {
; ALL-LABEL: avgrho:
; ALL:       # %bb.0:
; ALL-NEXT:    avgrhq $r0 = $r2, $r0
; ALL-NEXT:    avgrhq $r1 = $r3, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %3 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4 = shufflevector <8 x i16> %1, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %3, <4 x i16> %4, i32 1)
  %6 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7 = shufflevector <8 x i16> %1, <8 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %6, <4 x i16> %7, i32 1)
  %9 = shufflevector <4 x i16> %5, <4 x i16> %8, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i16> %9
}

define <2 x i16> @avgrhp(<2 x i16> %0, <2 x i16> %1) {
; ALL-LABEL: avgrhp:
; ALL:       # %bb.0:
; ALL-NEXT:    avgrhq $r0 = $r1, $r0
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %3 = tail call <2 x i16> @llvm.kvx.avg.v2i16(<2 x i16> %0, <2 x i16> %1, i32 1)
  ret <2 x i16> %3
}

define <4 x i16> @avgrhq(<4 x i16> %0, <4 x i16> %1) {
; ALL-LABEL: avgrhq:
; ALL:       # %bb.0:
; ALL-NEXT:    avgrhq $r0 = $r1, $r0
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %3 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %0, <4 x i16> %1, i32 1)
  ret <4 x i16> %3
}

define <16 x i16> @avgrhx(<16 x i16> %0, <16 x i16> %1) {
; CV1-LABEL: avgrhx:
; CV1:       # %bb.0:
; CV1-NEXT:    avgrhq $r0 = $r4, $r0
; CV1-NEXT:    avgrhq $r1 = $r5, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    avgrhq $r2 = $r6, $r2
; CV1-NEXT:    avgrhq $r3 = $r7, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgrhx:
; CV2:       # %bb.0:
; CV2-NEXT:    avgrhq $r0 = $r4, $r0
; CV2-NEXT:    avgrhq $r1 = $r5, $r1
; CV2-NEXT:    avgrhq $r2 = $r6, $r2
; CV2-NEXT:    avgrhq $r3 = $r7, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4 = shufflevector <16 x i16> %1, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %3, <4 x i16> %4, i32 1)
  %6 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7 = shufflevector <16 x i16> %1, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %6, <4 x i16> %7, i32 1)
  %9 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %10 = shufflevector <16 x i16> %1, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %11 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %9, <4 x i16> %10, i32 1)
  %12 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %13 = shufflevector <16 x i16> %1, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %14 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %12, <4 x i16> %13, i32 1)
  %15 = shufflevector <4 x i16> %5, <4 x i16> %8, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16 = shufflevector <4 x i16> %11, <4 x i16> %14, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %17 = shufflevector <8 x i16> %15, <8 x i16> %16, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  ret <16 x i16> %17
}

define <8 x i16> @avguho(<8 x i16> %0, <8 x i16> %1) {
; ALL-LABEL: avguho:
; ALL:       # %bb.0:
; ALL-NEXT:    avguhq $r0 = $r2, $r0
; ALL-NEXT:    avguhq $r1 = $r3, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %3 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4 = shufflevector <8 x i16> %1, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %3, <4 x i16> %4, i32 2)
  %6 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7 = shufflevector <8 x i16> %1, <8 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %6, <4 x i16> %7, i32 2)
  %9 = shufflevector <4 x i16> %5, <4 x i16> %8, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i16> %9
}

define <2 x i16> @avguhp(<2 x i16> %0, <2 x i16> %1) {
; ALL-LABEL: avguhp:
; ALL:       # %bb.0:
; ALL-NEXT:    avguhq $r0 = $r1, $r0
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %3 = tail call <2 x i16> @llvm.kvx.avg.v2i16(<2 x i16> %0, <2 x i16> %1, i32 2)
  ret <2 x i16> %3
}

define <4 x i16> @avguhq(<4 x i16> %0, <4 x i16> %1) {
; ALL-LABEL: avguhq:
; ALL:       # %bb.0:
; ALL-NEXT:    avguhq $r0 = $r1, $r0
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %3 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %0, <4 x i16> %1, i32 2)
  ret <4 x i16> %3
}

define <16 x i16> @avguhx(<16 x i16> %0, <16 x i16> %1) {
; CV1-LABEL: avguhx:
; CV1:       # %bb.0:
; CV1-NEXT:    avguhq $r0 = $r4, $r0
; CV1-NEXT:    avguhq $r1 = $r5, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    avguhq $r2 = $r6, $r2
; CV1-NEXT:    avguhq $r3 = $r7, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avguhx:
; CV2:       # %bb.0:
; CV2-NEXT:    avguhq $r0 = $r4, $r0
; CV2-NEXT:    avguhq $r1 = $r5, $r1
; CV2-NEXT:    avguhq $r2 = $r6, $r2
; CV2-NEXT:    avguhq $r3 = $r7, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4 = shufflevector <16 x i16> %1, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %3, <4 x i16> %4, i32 2)
  %6 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7 = shufflevector <16 x i16> %1, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %6, <4 x i16> %7, i32 2)
  %9 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %10 = shufflevector <16 x i16> %1, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %11 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %9, <4 x i16> %10, i32 2)
  %12 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %13 = shufflevector <16 x i16> %1, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %14 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %12, <4 x i16> %13, i32 2)
  %15 = shufflevector <4 x i16> %5, <4 x i16> %8, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16 = shufflevector <4 x i16> %11, <4 x i16> %14, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %17 = shufflevector <8 x i16> %15, <8 x i16> %16, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  ret <16 x i16> %17
}

define <8 x i16> @avgruho(<8 x i16> %0, <8 x i16> %1) {
; ALL-LABEL: avgruho:
; ALL:       # %bb.0:
; ALL-NEXT:    avgruhq $r0 = $r2, $r0
; ALL-NEXT:    avgruhq $r1 = $r3, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %3 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4 = shufflevector <8 x i16> %1, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %3, <4 x i16> %4, i32 3)
  %6 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7 = shufflevector <8 x i16> %1, <8 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %6, <4 x i16> %7, i32 3)
  %9 = shufflevector <4 x i16> %5, <4 x i16> %8, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i16> %9
}

define <2 x i16> @avgruhp(<2 x i16> %0, <2 x i16> %1) {
; ALL-LABEL: avgruhp:
; ALL:       # %bb.0:
; ALL-NEXT:    avgruhq $r0 = $r1, $r0
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %3 = tail call <2 x i16> @llvm.kvx.avg.v2i16(<2 x i16> %0, <2 x i16> %1, i32 3)
  ret <2 x i16> %3
}

define <4 x i16> @avgruhq(<4 x i16> %0, <4 x i16> %1) {
; ALL-LABEL: avgruhq:
; ALL:       # %bb.0:
; ALL-NEXT:    avgruhq $r0 = $r1, $r0
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %3 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %0, <4 x i16> %1, i32 3)
  ret <4 x i16> %3
}

define <16 x i16> @avgruhx(<16 x i16> %0, <16 x i16> %1) {
; CV1-LABEL: avgruhx:
; CV1:       # %bb.0:
; CV1-NEXT:    avgruhq $r0 = $r4, $r0
; CV1-NEXT:    avgruhq $r1 = $r5, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    avgruhq $r2 = $r6, $r2
; CV1-NEXT:    avgruhq $r3 = $r7, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgruhx:
; CV2:       # %bb.0:
; CV2-NEXT:    avgruhq $r0 = $r4, $r0
; CV2-NEXT:    avgruhq $r1 = $r5, $r1
; CV2-NEXT:    avgruhq $r2 = $r6, $r2
; CV2-NEXT:    avgruhq $r3 = $r7, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4 = shufflevector <16 x i16> %1, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %3, <4 x i16> %4, i32 3)
  %6 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7 = shufflevector <16 x i16> %1, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %6, <4 x i16> %7, i32 3)
  %9 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %10 = shufflevector <16 x i16> %1, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %11 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %9, <4 x i16> %10, i32 3)
  %12 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %13 = shufflevector <16 x i16> %1, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %14 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %12, <4 x i16> %13, i32 3)
  %15 = shufflevector <4 x i16> %5, <4 x i16> %8, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16 = shufflevector <4 x i16> %11, <4 x i16> %14, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %17 = shufflevector <8 x i16> %15, <8 x i16> %16, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  ret <16 x i16> %17
}

define <8 x i32> @avgruwo(<8 x i32> %0, <8 x i32> %1) {
; CV1-LABEL: avgruwo:
; CV1:       # %bb.0:
; CV1-NEXT:    avgruwp $r0 = $r4, $r0
; CV1-NEXT:    avgruwp $r1 = $r5, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    avgruwp $r2 = $r6, $r2
; CV1-NEXT:    avgruwp $r3 = $r7, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgruwo:
; CV2:       # %bb.0:
; CV2-NEXT:    avgruwp $r0 = $r4, $r0
; CV2-NEXT:    avgruwp $r1 = $r5, $r1
; CV2-NEXT:    avgruwp $r2 = $r6, $r2
; CV2-NEXT:    avgruwp $r3 = $r7, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 0, i32 1>
  %4 = shufflevector <8 x i32> %1, <8 x i32> undef, <2 x i32> <i32 0, i32 1>
  %5 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %3, <2 x i32> %4, i32 3)
  %6 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 2, i32 3>
  %7 = shufflevector <8 x i32> %1, <8 x i32> undef, <2 x i32> <i32 2, i32 3>
  %8 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %6, <2 x i32> %7, i32 3)
  %9 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 4, i32 5>
  %10 = shufflevector <8 x i32> %1, <8 x i32> undef, <2 x i32> <i32 4, i32 5>
  %11 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %9, <2 x i32> %10, i32 3)
  %12 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 6, i32 7>
  %13 = shufflevector <8 x i32> %1, <8 x i32> undef, <2 x i32> <i32 6, i32 7>
  %14 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %12, <2 x i32> %13, i32 3)
  %15 = shufflevector <2 x i32> %5, <2 x i32> %8, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %16 = shufflevector <2 x i32> %11, <2 x i32> %14, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %17 = shufflevector <4 x i32> %15, <4 x i32> %16, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i32> %17
}

declare <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32>, <2 x i32>, i32)

define <2 x i32> @avgruwp(<2 x i32> %0, <2 x i32> %1) {
; ALL-LABEL: avgruwp:
; ALL:       # %bb.0:
; ALL-NEXT:    avgruwp $r0 = $r1, $r0
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %3 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %0, <2 x i32> %1, i32 3)
  ret <2 x i32> %3
}

define <4 x i32> @avgruwq(<4 x i32> %0, <4 x i32> %1) {
; ALL-LABEL: avgruwq:
; ALL:       # %bb.0:
; ALL-NEXT:    avgruwp $r0 = $r2, $r0
; ALL-NEXT:    avgruwp $r1 = $r3, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %3 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 0, i32 1>
  %4 = shufflevector <4 x i32> %1, <4 x i32> undef, <2 x i32> <i32 0, i32 1>
  %5 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %3, <2 x i32> %4, i32 3)
  %6 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 2, i32 3>
  %7 = shufflevector <4 x i32> %1, <4 x i32> undef, <2 x i32> <i32 2, i32 3>
  %8 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %6, <2 x i32> %7, i32 3)
  %9 = shufflevector <2 x i32> %5, <2 x i32> %8, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  ret <4 x i32> %9
}

define <8 x i32> @avgrwo(<8 x i32> %0, <8 x i32> %1) {
; CV1-LABEL: avgrwo:
; CV1:       # %bb.0:
; CV1-NEXT:    avgrwp $r0 = $r4, $r0
; CV1-NEXT:    avgrwp $r1 = $r5, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    avgrwp $r2 = $r6, $r2
; CV1-NEXT:    avgrwp $r3 = $r7, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgrwo:
; CV2:       # %bb.0:
; CV2-NEXT:    avgrwp $r0 = $r4, $r0
; CV2-NEXT:    avgrwp $r1 = $r5, $r1
; CV2-NEXT:    avgrwp $r2 = $r6, $r2
; CV2-NEXT:    avgrwp $r3 = $r7, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 0, i32 1>
  %4 = shufflevector <8 x i32> %1, <8 x i32> undef, <2 x i32> <i32 0, i32 1>
  %5 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %3, <2 x i32> %4, i32 1)
  %6 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 2, i32 3>
  %7 = shufflevector <8 x i32> %1, <8 x i32> undef, <2 x i32> <i32 2, i32 3>
  %8 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %6, <2 x i32> %7, i32 1)
  %9 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 4, i32 5>
  %10 = shufflevector <8 x i32> %1, <8 x i32> undef, <2 x i32> <i32 4, i32 5>
  %11 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %9, <2 x i32> %10, i32 1)
  %12 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 6, i32 7>
  %13 = shufflevector <8 x i32> %1, <8 x i32> undef, <2 x i32> <i32 6, i32 7>
  %14 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %12, <2 x i32> %13, i32 1)
  %15 = shufflevector <2 x i32> %5, <2 x i32> %8, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %16 = shufflevector <2 x i32> %11, <2 x i32> %14, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %17 = shufflevector <4 x i32> %15, <4 x i32> %16, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i32> %17
}

define <2 x i32> @avgrwp(<2 x i32> %0, <2 x i32> %1) {
; ALL-LABEL: avgrwp:
; ALL:       # %bb.0:
; ALL-NEXT:    avgrwp $r0 = $r1, $r0
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %3 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %0, <2 x i32> %1, i32 1)
  ret <2 x i32> %3
}

define <4 x i32> @avgrwq(<4 x i32> %0, <4 x i32> %1) {
; ALL-LABEL: avgrwq:
; ALL:       # %bb.0:
; ALL-NEXT:    avgrwp $r0 = $r2, $r0
; ALL-NEXT:    avgrwp $r1 = $r3, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %3 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 0, i32 1>
  %4 = shufflevector <4 x i32> %1, <4 x i32> undef, <2 x i32> <i32 0, i32 1>
  %5 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %3, <2 x i32> %4, i32 1)
  %6 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 2, i32 3>
  %7 = shufflevector <4 x i32> %1, <4 x i32> undef, <2 x i32> <i32 2, i32 3>
  %8 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %6, <2 x i32> %7, i32 1)
  %9 = shufflevector <2 x i32> %5, <2 x i32> %8, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  ret <4 x i32> %9
}

define <8 x i32> @avguwo(<8 x i32> %0, <8 x i32> %1) {
; CV1-LABEL: avguwo:
; CV1:       # %bb.0:
; CV1-NEXT:    avguwp $r0 = $r4, $r0
; CV1-NEXT:    avguwp $r1 = $r5, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    avguwp $r2 = $r6, $r2
; CV1-NEXT:    avguwp $r3 = $r7, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avguwo:
; CV2:       # %bb.0:
; CV2-NEXT:    avguwp $r0 = $r4, $r0
; CV2-NEXT:    avguwp $r1 = $r5, $r1
; CV2-NEXT:    avguwp $r2 = $r6, $r2
; CV2-NEXT:    avguwp $r3 = $r7, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 0, i32 1>
  %4 = shufflevector <8 x i32> %1, <8 x i32> undef, <2 x i32> <i32 0, i32 1>
  %5 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %3, <2 x i32> %4, i32 2)
  %6 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 2, i32 3>
  %7 = shufflevector <8 x i32> %1, <8 x i32> undef, <2 x i32> <i32 2, i32 3>
  %8 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %6, <2 x i32> %7, i32 2)
  %9 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 4, i32 5>
  %10 = shufflevector <8 x i32> %1, <8 x i32> undef, <2 x i32> <i32 4, i32 5>
  %11 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %9, <2 x i32> %10, i32 2)
  %12 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 6, i32 7>
  %13 = shufflevector <8 x i32> %1, <8 x i32> undef, <2 x i32> <i32 6, i32 7>
  %14 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %12, <2 x i32> %13, i32 2)
  %15 = shufflevector <2 x i32> %5, <2 x i32> %8, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %16 = shufflevector <2 x i32> %11, <2 x i32> %14, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %17 = shufflevector <4 x i32> %15, <4 x i32> %16, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i32> %17
}

define <2 x i32> @avguwp(<2 x i32> %0, <2 x i32> %1) {
; ALL-LABEL: avguwp:
; ALL:       # %bb.0:
; ALL-NEXT:    avguwp $r0 = $r1, $r0
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %3 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %0, <2 x i32> %1, i32 2)
  ret <2 x i32> %3
}

define <4 x i32> @avguwq(<4 x i32> %0, <4 x i32> %1) {
; ALL-LABEL: avguwq:
; ALL:       # %bb.0:
; ALL-NEXT:    avguwp $r0 = $r2, $r0
; ALL-NEXT:    avguwp $r1 = $r3, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %3 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 0, i32 1>
  %4 = shufflevector <4 x i32> %1, <4 x i32> undef, <2 x i32> <i32 0, i32 1>
  %5 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %3, <2 x i32> %4, i32 2)
  %6 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 2, i32 3>
  %7 = shufflevector <4 x i32> %1, <4 x i32> undef, <2 x i32> <i32 2, i32 3>
  %8 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %6, <2 x i32> %7, i32 2)
  %9 = shufflevector <2 x i32> %5, <2 x i32> %8, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  ret <4 x i32> %9
}

define <8 x i32> @avgwo(<8 x i32> %0, <8 x i32> %1) {
; CV1-LABEL: avgwo:
; CV1:       # %bb.0:
; CV1-NEXT:    avgwp $r0 = $r4, $r0
; CV1-NEXT:    avgwp $r1 = $r5, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    avgwp $r2 = $r6, $r2
; CV1-NEXT:    avgwp $r3 = $r7, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgwo:
; CV2:       # %bb.0:
; CV2-NEXT:    avgwp $r0 = $r4, $r0
; CV2-NEXT:    avgwp $r1 = $r5, $r1
; CV2-NEXT:    avgwp $r2 = $r6, $r2
; CV2-NEXT:    avgwp $r3 = $r7, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 0, i32 1>
  %4 = shufflevector <8 x i32> %1, <8 x i32> undef, <2 x i32> <i32 0, i32 1>
  %5 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %3, <2 x i32> %4, i32 0)
  %6 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 2, i32 3>
  %7 = shufflevector <8 x i32> %1, <8 x i32> undef, <2 x i32> <i32 2, i32 3>
  %8 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %6, <2 x i32> %7, i32 0)
  %9 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 4, i32 5>
  %10 = shufflevector <8 x i32> %1, <8 x i32> undef, <2 x i32> <i32 4, i32 5>
  %11 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %9, <2 x i32> %10, i32 0)
  %12 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 6, i32 7>
  %13 = shufflevector <8 x i32> %1, <8 x i32> undef, <2 x i32> <i32 6, i32 7>
  %14 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %12, <2 x i32> %13, i32 0)
  %15 = shufflevector <2 x i32> %5, <2 x i32> %8, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %16 = shufflevector <2 x i32> %11, <2 x i32> %14, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %17 = shufflevector <4 x i32> %15, <4 x i32> %16, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i32> %17
}

define <2 x i32> @avgwp(<2 x i32> %0, <2 x i32> %1) {
; ALL-LABEL: avgwp:
; ALL:       # %bb.0:
; ALL-NEXT:    avgwp $r0 = $r1, $r0
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %3 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %0, <2 x i32> %1, i32 0)
  ret <2 x i32> %3
}

define <4 x i32> @avgwq(<4 x i32> %0, <4 x i32> %1) {
; ALL-LABEL: avgwq:
; ALL:       # %bb.0:
; ALL-NEXT:    avgwp $r0 = $r2, $r0
; ALL-NEXT:    avgwp $r1 = $r3, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %3 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 0, i32 1>
  %4 = shufflevector <4 x i32> %1, <4 x i32> undef, <2 x i32> <i32 0, i32 1>
  %5 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %3, <2 x i32> %4, i32 0)
  %6 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 2, i32 3>
  %7 = shufflevector <4 x i32> %1, <4 x i32> undef, <2 x i32> <i32 2, i32 3>
  %8 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %6, <2 x i32> %7, i32 0)
  %9 = shufflevector <2 x i32> %5, <2 x i32> %8, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  ret <4 x i32> %9
}

define i32 @avgw(i32 %0, i32 %1) {
; ALL-LABEL: avgw:
; ALL:       # %bb.0:
; ALL-NEXT:    avgw $r0 = $r0, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %3 = tail call i32 @llvm.kvx.avg.i32(i32 %0, i32 %1, i32 0)
  ret i32 %3
}

declare i32 @llvm.kvx.avg.i32(i32, i32, i32)

define i32 @avguw(i32 %0, i32 %1) {
; ALL-LABEL: avguw:
; ALL:       # %bb.0:
; ALL-NEXT:    avguw $r0 = $r0, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %3 = tail call i32 @llvm.kvx.avg.i32(i32 %0, i32 %1, i32 2)
  ret i32 %3
}

define i32 @avgrw(i32 %0, i32 %1) {
; ALL-LABEL: avgrw:
; ALL:       # %bb.0:
; ALL-NEXT:    avgrw $r0 = $r0, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %3 = tail call i32 @llvm.kvx.avg.i32(i32 %0, i32 %1, i32 1)
  ret i32 %3
}

define i32 @avgruw(i32 %0, i32 %1) {
; ALL-LABEL: avgruw:
; ALL:       # %bb.0:
; ALL-NEXT:    avgruw $r0 = $r0, $r1
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %3 = tail call i32 @llvm.kvx.avg.i32(i32 %0, i32 %1, i32 3)
  ret i32 %3
}

define <8 x i8> @avgbo_ri(<8 x i8> %0) {
; CV1-LABEL: avgbo_ri:
; CV1:       # %bb.0:
; CV1-NEXT:    make $r1 = 42
; CV1-NEXT:    andd.@ $r2 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r0, $r1
; CV1-NEXT:    andd.@ $r3 = $r1, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r2, $r3
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfz $r1 = $r0, 55, 48
; CV1-NEXT:    srld $r2 = $r0, 56
; CV1-NEXT:    extfz $r3 = $r0, 47, 40
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    extfz $r4 = $r0, 39, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    sraw $r2 = $r2, 1
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r1 = $r1, 1
; CV1-NEXT:    sraw $r3 = $r3, 1
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    extfz $r5 = $r0, 23, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r2, 15, 8
; CV1-NEXT:    srlw $r2 = $r0, 24
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    extfz $r3 = $r0, 15, 8
; CV1-NEXT:    insf $r4 = $r3, 15, 8
; CV1-NEXT:    sraw $r5 = $r5, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sraw $r2 = $r2, 1
; CV1-NEXT:    sraw $r3 = $r3, 1
; CV1-NEXT:    insf $r4 = $r1, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r0 = $r0, 1
; CV1-NEXT:    insf $r5 = $r2, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r3, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r5, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r4, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgbo_ri:
; CV2:       # %bb.0:
; CV2-NEXT:    avgbo $r0 = $r0, 42
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %2 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %0, <8 x i8> <i8 42, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, i32 0)
  ret <8 x i8> %2
}

define <2 x i8> @avgbp_ri(<2 x i8> %0) {
; CV1-LABEL: avgbp_ri:
; CV1:       # %bb.0:
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    ;;
; CV1-NEXT:    addhq $r0 = $r0, 42
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x401
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r0 = $r0, 1
; CV1-NEXT:    sraw $r1 = $r1, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r1, 15, 8
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgbp_ri:
; CV2:       # %bb.0:
; CV2-NEXT:    avgbo $r0 = $r0, 42
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %2 = tail call <2 x i8> @llvm.kvx.avg.v2i8(<2 x i8> %0, <2 x i8> <i8 42, i8 0>, i32 0)
  ret <2 x i8> %2
}

define <4 x i8> @avgbq_ri(<4 x i8> %0) {
; CV1-LABEL: avgbq_ri:
; CV1:       # %bb.0:
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    ;;
; CV1-NEXT:    addhq $r0 = $r0, 42
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    extfz $r3 = $r0, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sraw $r1 = $r1, 1
; CV1-NEXT:    sraw $r2 = $r2, 1
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r0 = $r0, 1
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    sraw $r3 = $r3, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r3, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r2, 31, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgbq_ri:
; CV2:       # %bb.0:
; CV2-NEXT:    avgbo $r0 = $r0, 42
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %2 = tail call <4 x i8> @llvm.kvx.avg.v4i8(<4 x i8> %0, <4 x i8> <i8 42, i8 0, i8 0, i8 0>, i32 0)
  ret <4 x i8> %2
}

define <16 x i8> @avgbx_ri(<16 x i8> %0) {
; CV1-LABEL: avgbx_ri:
; CV1:       # %bb.0:
; CV1-NEXT:    make $r2 = 42
; CV1-NEXT:    andd.@ $r3 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r4 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r2 = $r3, $r4
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r2, $r0
; CV1-NEXT:    ;;
; CV1-NEXT:    srld $r2 = $r0, 56
; CV1-NEXT:    extfz $r3 = $r0, 55, 48
; CV1-NEXT:    extfz $r4 = $r0, 47, 40
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r2 = $r2, 1
; CV1-NEXT:    sraw $r3 = $r3, 1
; CV1-NEXT:    extfz $r5 = $r0, 39, 32
; CV1-NEXT:    extfz $r6 = $r0, 23, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    srlw $r2 = $r0, 24
; CV1-NEXT:    insf $r3 = $r2, 15, 8
; CV1-NEXT:    extfz $r7 = $r0, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    sraw $r5 = $r5, 1
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sraw $r2 = $r2, 1
; CV1-NEXT:    sxbd $r7 = $r7
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r0 = $r0, 1
; CV1-NEXT:    sraw $r4 = $r6, 1
; CV1-NEXT:    insf $r5 = $r4, 15, 8
; CV1-NEXT:    sraw $r6 = $r7, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r6, 15, 8
; CV1-NEXT:    srld $r2 = $r1, 56
; CV1-NEXT:    insf $r4 = $r2, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r4, 31, 16
; CV1-NEXT:    insf $r5 = $r3, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    extfz $r3 = $r1, 55, 48
; CV1-NEXT:    extfz $r4 = $r1, 47, 40
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    extfz $r6 = $r1, 39, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r2 = $r2, 1
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r3 = $r3, 1
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    extfz $r7 = $r1, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    srlw $r2 = $r1, 24
; CV1-NEXT:    insf $r3 = $r2, 15, 8
; CV1-NEXT:    sraw $r6 = $r6, 1
; CV1-NEXT:    sxbd $r7 = $r7
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r1 = $r1
; CV1-NEXT:    extfz $r4 = $r1, 23, 16
; CV1-NEXT:    insf $r6 = $r4, 15, 8
; CV1-NEXT:    sraw $r7 = $r7, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    sraw $r2 = $r2, 1
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    insf $r6 = $r3, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r5, 63, 32
; CV1-NEXT:    sraw $r1 = $r1, 1
; CV1-NEXT:    insf $r4 = $r2, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r7, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r4, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r6, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgbx_ri:
; CV2:       # %bb.0:
; CV2-NEXT:    avgbo $r0 = $r0, 42
; CV2-NEXT:    srabos $r1 = $r1, 1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %2 = shufflevector <16 x i8> %0, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %3 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %2, <8 x i8> <i8 42, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, i32 0)
  %4 = shufflevector <16 x i8> %0, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %4, <8 x i8> zeroinitializer, i32 0)
  %6 = shufflevector <8 x i8> %3, <8 x i8> %5, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  ret <16 x i8> %6
}

define <32 x i8> @avgbv_ri(<32 x i8> %0) {
; CV1-LABEL: avgbv_ri:
; CV1:       # %bb.0:
; CV1-NEXT:    make $r4 = 42
; CV1-NEXT:    andd.@ $r5 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r0, $r4
; CV1-NEXT:    andd.@ $r6 = $r4, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r4 = $r5, $r6
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r4, $r0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfz $r4 = $r0, 55, 48
; CV1-NEXT:    srld $r5 = $r0, 56
; CV1-NEXT:    extfz $r6 = $r0, 47, 40
; CV1-NEXT:    srlw $r9 = $r0, 24
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    extfz $r7 = $r0, 39, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    sraw $r5 = $r5, 1
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    sraw $r6 = $r6, 1
; CV1-NEXT:    sxbd $r7 = $r7
; CV1-NEXT:    extfz $r8 = $r0, 23, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    insf $r4 = $r5, 15, 8
; CV1-NEXT:    extfz $r5 = $r0, 15, 8
; CV1-NEXT:    sraw $r7 = $r7, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r8 = $r8
; CV1-NEXT:    sxbd $r9 = $r9
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    sraw $r6 = $r9, 1
; CV1-NEXT:    insf $r7 = $r6, 15, 8
; CV1-NEXT:    sraw $r8 = $r8, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sraw $r4 = $r5, 1
; CV1-NEXT:    insf $r7 = $r4, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r0 = $r0, 1
; CV1-NEXT:    extfz $r5 = $r1, 55, 48
; CV1-NEXT:    insf $r8 = $r6, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r4, 15, 8
; CV1-NEXT:    srld $r4 = $r1, 56
; CV1-NEXT:    extfz $r6 = $r1, 47, 40
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r8, 31, 16
; CV1-NEXT:    extfz $r8 = $r1, 39, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    sraw $r5 = $r5, 1
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    sxbd $r8 = $r8
; CV1-NEXT:    ;;
; CV1-NEXT:    srlw $r4 = $r1, 24
; CV1-NEXT:    insf $r5 = $r4, 15, 8
; CV1-NEXT:    sraw $r6 = $r6, 1
; CV1-NEXT:    sraw $r8 = $r8, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    extfz $r6 = $r1, 15, 8
; CV1-NEXT:    insf $r8 = $r6, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r1 = $r1
; CV1-NEXT:    extfz $r5 = $r1, 23, 16
; CV1-NEXT:    insf $r8 = $r5, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    sraw $r5 = $r5, 1
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r7, 63, 32
; CV1-NEXT:    sraw $r1 = $r1, 1
; CV1-NEXT:    insf $r5 = $r4, 15, 8
; CV1-NEXT:    sraw $r6 = $r6, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r6, 15, 8
; CV1-NEXT:    srld $r4 = $r2, 56
; CV1-NEXT:    extfz $r6 = $r2, 47, 40
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r5, 31, 16
; CV1-NEXT:    extfz $r5 = $r2, 55, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    extfz $r7 = $r2, 39, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r8, 63, 32
; CV1-NEXT:    sraw $r5 = $r5, 1
; CV1-NEXT:    sraw $r6 = $r6, 1
; CV1-NEXT:    sxbd $r7 = $r7
; CV1-NEXT:    ;;
; CV1-NEXT:    srlw $r4 = $r2, 24
; CV1-NEXT:    insf $r5 = $r4, 15, 8
; CV1-NEXT:    sraw $r7 = $r7, 1
; CV1-NEXT:    extfz $r8 = $r3, 39, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    extfz $r6 = $r2, 15, 8
; CV1-NEXT:    insf $r7 = $r6, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r2 = $r2
; CV1-NEXT:    extfz $r5 = $r2, 23, 16
; CV1-NEXT:    insf $r7 = $r5, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    sraw $r5 = $r5, 1
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r2 = $r2, 1
; CV1-NEXT:    srld $r4 = $r3, 56
; CV1-NEXT:    insf $r5 = $r4, 15, 8
; CV1-NEXT:    sraw $r6 = $r6, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r6, 15, 8
; CV1-NEXT:    extfz $r6 = $r3, 47, 40
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r5, 31, 16
; CV1-NEXT:    extfz $r5 = $r3, 55, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    sraw $r5 = $r5, 1
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    sxbd $r8 = $r8
; CV1-NEXT:    ;;
; CV1-NEXT:    srlw $r4 = $r3, 24
; CV1-NEXT:    insf $r5 = $r4, 15, 8
; CV1-NEXT:    sraw $r6 = $r6, 1
; CV1-NEXT:    sraw $r8 = $r8, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    extfz $r6 = $r3, 23, 16
; CV1-NEXT:    insf $r8 = $r6, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r3 = $r3
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    extfz $r9 = $r3, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    sxbd $r9 = $r9
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    sraw $r6 = $r6, 1
; CV1-NEXT:    insf $r8 = $r5, 31, 16
; CV1-NEXT:    sraw $r9 = $r9, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r7, 63, 32
; CV1-NEXT:    sraw $r3 = $r3, 1
; CV1-NEXT:    insf $r6 = $r4, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r3 = $r9, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r3 = $r6, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r3 = $r8, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgbv_ri:
; CV2:       # %bb.0:
; CV2-NEXT:    avgbo $r0 = $r0, 42
; CV2-NEXT:    srabos $r1 = $r1, 1
; CV2-NEXT:    srabos $r2 = $r2, 1
; CV2-NEXT:    srabos $r3 = $r3, 1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %2 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %3 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %2, <8 x i8> <i8 42, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, i32 0)
  %4 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %4, <8 x i8> zeroinitializer, i32 0)
  %6 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %7 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %6, <8 x i8> zeroinitializer, i32 0)
  %8 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  %9 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %8, <8 x i8> zeroinitializer, i32 0)
  %10 = shufflevector <8 x i8> %3, <8 x i8> %5, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %11 = shufflevector <8 x i8> %7, <8 x i8> %9, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %12 = shufflevector <16 x i8> %10, <16 x i8> %11, <32 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  ret <32 x i8> %12
}

define <8 x i8> @avgrbo_ri(<8 x i8> %0) {
; CV1-LABEL: avgrbo_ri:
; CV1:       # %bb.0:
; CV1-NEXT:    make $r1 = 0x10101010101012b
; CV1-NEXT:    andd.@ $r2 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r0, $r1
; CV1-NEXT:    andd.@ $r3 = $r1, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r2, $r3
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfz $r1 = $r0, 55, 48
; CV1-NEXT:    srld $r2 = $r0, 56
; CV1-NEXT:    extfz $r3 = $r0, 47, 40
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    extfz $r4 = $r0, 39, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    sraw $r2 = $r2, 1
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r1 = $r1, 1
; CV1-NEXT:    sraw $r3 = $r3, 1
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    extfz $r5 = $r0, 23, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r2, 15, 8
; CV1-NEXT:    srlw $r2 = $r0, 24
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    extfz $r3 = $r0, 15, 8
; CV1-NEXT:    insf $r4 = $r3, 15, 8
; CV1-NEXT:    sraw $r5 = $r5, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sraw $r2 = $r2, 1
; CV1-NEXT:    sraw $r3 = $r3, 1
; CV1-NEXT:    insf $r4 = $r1, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r0 = $r0, 1
; CV1-NEXT:    insf $r5 = $r2, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r3, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r5, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r4, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgrbo_ri:
; CV2:       # %bb.0:
; CV2-NEXT:    make $r1 = 0x10101010101012b
; CV2-NEXT:    ;;
; CV2-NEXT:    avgbo $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %2 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %0, <8 x i8> <i8 42, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, i32 1)
  ret <8 x i8> %2
}

define <2 x i8> @avgrbp_ri(<2 x i8> %0) {
; CV1-LABEL: avgrbp_ri:
; CV1:       # %bb.0:
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    ;;
; CV1-NEXT:    addhq $r0 = $r0, 0x1002b
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x401
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r0 = $r0, 1
; CV1-NEXT:    sraw $r1 = $r1, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r1, 15, 8
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgrbp_ri:
; CV2:       # %bb.0:
; CV2-NEXT:    avgbo $r0 = $r0, 299
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %2 = tail call <2 x i8> @llvm.kvx.avg.v2i8(<2 x i8> %0, <2 x i8> <i8 42, i8 0>, i32 1)
  ret <2 x i8> %2
}

define <4 x i8> @avgrbq_ri(<4 x i8> %0) {
; CV1-LABEL: avgrbq_ri:
; CV1:       # %bb.0:
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    make $r1 = 0x100010001002b
; CV1-NEXT:    ;;
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    extfz $r3 = $r0, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sraw $r1 = $r1, 1
; CV1-NEXT:    sraw $r2 = $r2, 1
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r0 = $r0, 1
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    sraw $r3 = $r3, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r3, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r2, 31, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgrbq_ri:
; CV2:       # %bb.0:
; CV2-NEXT:    avgbo $r0 = $r0, 0x101012b
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %2 = tail call <4 x i8> @llvm.kvx.avg.v4i8(<4 x i8> %0, <4 x i8> <i8 42, i8 0, i8 0, i8 0>, i32 1)
  ret <4 x i8> %2
}

define <16 x i8> @avgrbx_ri(<16 x i8> %0) {
; CV1-LABEL: avgrbx_ri:
; CV1:       # %bb.0:
; CV1-NEXT:    make $r2 = 0x10101010101012b
; CV1-NEXT:    andd.@ $r3 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r4 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r2 = $r3, $r4
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r2, $r0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfz $r2 = $r0, 55, 48
; CV1-NEXT:    extfz $r3 = $r0, 47, 40
; CV1-NEXT:    srld $r4 = $r0, 56
; CV1-NEXT:    srlw $r7 = $r0, 24
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    extfz $r5 = $r0, 39, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r2 = $r2, 1
; CV1-NEXT:    sraw $r3 = $r3, 1
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    extfz $r6 = $r0, 23, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    insf $r2 = $r4, 15, 8
; CV1-NEXT:    extfz $r4 = $r0, 15, 8
; CV1-NEXT:    sraw $r5 = $r5, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    make $r3 = 0x101010101010101
; CV1-NEXT:    insf $r5 = $r3, 15, 8
; CV1-NEXT:    sxbd $r7 = $r7
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r2 = $r3, 0x7f7f7f7f
; CV1-NEXT:    xord $r3 = $r1, $r3
; CV1-NEXT:    insf $r5 = $r2, 31, 16
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    andd.@ $r1 = $r1, 0x7f7f7f7f
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    sraw $r7 = $r7, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r0 = $r0, 1
; CV1-NEXT:    addd $r1 = $r1, $r2
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    sraw $r6 = $r6, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r4, 15, 8
; CV1-NEXT:    andd.@ $r2 = $r3, 0x80808080
; CV1-NEXT:    insf $r6 = $r7, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r6, 31, 16
; CV1-NEXT:    xord $r1 = $r1, $r2
; CV1-NEXT:    ;;
; CV1-NEXT:    srld $r2 = $r1, 56
; CV1-NEXT:    extfz $r3 = $r1, 55, 48
; CV1-NEXT:    extfz $r4 = $r1, 47, 40
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    extfz $r6 = $r1, 39, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r2 = $r2, 1
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r3 = $r3, 1
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    extfz $r7 = $r1, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    srlw $r2 = $r1, 24
; CV1-NEXT:    insf $r3 = $r2, 15, 8
; CV1-NEXT:    sraw $r6 = $r6, 1
; CV1-NEXT:    sxbd $r7 = $r7
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r1 = $r1
; CV1-NEXT:    extfz $r4 = $r1, 23, 16
; CV1-NEXT:    insf $r6 = $r4, 15, 8
; CV1-NEXT:    sraw $r7 = $r7, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    sraw $r2 = $r2, 1
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    insf $r6 = $r3, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r5, 63, 32
; CV1-NEXT:    sraw $r1 = $r1, 1
; CV1-NEXT:    insf $r4 = $r2, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r7, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r4, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r6, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgrbx_ri:
; CV2:       # %bb.0:
; CV2-NEXT:    avgbo.@ $r1 = $r1, 0x1010101
; CV2-NEXT:    make $r2 = 0x10101010101012b
; CV2-NEXT:    ;;
; CV2-NEXT:    avgbo $r0 = $r2, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %2 = shufflevector <16 x i8> %0, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %3 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %2, <8 x i8> <i8 42, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, i32 1)
  %4 = shufflevector <16 x i8> %0, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %4, <8 x i8> zeroinitializer, i32 1)
  %6 = shufflevector <8 x i8> %3, <8 x i8> %5, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  ret <16 x i8> %6
}

define <32 x i8> @avgrbv_ri(<32 x i8> %0) {
; CV1-LABEL: avgrbv_ri:
; CV1:       # %bb.0:
; CV1-NEXT:    make $r4 = 0x10101010101012b
; CV1-NEXT:    andd.@ $r5 = $r0, 0x7f7f7f7f
; CV1-NEXT:    andd.@ $r11 = $r1, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r0, $r4
; CV1-NEXT:    andd.@ $r6 = $r4, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r4 = $r5, $r6
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r4, $r0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfz $r5 = $r0, 47, 40
; CV1-NEXT:    extfz $r6 = $r0, 39, 32
; CV1-NEXT:    srld $r7 = $r0, 56
; CV1-NEXT:    srlw $r10 = $r0, 24
; CV1-NEXT:    ;;
; CV1-NEXT:    extfz $r4 = $r0, 55, 48
; CV1-NEXT:    extfz $r8 = $r0, 23, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r5 = $r5, 1
; CV1-NEXT:    sraw $r6 = $r6, 1
; CV1-NEXT:    sxbd $r7 = $r7
; CV1-NEXT:    extfz $r9 = $r0, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    sraw $r7 = $r7, 1
; CV1-NEXT:    sxbd $r8 = $r8
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    sxbd $r9 = $r9
; CV1-NEXT:    sxbd $r10 = $r10
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sraw $r5 = $r8, 1
; CV1-NEXT:    insf $r6 = $r5, 15, 8
; CV1-NEXT:    make $r8 = 0x101010101010101
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r0 = $r0, 1
; CV1-NEXT:    insf $r4 = $r7, 15, 8
; CV1-NEXT:    sraw $r7 = $r9, 1
; CV1-NEXT:    sraw $r10 = $r10, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r7, 15, 8
; CV1-NEXT:    xord $r1 = $r1, $r8
; CV1-NEXT:    insf $r5 = $r10, 15, 8
; CV1-NEXT:    andd.@ $r9 = $r8, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r5, 31, 16
; CV1-NEXT:    andd.@ $r1 = $r1, 0x80808080
; CV1-NEXT:    insf $r6 = $r4, 31, 16
; CV1-NEXT:    addd $r7 = $r11, $r9
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r6, 63, 32
; CV1-NEXT:    xord $r1 = $r7, $r1
; CV1-NEXT:    xord $r2 = $r2, $r8
; CV1-NEXT:    andd.@ $r11 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r2 = $r2, 0x80808080
; CV1-NEXT:    srld $r4 = $r1, 56
; CV1-NEXT:    extfz $r5 = $r1, 55, 48
; CV1-NEXT:    extfz $r6 = $r1, 47, 40
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r3 = $r3, 0x7f7f7f7f
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    extfz $r7 = $r1, 39, 32
; CV1-NEXT:    xord $r8 = $r3, $r8
; CV1-NEXT:    ;;
; CV1-NEXT:    addd $r3 = $r3, $r9
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r5 = $r5, 1
; CV1-NEXT:    sraw $r6 = $r6, 1
; CV1-NEXT:    sxbd $r7 = $r7
; CV1-NEXT:    extfz $r10 = $r1, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    srlw $r4 = $r1, 24
; CV1-NEXT:    insf $r5 = $r4, 15, 8
; CV1-NEXT:    sraw $r7 = $r7, 1
; CV1-NEXT:    sxbd $r10 = $r10
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r1 = $r1
; CV1-NEXT:    extfz $r6 = $r1, 23, 16
; CV1-NEXT:    insf $r7 = $r6, 15, 8
; CV1-NEXT:    sraw $r10 = $r10, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    andd.@ $r8 = $r8, 0x80808080
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    sraw $r6 = $r6, 1
; CV1-NEXT:    insf $r7 = $r5, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r1 = $r1, 1
; CV1-NEXT:    xord $r3 = $r3, $r8
; CV1-NEXT:    addd $r4 = $r11, $r9
; CV1-NEXT:    insf $r6 = $r4, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r10, 15, 8
; CV1-NEXT:    xord $r2 = $r4, $r2
; CV1-NEXT:    extfz $r8 = $r3, 39, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r6, 31, 16
; CV1-NEXT:    srld $r4 = $r2, 56
; CV1-NEXT:    extfz $r5 = $r2, 55, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r7, 63, 32
; CV1-NEXT:    extfz $r6 = $r2, 47, 40
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    extfz $r7 = $r2, 39, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r5 = $r5, 1
; CV1-NEXT:    sraw $r6 = $r6, 1
; CV1-NEXT:    sxbd $r7 = $r7
; CV1-NEXT:    sxbd $r8 = $r8
; CV1-NEXT:    ;;
; CV1-NEXT:    srlw $r4 = $r2, 24
; CV1-NEXT:    insf $r5 = $r4, 15, 8
; CV1-NEXT:    sraw $r7 = $r7, 1
; CV1-NEXT:    sraw $r8 = $r8, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    extfz $r6 = $r2, 15, 8
; CV1-NEXT:    insf $r7 = $r6, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r2 = $r2
; CV1-NEXT:    extfz $r5 = $r2, 23, 16
; CV1-NEXT:    insf $r7 = $r5, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    sraw $r5 = $r5, 1
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r2 = $r2, 1
; CV1-NEXT:    srld $r4 = $r3, 56
; CV1-NEXT:    insf $r5 = $r4, 15, 8
; CV1-NEXT:    sraw $r6 = $r6, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r6, 15, 8
; CV1-NEXT:    extfz $r6 = $r3, 47, 40
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r5, 31, 16
; CV1-NEXT:    extfz $r5 = $r3, 55, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    ;;
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    sraw $r5 = $r5, 1
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    extfz $r9 = $r3, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    srlw $r4 = $r3, 24
; CV1-NEXT:    insf $r5 = $r4, 15, 8
; CV1-NEXT:    sraw $r6 = $r6, 1
; CV1-NEXT:    sxbd $r9 = $r9
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r3 = $r3
; CV1-NEXT:    extfz $r6 = $r3, 23, 16
; CV1-NEXT:    insf $r8 = $r6, 15, 8
; CV1-NEXT:    sraw $r9 = $r9, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    ;;
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    sraw $r4 = $r4, 1
; CV1-NEXT:    sraw $r6 = $r6, 1
; CV1-NEXT:    insf $r8 = $r5, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r7, 63, 32
; CV1-NEXT:    sraw $r3 = $r3, 1
; CV1-NEXT:    insf $r6 = $r4, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r3 = $r9, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r3 = $r6, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r3 = $r8, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgrbv_ri:
; CV2:       # %bb.0:
; CV2-NEXT:    avgbo.@ $r1 = $r1, 0x1010101
; CV2-NEXT:    avgbo.@ $r2 = $r2, 0x1010101
; CV2-NEXT:    make $r4 = 0x10101010101012b
; CV2-NEXT:    ;;
; CV2-NEXT:    avgbo $r0 = $r4, $r0
; CV2-NEXT:    avgbo.@ $r3 = $r3, 0x1010101
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %2 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %3 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %2, <8 x i8> <i8 42, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, i32 1)
  %4 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %4, <8 x i8> zeroinitializer, i32 1)
  %6 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %7 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %6, <8 x i8> zeroinitializer, i32 1)
  %8 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  %9 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %8, <8 x i8> zeroinitializer, i32 1)
  %10 = shufflevector <8 x i8> %3, <8 x i8> %5, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %11 = shufflevector <8 x i8> %7, <8 x i8> %9, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %12 = shufflevector <16 x i8> %10, <16 x i8> %11, <32 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  ret <32 x i8> %12
}

define <8 x i8> @avgubo_ri(<8 x i8> %0) {
; CV1-LABEL: avgubo_ri:
; CV1:       # %bb.0:
; CV1-NEXT:    make $r1 = 42
; CV1-NEXT:    andd.@ $r3 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r0, $r1
; CV1-NEXT:    andd.@ $r2 = $r1, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0xff00ff00
; CV1-NEXT:    andd.@ $r1 = $r0, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    srld $r0 = $r0, 1
; CV1-NEXT:    srld $r1 = $r1, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0xff00ff00
; CV1-NEXT:    andd.@ $r1 = $r1, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    ord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgubo_ri:
; CV2:       # %bb.0:
; CV2-NEXT:    avgubo $r0 = $r0, 42
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %2 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %0, <8 x i8> <i8 42, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, i32 2)
  ret <8 x i8> %2
}

define <2 x i8> @avgubp_ri(<2 x i8> %0) {
; CV1-LABEL: avgubp_ri:
; CV1:       # %bb.0:
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    ;;
; CV1-NEXT:    addhq $r0 = $r0, 42
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x401
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    andw $r0 = $r0, 254
; CV1-NEXT:    andw $r1 = $r1, 254
; CV1-NEXT:    ;;
; CV1-NEXT:    srlw $r0 = $r0, 1
; CV1-NEXT:    srlw $r1 = $r1, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r1, 15, 8
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgubp_ri:
; CV2:       # %bb.0:
; CV2-NEXT:    avgubo $r0 = $r0, 42
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %2 = tail call <2 x i8> @llvm.kvx.avg.v2i8(<2 x i8> %0, <2 x i8> <i8 42, i8 0>, i32 2)
  ret <2 x i8> %2
}

define <4 x i8> @avgubq_ri(<4 x i8> %0) {
; CV1-LABEL: avgubq_ri:
; CV1:       # %bb.0:
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    ;;
; CV1-NEXT:    addhq $r0 = $r0, 42
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    extfz $r3 = $r0, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    andw $r0 = $r0, 254
; CV1-NEXT:    andw $r1 = $r1, 254
; CV1-NEXT:    andw $r2 = $r2, 254
; CV1-NEXT:    andw $r3 = $r3, 254
; CV1-NEXT:    ;;
; CV1-NEXT:    srlw $r0 = $r0, 1
; CV1-NEXT:    srlw $r1 = $r1, 1
; CV1-NEXT:    srlw $r2 = $r2, 1
; CV1-NEXT:    srlw $r3 = $r3, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r3, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r2, 31, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgubq_ri:
; CV2:       # %bb.0:
; CV2-NEXT:    avgubo $r0 = $r0, 42
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %2 = tail call <4 x i8> @llvm.kvx.avg.v4i8(<4 x i8> %0, <4 x i8> <i8 42, i8 0, i8 0, i8 0>, i32 2)
  ret <4 x i8> %2
}

define <16 x i8> @avgubx_ri(<16 x i8> %0) {
; CV1-LABEL: avgubx_ri:
; CV1:       # %bb.0:
; CV1-NEXT:    make $r2 = 42
; CV1-NEXT:    andd.@ $r4 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r3 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    andd.@ $r1 = $r1, 0xff00ff00
; CV1-NEXT:    addd $r2 = $r4, $r3
; CV1-NEXT:    andd.@ $r3 = $r1, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r2, $r0
; CV1-NEXT:    srld $r1 = $r1, 1
; CV1-NEXT:    srld $r3 = $r3, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0xff00ff00
; CV1-NEXT:    andd.@ $r1 = $r1, 0xff00ff00
; CV1-NEXT:    andd.@ $r2 = $r0, 0xff00ff
; CV1-NEXT:    andd.@ $r3 = $r3, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    srld $r0 = $r0, 1
; CV1-NEXT:    ord $r1 = $r1, $r3
; CV1-NEXT:    srld $r2 = $r2, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0xff00ff00
; CV1-NEXT:    andd.@ $r2 = $r2, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    ord $r0 = $r0, $r2
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgubx_ri:
; CV2:       # %bb.0:
; CV2-NEXT:    avgubo $r0 = $r0, 42
; CV2-NEXT:    srlbos $r1 = $r1, 1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %2 = shufflevector <16 x i8> %0, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %3 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %2, <8 x i8> <i8 42, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, i32 2)
  %4 = shufflevector <16 x i8> %0, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %4, <8 x i8> zeroinitializer, i32 2)
  %6 = shufflevector <8 x i8> %3, <8 x i8> %5, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  ret <16 x i8> %6
}

define <32 x i8> @avgubv_ri(<32 x i8> %0) {
; CV1-LABEL: avgubv_ri:
; CV1:       # %bb.0:
; CV1-NEXT:    make $r4 = 42
; CV1-NEXT:    andd.@ $r5 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r0, $r4
; CV1-NEXT:    andd.@ $r1 = $r1, 0xff00ff00
; CV1-NEXT:    andd.@ $r4 = $r1, 0xff00ff
; CV1-NEXT:    andd.@ $r6 = $r4, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    andd.@ $r2 = $r2, 0xff00ff00
; CV1-NEXT:    addd $r5 = $r5, $r6
; CV1-NEXT:    andd.@ $r6 = $r2, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r5, $r0
; CV1-NEXT:    andd.@ $r3 = $r3, 0xff00ff00
; CV1-NEXT:    srld $r4 = $r4, 1
; CV1-NEXT:    andd.@ $r5 = $r3, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0xff00ff00
; CV1-NEXT:    srld $r1 = $r1, 1
; CV1-NEXT:    srld $r6 = $r6, 1
; CV1-NEXT:    andd.@ $r7 = $r0, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    srld $r0 = $r0, 1
; CV1-NEXT:    srld $r2 = $r2, 1
; CV1-NEXT:    srld $r5 = $r5, 1
; CV1-NEXT:    srld $r7 = $r7, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r1 = $r1, 0xff00ff00
; CV1-NEXT:    srld $r3 = $r3, 1
; CV1-NEXT:    andd.@ $r4 = $r4, 0xff00ff
; CV1-NEXT:    andd.@ $r7 = $r7, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0xff00ff00
; CV1-NEXT:    andd.@ $r2 = $r2, 0xff00ff00
; CV1-NEXT:    andd.@ $r5 = $r5, 0xff00ff
; CV1-NEXT:    andd.@ $r6 = $r6, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    ord $r0 = $r0, $r7
; CV1-NEXT:    ord $r1 = $r1, $r4
; CV1-NEXT:    ord $r2 = $r2, $r6
; CV1-NEXT:    andd.@ $r3 = $r3, 0xff00ff00
; CV1-NEXT:    ;;
; CV1-NEXT:    ord $r3 = $r3, $r5
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgubv_ri:
; CV2:       # %bb.0:
; CV2-NEXT:    avgubo $r0 = $r0, 42
; CV2-NEXT:    srlbos $r1 = $r1, 1
; CV2-NEXT:    srlbos $r2 = $r2, 1
; CV2-NEXT:    srlbos $r3 = $r3, 1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %2 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %3 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %2, <8 x i8> <i8 42, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, i32 2)
  %4 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %4, <8 x i8> zeroinitializer, i32 2)
  %6 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %7 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %6, <8 x i8> zeroinitializer, i32 2)
  %8 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  %9 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %8, <8 x i8> zeroinitializer, i32 2)
  %10 = shufflevector <8 x i8> %3, <8 x i8> %5, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %11 = shufflevector <8 x i8> %7, <8 x i8> %9, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %12 = shufflevector <16 x i8> %10, <16 x i8> %11, <32 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  ret <32 x i8> %12
}

define <8 x i8> @avgrubo_ri(<8 x i8> %0) {
; CV1-LABEL: avgrubo_ri:
; CV1:       # %bb.0:
; CV1-NEXT:    make $r1 = 0x10101010101012b
; CV1-NEXT:    andd.@ $r3 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r0, $r1
; CV1-NEXT:    andd.@ $r2 = $r1, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0xff00ff00
; CV1-NEXT:    andd.@ $r1 = $r0, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    srld $r0 = $r0, 1
; CV1-NEXT:    srld $r1 = $r1, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0xff00ff00
; CV1-NEXT:    andd.@ $r1 = $r1, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    ord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgrubo_ri:
; CV2:       # %bb.0:
; CV2-NEXT:    make $r1 = 0x10101010101012b
; CV2-NEXT:    ;;
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;;
; CV2-NEXT:    srlbos $r0 = $r0, 1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %2 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %0, <8 x i8> <i8 42, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, i32 3)
  ret <8 x i8> %2
}

define <2 x i8> @avgrubp_ri(<2 x i8> %0) {
; CV1-LABEL: avgrubp_ri:
; CV1:       # %bb.0:
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    ;;
; CV1-NEXT:    addhq $r0 = $r0, 0x1002b
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x401
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    andw $r0 = $r0, 254
; CV1-NEXT:    andw $r1 = $r1, 254
; CV1-NEXT:    ;;
; CV1-NEXT:    srlw $r0 = $r0, 1
; CV1-NEXT:    srlw $r1 = $r1, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r1, 15, 8
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgrubp_ri:
; CV2:       # %bb.0:
; CV2-NEXT:    addbo $r0 = $r0, 299
; CV2-NEXT:    ;;
; CV2-NEXT:    srlbos $r0 = $r0, 1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %2 = tail call <2 x i8> @llvm.kvx.avg.v2i8(<2 x i8> %0, <2 x i8> <i8 42, i8 0>, i32 3)
  ret <2 x i8> %2
}

define <4 x i8> @avgrubq_ri(<4 x i8> %0) {
; CV1-LABEL: avgrubq_ri:
; CV1:       # %bb.0:
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    make $r1 = 0x100010001002b
; CV1-NEXT:    ;;
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    ;;
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    extfz $r3 = $r0, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    andw $r0 = $r0, 254
; CV1-NEXT:    andw $r1 = $r1, 254
; CV1-NEXT:    andw $r2 = $r2, 254
; CV1-NEXT:    andw $r3 = $r3, 254
; CV1-NEXT:    ;;
; CV1-NEXT:    srlw $r0 = $r0, 1
; CV1-NEXT:    srlw $r1 = $r1, 1
; CV1-NEXT:    srlw $r2 = $r2, 1
; CV1-NEXT:    srlw $r3 = $r3, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r3, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r2, 31, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgrubq_ri:
; CV2:       # %bb.0:
; CV2-NEXT:    addbo $r0 = $r0, 0x101012b
; CV2-NEXT:    ;;
; CV2-NEXT:    srlbos $r0 = $r0, 1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %2 = tail call <4 x i8> @llvm.kvx.avg.v4i8(<4 x i8> %0, <4 x i8> <i8 42, i8 0, i8 0, i8 0>, i32 3)
  ret <4 x i8> %2
}

define <16 x i8> @avgrubx_ri(<16 x i8> %0) {
; CV1-LABEL: avgrubx_ri:
; CV1:       # %bb.0:
; CV1-NEXT:    make $r2 = 0x10101010101012b
; CV1-NEXT:    andd.@ $r3 = $r0, 0x7f7f7f7f
; CV1-NEXT:    make $r4 = 0x101010101010101
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r2 = $r4, 0x7f7f7f7f
; CV1-NEXT:    xord $r4 = $r1, $r4
; CV1-NEXT:    andd.@ $r5 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    andd.@ $r1 = $r1, 0x7f7f7f7f
; CV1-NEXT:    addd $r3 = $r3, $r5
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r3, $r0
; CV1-NEXT:    addd $r1 = $r1, $r2
; CV1-NEXT:    andd.@ $r2 = $r4, 0x80808080
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0xff00ff00
; CV1-NEXT:    xord $r1 = $r1, $r2
; CV1-NEXT:    andd.@ $r2 = $r0, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    srld $r0 = $r0, 1
; CV1-NEXT:    andd.@ $r1 = $r1, 0xff00ff00
; CV1-NEXT:    srld $r2 = $r2, 1
; CV1-NEXT:    andd.@ $r3 = $r1, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0xff00ff00
; CV1-NEXT:    srld $r1 = $r1, 1
; CV1-NEXT:    andd.@ $r2 = $r2, 0xff00ff
; CV1-NEXT:    srld $r3 = $r3, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    ord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r1 = $r1, 0xff00ff00
; CV1-NEXT:    andd.@ $r3 = $r3, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    ord $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgrubx_ri:
; CV2:       # %bb.0:
; CV2-NEXT:    addbo.@ $r1 = $r1, 0x1010101
; CV2-NEXT:    make $r2 = 0x10101010101012b
; CV2-NEXT:    ;;
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    srlbos $r1 = $r1, 1
; CV2-NEXT:    ;;
; CV2-NEXT:    srlbos $r0 = $r0, 1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %2 = shufflevector <16 x i8> %0, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %3 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %2, <8 x i8> <i8 42, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, i32 3)
  %4 = shufflevector <16 x i8> %0, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %4, <8 x i8> zeroinitializer, i32 3)
  %6 = shufflevector <8 x i8> %3, <8 x i8> %5, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  ret <16 x i8> %6
}

define <32 x i8> @avgrubv_ri(<32 x i8> %0) {
; CV1-LABEL: avgrubv_ri:
; CV1:       # %bb.0:
; CV1-NEXT:    make $r4 = 0x10101010101012b
; CV1-NEXT:    andd.@ $r5 = $r0, 0x7f7f7f7f
; CV1-NEXT:    andd.@ $r7 = $r1, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r0, $r4
; CV1-NEXT:    make $r4 = 0x101010101010101
; CV1-NEXT:    andd.@ $r6 = $r4, 0x7f7f7f7f
; CV1-NEXT:    andd.@ $r8 = $r3, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r5 = $r5, $r6
; CV1-NEXT:    andd.@ $r6 = $r2, 0x7f7f7f7f
; CV1-NEXT:    andd.@ $r9 = $r4, 0x7f7f7f7f
; CV1-NEXT:    ;;
; CV1-NEXT:    xord $r0 = $r5, $r0
; CV1-NEXT:    xord $r1 = $r1, $r4
; CV1-NEXT:    xord $r2 = $r2, $r4
; CV1-NEXT:    xord $r3 = $r3, $r4
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r1 = $r1, 0x80808080
; CV1-NEXT:    addd $r5 = $r7, $r9
; CV1-NEXT:    addd $r6 = $r6, $r9
; CV1-NEXT:    andd.@ $r7 = $r0, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r2 = $r2, 0x80808080
; CV1-NEXT:    andd.@ $r3 = $r3, 0x80808080
; CV1-NEXT:    srld $r7 = $r7, 1
; CV1-NEXT:    addd $r8 = $r8, $r9
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0xff00ff00
; CV1-NEXT:    xord $r1 = $r5, $r1
; CV1-NEXT:    xord $r2 = $r6, $r2
; CV1-NEXT:    xord $r3 = $r8, $r3
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r1 = $r1, 0xff00ff00
; CV1-NEXT:    andd.@ $r4 = $r1, 0xff00ff
; CV1-NEXT:    andd.@ $r5 = $r7, 0xff00ff
; CV1-NEXT:    andd.@ $r6 = $r2, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    srld $r0 = $r0, 1
; CV1-NEXT:    andd.@ $r2 = $r2, 0xff00ff00
; CV1-NEXT:    andd.@ $r3 = $r3, 0xff00ff00
; CV1-NEXT:    andd.@ $r7 = $r3, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    srld $r1 = $r1, 1
; CV1-NEXT:    srld $r2 = $r2, 1
; CV1-NEXT:    srld $r4 = $r4, 1
; CV1-NEXT:    srld $r6 = $r6, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r0 = $r0, 0xff00ff00
; CV1-NEXT:    srld $r3 = $r3, 1
; CV1-NEXT:    andd.@ $r4 = $r4, 0xff00ff
; CV1-NEXT:    srld $r7 = $r7, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    andd.@ $r1 = $r1, 0xff00ff00
; CV1-NEXT:    andd.@ $r2 = $r2, 0xff00ff00
; CV1-NEXT:    andd.@ $r6 = $r6, 0xff00ff
; CV1-NEXT:    andd.@ $r7 = $r7, 0xff00ff
; CV1-NEXT:    ;;
; CV1-NEXT:    ord $r0 = $r0, $r5
; CV1-NEXT:    ord $r1 = $r1, $r4
; CV1-NEXT:    ord $r2 = $r2, $r6
; CV1-NEXT:    andd.@ $r3 = $r3, 0xff00ff00
; CV1-NEXT:    ;;
; CV1-NEXT:    ord $r3 = $r3, $r7
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgrubv_ri:
; CV2:       # %bb.0:
; CV2-NEXT:    addbo.@ $r1 = $r1, 0x1010101
; CV2-NEXT:    addbo.@ $r2 = $r2, 0x1010101
; CV2-NEXT:    make $r4 = 0x10101010101012b
; CV2-NEXT:    ;;
; CV2-NEXT:    addbo $r0 = $r0, $r4
; CV2-NEXT:    srlbos $r1 = $r1, 1
; CV2-NEXT:    srlbos $r2 = $r2, 1
; CV2-NEXT:    addbo.@ $r3 = $r3, 0x1010101
; CV2-NEXT:    ;;
; CV2-NEXT:    srlbos $r0 = $r0, 1
; CV2-NEXT:    srlbos $r3 = $r3, 1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %2 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %3 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %2, <8 x i8> <i8 42, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, i32 3)
  %4 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %5 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %4, <8 x i8> zeroinitializer, i32 3)
  %6 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %7 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %6, <8 x i8> zeroinitializer, i32 3)
  %8 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  %9 = tail call <8 x i8> @llvm.kvx.avg.v8i8(<8 x i8> %8, <8 x i8> zeroinitializer, i32 3)
  %10 = shufflevector <8 x i8> %3, <8 x i8> %5, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %11 = shufflevector <8 x i8> %7, <8 x i8> %9, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %12 = shufflevector <16 x i8> %10, <16 x i8> %11, <32 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  ret <32 x i8> %12
}

define <8 x i16> @avgho_ri(<8 x i16> %0) {
; ALL-LABEL: avgho_ri:
; ALL:       # %bb.0:
; ALL-NEXT:    avghq $r0 = $r0, 42
; ALL-NEXT:    srahqs $r1 = $r1, 1
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %2 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %2, <4 x i16> <i16 42, i16 0, i16 0, i16 0>, i32 0)
  %4 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %5 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %4, <4 x i16> zeroinitializer, i32 0)
  %6 = shufflevector <4 x i16> %3, <4 x i16> %5, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i16> %6
}

define <2 x i16> @avghp_ri(<2 x i16> %0) {
; ALL-LABEL: avghp_ri:
; ALL:       # %bb.0:
; ALL-NEXT:    avghq $r0 = $r0, 42
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %2 = tail call <2 x i16> @llvm.kvx.avg.v2i16(<2 x i16> %0, <2 x i16> <i16 42, i16 0>, i32 0)
  ret <2 x i16> %2
}

define <4 x i16> @avghq_ri(<4 x i16> %0) {
; ALL-LABEL: avghq_ri:
; ALL:       # %bb.0:
; ALL-NEXT:    avghq $r0 = $r0, 42
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %2 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %0, <4 x i16> <i16 42, i16 0, i16 0, i16 0>, i32 0)
  ret <4 x i16> %2
}

define <16 x i16> @avghx_ri(<16 x i16> %0) {
; CV1-LABEL: avghx_ri:
; CV1:       # %bb.0:
; CV1-NEXT:    avghq $r0 = $r0, 42
; CV1-NEXT:    srahqs $r1 = $r1, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    srahqs $r2 = $r2, 1
; CV1-NEXT:    srahqs $r3 = $r3, 1
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avghx_ri:
; CV2:       # %bb.0:
; CV2-NEXT:    avghq $r0 = $r0, 42
; CV2-NEXT:    srahqs $r1 = $r1, 1
; CV2-NEXT:    srahqs $r2 = $r2, 1
; CV2-NEXT:    srahqs $r3 = $r3, 1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %2 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %2, <4 x i16> <i16 42, i16 0, i16 0, i16 0>, i32 0)
  %4 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %5 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %4, <4 x i16> zeroinitializer, i32 0)
  %6 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %6, <4 x i16> zeroinitializer, i32 0)
  %8 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %9 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %8, <4 x i16> zeroinitializer, i32 0)
  %10 = shufflevector <4 x i16> %3, <4 x i16> %5, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %11 = shufflevector <4 x i16> %7, <4 x i16> %9, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %12 = shufflevector <8 x i16> %10, <8 x i16> %11, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  ret <16 x i16> %12
}

define <8 x i16> @avgrho_ri(<8 x i16> %0) {
; ALL-LABEL: avgrho_ri:
; ALL:       # %bb.0:
; ALL-NEXT:    avghq.@ $r1 = $r1, 0x10001
; ALL-NEXT:    make $r2 = 0x100010001002b
; ALL-NEXT:    ;;
; ALL-NEXT:    avghq $r0 = $r2, $r0
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %2 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %2, <4 x i16> <i16 42, i16 0, i16 0, i16 0>, i32 1)
  %4 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %5 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %4, <4 x i16> zeroinitializer, i32 1)
  %6 = shufflevector <4 x i16> %3, <4 x i16> %5, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i16> %6
}

define <2 x i16> @avgrhp_ri(<2 x i16> %0) {
; ALL-LABEL: avgrhp_ri:
; ALL:       # %bb.0:
; ALL-NEXT:    avghq $r0 = $r0, 0x1002b
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %2 = tail call <2 x i16> @llvm.kvx.avg.v2i16(<2 x i16> %0, <2 x i16> <i16 42, i16 0>, i32 1)
  ret <2 x i16> %2
}

define <4 x i16> @avgrhq_ri(<4 x i16> %0) {
; ALL-LABEL: avgrhq_ri:
; ALL:       # %bb.0:
; ALL-NEXT:    make $r1 = 0x100010001002b
; ALL-NEXT:    ;;
; ALL-NEXT:    avghq $r0 = $r1, $r0
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %2 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %0, <4 x i16> <i16 42, i16 0, i16 0, i16 0>, i32 1)
  ret <4 x i16> %2
}

define <16 x i16> @avgrhx_ri(<16 x i16> %0) {
; ALL-LABEL: avgrhx_ri:
; ALL:       # %bb.0:
; ALL-NEXT:    avghq.@ $r1 = $r1, 0x10001
; ALL-NEXT:    avghq.@ $r2 = $r2, 0x10001
; ALL-NEXT:    make $r4 = 0x100010001002b
; ALL-NEXT:    ;;
; ALL-NEXT:    avghq $r0 = $r4, $r0
; ALL-NEXT:    avghq.@ $r3 = $r3, 0x10001
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %2 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %2, <4 x i16> <i16 42, i16 0, i16 0, i16 0>, i32 1)
  %4 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %5 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %4, <4 x i16> zeroinitializer, i32 1)
  %6 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %6, <4 x i16> zeroinitializer, i32 1)
  %8 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %9 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %8, <4 x i16> zeroinitializer, i32 1)
  %10 = shufflevector <4 x i16> %3, <4 x i16> %5, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %11 = shufflevector <4 x i16> %7, <4 x i16> %9, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %12 = shufflevector <8 x i16> %10, <8 x i16> %11, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  ret <16 x i16> %12
}

define <8 x i16> @avguho_ri(<8 x i16> %0) {
; ALL-LABEL: avguho_ri:
; ALL:       # %bb.0:
; ALL-NEXT:    avguhq $r0 = $r0, 42
; ALL-NEXT:    srlhqs $r1 = $r1, 1
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %2 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %2, <4 x i16> <i16 42, i16 0, i16 0, i16 0>, i32 2)
  %4 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %5 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %4, <4 x i16> zeroinitializer, i32 2)
  %6 = shufflevector <4 x i16> %3, <4 x i16> %5, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i16> %6
}

define <2 x i16> @avguhp_ri(<2 x i16> %0) {
; ALL-LABEL: avguhp_ri:
; ALL:       # %bb.0:
; ALL-NEXT:    avguhq $r0 = $r0, 42
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %2 = tail call <2 x i16> @llvm.kvx.avg.v2i16(<2 x i16> %0, <2 x i16> <i16 42, i16 0>, i32 2)
  ret <2 x i16> %2
}

define <4 x i16> @avguhq_ri(<4 x i16> %0) {
; ALL-LABEL: avguhq_ri:
; ALL:       # %bb.0:
; ALL-NEXT:    avguhq $r0 = $r0, 42
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %2 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %0, <4 x i16> <i16 42, i16 0, i16 0, i16 0>, i32 2)
  ret <4 x i16> %2
}

define <16 x i16> @avguhx_ri(<16 x i16> %0) {
; CV1-LABEL: avguhx_ri:
; CV1:       # %bb.0:
; CV1-NEXT:    avguhq $r0 = $r0, 42
; CV1-NEXT:    srlhqs $r1 = $r1, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    srlhqs $r2 = $r2, 1
; CV1-NEXT:    srlhqs $r3 = $r3, 1
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avguhx_ri:
; CV2:       # %bb.0:
; CV2-NEXT:    avguhq $r0 = $r0, 42
; CV2-NEXT:    srlhqs $r1 = $r1, 1
; CV2-NEXT:    srlhqs $r2 = $r2, 1
; CV2-NEXT:    srlhqs $r3 = $r3, 1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %2 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %2, <4 x i16> <i16 42, i16 0, i16 0, i16 0>, i32 2)
  %4 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %5 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %4, <4 x i16> zeroinitializer, i32 2)
  %6 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %6, <4 x i16> zeroinitializer, i32 2)
  %8 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %9 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %8, <4 x i16> zeroinitializer, i32 2)
  %10 = shufflevector <4 x i16> %3, <4 x i16> %5, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %11 = shufflevector <4 x i16> %7, <4 x i16> %9, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %12 = shufflevector <8 x i16> %10, <8 x i16> %11, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  ret <16 x i16> %12
}

define <8 x i16> @avgruho_ri(<8 x i16> %0) {
; ALL-LABEL: avgruho_ri:
; ALL:       # %bb.0:
; ALL-NEXT:    addhq.@ $r1 = $r1, 0x10001
; ALL-NEXT:    make $r2 = 0x100010001002b
; ALL-NEXT:    ;;
; ALL-NEXT:    addhq $r0 = $r0, $r2
; ALL-NEXT:    srlhqs $r1 = $r1, 1
; ALL-NEXT:    ;;
; ALL-NEXT:    srlhqs $r0 = $r0, 1
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %2 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %2, <4 x i16> <i16 42, i16 0, i16 0, i16 0>, i32 3)
  %4 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %5 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %4, <4 x i16> zeroinitializer, i32 3)
  %6 = shufflevector <4 x i16> %3, <4 x i16> %5, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i16> %6
}

define <2 x i16> @avgruhp_ri(<2 x i16> %0) {
; ALL-LABEL: avgruhp_ri:
; ALL:       # %bb.0:
; ALL-NEXT:    addhq $r0 = $r0, 0x1002b
; ALL-NEXT:    ;;
; ALL-NEXT:    srlhqs $r0 = $r0, 1
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %2 = tail call <2 x i16> @llvm.kvx.avg.v2i16(<2 x i16> %0, <2 x i16> <i16 42, i16 0>, i32 3)
  ret <2 x i16> %2
}

define <4 x i16> @avgruhq_ri(<4 x i16> %0) {
; ALL-LABEL: avgruhq_ri:
; ALL:       # %bb.0:
; ALL-NEXT:    make $r1 = 0x100010001002b
; ALL-NEXT:    ;;
; ALL-NEXT:    addhq $r0 = $r0, $r1
; ALL-NEXT:    ;;
; ALL-NEXT:    srlhqs $r0 = $r0, 1
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %2 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %0, <4 x i16> <i16 42, i16 0, i16 0, i16 0>, i32 3)
  ret <4 x i16> %2
}

define <16 x i16> @avgruhx_ri(<16 x i16> %0) {
; ALL-LABEL: avgruhx_ri:
; ALL:       # %bb.0:
; ALL-NEXT:    addhq.@ $r1 = $r1, 0x10001
; ALL-NEXT:    addhq.@ $r2 = $r2, 0x10001
; ALL-NEXT:    make $r4 = 0x100010001002b
; ALL-NEXT:    ;;
; ALL-NEXT:    addhq $r0 = $r0, $r4
; ALL-NEXT:    srlhqs $r1 = $r1, 1
; ALL-NEXT:    srlhqs $r2 = $r2, 1
; ALL-NEXT:    addhq.@ $r3 = $r3, 0x10001
; ALL-NEXT:    ;;
; ALL-NEXT:    srlhqs $r0 = $r0, 1
; ALL-NEXT:    srlhqs $r3 = $r3, 1
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %2 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %3 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %2, <4 x i16> <i16 42, i16 0, i16 0, i16 0>, i32 3)
  %4 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %5 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %4, <4 x i16> zeroinitializer, i32 3)
  %6 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %7 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %6, <4 x i16> zeroinitializer, i32 3)
  %8 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %9 = tail call <4 x i16> @llvm.kvx.avg.v4i16(<4 x i16> %8, <4 x i16> zeroinitializer, i32 3)
  %10 = shufflevector <4 x i16> %3, <4 x i16> %5, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %11 = shufflevector <4 x i16> %7, <4 x i16> %9, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %12 = shufflevector <8 x i16> %10, <8 x i16> %11, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  ret <16 x i16> %12
}

define <8 x i32> @avgruwo_ri(<8 x i32> %0) {
; ALL-LABEL: avgruwo_ri:
; ALL:       # %bb.0:
; ALL-NEXT:    addwp.@ $r1 = $r1, 1
; ALL-NEXT:    addwp.@ $r2 = $r2, 1
; ALL-NEXT:    make $r4 = 0x10000002b
; ALL-NEXT:    ;;
; ALL-NEXT:    addwp $r0 = $r0, $r4
; ALL-NEXT:    srlwps $r1 = $r1, 1
; ALL-NEXT:    srlwps $r2 = $r2, 1
; ALL-NEXT:    addwp.@ $r3 = $r3, 1
; ALL-NEXT:    ;;
; ALL-NEXT:    srlwps $r0 = $r0, 1
; ALL-NEXT:    srlwps $r3 = $r3, 1
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %2 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 0, i32 1>
  %3 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %2, <2 x i32> <i32 42, i32 0>, i32 3)
  %4 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 2, i32 3>
  %5 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %4, <2 x i32> zeroinitializer, i32 3)
  %6 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 4, i32 5>
  %7 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %6, <2 x i32> zeroinitializer, i32 3)
  %8 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 6, i32 7>
  %9 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %8, <2 x i32> zeroinitializer, i32 3)
  %10 = shufflevector <2 x i32> %3, <2 x i32> %5, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %11 = shufflevector <2 x i32> %7, <2 x i32> %9, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %12 = shufflevector <4 x i32> %10, <4 x i32> %11, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i32> %12
}

define <2 x i32> @avgruwp_ri(<2 x i32> %0) {
; ALL-LABEL: avgruwp_ri:
; ALL:       # %bb.0:
; ALL-NEXT:    make $r1 = 0x10000002b
; ALL-NEXT:    ;;
; ALL-NEXT:    addwp $r0 = $r0, $r1
; ALL-NEXT:    ;;
; ALL-NEXT:    srlwps $r0 = $r0, 1
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %2 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %0, <2 x i32> <i32 42, i32 0>, i32 3)
  ret <2 x i32> %2
}

define <4 x i32> @avgruwq_ri(<4 x i32> %0) {
; ALL-LABEL: avgruwq_ri:
; ALL:       # %bb.0:
; ALL-NEXT:    addwp.@ $r1 = $r1, 1
; ALL-NEXT:    make $r2 = 0x10000002b
; ALL-NEXT:    ;;
; ALL-NEXT:    addwp $r0 = $r0, $r2
; ALL-NEXT:    srlwps $r1 = $r1, 1
; ALL-NEXT:    ;;
; ALL-NEXT:    srlwps $r0 = $r0, 1
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %2 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 0, i32 1>
  %3 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %2, <2 x i32> <i32 42, i32 0>, i32 3)
  %4 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 2, i32 3>
  %5 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %4, <2 x i32> zeroinitializer, i32 3)
  %6 = shufflevector <2 x i32> %3, <2 x i32> %5, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  ret <4 x i32> %6
}

define <8 x i32> @avgrwo_ri(<8 x i32> %0) {
; CV1-LABEL: avgrwo_ri:
; CV1:       # %bb.0:
; CV1-NEXT:    avgrwp $r0 = $r0, 42
; CV1-NEXT:    avgrwp $r1 = $r1, 0
; CV1-NEXT:    ;;
; CV1-NEXT:    avgrwp $r2 = $r2, 0
; CV1-NEXT:    avgrwp $r3 = $r3, 0
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgrwo_ri:
; CV2:       # %bb.0:
; CV2-NEXT:    avgrwp $r0 = $r0, 42
; CV2-NEXT:    avgrwp $r1 = $r1, 0
; CV2-NEXT:    avgrwp $r2 = $r2, 0
; CV2-NEXT:    avgrwp $r3 = $r3, 0
; CV2-NEXT:    ;;
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %2 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 0, i32 1>
  %3 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %2, <2 x i32> <i32 42, i32 0>, i32 1)
  %4 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 2, i32 3>
  %5 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %4, <2 x i32> zeroinitializer, i32 1)
  %6 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 4, i32 5>
  %7 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %6, <2 x i32> zeroinitializer, i32 1)
  %8 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 6, i32 7>
  %9 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %8, <2 x i32> zeroinitializer, i32 1)
  %10 = shufflevector <2 x i32> %3, <2 x i32> %5, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %11 = shufflevector <2 x i32> %7, <2 x i32> %9, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %12 = shufflevector <4 x i32> %10, <4 x i32> %11, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i32> %12
}

define <2 x i32> @avgrwp_ri(<2 x i32> %0) {
; ALL-LABEL: avgrwp_ri:
; ALL:       # %bb.0:
; ALL-NEXT:    avgrwp $r0 = $r0, 42
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %2 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %0, <2 x i32> <i32 42, i32 0>, i32 1)
  ret <2 x i32> %2
}

define <4 x i32> @avgrwq_ri(<4 x i32> %0) {
; ALL-LABEL: avgrwq_ri:
; ALL:       # %bb.0:
; ALL-NEXT:    avgrwp $r0 = $r0, 42
; ALL-NEXT:    avgrwp $r1 = $r1, 0
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %2 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 0, i32 1>
  %3 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %2, <2 x i32> <i32 42, i32 0>, i32 1)
  %4 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 2, i32 3>
  %5 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %4, <2 x i32> zeroinitializer, i32 1)
  %6 = shufflevector <2 x i32> %3, <2 x i32> %5, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  ret <4 x i32> %6
}

define <8 x i32> @avguwo_ri(<8 x i32> %0) {
; CV1-LABEL: avguwo_ri:
; CV1:       # %bb.0:
; CV1-NEXT:    avguwp $r0 = $r0, 42
; CV1-NEXT:    srlwps $r1 = $r1, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    srlwps $r2 = $r2, 1
; CV1-NEXT:    srlwps $r3 = $r3, 1
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avguwo_ri:
; CV2:       # %bb.0:
; CV2-NEXT:    avguwp $r0 = $r0, 42
; CV2-NEXT:    srlwps $r1 = $r1, 1
; CV2-NEXT:    srlwps $r2 = $r2, 1
; CV2-NEXT:    srlwps $r3 = $r3, 1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %2 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 0, i32 1>
  %3 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %2, <2 x i32> <i32 42, i32 0>, i32 2)
  %4 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 2, i32 3>
  %5 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %4, <2 x i32> zeroinitializer, i32 2)
  %6 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 4, i32 5>
  %7 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %6, <2 x i32> zeroinitializer, i32 2)
  %8 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 6, i32 7>
  %9 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %8, <2 x i32> zeroinitializer, i32 2)
  %10 = shufflevector <2 x i32> %3, <2 x i32> %5, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %11 = shufflevector <2 x i32> %7, <2 x i32> %9, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %12 = shufflevector <4 x i32> %10, <4 x i32> %11, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i32> %12
}

define <2 x i32> @avguwp_ri(<2 x i32> %0) {
; ALL-LABEL: avguwp_ri:
; ALL:       # %bb.0:
; ALL-NEXT:    avguwp $r0 = $r0, 42
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %2 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %0, <2 x i32> <i32 42, i32 0>, i32 2)
  ret <2 x i32> %2
}

define <4 x i32> @avguwq_ri(<4 x i32> %0) {
; ALL-LABEL: avguwq_ri:
; ALL:       # %bb.0:
; ALL-NEXT:    avguwp $r0 = $r0, 42
; ALL-NEXT:    srlwps $r1 = $r1, 1
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %2 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 0, i32 1>
  %3 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %2, <2 x i32> <i32 42, i32 0>, i32 2)
  %4 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 2, i32 3>
  %5 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %4, <2 x i32> zeroinitializer, i32 2)
  %6 = shufflevector <2 x i32> %3, <2 x i32> %5, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  ret <4 x i32> %6
}

define <8 x i32> @avgwo_ri(<8 x i32> %0) {
; CV1-LABEL: avgwo_ri:
; CV1:       # %bb.0:
; CV1-NEXT:    avgwp $r0 = $r0, 42
; CV1-NEXT:    srawps $r1 = $r1, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    srawps $r2 = $r2, 1
; CV1-NEXT:    srawps $r3 = $r3, 1
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: avgwo_ri:
; CV2:       # %bb.0:
; CV2-NEXT:    avgwp $r0 = $r0, 42
; CV2-NEXT:    srawps $r1 = $r1, 1
; CV2-NEXT:    srawps $r2 = $r2, 1
; CV2-NEXT:    srawps $r3 = $r3, 1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %2 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 0, i32 1>
  %3 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %2, <2 x i32> <i32 42, i32 0>, i32 0)
  %4 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 2, i32 3>
  %5 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %4, <2 x i32> zeroinitializer, i32 0)
  %6 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 4, i32 5>
  %7 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %6, <2 x i32> zeroinitializer, i32 0)
  %8 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 6, i32 7>
  %9 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %8, <2 x i32> zeroinitializer, i32 0)
  %10 = shufflevector <2 x i32> %3, <2 x i32> %5, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %11 = shufflevector <2 x i32> %7, <2 x i32> %9, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %12 = shufflevector <4 x i32> %10, <4 x i32> %11, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i32> %12
}

define <2 x i32> @avgwp_ri(<2 x i32> %0) {
; ALL-LABEL: avgwp_ri:
; ALL:       # %bb.0:
; ALL-NEXT:    avgwp $r0 = $r0, 42
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %2 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %0, <2 x i32> <i32 42, i32 0>, i32 0)
  ret <2 x i32> %2
}

define <4 x i32> @avgwq_ri(<4 x i32> %0) {
; ALL-LABEL: avgwq_ri:
; ALL:       # %bb.0:
; ALL-NEXT:    avgwp $r0 = $r0, 42
; ALL-NEXT:    srawps $r1 = $r1, 1
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %2 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 0, i32 1>
  %3 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %2, <2 x i32> <i32 42, i32 0>, i32 0)
  %4 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 2, i32 3>
  %5 = tail call <2 x i32> @llvm.kvx.avg.v2i32(<2 x i32> %4, <2 x i32> zeroinitializer, i32 0)
  %6 = shufflevector <2 x i32> %3, <2 x i32> %5, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  ret <4 x i32> %6
}

define i32 @avgw_ri(i32 %0) {
; ALL-LABEL: avgw_ri:
; ALL:       # %bb.0:
; ALL-NEXT:    avgw $r0 = $r0, 42
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %2 = tail call i32 @llvm.kvx.avg.i32(i32 %0, i32 42, i32 0)
  ret i32 %2
}

define i32 @avguw_ri(i32 %0) {
; ALL-LABEL: avguw_ri:
; ALL:       # %bb.0:
; ALL-NEXT:    avguw $r0 = $r0, 42
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %2 = tail call i32 @llvm.kvx.avg.i32(i32 %0, i32 42, i32 2)
  ret i32 %2
}

define i32 @avgrw_ri(i32 %0) {
; ALL-LABEL: avgrw_ri:
; ALL:       # %bb.0:
; ALL-NEXT:    avgw $r0 = $r0, 43
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %2 = tail call i32 @llvm.kvx.avg.i32(i32 %0, i32 42, i32 1)
  ret i32 %2
}

define i32 @avgruw_ri(i32 %0) {
; ALL-LABEL: avgruw_ri:
; ALL:       # %bb.0:
; ALL-NEXT:    addw $r0 = $r0, 43
; ALL-NEXT:    ;;
; ALL-NEXT:    srlw $r0 = $r0, 1
; ALL-NEXT:    ret
; ALL-NEXT:    ;;
  %2 = tail call i32 @llvm.kvx.avg.i32(i32 %0, i32 42, i32 3)
  ret i32 %2
}

