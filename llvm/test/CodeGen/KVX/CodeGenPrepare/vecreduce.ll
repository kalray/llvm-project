; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mcpu=kv3-1 -o - %s -O2 | FileCheck %s --check-prefixes=ALL,CV1
; RUN: llc -mcpu=kv3-2 -o - %s -O2 | FileCheck %s --check-prefixes=ALL,CV2
; RUN: clang -O2 -march=kv3-1 -c -o /dev/null %s
; RUN: clang -O2 -march=kv3-2 -c -o /dev/null %s

target triple = "kvx-kalray-cos"

define i1 @v16_vecreduce_and_cmp_ne_i8(ptr %p0, ptr %p1) {
; CV1-LABEL: v16_vecreduce_and_cmp_ne_i8:
; CV1:       # %bb.0:
; CV1-NEXT:    lq $r2r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lq $r0r1 = 0[$r1]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r3 = $r3, 0x8000400020001
; CV1-NEXT:    sbmm8 $r4 = $r3, 0x80004000200010
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r2 = $r2, 0x8000400020001
; CV1-NEXT:    sbmm8 $r5 = $r2, 0x80004000200010
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x8000400020001
; CV1-NEXT:    sbmm8 $r6 = $r1, 0x80004000200010
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x8000400020001
; CV1-NEXT:    compnhq.eq $r1 = $r3, $r1
; CV1-NEXT:    compnhq.eq $r3 = $r4, $r6
; CV1-NEXT:    sbmm8 $r7 = $r0, 0x80004000200010
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    compnhq.eq $r0 = $r2, $r0
; CV1-NEXT:    ord $r1 = $r3, $r1
; CV1-NEXT:    compnhq.eq $r2 = $r5, $r7
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    ord $r0 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    lnord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 9)
;
; CV2-LABEL: v16_vecreduce_and_cmp_ne_i8:
; CV2:       # %bb.0:
; CV2-NEXT:    lq $r2r3 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lq $r0r1 = 0[$r1]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    compnbo.eq $r0 = $r2, $r0
; CV2-NEXT:    compnbo.eq $r1 = $r3, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    lnord $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 5)
  %l1 = load <16 x i8>, ptr %p0
  %l2 = load <16 x i8>, ptr %p1
  %c = icmp eq <16 x i8> %l1, %l2
  %1 = bitcast <16 x i1> %c to i16
  %2 = icmp eq i16 %1, 0
  ret i1 %2
}

define i1 @v16_vecreduce_and_cmp_eq_i8(ptr %p0, ptr %p1) {
; CV1-LABEL: v16_vecreduce_and_cmp_eq_i8:
; CV1:       # %bb.0:
; CV1-NEXT:    lq $r2r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lq $r0r1 = 0[$r1]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r3 = $r3, 0x8000400020001
; CV1-NEXT:    sbmm8 $r4 = $r3, 0x80004000200010
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r2 = $r2, 0x8000400020001
; CV1-NEXT:    sbmm8 $r5 = $r2, 0x80004000200010
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x8000400020001
; CV1-NEXT:    sbmm8 $r6 = $r1, 0x80004000200010
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x8000400020001
; CV1-NEXT:    compnhq.ne $r1 = $r3, $r1
; CV1-NEXT:    compnhq.ne $r3 = $r4, $r6
; CV1-NEXT:    sbmm8 $r7 = $r0, 0x80004000200010
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    compnhq.ne $r0 = $r2, $r0
; CV1-NEXT:    ord $r1 = $r3, $r1
; CV1-NEXT:    compnhq.ne $r2 = $r5, $r7
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    ord $r0 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    lnord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 9)
;
; CV2-LABEL: v16_vecreduce_and_cmp_eq_i8:
; CV2:       # %bb.0:
; CV2-NEXT:    lq $r2r3 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lq $r0r1 = 0[$r1]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    compnbo.ne $r0 = $r2, $r0
; CV2-NEXT:    compnbo.ne $r1 = $r3, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    lnord $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 5)
  %l1 = load <16 x i8>, ptr %p0
  %l2 = load <16 x i8>, ptr %p1
  %c = icmp ne <16 x i8> %l1, %l2
  %1 = bitcast <16 x i1> %c to i16
  %2 = icmp eq i16 %1, 0
  ret i1 %2
}

define i1 @v16_vecreduce_and_cmp_ult_i8(ptr %p0, ptr %p1) {
; CV1-LABEL: v16_vecreduce_and_cmp_ult_i8:
; CV1:       # %bb.0:
; CV1-NEXT:    lq $r2r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lq $r0r1 = 0[$r1]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r3 = $r3, 0x8000400020001
; CV1-NEXT:    sbmm8 $r4 = $r3, 0x80004000200010
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r2 = $r2, 0x8000400020001
; CV1-NEXT:    sbmm8 $r5 = $r2, 0x80004000200010
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x8000400020001
; CV1-NEXT:    sbmm8 $r6 = $r1, 0x80004000200010
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x8000400020001
; CV1-NEXT:    compnhq.geu $r1 = $r3, $r1
; CV1-NEXT:    compnhq.geu $r3 = $r4, $r6
; CV1-NEXT:    sbmm8 $r7 = $r0, 0x80004000200010
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    compnhq.geu $r0 = $r2, $r0
; CV1-NEXT:    ord $r1 = $r3, $r1
; CV1-NEXT:    compnhq.geu $r2 = $r5, $r7
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    ord $r0 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    lnord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 9)
;
; CV2-LABEL: v16_vecreduce_and_cmp_ult_i8:
; CV2:       # %bb.0:
; CV2-NEXT:    lq $r2r3 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lq $r0r1 = 0[$r1]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    compnbo.geu $r0 = $r2, $r0
; CV2-NEXT:    compnbo.geu $r1 = $r3, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    lnord $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 5)
  %l1 = load <16 x i8>, ptr %p0
  %l2 = load <16 x i8>, ptr %p1
  %c = icmp uge <16 x i8> %l1, %l2
  %1 = bitcast <16 x i1> %c to i16
  %2 = icmp eq i16 %1, 0
  ret i1 %2
}

define i1 @v16_vecreduce_and_cmp_ule_i8(ptr %p0, ptr %p1) {
; CV1-LABEL: v16_vecreduce_and_cmp_ule_i8:
; CV1:       # %bb.0:
; CV1-NEXT:    lq $r2r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lq $r0r1 = 0[$r1]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r3 = $r3, 0x8000400020001
; CV1-NEXT:    sbmm8 $r4 = $r3, 0x80004000200010
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r2 = $r2, 0x8000400020001
; CV1-NEXT:    sbmm8 $r5 = $r2, 0x80004000200010
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x8000400020001
; CV1-NEXT:    sbmm8 $r6 = $r1, 0x80004000200010
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x8000400020001
; CV1-NEXT:    compnhq.gtu $r1 = $r3, $r1
; CV1-NEXT:    compnhq.gtu $r3 = $r4, $r6
; CV1-NEXT:    sbmm8 $r7 = $r0, 0x80004000200010
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    compnhq.gtu $r0 = $r2, $r0
; CV1-NEXT:    ord $r1 = $r3, $r1
; CV1-NEXT:    compnhq.gtu $r2 = $r5, $r7
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    ord $r0 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    lnord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 9)
;
; CV2-LABEL: v16_vecreduce_and_cmp_ule_i8:
; CV2:       # %bb.0:
; CV2-NEXT:    lq $r2r3 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lq $r0r1 = 0[$r1]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    compnbo.gtu $r0 = $r2, $r0
; CV2-NEXT:    compnbo.gtu $r1 = $r3, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    lnord $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 5)
  %l1 = load <16 x i8>, ptr %p0
  %l2 = load <16 x i8>, ptr %p1
  %c = icmp ugt <16 x i8> %l1, %l2
  %1 = bitcast <16 x i1> %c to i16
  %2 = icmp eq i16 %1, 0
  ret i1 %2
}

define i1 @v16_vecreduce_and_cmp_slt_i8(ptr %p0, ptr %p1) {
; CV1-LABEL: v16_vecreduce_and_cmp_slt_i8:
; CV1:       # %bb.0:
; CV1-NEXT:    lq $r2r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lq $r0r1 = 0[$r1]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sxlbhq $r3 = $r3
; CV1-NEXT:    sxmbhq $r4 = $r3
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sxlbhq $r2 = $r2
; CV1-NEXT:    sxmbhq $r5 = $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    sxmbhq $r6 = $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    compnhq.ge $r1 = $r3, $r1
; CV1-NEXT:    compnhq.ge $r3 = $r4, $r6
; CV1-NEXT:    sxmbhq $r7 = $r0
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    compnhq.ge $r0 = $r2, $r0
; CV1-NEXT:    ord $r1 = $r3, $r1
; CV1-NEXT:    compnhq.ge $r2 = $r5, $r7
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    ord $r0 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    lnord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 9)
;
; CV2-LABEL: v16_vecreduce_and_cmp_slt_i8:
; CV2:       # %bb.0:
; CV2-NEXT:    lq $r2r3 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lq $r0r1 = 0[$r1]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    compnbo.ge $r0 = $r2, $r0
; CV2-NEXT:    compnbo.ge $r1 = $r3, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    lnord $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 5)
  %l1 = load <16 x i8>, ptr %p0
  %l2 = load <16 x i8>, ptr %p1
  %c = icmp sge <16 x i8> %l1, %l2
  %1 = bitcast <16 x i1> %c to i16
  %2 = icmp eq i16 %1, 0
  ret i1 %2
}

define i1 @v16_vecreduce_and_cmp_sle_i8(ptr %p0, ptr %p1) {
; CV1-LABEL: v16_vecreduce_and_cmp_sle_i8:
; CV1:       # %bb.0:
; CV1-NEXT:    lq $r2r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lq $r0r1 = 0[$r1]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sxlbhq $r3 = $r3
; CV1-NEXT:    sxmbhq $r4 = $r3
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sxlbhq $r2 = $r2
; CV1-NEXT:    sxmbhq $r5 = $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    sxmbhq $r6 = $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    compnhq.gt $r1 = $r3, $r1
; CV1-NEXT:    compnhq.gt $r3 = $r4, $r6
; CV1-NEXT:    sxmbhq $r7 = $r0
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    compnhq.gt $r0 = $r2, $r0
; CV1-NEXT:    ord $r1 = $r3, $r1
; CV1-NEXT:    compnhq.gt $r2 = $r5, $r7
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    ord $r0 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    lnord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 9)
;
; CV2-LABEL: v16_vecreduce_and_cmp_sle_i8:
; CV2:       # %bb.0:
; CV2-NEXT:    lq $r2r3 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lq $r0r1 = 0[$r1]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    compnbo.gt $r0 = $r2, $r0
; CV2-NEXT:    compnbo.gt $r1 = $r3, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    lnord $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 5)
  %l1 = load <16 x i8>, ptr %p0
  %l2 = load <16 x i8>, ptr %p1
  %c = icmp sgt <16 x i8> %l1, %l2
  %1 = bitcast <16 x i1> %c to i16
  %2 = icmp eq i16 %1, 0
  ret i1 %2
}

define i1 @v16_vecreduce_or_cmp_ne_i8(ptr %p0, ptr %p1) {
; CV1-LABEL: v16_vecreduce_or_cmp_ne_i8:
; CV1:       # %bb.0:
; CV1-NEXT:    lq $r2r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lq $r0r1 = 0[$r1]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r3 = $r3, 0x8000400020001
; CV1-NEXT:    sbmm8 $r4 = $r3, 0x80004000200010
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r2 = $r2, 0x8000400020001
; CV1-NEXT:    sbmm8 $r5 = $r2, 0x80004000200010
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x8000400020001
; CV1-NEXT:    sbmm8 $r6 = $r1, 0x80004000200010
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x8000400020001
; CV1-NEXT:    compnhq.ne $r1 = $r3, $r1
; CV1-NEXT:    compnhq.ne $r3 = $r4, $r6
; CV1-NEXT:    sbmm8 $r7 = $r0, 0x80004000200010
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    compnhq.ne $r0 = $r2, $r0
; CV1-NEXT:    ord $r1 = $r3, $r1
; CV1-NEXT:    compnhq.ne $r2 = $r5, $r7
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    ord $r0 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    lord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 9)
;
; CV2-LABEL: v16_vecreduce_or_cmp_ne_i8:
; CV2:       # %bb.0:
; CV2-NEXT:    lq $r2r3 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lq $r0r1 = 0[$r1]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    compnbo.ne $r0 = $r2, $r0
; CV2-NEXT:    compnbo.ne $r1 = $r3, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    lord $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 5)
  %l1 = load <16 x i8>, ptr %p0
  %l2 = load <16 x i8>, ptr %p1
  %c = icmp ne <16 x i8> %l1, %l2
  %1 = bitcast <16 x i1> %c to i16
  %2 = icmp ne i16 %1, 0
  ret i1 %2
}

define i1 @v16_vecreduce_or_cmp_eq_i8(ptr %p0, ptr %p1) {
; CV1-LABEL: v16_vecreduce_or_cmp_eq_i8:
; CV1:       # %bb.0:
; CV1-NEXT:    lq $r2r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lq $r0r1 = 0[$r1]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r3 = $r3, 0x8000400020001
; CV1-NEXT:    sbmm8 $r4 = $r3, 0x80004000200010
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r2 = $r2, 0x8000400020001
; CV1-NEXT:    sbmm8 $r5 = $r2, 0x80004000200010
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x8000400020001
; CV1-NEXT:    sbmm8 $r6 = $r1, 0x80004000200010
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x8000400020001
; CV1-NEXT:    compnhq.eq $r1 = $r3, $r1
; CV1-NEXT:    compnhq.eq $r3 = $r4, $r6
; CV1-NEXT:    sbmm8 $r7 = $r0, 0x80004000200010
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    compnhq.eq $r0 = $r2, $r0
; CV1-NEXT:    ord $r1 = $r3, $r1
; CV1-NEXT:    compnhq.eq $r2 = $r5, $r7
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    ord $r0 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    lord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 9)
;
; CV2-LABEL: v16_vecreduce_or_cmp_eq_i8:
; CV2:       # %bb.0:
; CV2-NEXT:    lq $r2r3 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lq $r0r1 = 0[$r1]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    compnbo.eq $r0 = $r2, $r0
; CV2-NEXT:    compnbo.eq $r1 = $r3, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    lord $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 5)
  %l1 = load <16 x i8>, ptr %p0
  %l2 = load <16 x i8>, ptr %p1
  %c = icmp eq <16 x i8> %l1, %l2
  %1 = bitcast <16 x i1> %c to i16
  %2 = icmp ne i16 %1, 0
  ret i1 %2
}

define i1 @v16_vecreduce_or_cmp_ult_i8(ptr %p0, ptr %p1) {
; CV1-LABEL: v16_vecreduce_or_cmp_ult_i8:
; CV1:       # %bb.0:
; CV1-NEXT:    lq $r2r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lq $r0r1 = 0[$r1]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r3 = $r3, 0x8000400020001
; CV1-NEXT:    sbmm8 $r4 = $r3, 0x80004000200010
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r2 = $r2, 0x8000400020001
; CV1-NEXT:    sbmm8 $r5 = $r2, 0x80004000200010
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x8000400020001
; CV1-NEXT:    sbmm8 $r6 = $r1, 0x80004000200010
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x8000400020001
; CV1-NEXT:    compnhq.ltu $r1 = $r3, $r1
; CV1-NEXT:    compnhq.ltu $r3 = $r4, $r6
; CV1-NEXT:    sbmm8 $r7 = $r0, 0x80004000200010
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    compnhq.ltu $r0 = $r2, $r0
; CV1-NEXT:    ord $r1 = $r3, $r1
; CV1-NEXT:    compnhq.ltu $r2 = $r5, $r7
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    ord $r0 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    lord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 9)
;
; CV2-LABEL: v16_vecreduce_or_cmp_ult_i8:
; CV2:       # %bb.0:
; CV2-NEXT:    lq $r2r3 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lq $r0r1 = 0[$r1]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    compnbo.ltu $r0 = $r2, $r0
; CV2-NEXT:    compnbo.ltu $r1 = $r3, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    lord $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 5)
  %l1 = load <16 x i8>, ptr %p0
  %l2 = load <16 x i8>, ptr %p1
  %c = icmp ult <16 x i8> %l1, %l2
  %1 = bitcast <16 x i1> %c to i16
  %2 = icmp ne i16 %1, 0
  ret i1 %2
}

define i1 @v16_vecreduce_or_cmp_ule_i8(ptr %p0, ptr %p1) {
; CV1-LABEL: v16_vecreduce_or_cmp_ule_i8:
; CV1:       # %bb.0:
; CV1-NEXT:    lq $r2r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lq $r0r1 = 0[$r1]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r3 = $r3, 0x8000400020001
; CV1-NEXT:    sbmm8 $r4 = $r3, 0x80004000200010
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r2 = $r2, 0x8000400020001
; CV1-NEXT:    sbmm8 $r5 = $r2, 0x80004000200010
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x8000400020001
; CV1-NEXT:    sbmm8 $r6 = $r1, 0x80004000200010
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x8000400020001
; CV1-NEXT:    compnhq.leu $r1 = $r3, $r1
; CV1-NEXT:    compnhq.leu $r3 = $r4, $r6
; CV1-NEXT:    sbmm8 $r7 = $r0, 0x80004000200010
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    compnhq.leu $r0 = $r2, $r0
; CV1-NEXT:    ord $r1 = $r3, $r1
; CV1-NEXT:    compnhq.leu $r2 = $r5, $r7
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    ord $r0 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    lord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 9)
;
; CV2-LABEL: v16_vecreduce_or_cmp_ule_i8:
; CV2:       # %bb.0:
; CV2-NEXT:    lq $r2r3 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lq $r0r1 = 0[$r1]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    compnbo.leu $r0 = $r2, $r0
; CV2-NEXT:    compnbo.leu $r1 = $r3, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    lord $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 5)
  %l1 = load <16 x i8>, ptr %p0
  %l2 = load <16 x i8>, ptr %p1
  %c = icmp ule <16 x i8> %l1, %l2
  %1 = bitcast <16 x i1> %c to i16
  %2 = icmp ne i16 %1, 0
  ret i1 %2
}

define i1 @v16_vecreduce_or_cmp_slt_i8(ptr %p0, ptr %p1) {
; CV1-LABEL: v16_vecreduce_or_cmp_slt_i8:
; CV1:       # %bb.0:
; CV1-NEXT:    lq $r2r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lq $r0r1 = 0[$r1]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sxlbhq $r3 = $r3
; CV1-NEXT:    sxmbhq $r4 = $r3
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sxlbhq $r2 = $r2
; CV1-NEXT:    sxmbhq $r5 = $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    sxmbhq $r6 = $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    compnhq.lt $r1 = $r3, $r1
; CV1-NEXT:    compnhq.lt $r3 = $r4, $r6
; CV1-NEXT:    sxmbhq $r7 = $r0
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    compnhq.lt $r0 = $r2, $r0
; CV1-NEXT:    ord $r1 = $r3, $r1
; CV1-NEXT:    compnhq.lt $r2 = $r5, $r7
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    ord $r0 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    lord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 9)
;
; CV2-LABEL: v16_vecreduce_or_cmp_slt_i8:
; CV2:       # %bb.0:
; CV2-NEXT:    lq $r2r3 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lq $r0r1 = 0[$r1]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    compnbo.lt $r0 = $r2, $r0
; CV2-NEXT:    compnbo.lt $r1 = $r3, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    lord $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 5)
  %l1 = load <16 x i8>, ptr %p0
  %l2 = load <16 x i8>, ptr %p1
  %c = icmp slt <16 x i8> %l1, %l2
  %1 = bitcast <16 x i1> %c to i16
  %2 = icmp ne i16 %1, 0
  ret i1 %2
}

define i1 @v16_vecreduce_or_cmp_sle_i8(ptr %p0, ptr %p1) {
; CV1-LABEL: v16_vecreduce_or_cmp_sle_i8:
; CV1:       # %bb.0:
; CV1-NEXT:    lq $r2r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lq $r0r1 = 0[$r1]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sxlbhq $r3 = $r3
; CV1-NEXT:    sxmbhq $r4 = $r3
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sxlbhq $r2 = $r2
; CV1-NEXT:    sxmbhq $r5 = $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    sxmbhq $r6 = $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    compnhq.le $r1 = $r3, $r1
; CV1-NEXT:    compnhq.le $r3 = $r4, $r6
; CV1-NEXT:    sxmbhq $r7 = $r0
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    compnhq.le $r0 = $r2, $r0
; CV1-NEXT:    ord $r1 = $r3, $r1
; CV1-NEXT:    compnhq.le $r2 = $r5, $r7
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    ord $r0 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    lord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 9)
;
; CV2-LABEL: v16_vecreduce_or_cmp_sle_i8:
; CV2:       # %bb.0:
; CV2-NEXT:    lq $r2r3 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lq $r0r1 = 0[$r1]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    compnbo.le $r0 = $r2, $r0
; CV2-NEXT:    compnbo.le $r1 = $r3, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    lord $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 5)
  %l1 = load <16 x i8>, ptr %p0
  %l2 = load <16 x i8>, ptr %p1
  %c = icmp sle <16 x i8> %l1, %l2
  %1 = bitcast <16 x i1> %c to i16
  %2 = icmp ne i16 %1, 0
  ret i1 %2
}


define i1 @v4_vecreduce_or_cmp_sle_i16(ptr %p0, ptr %p1) {
; ALL-LABEL: v4_vecreduce_or_cmp_sle_i16:
; ALL:       # %bb.0:
; ALL-NEXT:    ld $r0 = 0[$r0]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    ld $r1 = 0[$r1]
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    compnhq.le $r0 = $r0, $r1
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    compd.ne $r0 = $r0, 0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 5)
  %l1 = load <4 x i16>, ptr %p0
  %l2 = load <4 x i16>, ptr %p1
  %c = icmp sle <4 x i16> %l1, %l2
  %1 = bitcast <4 x i1> %c to i4
  %2 = icmp ne i4 %1, 0
  ret i1 %2
}

define i1 @v2_vecreduce_or_cmp_sle_i16(ptr %p0, ptr %p1) {
; ALL-LABEL: v2_vecreduce_or_cmp_sle_i16:
; ALL:       # %bb.0:
; ALL-NEXT:    lwz $r0 = 0[$r0]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    lwz $r1 = 0[$r1]
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    compnhq.le $r0 = $r0, $r1
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    zxwd $r0 = $r0
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    compw.ne $r0 = $r0, 0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 6)
  %l1 = load <2 x i16>, ptr %p0
  %l2 = load <2 x i16>, ptr %p1
  %c = icmp sle <2 x i16> %l1, %l2
  %1 = bitcast <2 x i1> %c to i2
  %2 = icmp ne i2 %1, 0
  ret i1 %2
}

define i1 @v8_vecreduce_or_cmp_sle_i8(ptr %p0, ptr %p1) {
; CV1-LABEL: v8_vecreduce_or_cmp_sle_i8:
; CV1:       # %bb.0:
; CV1-NEXT:    ld $r0 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    ld $r1 = 0[$r1]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxmbhq $r2 = $r0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    sxmbhq $r3 = $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    compnhq.le $r0 = $r0, $r1
; CV1-NEXT:    compnhq.le $r1 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    lord $r0 = $r1, $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: v8_vecreduce_or_cmp_sle_i8:
; CV2:       # %bb.0:
; CV2-NEXT:    ld $r0 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    ld $r1 = 0[$r1]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    compnbo.le $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    compd.ne $r0 = $r0, 0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 5)
  %l1 = load <8 x i8>, ptr %p0
  %l2 = load <8 x i8>, ptr %p1
  %c = icmp sle <8 x i8> %l1, %l2
  %1 = bitcast <8 x i1> %c to i8
  %2 = icmp ne i8 %1, 0
  ret i1 %2
}

define i1 @v4_vecreduce_or_cmp_sle_i8(ptr %p0, ptr %p1) {
; CV1-LABEL: v4_vecreduce_or_cmp_sle_i8:
; CV1:       # %bb.0:
; CV1-NEXT:    lwz $r0 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lwz $r1 = 0[$r1]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    compnhq.le $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    compd.ne $r0 = $r0, 0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: v4_vecreduce_or_cmp_sle_i8:
; CV2:       # %bb.0:
; CV2-NEXT:    lwz $r0 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lwz $r1 = 0[$r1]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    compnbo.le $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    zxwd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    compw.ne $r0 = $r0, 0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 6)
  %l1 = load <4 x i8>, ptr %p0
  %l2 = load <4 x i8>, ptr %p1
  %c = icmp sle <4 x i8> %l1, %l2
  %1 = bitcast <4 x i1> %c to i4
  %2 = icmp ne i4 %1, 0
  ret i1 %2
}

define i1 @v2_vecreduce_or_cmp_sle_i8(ptr %p0, ptr %p1) {
; CV1-LABEL: v2_vecreduce_or_cmp_sle_i8:
; CV1:       # %bb.0:
; CV1-NEXT:    lhz $r0 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lhz $r1 = 0[$r1]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    compnhq.le $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    compw.ne $r0 = $r0, 0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: v2_vecreduce_or_cmp_sle_i8:
; CV2:       # %bb.0:
; CV2-NEXT:    lhz $r0 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lhz $r1 = 0[$r1]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    compnbo.le $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    compw.ne $r0 = $r0, 0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 6)
  %l1 = load <2 x i8>, ptr %p0
  %l2 = load <2 x i8>, ptr %p1
  %c = icmp sle <2 x i8> %l1, %l2
  %1 = bitcast <2 x i1> %c to i2
  %2 = icmp ne i2 %1, 0
  ret i1 %2
}

define i1 @v8_vecreduce_or_cmp_sle_i32(ptr %p0, ptr %p1) {
; ALL-LABEL: v8_vecreduce_or_cmp_sle_i32:
; ALL:       # %bb.0:
; ALL-NEXT:    lo $r4r5r6r7 = 0[$r0]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    lo $r0r1r2r3 = 0[$r1]
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    compnwp.le $r0 = $r4, $r0
; ALL-NEXT:    compnwp.le $r1 = $r5, $r1
; ALL-NEXT:    compnwp.le $r2 = $r6, $r2
; ALL-NEXT:    compnwp.le $r3 = $r7, $r3
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    ord $r0 = $r1, $r0
; ALL-NEXT:    ord $r2 = $r3, $r2
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    lord $r0 = $r0, $r2
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 6)
  %l1 = load <8 x i32>, ptr %p0
  %l2 = load <8 x i32>, ptr %p1
  %c = icmp sle <8 x i32> %l1, %l2
  %1 = bitcast <8 x i1> %c to i8
  %2 = icmp ne i8 %1, 0
  ret i1 %2
}

define i1 @v4_vecreduce_or_cmp_sle_i32(ptr %p0, ptr %p1) {
; ALL-LABEL: v4_vecreduce_or_cmp_sle_i32:
; ALL:       # %bb.0:
; ALL-NEXT:    lq $r2r3 = 0[$r0]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    lq $r0r1 = 0[$r1]
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    compnwp.le $r0 = $r2, $r0
; ALL-NEXT:    compnwp.le $r1 = $r3, $r1
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    lord $r0 = $r1, $r0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 5)
  %l1 = load <4 x i32>, ptr %p0
  %l2 = load <4 x i32>, ptr %p1
  %c = icmp sle <4 x i32> %l1, %l2
  %1 = bitcast <4 x i1> %c to i4
  %2 = icmp ne i4 %1, 0
  ret i1 %2
}

define i1 @v2_vecreduce_or_cmp_sle_i32(ptr %p0, ptr %p1) {
; ALL-LABEL: v2_vecreduce_or_cmp_sle_i32:
; ALL:       # %bb.0:
; ALL-NEXT:    ld $r0 = 0[$r0]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    ld $r1 = 0[$r1]
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    compnwp.le $r0 = $r0, $r1
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    compd.ne $r0 = $r0, 0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 5)
  %l1 = load <2 x i32>, ptr %p0
  %l2 = load <2 x i32>, ptr %p1
  %c = icmp sle <2 x i32> %l1, %l2
  %1 = bitcast <2 x i1> %c to i2
  %2 = icmp ne i2 %1, 0
  ret i1 %2
}

define i1 @v2_vecreduce_or_cmp_sle_i64(ptr %p0, ptr %p1) {
; ALL-LABEL: v2_vecreduce_or_cmp_sle_i64:
; ALL:       # %bb.0:
; ALL-NEXT:    lq $r2r3 = 0[$r0]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    lq $r0r1 = 0[$r1]
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    compd.le $r0 = $r2, $r0
; ALL-NEXT:    compd.le $r1 = $r3, $r1
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    orw $r0 = $r1, $r0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 5)
  %l1 = load <2 x i64>, ptr %p0
  %l2 = load <2 x i64>, ptr %p1
  %c = icmp sle <2 x i64> %l1, %l2
  %1 = bitcast <2 x i1> %c to i2
  %2 = icmp ne i2 %1, 0
  ret i1 %2
}

define i1 @v4_vecreduce_and_cmp_sle_i64(ptr %p0, ptr %p1) {
; ALL-LABEL: v4_vecreduce_and_cmp_sle_i64:
; ALL:       # %bb.0:
; ALL-NEXT:    lo $r4r5r6r7 = 0[$r0]
; ALL-NEXT:    ;; # (end cycle 0)
; ALL-NEXT:    lo $r0r1r2r3 = 0[$r1]
; ALL-NEXT:    ;; # (end cycle 1)
; ALL-NEXT:    compd.gt $r0 = $r4, $r0
; ALL-NEXT:    compd.gt $r1 = $r5, $r1
; ALL-NEXT:    compd.gt $r2 = $r6, $r2
; ALL-NEXT:    compd.gt $r3 = $r7, $r3
; ALL-NEXT:    ;; # (end cycle 4)
; ALL-NEXT:    orw $r0 = $r1, $r0
; ALL-NEXT:    orw $r2 = $r3, $r2
; ALL-NEXT:    ;; # (end cycle 5)
; ALL-NEXT:    orw $r0 = $r0, $r2
; ALL-NEXT:    ;; # (end cycle 6)
; ALL-NEXT:    compd.eq $r0 = $r0, 0
; ALL-NEXT:    ret
; ALL-NEXT:    ;; # (end cycle 7)
  %l1 = load <4 x i64>, ptr %p0
  %l2 = load <4 x i64>, ptr %p1
  %c = icmp sgt <4 x i64> %l1, %l2
  %1 = bitcast <4 x i1> %c to i4
  %2 = icmp eq i4 %1, 0
  ret i1 %2
}

define i32 @ctpop_to_add_reduce(<8 x i32> %v0, i32 %c) {
; CV1-LABEL: ctpop_to_add_reduce:
; CV1:       # %bb.0:
; CV1-NEXT:    make $r5 = 0
; CV1-NEXT:    make $r6 = 0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    compnwp.ne $r0 = $r0, $r6
; CV1-NEXT:    compnwp.ne $r1 = $r1, $r5
; CV1-NEXT:    compnwp.ne $r2 = $r2, $r6
; CV1-NEXT:    compnwp.ne $r3 = $r3, $r5
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    addwp $r0 = $r0, $r2
; CV1-NEXT:    addwp $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    addwp $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srad $r1 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    addw $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbfw $r0 = $r0, $r4
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: ctpop_to_add_reduce:
; CV2:       # %bb.0:
; CV2-NEXT:    make $r5 = 0
; CV2-NEXT:    make $r6 = 0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    compnwp.ne $r0 = $r0, $r6
; CV2-NEXT:    compnwp.ne $r1 = $r1, $r5
; CV2-NEXT:    compnwp.ne $r2 = $r2, $r6
; CV2-NEXT:    compnwp.ne $r3 = $r3, $r5
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    addwp $r0 = $r0, $r2
; CV2-NEXT:    addwp $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    addrwpd $r0 = $r0
; CV2-NEXT:    addrwpd $r1 = $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    addw $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbfw $r0 = $r0, $r4
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 5)
  %cmp = icmp ne <8 x i32> %v0, zeroinitializer
  %b = bitcast <8 x i1> %cmp to i8
  %pop = tail call i8 @llvm.ctpop.i8(i8 %b)
  %z = zext i8 %pop to i32
  %r = add i32 %c, %z
  ret i32 %r
}

declare i8 @llvm.ctpop.i8(i8) nounwind readnone
