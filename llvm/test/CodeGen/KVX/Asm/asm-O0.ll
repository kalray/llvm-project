; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -o - %s | FileCheck %s

; RUN: clang -c -o /dev/null %s

target triple = "kvx-kalray-cos"

define i64 @asm_clobber_single_none(<2 x i64> %v, i64 %A) {
; CHECK-LABEL: asm_clobber_single_none:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -64
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    sd 40[$r12] = $r2
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    sq 48[$r12] = $r0r1
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:     # (here cycle 3)
; CHECK-NEXT:    #APP
; CHECK-NEXT:    copyq $r4r5 = $r0, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    ld $r0 = 40[$r12]
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    sq 16[$r12] = $r4r5
; CHECK-NEXT:    addd $r12 = $r12, 64
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 5)
entry:
  %v.addr = alloca <2 x i64>, align 16
  %A.addr = alloca i64, align 8
  %out = alloca <2 x i64>, align 16
  store <2 x i64> %v, ptr %v.addr, align 16
  store i64 %A, ptr %A.addr, align 8
  %0 = load <2 x i64>, ptr %v.addr, align 16
  %vecext = extractelement <2 x i64> %0, i32 0
  %1 = load <2 x i64>, ptr %v.addr, align 16
  %vecext1 = extractelement <2 x i64> %1, i32 1
  %2 = call <2 x i64> asm sideeffect "copyq $0 = $1, $2", "=r,r,r,~{$r2}"(i64 %vecext, i64 %vecext1)
  store <2 x i64> %2, ptr %out, align 16
  %3 = load i64, ptr %A.addr, align 8
  ret i64 %3
}

define i64 @asm_clobber_single_single(i64 %A) {
; CHECK-LABEL: asm_clobber_single_single:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    copyd $r1 = $r0
; CHECK-NEXT:    addd $r12 = $r12, -32
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    sd 24[$r12] = $r1
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:     # (here cycle 2)
; CHECK-NEXT:    #APP
; CHECK-NEXT:    copyq $r2r3 = $r1, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    ld $r0 = 24[$r12]
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    sq 0[$r12] = $r2r3
; CHECK-NEXT:    addd $r12 = $r12, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 4)
entry:
  %A.addr = alloca i64, align 8
  %v2i64 = alloca <2 x i64>, align 16
  store i64 %A, ptr %A.addr, align 8
  %0 = load i64, ptr %A.addr, align 8
  %1 = call <2 x i64> asm sideeffect "copyq $0 = $1, $1", "=r,r,~{$r0}"(i64 %0)
  store <2 x i64> %1, ptr %v2i64, align 16
  %2 = load i64, ptr %A.addr, align 8
  ret i64 %2
}

define ptr @asm_clobber_single_pair(ptr %A) {
; CHECK-LABEL: asm_clobber_single_pair:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    copyd $r2 = $r0
; CHECK-NEXT:    addd $r12 = $r12, -32
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    sd 24[$r12] = $r2
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:     # (here cycle 2)
; CHECK-NEXT:    #APP
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    ld $r0 = 24[$r12]
; CHECK-NEXT:    addd $r12 = $r12, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
entry:
  %A.addr = alloca ptr, align 8
  store ptr %A, ptr %A.addr, align 8
  %0 = load ptr, ptr %A.addr, align 8
  call void asm sideeffect "", "r,~{$r0r1}"(ptr %0)
  %1 = load ptr, ptr %A.addr, align 8
  ret ptr %1
}

define ptr @asm_clobber_single_quad(ptr %A, ptr %B, ptr %C) {
; CHECK-LABEL: asm_clobber_single_quad:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    copyd $r4 = $r1
; CHECK-NEXT:    addd $r12 = $r12, -32
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    sd 24[$r12] = $r0
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    sd 16[$r12] = $r4
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    sd 8[$r12] = $r2
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:     # (here cycle 4)
; CHECK-NEXT:    #APP
; CHECK-NEXT:    copyd $r0 = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    ld $r0 = 8[$r12]
; CHECK-NEXT:    addd $r12 = $r12, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 5)
entry:
  %A.addr = alloca ptr, align 8
  %B.addr = alloca ptr, align 8
  %C.addr = alloca ptr, align 8
  store ptr %A, ptr %A.addr, align 8
  store ptr %B, ptr %B.addr, align 8
  store ptr %C, ptr %C.addr, align 8
  %0 = load ptr, ptr %B.addr, align 8
  call void asm sideeffect "copyd $$r0 = $0", "r,~{$r0r1r2r3}"(ptr %0)
  %1 = load ptr, ptr %C.addr, align 8
  ret ptr %1
}

define <2 x i64> @asm_clobber_double_single(<2 x i64> %a) {
; CHECK-LABEL: asm_clobber_double_single:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -32
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    sq 16[$r12] = $r0r1
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    lq $r2r3 = 16[$r12]
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:     # (here cycle 3)
; CHECK-NEXT:    #APP
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    lq $r0r1 = 16[$r12]
; CHECK-NEXT:    addd $r12 = $r12, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 4)
entry:
  %a.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, ptr %a.addr, align 16
  %0 = load <2 x i64>, ptr %a.addr, align 16
  call void asm sideeffect "", "r,~{$r1}"(<2 x i64> %0)
  %1 = load <2 x i64>, ptr %a.addr, align 16
  ret <2 x i64> %1
}

define <2 x i64> @asm_clobber_double_double(<2 x i64> %a) {
; CHECK-LABEL: asm_clobber_double_double:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -32
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    sq 16[$r12] = $r0r1
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:     # (here cycle 2)
; CHECK-NEXT:    #APP
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    lq $r0r1 = 16[$r12]
; CHECK-NEXT:    addd $r12 = $r12, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
entry:
  %a.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, ptr %a.addr, align 16
  call void asm sideeffect "", "~{$r0r1}"()
  %0 = load <2 x i64>, ptr %a.addr, align 16
  ret <2 x i64> %0
}

define <2 x i64> @asm_clobber_double_quad(<2 x i64> %a) {
; CHECK-LABEL: asm_clobber_double_quad:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -32
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    sq 16[$r12] = $r0r1
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:     # (here cycle 2)
; CHECK-NEXT:    #APP
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    lq $r0r1 = 16[$r12]
; CHECK-NEXT:    addd $r12 = $r12, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
entry:
  %a.addr = alloca <2 x i64>, align 16
  store <2 x i64> %a, ptr %a.addr, align 16
  call void asm sideeffect "", "~{$r0r1r2r3}"()
  %0 = load <2 x i64>, ptr %a.addr, align 16
  ret <2 x i64> %0
}

define float @asm_clobber_multiple_quad(float %a, <2 x i64> %b, <4 x i64> %c) {
; CHECK-LABEL: asm_clobber_multiple_quad:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -64
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    sd 56[$r12] = $r16
; CHECK-NEXT:    copyd $r7 = $r6
; CHECK-NEXT:    copyd $r9 = $r2
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    sw 52[$r12] = $r0
; CHECK-NEXT:    copyd $r6 = $r5
; CHECK-NEXT:    copyd $r8 = $r1
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    sq 32[$r12] = $r8r9
; CHECK-NEXT:    copyd $r4 = $r3
; CHECK-NEXT:    copyd $r5 = $r4
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    so 0[$r12] = $r4r5r6r7
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:     # (here cycle 5)
; CHECK-NEXT:    #APP
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    lq $r0r1 = 32[$r12]
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    ld $r2 = 0[$r12]
; CHECK-NEXT:    ;; # (end cycle 7)
; CHECK-NEXT:    addd $r0 = $r0, $r1
; CHECK-NEXT:    ;; # (end cycle 9)
; CHECK-NEXT:    addd $r0 = $r0, $r2
; CHECK-NEXT:    call __floatdisf
; CHECK-NEXT:    ;; # (end cycle 10)
; CHECK-NEXT:    lwz $r1 = 52[$r12]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    faddw $r0 = $r1, $r0
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    sw 52[$r12] = $r0
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    ld $r16 = 56[$r12]
; CHECK-NEXT:    ;; # (end cycle 7)
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 64
; CHECK-NEXT:    ;; # (end cycle 12)
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %a.addr = alloca float, align 4
  %b.addr = alloca <2 x i64>, align 16
  %c.addr = alloca <4 x i64>, align 32
  store float %a, ptr %a.addr, align 4
  store <2 x i64> %b, ptr %b.addr, align 16
  store <4 x i64> %c, ptr %c.addr, align 32
  call void asm sideeffect "", "~{$r0r1r2r3}"()
  %0 = load <2 x i64>, ptr %b.addr, align 16
  %vecext = extractelement <2 x i64> %0, i32 0
  %1 = load <2 x i64>, ptr %b.addr, align 16
  %vecext1 = extractelement <2 x i64> %1, i32 1
  %add = add nsw i64 %vecext, %vecext1
  %2 = load <4 x i64>, ptr %c.addr, align 32
  %vecext2 = extractelement <4 x i64> %2, i32 0
  %add3 = add nsw i64 %add, %vecext2
  %conv = sitofp i64 %add3 to float
  %3 = load float, ptr %a.addr, align 4
  %add4 = fadd float %3, %conv
  store float %add4, ptr %a.addr, align 4
  %4 = load float, ptr %a.addr, align 4
  ret float %4
}

define void @asm_clobber_quad_single(<4 x i64> %a) {
; CHECK-LABEL: asm_clobber_quad_single:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -32
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    so 0[$r12] = $r0r1r2r3
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:     # (here cycle 2)
; CHECK-NEXT:    #APP
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    addd $r12 = $r12, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
entry:
  %a.addr = alloca <4 x i64>, align 32
  store <4 x i64> %a, ptr %a.addr, align 32
  call void asm sideeffect "", "~{$r0}"()
  ret void
}

define <4 x i64> @asm_clobber_quad_double(<4 x i64> %a) {
; CHECK-LABEL: asm_clobber_quad_double:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -32
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    so 0[$r12] = $r0r1r2r3
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:     # (here cycle 2)
; CHECK-NEXT:    #APP
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    lo $r0r1r2r3 = 0[$r12]
; CHECK-NEXT:    addd $r12 = $r12, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
entry:
  %a.addr = alloca <4 x i64>, align 32
  store <4 x i64> %a, ptr %a.addr, align 32
  %0 = load <4 x i64>, ptr %a.addr, align 32
  call void asm sideeffect "", "r"(<4 x i64> %0)
  %1 = load <4 x i64>, ptr %a.addr, align 32
  ret <4 x i64> %1
}

define <4 x i64> @asm_clobber_quad_quad(<4 x i64> %a) {
; CHECK-LABEL: asm_clobber_quad_quad:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -32
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    so 0[$r12] = $r0r1r2r3
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:     # (here cycle 2)
; CHECK-NEXT:    #APP
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    lo $r0r1r2r3 = 0[$r12]
; CHECK-NEXT:    addd $r12 = $r12, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
entry:
  %a.addr = alloca <4 x i64>, align 32
  store <4 x i64> %a, ptr %a.addr, align 32
  call void asm sideeffect "", "~{$r0r1r2r3}"()
  %0 = load <4 x i64>, ptr %a.addr, align 32
  ret <4 x i64> %0
}

define <4 x i64> @asm_clobber_quad_quad_use(<4 x i64> %a) {
; CHECK-LABEL: asm_clobber_quad_quad_use:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -32
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    so 0[$r12] = $r0r1r2r3
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:     # (here cycle 2)
; CHECK-NEXT:    #APP
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    lo $r0r1r2r3 = 0[$r12]
; CHECK-NEXT:    addd $r12 = $r12, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 3)
entry:
  %a.addr = alloca <4 x i64>, align 32
  store <4 x i64> %a, ptr %a.addr, align 32
  call void asm sideeffect "", "~{$r0},~{$r2r3}"()
  %0 = load <4 x i64>, ptr %a.addr, align 32
  ret <4 x i64> %0
}

define i64 @local_regs(i32 %a, i64 %b, i64 %c, i64 %d, i64 %e, i64 %f, i64 %g) {
; CHECK-LABEL: local_regs:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    copyd $r7 = $r1
; CHECK-NEXT:    zxwd $r8 = $r0
; CHECK-NEXT:    addd $r12 = $r12, -128
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    sw 124[$r12] = $r0
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    sd 112[$r12] = $r7
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    sd 104[$r12] = $r2
; CHECK-NEXT:    ;; # (end cycle 3)
; CHECK-NEXT:    sd 96[$r12] = $r3
; CHECK-NEXT:    ;; # (end cycle 4)
; CHECK-NEXT:    sd 88[$r12] = $r4
; CHECK-NEXT:    ;; # (end cycle 5)
; CHECK-NEXT:    sd 80[$r12] = $r5
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    sd 72[$r12] = $r6
; CHECK-NEXT:    ;; # (end cycle 7)
; CHECK-NEXT:    sd 64[$r12] = $r7
; CHECK-NEXT:    ;; # (end cycle 8)
; CHECK-NEXT:    sd 56[$r12] = $r2
; CHECK-NEXT:    ;; # (end cycle 9)
; CHECK-NEXT:    sd 48[$r12] = $r3
; CHECK-NEXT:    ;; # (end cycle 10)
; CHECK-NEXT:    sd 40[$r12] = $r4
; CHECK-NEXT:    ;; # (end cycle 11)
; CHECK-NEXT:    sd 32[$r12] = $r5
; CHECK-NEXT:    ;; # (end cycle 12)
; CHECK-NEXT:    sd 24[$r12] = $r6
; CHECK-NEXT:    copyd $r1 = $r2
; CHECK-NEXT:    copyd $r2 = $r3
; CHECK-NEXT:    copyd $r3 = $r4
; CHECK-NEXT:    ;; # (end cycle 13)
; CHECK-NEXT:    copyd $r0 = $r7
; CHECK-NEXT:    copyd $r4 = $r5
; CHECK-NEXT:    copyd $r5 = $r6
; CHECK-NEXT:    ;; # (end cycle 14)
; CHECK-NEXT:     # (here cycle 15)
; CHECK-NEXT:    #APP
; CHECK-NEXT:    scall $r8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    sd 64[$r12] = $r0
; CHECK-NEXT:    addd $r12 = $r12, 128
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 16)
entry:
  %a.addr = alloca i32, align 4
  %b.addr = alloca i64, align 8
  %c.addr = alloca i64, align 8
  %d.addr = alloca i64, align 8
  %e.addr = alloca i64, align 8
  %f.addr = alloca i64, align 8
  %g.addr = alloca i64, align 8
  %arg0 = alloca i64, align 8
  %arg1 = alloca i64, align 8
  %arg2 = alloca i64, align 8
  %arg3 = alloca i64, align 8
  %arg4 = alloca i64, align 8
  %arg5 = alloca i64, align 8
  store i32 %a, ptr %a.addr, align 4
  store i64 %b, ptr %b.addr, align 8
  store i64 %c, ptr %c.addr, align 8
  store i64 %d, ptr %d.addr, align 8
  store i64 %e, ptr %e.addr, align 8
  store i64 %f, ptr %f.addr, align 8
  store i64 %g, ptr %g.addr, align 8
  %0 = load i64, ptr %b.addr, align 8
  store i64 %0, ptr %arg0, align 8
  %1 = load i64, ptr %c.addr, align 8
  store i64 %1, ptr %arg1, align 8
  %2 = load i64, ptr %d.addr, align 8
  store i64 %2, ptr %arg2, align 8
  %3 = load i64, ptr %e.addr, align 8
  store i64 %3, ptr %arg3, align 8
  %4 = load i64, ptr %f.addr, align 8
  store i64 %4, ptr %arg4, align 8
  %5 = load i64, ptr %g.addr, align 8
  store i64 %5, ptr %arg5, align 8
  %6 = load i64, ptr %arg0, align 8
  %7 = load i64, ptr %arg1, align 8
  %8 = load i64, ptr %arg2, align 8
  %9 = load i64, ptr %arg3, align 8
  %10 = load i64, ptr %arg4, align 8
  %11 = load i64, ptr %arg5, align 8
  %12 = load i32, ptr %a.addr, align 4
  %13 = call i64 asm sideeffect "scall $6\0A\09;;", "={$r0},{$r1},{$r2},{$r3},{$r4},{$r5},ir,0,~{memory}"(i64 %7, i64 %8, i64 %9, i64 %10, i64 %11, i32 %12, i64 %6)
  store i64 %13, ptr %arg0, align 8
  %14 = load i64, ptr %arg0, align 8
  ret i64 %14
}
