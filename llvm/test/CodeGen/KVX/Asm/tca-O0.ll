; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -O0 -o - %s | FileCheck %s
; RUN: clang -c -o /dev/null %s

target triple = "kvx-kalray-cos"

define void @asm_tca(i8* %v, i64 %A) {
; CHECK-LABEL: asm_tca:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -128
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r2 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r1 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 120[$r12] = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 112[$r12] = $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r3 = $r2, 1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 104[$r12] = $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #APP
; CHECK-NEXT:    xlo.u $a0 = $r2[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xlo.us $a1 = $r3[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    xso 64[$r12] = $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xso 32[$r12] = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r0 = 120[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #APP
; CHECK-NEXT:    xcopyo $a0 = $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xso 0[$r0] = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    xso 0[$r12] = $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r12 = $r12, 128
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %v.addr = alloca i8*, align 8
  %A.addr = alloca i64, align 8
  %B = alloca i64, align 8
  %out1 = alloca <256 x i1>, align 32
  %out2 = alloca <256 x i1>, align 32
  %out3 = alloca <256 x i1>, align 32
  store i8* %v, i8** %v.addr, align 8
  store i64 %A, i64* %A.addr, align 8
  %0 = load i64, i64* %A.addr, align 8
  %add = add nsw i64 %0, 1
  store i64 %add, i64* %B, align 8
  %1 = load i8*, i8** %v.addr, align 8
  %2 = load i64, i64* %A.addr, align 8
  %3 = load i64, i64* %B, align 8
  %4 = call { <256 x i1>, <256 x i1> } asm sideeffect "xlo.u $0 = $3[$2]\0A\09;;\0A\09xlo.us $1 = $4[$2]\0A\09;;", "=x,=x,r,r,r,~{$r0}"(i8* %1, i64 %2, i64 %3)
  %asmresult = extractvalue { <256 x i1>, <256 x i1> } %4, 0
  %asmresult1 = extractvalue { <256 x i1>, <256 x i1> } %4, 1
  store <256 x i1> %asmresult, <256 x i1>* %out1, align 32
  store <256 x i1> %asmresult1, <256 x i1>* %out2, align 32
  %5 = load <256 x i1>, <256 x i1>* %out1, align 32
  %6 = load <256 x i1>, <256 x i1>* %out2, align 32
  %7 = load i8*, i8** %v.addr, align 8
  %8 = call <256 x i1> asm sideeffect "xcopyo $0 = $1\0A\09;;\0A\09xso 0[$3] = $2", "=x,x,x,r"(<256 x i1> %5, <256 x i1> %6, i8* %7)
  store <256 x i1> %8, <256 x i1>* %out3, align 32
  ret void
}

define void @asm_clobber_vec_vec(i64 %A) {
; CHECK-LABEL: asm_clobber_vec_vec:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -128
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 120[$r12] = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xlo.u $a1 = 32[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #APP
; CHECK-NEXT:    xcopyo $a1 = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    xso 0[$r12] = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xlo.u $a0 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xso 64[$r12] = $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r12 = $r12, 128
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %A.addr = alloca i64, align 8
  %v4i64 = alloca <256 x i1>, align 32
  %tcav4i64 = alloca <256 x i1>, align 32
  store i64 %A, i64* %A.addr, align 8
  %0 = load <256 x i1>, <256 x i1>* %tcav4i64, align 32
  %1 = call <256 x i1> asm sideeffect "xcopyo $0 = $1", "=x,x,~{$a0}"(<256 x i1> %0)
  store <256 x i1> %1, <256 x i1>* %v4i64, align 32
  ret void
}

define void @asm_clobber_vec_block(i64 %A) {
; CHECK-LABEL: asm_clobber_vec_block:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -128
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 120[$r12] = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xlo.u $a1 = 32[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #APP
; CHECK-NEXT:    xcopyo $a1 = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    xso 0[$r12] = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xlo.u $a0 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xso 64[$r12] = $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r12 = $r12, 128
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %A.addr = alloca i64, align 8
  %v4i64 = alloca <256 x i1>, align 32
  %tcav4i64 = alloca <256 x i1>, align 32
  store i64 %A, i64* %A.addr, align 8
  %0 = load <256 x i1>, <256 x i1>* %tcav4i64, align 32
  %1 = call <256 x i1> asm sideeffect "xcopyo $0 = $1", "=x,x,~{$a0.hi}"(<256 x i1> %0)
  store <256 x i1> %1, <256 x i1>* %v4i64, align 32
  ret void
}

define void @asm_clobber_wide_vec(<256 x i1>* %a) {
; CHECK-LABEL: asm_clobber_wide_vec:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 24[$r12] = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xlo.u $a2 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #APP
; CHECK-NEXT:    xcopyo $a2 = $a2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    addd $r12 = $r12, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %a.addr = alloca <256 x i1>*, align 8
  store <256 x i1>* %a, <256 x i1>** %a.addr, align 8
  %0 = load <256 x i1>*, <256 x i1>** %a.addr, align 8
  %arrayidx = getelementptr inbounds <256 x i1>, <256 x i1>* %0, i64 0
  %1 = load <256 x i1>, <256 x i1>* %arrayidx, align 32
  call void asm sideeffect "xcopyo $0 = $0", "x,~{$r0r1r2r3},~{$a0a1}"(<256 x i1> %1)
  ret void
}

define void @asm_clobber_multiple_quad(<256 x i1>* %c, <256 x i1>* %b) {
; CHECK-LABEL: asm_clobber_multiple_quad:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -96
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 64[$r12] = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 72[$r12] = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 88[$r12] = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 80[$r12] = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xlo.u $a4 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #APP
; CHECK-NEXT:    xcopyo $a4 = $a5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xcopyo $a5 = $a4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    xso 32[$r12] = $a5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xso 0[$r12] = $a4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xlo.u $a1 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xlo.u $a0 = 32[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r1 = 64[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r0 = 72[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xso 0[$r0] = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xso 0[$r1] = $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r12 = $r12, 96
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %c.addr = alloca <256 x i1>*, align 8
  %b.addr = alloca <256 x i1>*, align 8
  store <256 x i1>* %c, <256 x i1>** %c.addr, align 8
  store <256 x i1>* %b, <256 x i1>** %b.addr, align 8
  %0 = load <256 x i1>*, <256 x i1>** %c.addr, align 8
  %arrayidx = getelementptr inbounds <256 x i1>, <256 x i1>* %0, i64 0
  %1 = load <256 x i1>*, <256 x i1>** %b.addr, align 8
  %arrayidx1 = getelementptr inbounds <256 x i1>, <256 x i1>* %1, i64 0
  %2 = load <256 x i1>*, <256 x i1>** %c.addr, align 8
  %arrayidx2 = getelementptr inbounds <256 x i1>, <256 x i1>* %2, i64 0
  %3 = load <256 x i1>, <256 x i1>* %arrayidx2, align 32
  %4 = call { <256 x i1>, <256 x i1> } asm sideeffect "xcopyo $0 = $1\0A\09;;\0A\09xcopyo $1 = $0", "=x,=x,x,~{$r0r1r2r3},~{$a0a1a2a3}"(<256 x i1> %3)
  %asmresult = extractvalue { <256 x i1>, <256 x i1> } %4, 0
  %asmresult3 = extractvalue { <256 x i1>, <256 x i1> } %4, 1
  store <256 x i1> %asmresult, <256 x i1>* %arrayidx, align 32
  store <256 x i1> %asmresult3, <256 x i1>* %arrayidx1, align 32
  ret void
}

define <256 x i1>* @asm_clobber_quad_matrix(<256 x i1>* %a) {
; CHECK-LABEL: asm_clobber_quad_matrix:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 24[$r12] = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xlo.u $a4 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #APP
; CHECK-NEXT:    xso 0[$r3] = $a4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    ld $r0 = 24[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r12 = $r12, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %a.addr = alloca <256 x i1>*, align 8
  store <256 x i1>* %a, <256 x i1>** %a.addr, align 8
  %0 = load <256 x i1>*, <256 x i1>** %a.addr, align 8
  %arrayidx = getelementptr inbounds <256 x i1>, <256 x i1>* %0, i64 0
  %1 = load <256 x i1>, <256 x i1>* %arrayidx, align 32
  call void asm sideeffect "xso 0[$$r3] = $0", "x,~{$r0r1r2r3},~{$a0a1a2a3}"(<256 x i1> %1)
  %2 = load <256 x i1>*, <256 x i1>** %a.addr, align 8
  ret <256 x i1>* %2
}

define void @use_wide_reg(<512 x i1>* %w, <256 x i1>* %v) {
; CHECK-LABEL: use_wide_reg:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -192
; CHECK-NEXT:    ;;
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 184[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 176[$r12] = $r14
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r14 = $r12, 176
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 168[$r12] = $r31
; CHECK-NEXT:    ;;
; CHECK-NEXT:    andd $r31 = $r12, -64
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 144[$r31] = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 160[$r31] = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 152[$r31] = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xlo.u $a0 = 32[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xlo.u $a4 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    # kill: def $a4 killed $a4 def $w2
; CHECK-NEXT:    xcopyo $a5 = $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xlo.u $a6 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #APP
; CHECK-NEXT:    xmma484bw $a4a5 = $a4a5, $a6, $a6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    xso 64[$r31] = $a4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xso 96[$r31] = $a5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xlo.u $a0 = 64[$r31]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xlo.u $a1 = 96[$r31]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r0 = 144[$r31]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xcopyo $a2 = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xso 32[$r0] = $a2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    # kill: def $a0 killed $a0 killed $w0
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r12 = $r14, -176
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r31 = 168[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r14 = 176[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 184[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r12 = $r12, 192
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %w.addr = alloca <512 x i1>*, align 8
  %v.addr = alloca <256 x i1>*, align 8
  store <512 x i1>* %w, <512 x i1>** %w.addr, align 8
  store <256 x i1>* %v, <256 x i1>** %v.addr, align 8
  %0 = load <512 x i1>*, <512 x i1>** %w.addr, align 8
  %arrayidx = getelementptr inbounds <512 x i1>, <512 x i1>* %0, i64 0
  %1 = load <512 x i1>, <512 x i1>* %arrayidx, align 32
  %2 = load <256 x i1>*, <256 x i1>** %v.addr, align 8
  %arrayidx1 = getelementptr inbounds <256 x i1>, <256 x i1>* %2, i64 0
  %3 = load <256 x i1>, <256 x i1>* %arrayidx1, align 32
  %4 = call <512 x i1> asm sideeffect "xmma484bw $0 = $0, $1, $1", "=x,x,0,~{$r0r1r2r3},~{$a0a1a2a3}"(<256 x i1> %3, <512 x i1> %1)
  store <512 x i1> %4, <512 x i1>* %arrayidx, align 32
  ret void
}

define void @use_matrix_reg(<1024 x i1>* %x) {
; CHECK-LABEL: use_matrix_reg:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -384
; CHECK-NEXT:    ;;
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 376[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 368[$r12] = $r14
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r14 = $r12, 368
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 360[$r12] = $r31
; CHECK-NEXT:    ;;
; CHECK-NEXT:    andd $r31 = $r12, -128
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 344[$r31] = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 352[$r31] = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xlo.u $a2 = 96[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xlo.u $a0 = 64[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    # kill: def $a0 killed $a0 def $w0
; CHECK-NEXT:    xcopyo $a1 = $a2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xlo.u $a2 = 32[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xlo.u $a4 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    # kill: def $a4 killed $a4 def $w2
; CHECK-NEXT:    xcopyo $a5 = $a2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    # kill: def $w2 killed $w2 def $x1
; CHECK-NEXT:    xcopyo $a6 = $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xcopyo $a7 = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #APP
; CHECK-NEXT:    xmt44d $a4a5a6a7 = $a4a5a6a7
; CHECK-NEXT:    ;;
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    xso 128[$r31] = $a4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xso 160[$r31] = $a5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xso 192[$r31] = $a6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xso 224[$r31] = $a7
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xlo.u $a0 = 128[$r31]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xlo.u $a1 = 160[$r31]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xlo.u $a2 = 192[$r31]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xlo.u $a3 = 224[$r31]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r0 = 344[$r31]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xcopyo $a4 = $a2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xcopyo $a5 = $a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xcopyo $a6 = $a5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xso 96[$r0] = $a6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    # kill: def $a4 killed $a4 killed $w2
; CHECK-NEXT:    xso 64[$r0] = $a4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    # kill: def $w0 killed $w0 killed $x0
; CHECK-NEXT:    xcopyo $a2 = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xso 32[$r0] = $a2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    # kill: def $a0 killed $a0 killed $w0
; CHECK-NEXT:    xso 0[$r0] = $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r12 = $r14, -368
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r31 = 360[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r14 = 368[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 376[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r12 = $r12, 384
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %x.addr = alloca <1024 x i1>*, align 8
  store <1024 x i1>* %x, <1024 x i1>** %x.addr, align 8
  %0 = load <1024 x i1>*, <1024 x i1>** %x.addr, align 8
  %arrayidx = getelementptr inbounds <1024 x i1>, <1024 x i1>* %0, i64 0
  %1 = load <1024 x i1>, <1024 x i1>* %arrayidx, align 128
  %2 = call <1024 x i1> asm sideeffect "xmt44d $0 = $0", "=x,0,~{$r0r1r2r3},~{$a0a1a2a3}"(<1024 x i1> %1)
  store <1024 x i1> %2, <1024 x i1>* %arrayidx, align 128
  ret void
}
