; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -O3 < %s | FileCheck %s
; XFAIL: *
target triple = "kvx-kalray-cos"

define <16 x i8> @test_movetobvhi_v8i8(<8 x i8> %a, <8 x i8> %b) {
; CHECK-LABEL: test_movetobvhi_v8i8:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    movetq $a0_hi = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %0 = bitcast <8 x i8> %a to i64
  %1 = bitcast <8 x i8> %b to i64
  %2 = tail call <2 x i64> @llvm.kvx.movetohi(i64 %0, i64 %1)
  %3 = bitcast <2 x i64> %2 to <16 x i8>
  ret <16 x i8> %3
}

declare <2 x i64> @llvm.kvx.movetohi(i64, i64)

define <16 x i8> @test_movetobvlo_v8i8(<8 x i8> %a, <8 x i8> %b) {
; CHECK-LABEL: test_movetobvlo_v8i8:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    movetq $a0_lo = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %0 = bitcast <8 x i8> %a to i64
  %1 = bitcast <8 x i8> %b to i64
  %2 = tail call <2 x i64> @llvm.kvx.movetolo(i64 %0, i64 %1)
  %3 = bitcast <2 x i64> %2 to <16 x i8>
  ret <16 x i8> %3
}

declare <2 x i64> @llvm.kvx.movetolo(i64, i64)

define <8 x i16> @test_movetobvhi_v4i16(<4 x i16> %a, <4 x i16> %b) {
; CHECK-LABEL: test_movetobvhi_v4i16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    movetq $a0_hi = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %0 = bitcast <4 x i16> %a to i64
  %1 = bitcast <4 x i16> %b to i64
  %2 = tail call <2 x i64> @llvm.kvx.movetohi(i64 %0, i64 %1)
  %3 = bitcast <2 x i64> %2 to <8 x i16>
  ret <8 x i16> %3
}

define <8 x i16> @test_movetobvlo_v4i16(<4 x i16> %a, <4 x i16> %b) {
; CHECK-LABEL: test_movetobvlo_v4i16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    movetq $a0_lo = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %0 = bitcast <4 x i16> %a to i64
  %1 = bitcast <4 x i16> %b to i64
  %2 = tail call <2 x i64> @llvm.kvx.movetolo(i64 %0, i64 %1)
  %3 = bitcast <2 x i64> %2 to <8 x i16>
  ret <8 x i16> %3
}

; TODO: Check bad codegen for 2xi32
define <4 x i32> @test_movetobvhi_v2i32(<2 x i32> %a, <2 x i32> %b) {
; CHECK-LABEL: test_movetobvhi_v2i32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    sq 0[$r12] = $r0r1
; CHECK-NEXT:    movetq $a0_hi = $r0, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r0 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r1 = 8[$r12]
; CHECK-NEXT:    addd $r12 = $r12, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %0 = bitcast <2 x i32> %a to i64
  %1 = bitcast <2 x i32> %b to i64
  %2 = tail call <2 x i64> @llvm.kvx.movetohi(i64 %0, i64 %1)
  %3 = bitcast <2 x i64> %2 to <4 x i32>
  ret <4 x i32> %3
}

define <4 x i32> @test_movetobvlo_v2i32(<2 x i32> %a, <2 x i32> %b) {
; CHECK-LABEL: test_movetobvlo_v2i32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    sq 0[$r12] = $r0r1
; CHECK-NEXT:    movetq $a0_lo = $r0, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r0 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r1 = 8[$r12]
; CHECK-NEXT:    addd $r12 = $r12, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %0 = bitcast <2 x i32> %a to i64
  %1 = bitcast <2 x i32> %b to i64
  %2 = tail call <2 x i64> @llvm.kvx.movetolo(i64 %0, i64 %1)
  %3 = bitcast <2 x i64> %2 to <4 x i32>
  ret <4 x i32> %3
}

define <2 x i64> @test_movetobvhi_i64(i64 %a, i64 %b) {
; CHECK-LABEL: test_movetobvhi_i64:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    movetq $a0_hi = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %0 = tail call <2 x i64> @llvm.kvx.movetohi(i64 %a, i64 %b)
  ret <2 x i64> %0
}

define <2 x i64> @test_movetobvlo_i64(i64 %a, i64 %b) {
; CHECK-LABEL: test_movetobvlo_i64:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    movetq $a0_lo = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %0 = tail call <2 x i64> @llvm.kvx.movetolo(i64 %a, i64 %b)
  ret <2 x i64> %0
}

define <2 x i64> @test_movetobvhi_v2f32(<2 x float> %a, <2 x float> %b) {
; CHECK-LABEL: test_movetobvhi_v2f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    movetq $a0_hi = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %0 = bitcast <2 x float> %a to i64
  %1 = bitcast <2 x float> %b to i64
  %2 = tail call <2 x i64> @llvm.kvx.movetohi(i64 %0, i64 %1)
  ret <2 x i64> %2
}

define <2 x i64> @test_movetobvlo_v2f32(<2 x float> %a, <2 x float> %b) {
; CHECK-LABEL: test_movetobvlo_v2f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    movetq $a0_lo = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %0 = bitcast <2 x float> %a to i64
  %1 = bitcast <2 x float> %b to i64
  %2 = tail call <2 x i64> @llvm.kvx.movetolo(i64 %0, i64 %1)
  ret <2 x i64> %2
}

define <2 x double> @test_movetobvhi_f64(i64 %a.coerce, i64 %b.coerce) {
; CHECK-LABEL: test_movetobvhi_f64:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    movetq $a0_hi = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %0 = tail call <2 x i64> @llvm.kvx.movetohi(i64 %a.coerce, i64 %b.coerce)
  %1 = bitcast <2 x i64> %0 to <2 x double>
  ret <2 x double> %1
}

define <2 x double> @test_movetobvlo_f64(i64 %a.coerce, i64 %b.coerce) {
; CHECK-LABEL: test_movetobvlo_f64:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    movetq $a0_lo = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %0 = tail call <2 x i64> @llvm.kvx.movetolo(i64 %a.coerce, i64 %b.coerce)
  %1 = bitcast <2 x i64> %0 to <2 x double>
  ret <2 x double> %1
}
