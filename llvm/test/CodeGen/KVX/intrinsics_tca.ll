; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -O3 < %s | FileCheck %s
target triple = "kvx-kalray-cos"

declare <256 x i1> @llvm.kvx.movetohi(<256 x i1>, i64, i64)
define void @test_movetohi(i64 %a, i64 %b, <256 x i1>* %p0, <256 x i1>* %p1) {
; CHECK-LABEL: test_movetohi:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    lv $a0 = 0[$r2]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a1 = 0[$r3]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    movetq $a1_hi = $r0, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    movetq $a0_hi = $r1, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r2] = $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r3] = $a1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %v0 = load <256 x i1>, <256 x i1>* %p0
  %v1 = load <256 x i1>, <256 x i1>* %p1
  %v2 = tail call <256 x i1> @llvm.kvx.movetohi(<256 x i1> %v1, i64 %a, i64 %b)
  %v3 = tail call <256 x i1> @llvm.kvx.movetohi(<256 x i1> %v0, i64 %b, i64 %a)
  store <256 x i1> %v3, <256 x i1>* %p0, align 32
  store <256 x i1> %v2, <256 x i1>* %p1, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.movetolo(<256 x i1>, i64, i64)
define void @test_movetolo(i64 %a, i64 %b, <256 x i1>* %p0, <256 x i1>* %p1) {
; CHECK-LABEL: test_movetolo:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    lv $a0 = 0[$r2]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a1 = 0[$r3]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    movetq $a1_lo = $r0, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    movetq $a0_lo = $r1, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r2] = $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r3] = $a1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %v0 = load <256 x i1>, <256 x i1>* %p0
  %v1 = load <256 x i1>, <256 x i1>* %p1
  %v2 = tail call <256 x i1> @llvm.kvx.movetolo(<256 x i1> %v1, i64 %a, i64 %b)
  %v3 = tail call <256 x i1> @llvm.kvx.movetolo(<256 x i1> %v0, i64 %b, i64 %a)
  store <256 x i1> %v3, <256 x i1>* %p0, align 32
  store <256 x i1> %v2, <256 x i1>* %p1, align 32
  ret void
}

define void @test_movetohilo(i64 %a, i64 %b, i64 %c, i64 %d, <256 x i1>* %p0) {
; CHECK-LABEL: test_movetohilo:
; CHECK:       # %bb.0:
; CHECK-NEXT:    movetq $a0_lo = $r0, $r1
; CHECK-NEXT:    movetq $a0_hi = $r2, $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r4] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %v1 = tail call <256 x i1> @llvm.kvx.movetolo(<256 x i1> undef, i64 %a, i64 %b)
  %v2 = tail call <256 x i1> @llvm.kvx.movetohi(<256 x i1> %v1, i64 %c, i64 %d)
  store <256 x i1> %v2, <256 x i1>* %p0, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.moveto(i64, i64, i64, i64)
define void @test_moveto(i64 %a, i64 %b, i64 %c, i64 %d, <256 x i1>* %p0, <256 x i1>* %p1){
; CHECK-LABEL: test_moveto:
; CHECK:       # %bb.0:
; CHECK-NEXT:    movetq $a0_lo = $r2, $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    movetq $a1_lo = $r1, $r0
; CHECK-NEXT:    movetq $a0_hi = $r0, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    movetq $a1_hi = $r3, $r2
; CHECK-NEXT:    sv 0[$r4] = $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r5] = $a1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %v1 = tail call <256 x i1> @llvm.kvx.moveto( i64 %a, i64 %b, i64 %c, i64 %d)
  %v2 = tail call <256 x i1> @llvm.kvx.moveto( i64 %d, i64 %c, i64 %b, i64 %a)
  store <256 x i1> %v1, <256 x i1>* %p0, align 32
  store <256 x i1> %v2, <256 x i1>* %p1, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.moveoto(<4 x i64>)
define void @test_moveoto(<4 x i64> %r, <256 x i1>* %p0){
; CHECK-LABEL: test_moveoto:
; CHECK:       # %bb.0:
; CHECK-NEXT:    movetq $a0_lo = $r0, $r1
; CHECK-NEXT:    movetq $a0_hi = $r2, $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r4] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %v0 = tail call <256 x i1> @llvm.kvx.moveoto(<4 x i64> %r)
  store <256 x i1> %v0, <256 x i1>* %p0, align 32
  ret void
}

declare <4 x i64> @llvm.kvx.movefo(<256 x i1>)
define <4 x i64> @test_movefo(<256 x i1>* %p0){
; CHECK-LABEL: test_movefo:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lv $a0 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    movefo $r0r1r2r3 = $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %v0 = load <256 x i1>, <256 x i1>* %p0
  %v1 = tail call <4 x i64> @llvm.kvx.movefo(<256 x i1> %v0)
  ret <4 x i64> %v1
}

declare <4 x i64> @llvm.kvx.alignov(<256 x i1>, <256 x i1>, i64)
define <4 x i64> @test_alignovi(<256 x i1>* %p0, <256 x i1>* %p1){
; CHECK-LABEL: test_alignovi:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lv $a0 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a1 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    aligno $r0r1r2r3 = $a0, $a1, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %v0 = load <256 x i1>, <256 x i1>* %p0
  %v1 = load <256 x i1>, <256 x i1>* %p1
  %v2 = tail call <4 x i64> @llvm.kvx.alignov(<256 x i1> %v0, <256 x i1> %v1, i64 16)
  ret <4 x i64> %v2
}

define <4 x i64> @test_alignovr(<256 x i1>* %p0, <256 x i1>* %p1, i64 %s){
; CHECK-LABEL: test_alignovr:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lv $a0 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a1 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    aligno $r0r1r2r3 = $a0, $a1, $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %v0 = load <256 x i1>, <256 x i1>* %p0
  %v1 = load <256 x i1>, <256 x i1>* %p1
  %v2 = tail call <4 x i64> @llvm.kvx.alignov(<256 x i1> %v0, <256 x i1> %v1, i64 %s)
  ret <4 x i64> %v2
}

declare <256 x i1> @llvm.kvx.alignv(<256 x i1>, <256 x i1>, i64 immarg)
define void @test_alignvi(<256 x i1>* %p0, <256 x i1>* %p1){
; CHECK-LABEL: test_alignvi:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lv $a0 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a1 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    alignv $a0 = $a0, $a1, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %v0 = load <256 x i1>, <256 x i1>* %p0
  %v1 = load <256 x i1>, <256 x i1>* %p1
  %v2 = tail call <256 x i1> @llvm.kvx.alignv(<256 x i1> %v0, <256 x i1> %v1, i64 16)
  store <256 x i1> %v2, <256 x i1>* %p0, align 32
  ret void
}

define void @test_alignvr(<256 x i1>* %p0, <256 x i1>* %p1, i64 %s){
; CHECK-LABEL: test_alignvr:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lv $a0 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a1 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    alignv $a0 = $a0, $a1, $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %v0 = load <256 x i1>, <256 x i1>* %p0
  %v1 = load <256 x i1>, <256 x i1>* %p1
  %v2 = tail call <256 x i1> @llvm.kvx.alignv(<256 x i1> %v0, <256 x i1> %v1, i64 %s)
  store <256 x i1> %v2, <256 x i1>* %p0, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.convdhv0(<256 x i1>, <1024 x i1>, i32, i32)
define void @test_convdhv0(<256 x i1>* %p0, <1024 x i1>* %p1){
; CHECK-LABEL: test_convdhv0:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lv $a3 = 96[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a2 = 64[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a1 = 32[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a0 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    convdhv0.rhu.satu $a0_lo = $a0a1a2a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %m0 = load <1024 x i1>, <1024 x i1>* %p1
  %v1 = tail call <256 x i1> @llvm.kvx.convdhv0(<256 x i1> undef, <1024 x i1> %m0, i32 4, i32 1)
  store <256 x i1> %v1, <256 x i1>* %p0, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.convdhv1(<256 x i1>, <1024 x i1>, i32, i32)
define void @test_convdhv1(<256 x i1>* %p0, <1024 x i1>* %p1){
; CHECK-LABEL: test_convdhv1:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lv $a3 = 96[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a2 = 64[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a1 = 32[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a0 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    convdhv1.rz.satu $a0_hi = $a0a1a2a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %m0 = load <1024 x i1>, <1024 x i1>* %p1
  %v1 = tail call <256 x i1> @llvm.kvx.convdhv1(<256 x i1> undef, <1024 x i1> %m0, i32 3, i32 1)
  store <256 x i1> %v1, <256 x i1>* %p0, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.convdhv(<1024 x i1>, i32, i32)
define void @test_convdhv(<256 x i1>* %p0, <1024 x i1>* %p1){
; CHECK-LABEL: test_convdhv:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lv $a3 = 96[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a2 = 64[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a1 = 32[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a0 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    convdhv0.rd.sat $a4_lo = $a0a1a2a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    convdhv1.rd.sat $a4_hi = $a0a1a2a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a4
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %m0 = load <1024 x i1>, <1024 x i1>* %p1
  %v1 = tail call <256 x i1> @llvm.kvx.convdhv(<1024 x i1> %m0, i32 2, i32 0)
  store <256 x i1> %v1, <256 x i1>* %p0, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.convwbv0(<256 x i1>, <1024 x i1>, i32, i32)
define void @test_convwbv0(<256 x i1>* %p0){
; CHECK-LABEL: test_convwbv0:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lv $a0 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    convwbv0.rn.sat $a0_x = $a0a1a2a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %v0 = load <256 x i1>, <256 x i1>* %p0
  %v1 = tail call <256 x i1> @llvm.kvx.convwbv0(<256 x i1> %v0, <1024 x i1> undef, i32 0, i32 0)
  store <256 x i1> %v1, <256 x i1>* %p0, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.convwbv1(<256 x i1>, <1024 x i1>, i32, i32)
define void @test_convwbv1(<256 x i1>* %p0){
; CHECK-LABEL: test_convwbv1:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lv $a0 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    convwbv1.ru.satu $a0_y = $a0a1a2a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %v0 = load <256 x i1>, <256 x i1>* %p0
  %v1 = tail call <256 x i1> @llvm.kvx.convwbv1(<256 x i1> %v0, <1024 x i1> undef, i32 1, i32 1)
  store <256 x i1> %v1, <256 x i1>* %p0, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.convwbv2(<256 x i1>, <1024 x i1>, i32, i32)
define void @test_convwbv2(<256 x i1>* %p0){
; CHECK-LABEL: test_convwbv2:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lv $a0 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    convwbv2.rd.sat $a0_z = $a0a1a2a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %v0 = load <256 x i1>, <256 x i1>* %p0
  %v1 = tail call <256 x i1> @llvm.kvx.convwbv2(<256 x i1> %v0, <1024 x i1> undef, i32 2, i32 0)
  store <256 x i1> %v1, <256 x i1>* %p0, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.convwbv3(<256 x i1>, <1024 x i1>, i32, i32)
define void @test_convwbv3(<256 x i1>* %p0){
; CHECK-LABEL: test_convwbv3:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lv $a0 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    convwbv3.rz.sat $a0_t = $a0a1a2a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %v0 = load <256 x i1>, <256 x i1>* %p0
  %v1 = tail call <256 x i1> @llvm.kvx.convwbv3(<256 x i1> %v0, <1024 x i1> undef, i32 3, i32 0)
  store <256 x i1> %v1, <256 x i1>* %p0, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.convwbv(<1024 x i1>, i32, i32)
define void @test_convwbv(<256 x i1>* %p0){
; CHECK-LABEL: test_convwbv:
; CHECK:       # %bb.0:
; CHECK-NEXT:    convwbv0.rhu.sat $a0_x = $a0a1a2a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    convwbv1.rhu.sat $a0_y = $a0a1a2a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    convwbv2.rhu.sat $a0_z = $a0a1a2a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    convwbv3.rhu.sat $a0_t = $a0a1a2a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %v1 = tail call <256 x i1> @llvm.kvx.convwbv(<1024 x i1> undef, i32 4, i32 0)
  store <256 x i1> %v1, <256 x i1>* %p0, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.fmma242hw0(<256 x i1>, <512 x i1>, <256 x i1>, <256 x i1>)
define void @test_fmma242hw0(<256 x i1>* %p0){
; CHECK-LABEL: test_fmma242hw0:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lv $a0 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmma242hw0 $a0_lo = $a0a1, $a0, $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %v0 = load <256 x i1>, <256 x i1>* %p0
  %v1 = tail call <256 x i1> @llvm.kvx.fmma242hw0(<256 x i1> %v0, <512 x i1> undef, <256 x i1> undef, <256 x i1> undef)
  store <256 x i1> %v1, <256 x i1>* %p0, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.fmma242hw1(<256 x i1>, <512 x i1>, <256 x i1>, <256 x i1>)
define void @test_fmma242hw1(<256 x i1>* %p0){
; CHECK-LABEL: test_fmma242hw1:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lv $a0 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmma242hw1 $a0_hi = $a0a1, $a0, $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %v0 = load <256 x i1>, <256 x i1>* %p0
  %v1 = tail call <256 x i1> @llvm.kvx.fmma242hw1(<256 x i1> %v0, <512 x i1> undef, <256 x i1> undef, <256 x i1> undef)
  store <256 x i1> %v1, <256 x i1>* %p0, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.fmma242hw2(<256 x i1>, <512 x i1>, <256 x i1>, <256 x i1>)
define void @test_fmma242hw2(<256 x i1>* %p0){
; CHECK-LABEL: test_fmma242hw2:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lv $a0 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmma242hw2 $a0_lo = $a0a1, $a0, $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %v0 = load <256 x i1>, <256 x i1>* %p0
  %v1 = tail call <256 x i1> @llvm.kvx.fmma242hw2(<256 x i1> %v0, <512 x i1> undef, <256 x i1> undef, <256 x i1> undef)
  store <256 x i1> %v1, <256 x i1>* %p0, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.fmma242hw3(<256 x i1>, <512 x i1>, <256 x i1>, <256 x i1>)
define void @test_fmma242hw3(<256 x i1>* %p0){
; CHECK-LABEL: test_fmma242hw3:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lv $a0 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmma242hw3 $a0_hi = $a0a1, $a0, $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %v0 = load <256 x i1>, <256 x i1>* %p0
  %v1 = tail call <256 x i1> @llvm.kvx.fmma242hw3(<256 x i1> %v0, <512 x i1> undef, <256 x i1> undef, <256 x i1> undef)
  store <256 x i1> %v1, <256 x i1>* %p0, align 32
  ret void
}

declare <512 x i1> @llvm.kvx.fmma242hw(<512 x i1>, <256 x i1>, <256 x i1>)
define void @test_fmma242hw(<512 x i1>* %p0){
; CHECK-LABEL: test_fmma242hw:
; CHECK:       # %bb.0:
; CHECK-NEXT:    fmma242hw2 $a1_lo = $a0a1, $a0, $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmma242hw3 $a1_hi = $a0a1, $a0, $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmma242hw0 $a0_lo = $a0a1, $a0, $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmma242hw1 $a0_hi = $a0a1, $a0, $a0
; CHECK-NEXT:    sv 32[$r0] = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %v1 = tail call <512 x i1> @llvm.kvx.fmma242hw(<512 x i1> undef, <256 x i1> undef, <256 x i1> undef)
  store <512 x i1> %v1, <512 x i1>* %p0, align 32
  ret void
}

declare <1024 x i1> @llvm.kvx.mma444hbd0(<1024 x i1>, <256 x i1>, <256 x i1>)
define void @test_mma444hbd0(<1024 x i1>* %p0){
; CHECK-LABEL: test_mma444hbd0:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mma444hbd0 $a0a1a2a3 = $a0a1a2a3, $a0, $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 96[$r0] = $a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 64[$r0] = $a2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 32[$r0] = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %m1 = tail call <1024 x i1> @llvm.kvx.mma444hbd0(<1024 x i1> undef, <256 x i1> undef, <256 x i1> undef)
  store <1024 x i1> %m1, <1024 x i1>* %p0, align 32
  ret void
}

declare <1024 x i1> @llvm.kvx.mma444hbd1(<1024 x i1>, <256 x i1>, <256 x i1>)
define void @test_mma444hbd1(<1024 x i1>* %p0){
; CHECK-LABEL: test_mma444hbd1:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mma444hbd1 $a0a1a2a3 = $a0a1a2a3, $a0, $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 96[$r0] = $a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 64[$r0] = $a2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 32[$r0] = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %m1 = tail call <1024 x i1> @llvm.kvx.mma444hbd1(<1024 x i1> undef, <256 x i1> undef, <256 x i1> undef)
  store <1024 x i1> %m1, <1024 x i1>* %p0, align 32
  ret void
}

declare <1024 x i1> @llvm.kvx.mma444hd(<1024 x i1>, <256 x i1>, <256 x i1>)
define void @test_mma444hd(<1024 x i1>* %p0){
; CHECK-LABEL: test_mma444hd:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mma444hd $a0a1a2a3 = $a0a1a2a3, $a0, $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 96[$r0] = $a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 64[$r0] = $a2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 32[$r0] = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %m1 = tail call <1024 x i1> @llvm.kvx.mma444hd(<1024 x i1> undef, <256 x i1> undef, <256 x i1> undef)
  store <1024 x i1> %m1, <1024 x i1>* %p0, align 32
  ret void
}

declare <1024 x i1> @llvm.kvx.mma444suhbd0(<1024 x i1>, <256 x i1>, <256 x i1>)
define void @test_mma444suhbd0(<1024 x i1>* %p0){
; CHECK-LABEL: test_mma444suhbd0:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mma444suhbd0 $a0a1a2a3 = $a0a1a2a3, $a0, $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 96[$r0] = $a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 64[$r0] = $a2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 32[$r0] = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %m1 = tail call <1024 x i1> @llvm.kvx.mma444suhbd0(<1024 x i1> undef, <256 x i1> undef, <256 x i1> undef)
  store <1024 x i1> %m1, <1024 x i1>* %p0, align 32
  ret void
}

declare <1024 x i1> @llvm.kvx.mma444suhbd1(<1024 x i1>, <256 x i1>, <256 x i1>)
define void @test_mma444suhbd1(<1024 x i1>* %p0){
; CHECK-LABEL: test_mma444suhbd1:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mma444suhbd1 $a0a1a2a3 = $a0a1a2a3, $a0, $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 96[$r0] = $a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 64[$r0] = $a2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 32[$r0] = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %m1 = tail call <1024 x i1> @llvm.kvx.mma444suhbd1(<1024 x i1> undef, <256 x i1> undef, <256 x i1> undef)
  store <1024 x i1> %m1, <1024 x i1>* %p0, align 32
  ret void
}

declare <1024 x i1> @llvm.kvx.mma444suhd(<1024 x i1>, <256 x i1>, <256 x i1>)
define void @test_mma444suhd(<1024 x i1>* %p0){
; CHECK-LABEL: test_mma444suhd:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mma444suhd $a0a1a2a3 = $a0a1a2a3, $a0, $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 96[$r0] = $a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 64[$r0] = $a2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 32[$r0] = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %m1 = tail call <1024 x i1> @llvm.kvx.mma444suhd(<1024 x i1> undef, <256 x i1> undef, <256 x i1> undef)
  store <1024 x i1> %m1, <1024 x i1>* %p0, align 32
  ret void
}

declare <1024 x i1> @llvm.kvx.mma444uhbd0(<1024 x i1>, <256 x i1>, <256 x i1>)
define void @test_mma444uhbd0(<1024 x i1>* %p0){
; CHECK-LABEL: test_mma444uhbd0:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mma444uhbd0 $a0a1a2a3 = $a0a1a2a3, $a0, $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 96[$r0] = $a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 64[$r0] = $a2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 32[$r0] = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %m1 = tail call <1024 x i1> @llvm.kvx.mma444uhbd0(<1024 x i1> undef, <256 x i1> undef, <256 x i1> undef)
  store <1024 x i1> %m1, <1024 x i1>* %p0, align 32
  ret void
}

declare <1024 x i1> @llvm.kvx.mma444uhbd1(<1024 x i1>, <256 x i1>, <256 x i1>)
define void @test_mma444uhbd1(<1024 x i1>* %p0){
; CHECK-LABEL: test_mma444uhbd1:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mma444uhbd1 $a0a1a2a3 = $a0a1a2a3, $a0, $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 96[$r0] = $a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 64[$r0] = $a2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 32[$r0] = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %m1 = tail call <1024 x i1> @llvm.kvx.mma444uhbd1(<1024 x i1> undef, <256 x i1> undef, <256 x i1> undef)
  store <1024 x i1> %m1, <1024 x i1>* %p0, align 32
  ret void
}

declare <1024 x i1> @llvm.kvx.mma444uhd(<1024 x i1>, <256 x i1>, <256 x i1>)
define void @test_mma444uhd(<1024 x i1>* %p0){
; CHECK-LABEL: test_mma444uhd:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mma444uhd $a0a1a2a3 = $a0a1a2a3, $a0, $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 96[$r0] = $a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 64[$r0] = $a2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 32[$r0] = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %m1 = tail call <1024 x i1> @llvm.kvx.mma444uhd(<1024 x i1> undef, <256 x i1> undef, <256 x i1> undef)
  store <1024 x i1> %m1, <1024 x i1>* %p0, align 32
  ret void
}

declare <1024 x i1> @llvm.kvx.mma444ushbd0(<1024 x i1>, <256 x i1>, <256 x i1>)
define void @test_mma444ushbd0(<1024 x i1>* %p0){
; CHECK-LABEL: test_mma444ushbd0:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mma444ushbd0 $a0a1a2a3 = $a0a1a2a3, $a0, $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 96[$r0] = $a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 64[$r0] = $a2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 32[$r0] = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %m1 = tail call <1024 x i1> @llvm.kvx.mma444ushbd0(<1024 x i1> undef, <256 x i1> undef, <256 x i1> undef)
  store <1024 x i1> %m1, <1024 x i1>* %p0, align 32
  ret void
}

declare <1024 x i1> @llvm.kvx.mma444ushbd1(<1024 x i1>, <256 x i1>, <256 x i1>)
define void @test_mma444ushbd1(<1024 x i1>* %p0){
; CHECK-LABEL: test_mma444ushbd1:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mma444ushbd1 $a0a1a2a3 = $a0a1a2a3, $a0, $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 96[$r0] = $a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 64[$r0] = $a2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 32[$r0] = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %m1 = tail call <1024 x i1> @llvm.kvx.mma444ushbd1(<1024 x i1> undef, <256 x i1> undef, <256 x i1> undef)
  store <1024 x i1> %m1, <1024 x i1>* %p0, align 32
  ret void
}

declare <1024 x i1> @llvm.kvx.mma444ushd(<1024 x i1>, <256 x i1>, <256 x i1>)
define void @test_mma444ushd(<1024 x i1>* %p0){
; CHECK-LABEL: test_mma444ushd:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mma444ushd $a0a1a2a3 = $a0a1a2a3, $a0, $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 96[$r0] = $a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 64[$r0] = $a2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 32[$r0] = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %m1 = tail call <1024 x i1> @llvm.kvx.mma444ushd(<1024 x i1> undef, <256 x i1> undef, <256 x i1> undef)
  store <1024 x i1> %m1, <1024 x i1>* %p0, align 32
  ret void
}

declare <512 x i1> @llvm.kvx.mma484bw(<512 x i1>, <256 x i1>, <256 x i1>)
define void @test_mma484bw(<512 x i1>* %p0){
; CHECK-LABEL: test_mma484bw:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mma484bw $a0a1 = $a0a1, $a0, $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 32[$r0] = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %m1 = tail call <512 x i1> @llvm.kvx.mma484bw(<512 x i1> undef, <256 x i1> undef, <256 x i1> undef)
  store <512 x i1> %m1, <512 x i1>* %p0, align 32
  ret void
}

declare <512 x i1> @llvm.kvx.mma484subw(<512 x i1>, <256 x i1>, <256 x i1>)
define void @test_mma484subw(<512 x i1>* %p0){
; CHECK-LABEL: test_mma484subw:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mma484subw $a0a1 = $a0a1, $a0, $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 32[$r0] = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %m1 = tail call <512 x i1> @llvm.kvx.mma484subw(<512 x i1> undef, <256 x i1> undef, <256 x i1> undef)
  store <512 x i1> %m1, <512 x i1>* %p0, align 32
  ret void
}

declare <512 x i1> @llvm.kvx.mma484ubw(<512 x i1>, <256 x i1>, <256 x i1>)
define void @test_mma484ubw(<512 x i1>* %p0){
; CHECK-LABEL: test_mma484ubw:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mma484ubw $a0a1 = $a0a1, $a0, $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 32[$r0] = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %m1 = tail call <512 x i1> @llvm.kvx.mma484ubw(<512 x i1> undef, <256 x i1> undef, <256 x i1> undef)
  store <512 x i1> %m1, <512 x i1>* %p0, align 32
  ret void
}

declare <512 x i1> @llvm.kvx.mma484usbw(<512 x i1>, <256 x i1>, <256 x i1>)
define void @test_mma484usbw(<512 x i1>* %p0){
; CHECK-LABEL: test_mma484usbw:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mma484usbw $a0a1 = $a0a1, $a0, $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 32[$r0] = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %m1 = tail call <512 x i1> @llvm.kvx.mma484usbw(<512 x i1> undef, <256 x i1> undef, <256 x i1> undef)
  store <512 x i1> %m1, <512 x i1>* %p0, align 32
  ret void
}

declare <1024 x i1> @llvm.kvx.mt44d(<1024 x i1>)
define void @test_mt44d(<1024 x i1>* %p0){
; CHECK-LABEL: test_mt44d:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mt44d $a0a1a2a3 = $a0a1a2a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 96[$r0] = $a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 64[$r0] = $a2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 32[$r0] = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %m1 = tail call <1024 x i1> @llvm.kvx.mt44d(<1024 x i1> undef)
  store <1024 x i1> %m1, <1024 x i1>* %p0, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.fscalewv(<256 x i1>, i32, i32, i32)
define void @test_fscalewv(<256 x i1>* %p0){
; CHECK-LABEL: test_fscalewv:
; CHECK:       # %bb.0:
; CHECK-NEXT:    fscalewv.rn $a0 = $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %m1 = tail call <256 x i1> @llvm.kvx.fscalewv(<256 x i1> undef, i32 0, i32 0, i32 0)
  store <256 x i1> %m1, <256 x i1>* %p0, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.fnarrowwhv(<512 x i1>, i32, i32)
define void @test_fnarrowwhv(<256 x i1>* %p0){
; CHECK-LABEL: test_fnarrowwhv:
; CHECK:       # %bb.0:
; CHECK-NEXT:    fnarrowwhv.rn $a0 = $a0a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %m1 = tail call <256 x i1> @llvm.kvx.fnarrowwhv(<512 x i1> undef, i32 0, i32 0)
  store <256 x i1> %m1, <256 x i1>* %p0, align 32
  ret void
}


declare <256 x i1> @llvm.kvx.lv.cond(<256 x i1>, i8*, i64, i32, i32)
declare <1024 x i1> @llvm.kvx.lvc(<1024 x i1>, i8*, i32, i32)
declare <1024 x i1> @llvm.kvx.lvc.cond(<1024 x i1>, i8*, i32, i64, i32, i32)
declare { <4 x i64>, <256 x i1> } @llvm.kvx.swapvo(<4 x i64>, <256 x i1>) #2
declare void @llvm.kvx.sv.cond(i8*, <256 x i1>, i64, i32) #3

; Test generated from clang's intrinsics_tca.c
define <4 x i64> @test_tca_builtins(i64 %a, i64 %b, i64 %c, i64 %d, <256 x i1>* %v, <512 x i1>* %w, <1024 x i1>* %m) {
; CHECK-LABEL: test_tca_builtins:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    make $r1 = 4
; CHECK-NEXT:    make $r35 = 3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    movetq $a0_lo = $r35, $r1
; CHECK-NEXT:    make $r34 = 2
; CHECK-NEXT:    addd $r1 = $r4, 96
; CHECK-NEXT:    make $r33 = 1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    movetq $a0_hi = $r33, $r34
; CHECK-NEXT:    lv $a1 = 0[$r4]
; CHECK-NEXT:    make $r32 = 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a1 = 0[$r4]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r4] = $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a5 = 0[$r4]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a3 = 96[$r6]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a2 = 64[$r6]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a1 = 32[$r6]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a0 = 0[$r6]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a7 = 32[$r5]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a6 = 0[$r5]
; CHECK-NEXT:    convwbv0.ru.sat $a4_x = $a0a1a2a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    convwbv1.ru.sat $a4_y = $a0a1a2a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    convwbv2.ru.sat $a4_z = $a0a1a2a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    convwbv3.ru.sat $a4_t = $a0a1a2a3
; CHECK-NEXT:    movetq $a8_lo = $r32, $r33
; CHECK-NEXT:    movetq $a8_hi = $r34, $r35
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmma242hw2 $a11_lo = $a6a7, $a4, $a8
; CHECK-NEXT:    alignv $a5 = $a8, $a5, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmma242hw3 $a11_hi = $a6a7, $a4, $a8
; CHECK-NEXT:    aligno $r8r9r10r11 = $a8, $a5, 1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmma242hw0 $a10_lo = $a6a7, $a4, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmma242hw1 $a10_hi = $a6a7, $a4, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma484bw $a6a7 = $a10a11, $a8, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma484subw $a6a7 = $a6a7, $a8, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma444hbd0 $a0a1a2a3 = $a0a1a2a3, $a8, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fscalewv $a4 = $a4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma484ubw $a6a7 = $a6a7, $a8, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma444hbd1 $a0a1a2a3 = $a0a1a2a3, $a8, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma444hd $a0a1a2a3 = $a0a1a2a3, $a8, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma484usbw $a6a7 = $a6a7, $a8, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma444suhbd0 $a0a1a2a3 = $a0a1a2a3, $a8, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwhv.rn.s $a9 = $a6a7
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma444suhbd1 $a0a1a2a3 = $a0a1a2a3, $a8, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fscalewv.rna.relu $a4 = $a4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    movefo $r8r9r10r11 = $a4
; CHECK-NEXT:    movetq $a4_lo = $r8, $r9
; CHECK-NEXT:    movetq $a4_hi = $r10, $r11
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma444suhd $a0a1a2a3 = $a0a1a2a3, $a8, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma444uhbd0 $a0a1a2a3 = $a0a1a2a3, $a8, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma444uhbd1 $a0a1a2a3 = $a0a1a2a3, $a8, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fscalewv.relu $a4 = $a4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma444uhd $a0a1a2a3 = $a0a1a2a3, $a8, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma444ushbd0 $a0a1a2a3 = $a0a1a2a3, $a8, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma444ushbd1 $a0a1a2a3 = $a0a1a2a3, $a8, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma444ushd $a0a1a2a3 = $a0a1a2a3, $a8, $a8
; CHECK-NEXT:    lv.s.even $r0 ? $a4 = [$r1]
; CHECK-NEXT:    addd $r1 = $r4, 128
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mt44d $a0a1a2a3 = $a0a1a2a3
; CHECK-NEXT:    lv.s $a4 = 0[$r4]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv.c3.s $a0a1a2a3 = 0[$r1]
; CHECK-NEXT:    addd $r1 = $r4, 160
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv.c2.odd $r0 ? $a0a1a2a3 = [$r1]
; CHECK-NEXT:    addd $r0 = $r4, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r4] = $a4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv.even $r33 ? [$r0] = $a4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 32[$r5] = $a7
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r5] = $a6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 96[$r6] = $a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 64[$r6] = $a2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 32[$r6] = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r6] = $a0
; CHECK-NEXT:    copyd $r0 = $r8
; CHECK-NEXT:    copyd $r1 = $r9
; CHECK-NEXT:    copyd $r2 = $r10
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r3 = $r11
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %0 = load volatile <256 x i1>, <256 x i1>* %v, align 32
  %1 = call <256 x i1> @llvm.kvx.movetohi(<256 x i1> %0, i64 0, i64 1)
  %2 = load volatile <256 x i1>, <256 x i1>* %v, align 32
  %3 = call <256 x i1> @llvm.kvx.movetolo(<256 x i1> %2, i64 3, i64 2)
  %4 = call <256 x i1> @llvm.kvx.moveto(i64 1, i64 2, i64 3, i64 4)
  store volatile <256 x i1> %4, <256 x i1>* %v, align 32
  %5 = call <256 x i1> @llvm.kvx.moveoto(<4 x i64> <i64 0, i64 1, i64 2, i64 3>)
  %6 = load volatile <256 x i1>, <256 x i1>* %v, align 32
  %7 = call <256 x i1> @llvm.kvx.alignv(<256 x i1> %5, <256 x i1> %6, i64 16)
  %8 = call <4 x i64> @llvm.kvx.alignov(<256 x i1> %5, <256 x i1> %7, i64 1)
  %9 = load volatile <1024 x i1>, <1024 x i1>* %m, align 128
  %10 = call <256 x i1> @llvm.kvx.convdhv0(<256 x i1> %5, <1024 x i1> %9, i32 0, i32 0)
  %11 = call <256 x i1> @llvm.kvx.convdhv1(<256 x i1> %5, <1024 x i1> %9, i32 1, i32 1)
  %12 = call <256 x i1> @llvm.kvx.convdhv(<1024 x i1> %9, i32 4, i32 0)
  %13 = call <256 x i1> @llvm.kvx.convwbv0(<256 x i1> %12, <1024 x i1> %9, i32 1, i32 0)
  %14 = call <256 x i1> @llvm.kvx.convwbv1(<256 x i1> %12, <1024 x i1> %9, i32 2, i32 1)
  %15 = call <256 x i1> @llvm.kvx.convwbv2(<256 x i1> %12, <1024 x i1> %9, i32 3, i32 0)
  %16 = call <256 x i1> @llvm.kvx.convwbv3(<256 x i1> %12, <1024 x i1> %9, i32 4, i32 1)
  %17 = load volatile <512 x i1>, <512 x i1>* %w, align 64
  %18 = call <256 x i1> @llvm.kvx.convwbv(<1024 x i1> %9, i32 1, i32 0)
  %19 = call <256 x i1> @llvm.kvx.fmma242hw0(<256 x i1> %5, <512 x i1> %17, <256 x i1> %5, <256 x i1> %18)
  %20 = call <256 x i1> @llvm.kvx.fmma242hw1(<256 x i1> %18, <512 x i1> %17, <256 x i1> %5, <256 x i1> %18)
  %21 = call <256 x i1> @llvm.kvx.fmma242hw2(<256 x i1> %5, <512 x i1> %17, <256 x i1> %5, <256 x i1> %18)
  %22 = call <256 x i1> @llvm.kvx.fmma242hw3(<256 x i1> %18, <512 x i1> %17, <256 x i1> %5, <256 x i1> %18)
  %23 = call <512 x i1> @llvm.kvx.fmma242hw(<512 x i1> %17, <256 x i1> %18, <256 x i1> %5)
  %24 = call <1024 x i1> @llvm.kvx.mma444hbd0(<1024 x i1> %9, <256 x i1> %5, <256 x i1> %5)
  %25 = call <1024 x i1> @llvm.kvx.mma444hbd1(<1024 x i1> %24, <256 x i1> %5, <256 x i1> %5)
  %26 = call <1024 x i1> @llvm.kvx.mma444hd(<1024 x i1> %25, <256 x i1> %5, <256 x i1> %5)
  %27 = call <1024 x i1> @llvm.kvx.mma444suhbd0(<1024 x i1> %26, <256 x i1> %5, <256 x i1> %5)
  %28 = call <1024 x i1> @llvm.kvx.mma444suhbd1(<1024 x i1> %27, <256 x i1> %5, <256 x i1> %5)
  %29 = call <1024 x i1> @llvm.kvx.mma444suhd(<1024 x i1> %28, <256 x i1> %5, <256 x i1> %5)
  %30 = call <1024 x i1> @llvm.kvx.mma444uhbd0(<1024 x i1> %29, <256 x i1> %5, <256 x i1> %5)
  %31 = call <1024 x i1> @llvm.kvx.mma444uhbd1(<1024 x i1> %30, <256 x i1> %5, <256 x i1> %5)
  %32 = call <1024 x i1> @llvm.kvx.mma444uhd(<1024 x i1> %31, <256 x i1> %5, <256 x i1> %5)
  %33 = call <1024 x i1> @llvm.kvx.mma444ushbd0(<1024 x i1> %32, <256 x i1> %5, <256 x i1> %5)
  %34 = call <1024 x i1> @llvm.kvx.mma444ushbd1(<1024 x i1> %33, <256 x i1> %5, <256 x i1> %5)
  %35 = call <1024 x i1> @llvm.kvx.mma444ushd(<1024 x i1> %34, <256 x i1> %5, <256 x i1> %5)
  %36 = call <512 x i1> @llvm.kvx.mma484bw(<512 x i1> %23, <256 x i1> %5, <256 x i1> %5)
  %37 = call <512 x i1> @llvm.kvx.mma484subw(<512 x i1> %36, <256 x i1> %5, <256 x i1> %5)
  %38 = call <512 x i1> @llvm.kvx.mma484ubw(<512 x i1> %37, <256 x i1> %5, <256 x i1> %5)
  %39 = call <512 x i1> @llvm.kvx.mma484usbw(<512 x i1> %38, <256 x i1> %5, <256 x i1> %5)
  %40 = call <1024 x i1> @llvm.kvx.mt44d(<1024 x i1> %35)
  %41 = call <256 x i1> @llvm.kvx.fscalewv(<256 x i1> %18, i32 7, i32 0, i32 0)
  %42 = call <256 x i1> @llvm.kvx.fnarrowwhv(<512 x i1> %39, i32 0, i32 1)
  %43 = call <256 x i1> @llvm.kvx.fscalewv(<256 x i1> %41, i32 4, i32 0, i32 1)
  %44 = call { <4 x i64>, <256 x i1> } @llvm.kvx.swapvo(<4 x i64> %8, <256 x i1> %43)
  %45 = extractvalue { <4 x i64>, <256 x i1> } %44, 0
  %46 = extractvalue { <4 x i64>, <256 x i1> } %44, 1
  %47 = call <256 x i1> @llvm.kvx.fscalewv(<256 x i1> %46, i32 7, i32 0, i32 1)
  %arrayidx6 = getelementptr inbounds <256 x i1>, <256 x i1>* %v, i64 3
  %48 = bitcast <256 x i1>* %arrayidx6 to i8*
  %49 = call <256 x i1> @llvm.kvx.lv.cond(<256 x i1> %47, i8* nonnull %48, i64 %a, i32 1, i32 7)
  %50 = addrspacecast <256 x i1>* %v to <256 x i1> addrspace(258)*
  %51 = load <256 x i1>, <256 x i1> addrspace(258)* %50, align 32
  %arrayidx7 = getelementptr inbounds <256 x i1>, <256 x i1>* %v, i64 4
  %52 = bitcast <256 x i1>* %arrayidx7 to i8*
  %53 = call <1024 x i1> @llvm.kvx.lvc(<1024 x i1> %40, i8* nonnull %52, i32 3, i32 1)
  %arrayidx8 = getelementptr inbounds <256 x i1>, <256 x i1>* %v, i64 5
  %54 = bitcast <256 x i1>* %arrayidx8 to i8*
  %55 = call <1024 x i1> @llvm.kvx.lvc.cond(<1024 x i1> %53, i8* nonnull %54, i32 2, i64 %a, i32 0, i32 6)
  store <256 x i1> %51, <256 x i1>* %v, align 32
  %arrayidx10 = getelementptr inbounds <256 x i1>, <256 x i1>* %v, i64 1
  %56 = bitcast <256 x i1>* %arrayidx10 to i8*
  call void @llvm.kvx.sv.cond(i8* nonnull %56, <256 x i1> %51, i64 1, i32 7)
  store volatile <512 x i1> %39, <512 x i1>* %w, align 64
  store volatile <1024 x i1> %55, <1024 x i1>* %m, align 128
  ret <4 x i64> %45
}

define void @convdhv(<256 x i1>* nocapture %v, <1024 x i1>* nocapture readonly %m) {
; CHECK-LABEL: convdhv:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    lv $a0 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a7 = 96[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a6 = 64[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a5 = 32[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a4 = 0[$r1]
; CHECK-NEXT:    addd $r1 = $r1, 128
; CHECK-NEXT:    ;;
; CHECK-NEXT:    convdhv1.rn.sat $a0_hi = $a4a5a6a7
; CHECK-NEXT:    ;;
; CHECK-NEXT:    convdhv0.rn.satu $a0_lo = $a4a5a6a7
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 32[$r0] = $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a3 = 96[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a2 = 64[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a1 = 32[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a0 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    convdhv0.rz.sat $a4_lo = $a0a1a2a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    convdhv1.rz.sat $a4_hi = $a0a1a2a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 64[$r0] = $a4
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %0 = load <256 x i1>, <256 x i1>* %v, align 32
  %1 = load <1024 x i1>, <1024 x i1>* %m, align 128
  %2 = tail call <256 x i1> @llvm.kvx.convdhv1(<256 x i1> %0, <1024 x i1> %1, i32 0, i32 0)
  %3 = tail call <256 x i1> @llvm.kvx.convdhv0(<256 x i1> %2, <1024 x i1> %1, i32 0, i32 1)
  %arrayidx3 = getelementptr inbounds <256 x i1>, <256 x i1>* %v, i64 1
  store <256 x i1> %3, <256 x i1>* %arrayidx3, align 32
  %arrayidx4 = getelementptr inbounds <1024 x i1>, <1024 x i1>* %m, i64 1
  %4 = load <1024 x i1>, <1024 x i1>* %arrayidx4, align 128
  %5 = tail call <256 x i1> @llvm.kvx.convdhv(<1024 x i1> %4, i32 3, i32 0)
  %arrayidx5 = getelementptr inbounds <256 x i1>, <256 x i1>* %v, i64 2
  store <256 x i1> %5, <256 x i1>* %arrayidx5, align 32
  ret void
}

define void @convwbv(<256 x i1>* nocapture %v, <1024 x i1>* nocapture readonly %m) {
; CHECK-LABEL: convwbv:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    lv $a0 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a7 = 96[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a6 = 64[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a5 = 32[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a4 = 0[$r1]
; CHECK-NEXT:    addd $r1 = $r1, 128
; CHECK-NEXT:    ;;
; CHECK-NEXT:    convwbv1.rn.sat $a0_y = $a4a5a6a7
; CHECK-NEXT:    ;;
; CHECK-NEXT:    convwbv0.rn.satu $a0_x = $a4a5a6a7
; CHECK-NEXT:    ;;
; CHECK-NEXT:    convwbv2.rd.sat $a0_z = $a4a5a6a7
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyv $a1 = $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    convwbv3.rhu.satu $a1_t = $a4a5a6a7
; CHECK-NEXT:    ;;
; CHECK-NEXT:    convwbv3.rn.sat $a0_t = $a4a5a6a7
; CHECK-NEXT:    sv 32[$r0] = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a3 = 96[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a2 = 64[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a1 = 32[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a0 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    convwbv0.rz.satu $a4_x = $a0a1a2a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    convwbv1.rz.satu $a4_y = $a0a1a2a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    convwbv2.rz.satu $a4_z = $a0a1a2a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    convwbv3.rz.satu $a4_t = $a0a1a2a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 64[$r0] = $a4
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %0 = load <256 x i1>, <256 x i1>* %v, align 32
  %1 = load <1024 x i1>, <1024 x i1>* %m, align 128
  %2 = tail call <256 x i1> @llvm.kvx.convwbv1(<256 x i1> %0, <1024 x i1> %1, i32 0, i32 0)
  %3 = tail call <256 x i1> @llvm.kvx.convwbv0(<256 x i1> %2, <1024 x i1> %1, i32 0, i32 1)
  %4 = tail call <256 x i1> @llvm.kvx.convwbv2(<256 x i1> %3, <1024 x i1> %1, i32 2, i32 0)
  %5 = tail call <256 x i1> @llvm.kvx.convwbv3(<256 x i1> %4, <1024 x i1> %1, i32 4, i32 1)
  %arrayidx5 = getelementptr inbounds <256 x i1>, <256 x i1>* %v, i64 1
  store <256 x i1> %5, <256 x i1>* %arrayidx5, align 32
  %6 = tail call <256 x i1> @llvm.kvx.convwbv3(<256 x i1> %4, <1024 x i1> %1, i32 0, i32 0)
  store <256 x i1> %6, <256 x i1>* %v, align 32
  %arrayidx8 = getelementptr inbounds <1024 x i1>, <1024 x i1>* %m, i64 1
  %7 = load <1024 x i1>, <1024 x i1>* %arrayidx8, align 128
  %8 = tail call <256 x i1> @llvm.kvx.convwbv(<1024 x i1> %7, i32 3, i32 1)
  %arrayidx9 = getelementptr inbounds <256 x i1>, <256 x i1>* %v, i64 2
  store <256 x i1> %8, <256 x i1>* %arrayidx9, align 32
  ret void
}

define void @fmma242hw(<256 x i1>* nocapture %v, <512 x i1>* nocapture %w) {
; CHECK-LABEL: fmma242hw:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    lv $a0 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a3 = 32[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a2 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a1 = 32[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a4 = 64[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmma242hw0 $a0_lo = $a2a3, $a1, $a4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmma242hw1 $a0_hi = $a2a3, $a1, $a4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyv $a5 = $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyv $a6 = $a0
; CHECK-NEXT:    fmma242hw2 $a5_lo = $a2a3, $a1, $a4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmma242hw3 $a6_hi = $a2a3, $a5, $a4
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 32[$r0] = $a5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 64[$r0] = $a6
; CHECK-NEXT:    addd $r0 = $r1, 64
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a1 = 32[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a0 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmma242hw2 $a3_lo = $a0a1, $a6, $a5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmma242hw3 $a3_hi = $a0a1, $a6, $a5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmma242hw0 $a2_lo = $a0a1, $a6, $a5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmma242hw1 $a2_hi = $a0a1, $a6, $a5
; CHECK-NEXT:    sv 224[$r1] = $a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 192[$r1] = $a2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %0 = load <256 x i1>, <256 x i1>* %v, align 32
  %1 = load <512 x i1>, <512 x i1>* %w, align 64
  %arrayidx2 = getelementptr inbounds <256 x i1>, <256 x i1>* %v, i64 1
  %2 = load <256 x i1>, <256 x i1>* %arrayidx2, align 32
  %arrayidx3 = getelementptr inbounds <256 x i1>, <256 x i1>* %v, i64 2
  %3 = load <256 x i1>, <256 x i1>* %arrayidx3, align 32
  %4 = tail call <256 x i1> @llvm.kvx.fmma242hw0(<256 x i1> %0, <512 x i1> %1, <256 x i1> %2, <256 x i1> %3)
  %5 = tail call <256 x i1> @llvm.kvx.fmma242hw1(<256 x i1> %4, <512 x i1> %1, <256 x i1> %2, <256 x i1> %3)
  %6 = tail call <256 x i1> @llvm.kvx.fmma242hw2(<256 x i1> %5, <512 x i1> %1, <256 x i1> %2, <256 x i1> %3)
  store <256 x i1> %6, <256 x i1>* %arrayidx2, align 32
  %7 = tail call <256 x i1> @llvm.kvx.fmma242hw3(<256 x i1> %5, <512 x i1> %1, <256 x i1> %6, <256 x i1> %3)
  store <256 x i1> %7, <256 x i1>* %arrayidx3, align 32
  store <256 x i1> %5, <256 x i1>* %v, align 32
  %arrayidx16 = getelementptr inbounds <512 x i1>, <512 x i1>* %w, i64 1
  %8 = load <512 x i1>, <512 x i1>* %arrayidx16, align 64
  %9 = tail call <512 x i1> @llvm.kvx.fmma242hw(<512 x i1> %8, <256 x i1> %7, <256 x i1> %6)
  %arrayidx19 = getelementptr inbounds <512 x i1>, <512 x i1>* %w, i64 3
  store <512 x i1> %9, <512 x i1>* %arrayidx19, align 64
  ret void
}

define void @test(<256 x i1>* nocapture %v) {
; CHECK-LABEL: test:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    lv $a0 = 0[$r0]
; CHECK-NEXT:    make $r1 = 0
; CHECK-NEXT:    make $r2 = 1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    movetq $a0_hi = $r2, $r1
; CHECK-NEXT:    movetq $a0_lo = $r2, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %0 = load <256 x i1>, <256 x i1>* %v, align 32
  %1 = tail call <256 x i1> @llvm.kvx.movetohi(<256 x i1> %0, i64 1, i64 0)
  %2 = tail call <256 x i1> @llvm.kvx.movetolo(<256 x i1> %1, i64 1, i64 0)
  store <256 x i1> %2, <256 x i1>* %v, align 32
  ret void
}

define void @insertwm(<1024 x i1>* nocapture %a0, <512 x i1>* nocapture readonly %a1) {
; CHECK-LABEL: insertwm:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    lv $a3 = 96[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a2 = 64[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a1 = 32[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a0 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a1 = 32[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a0 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyv $a2 = $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyv $a3 = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 96[$r0] = $a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 64[$r0] = $a2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 32[$r0] = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %0 = load <1024 x i1>, <1024 x i1>* %a0, align 128
  %1 = load <512 x i1>, <512 x i1>* %a1, align 64
  %2 = tail call <1024 x i1> @llvm.kvx.insertwm(<1024 x i1> %0, <512 x i1> %1, i32 0)
  %3 = tail call <1024 x i1> @llvm.kvx.insertwm(<1024 x i1> %2, <512 x i1> %1, i32 1)
  store <1024 x i1> %3, <1024 x i1>* %a0, align 128
  ret void
}

declare <1024 x i1> @llvm.kvx.insertwm(<1024 x i1>, <512 x i1>, i32) #1

define void @insertvm(<1024 x i1>* nocapture %a0, <256 x i1>* nocapture readonly %a1) {
; CHECK-LABEL: insertvm:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    lv $a3 = 96[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a2 = 64[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a1 = 32[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a0 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a0 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyv $a1 = $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyv $a2 = $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyv $a3 = $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 96[$r0] = $a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 64[$r0] = $a2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 32[$r0] = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %0 = load <1024 x i1>, <1024 x i1>* %a0, align 128
  %1 = load <256 x i1>, <256 x i1>* %a1, align 32
  %2 = tail call <1024 x i1> @llvm.kvx.insertvm(<1024 x i1> %0, <256 x i1> %1, i32 0)
  %3 = tail call <1024 x i1> @llvm.kvx.insertvm(<1024 x i1> %2, <256 x i1> %1, i32 1)
  %4 = tail call <1024 x i1> @llvm.kvx.insertvm(<1024 x i1> %3, <256 x i1> %1, i32 2)
  %5 = tail call <1024 x i1> @llvm.kvx.insertvm(<1024 x i1> %4, <256 x i1> %1, i32 3)
  store <1024 x i1> %5, <1024 x i1>* %a0, align 128
  ret void
}

declare <1024 x i1> @llvm.kvx.insertvm(<1024 x i1>, <256 x i1>, i32) #1

define void @insertvw(<512 x i1>* nocapture %a0, <256 x i1>* nocapture readonly %a1) {
; CHECK-LABEL: insertvw:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    lv $a1 = 32[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a0 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a0 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyv $a1 = $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 32[$r0] = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %0 = load <512 x i1>, <512 x i1>* %a0, align 64
  %1 = load <256 x i1>, <256 x i1>* %a1, align 32
  %2 = tail call <512 x i1> @llvm.kvx.insertvw(<512 x i1> %0, <256 x i1> %1, i32 0)
  %3 = tail call <512 x i1> @llvm.kvx.insertvw(<512 x i1> %2, <256 x i1> %1, i32 1)
  store <512 x i1> %3, <512 x i1>* %a0, align 64
  ret void
}

declare <512 x i1> @llvm.kvx.insertvw(<512 x i1>, <256 x i1>, i32) #1

define void @movefmw(<512 x i1>* nocapture %o, <1024 x i1>* nocapture readonly %a0) {
; CHECK-LABEL: movefmw:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    lv $a3 = 96[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a2 = 64[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a1 = 32[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a0 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 32[$r0] = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 96[$r0] = $a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 64[$r0] = $a2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %0 = load <1024 x i1>, <1024 x i1>* %a0, align 128
  %1 = tail call <512 x i1> @llvm.kvx.movefmw(<1024 x i1> %0, i32 0)
  store <512 x i1> %1, <512 x i1>* %o, align 64
  %2 = tail call <512 x i1> @llvm.kvx.movefmw(<1024 x i1> %0, i32 1)
  %arrayidx3 = getelementptr inbounds <512 x i1>, <512 x i1>* %o, i64 1
  store <512 x i1> %2, <512 x i1>* %arrayidx3, align 64
  ret void
}

declare <512 x i1> @llvm.kvx.movefmw(<1024 x i1>, i32) #1

define void @movefmv(<256 x i1>* nocapture %o, <1024 x i1>* nocapture readonly %a0) {
; CHECK-LABEL: movefmv:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    lv $a3 = 96[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a2 = 64[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a1 = 32[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a0 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 32[$r0] = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 64[$r0] = $a2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 96[$r0] = $a3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %0 = load <1024 x i1>, <1024 x i1>* %a0, align 128
  %1 = tail call <256 x i1> @llvm.kvx.movefmv(<1024 x i1> %0, i32 0)
  store <256 x i1> %1, <256 x i1>* %o, align 32
  %2 = tail call <256 x i1> @llvm.kvx.movefmv(<1024 x i1> %0, i32 1)
  %arrayidx3 = getelementptr inbounds <256 x i1>, <256 x i1>* %o, i64 1
  store <256 x i1> %2, <256 x i1>* %arrayidx3, align 32
  %3 = tail call <256 x i1> @llvm.kvx.movefmv(<1024 x i1> %0, i32 2)
  %arrayidx5 = getelementptr inbounds <256 x i1>, <256 x i1>* %o, i64 2
  store <256 x i1> %3, <256 x i1>* %arrayidx5, align 32
  %4 = tail call <256 x i1> @llvm.kvx.movefmv(<1024 x i1> %0, i32 3)
  %arrayidx7 = getelementptr inbounds <256 x i1>, <256 x i1>* %o, i64 3
  store <256 x i1> %4, <256 x i1>* %arrayidx7, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.movefmv(<1024 x i1>, i32) #1

define void @movefwv(<256 x i1>* nocapture %o, <512 x i1>* nocapture readonly %a0) {
; CHECK-LABEL: movefwv:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    lv $a1 = 32[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a0 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 32[$r0] = $a1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %0 = load <512 x i1>, <512 x i1>* %a0, align 64
  %1 = tail call <256 x i1> @llvm.kvx.movefwv(<512 x i1> %0, i32 0)
  store <256 x i1> %1, <256 x i1>* %o, align 32
  %2 = tail call <256 x i1> @llvm.kvx.movefwv(<512 x i1> %0, i32 1)
  %arrayidx3 = getelementptr inbounds <256 x i1>, <256 x i1>* %o, i64 1
  store <256 x i1> %2, <256 x i1>* %arrayidx3, align 32
  ret void
}

declare <256 x i1> @llvm.kvx.movefwv(<512 x i1>, i32) #1

define void @buildfvm(<256 x i1>* nocapture readonly %a, <1024 x i1>* nocapture %M) {
; CHECK-LABEL: buildfvm:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    lv $a0 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a1 = 64[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a3 = 32[$r0]
; CHECK-NEXT:    copyv $a2 = $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 96[$r1] = $a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 64[$r1] = $a2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 32[$r1] = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r1] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %0 = load <256 x i1>, <256 x i1>* %a, align 32
  %arrayidx1 = getelementptr inbounds <256 x i1>, <256 x i1>* %a, i64 2
  %1 = load <256 x i1>, <256 x i1>* %arrayidx1, align 32
  %arrayidx3 = getelementptr inbounds <256 x i1>, <256 x i1>* %a, i64 1
  %2 = load <256 x i1>, <256 x i1>* %arrayidx3, align 32
  %3 = tail call <1024 x i1> @llvm.kvx.buildfvm(<256 x i1> %0, <256 x i1> %1, <256 x i1> %0, <256 x i1> %2)
  store <1024 x i1> %3, <1024 x i1>* %M, align 128
  ret void
}

declare <1024 x i1> @llvm.kvx.buildfvm(<256 x i1>, <256 x i1>, <256 x i1>, <256 x i1>) #1

define void @buildfwm(<512 x i1>* nocapture readonly %a, <1024 x i1>* nocapture %M) {
; CHECK-LABEL: buildfwm:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r2 = $r0, 128
; CHECK-NEXT:    addd $r0 = $r0, 64
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a1 = 32[$r2]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a0 = 0[$r2]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyv $a2 = $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyv $a3 = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 224[$r1] = $a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 192[$r1] = $a2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 160[$r1] = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 128[$r1] = $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a3 = 32[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a2 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 96[$r1] = $a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 64[$r1] = $a2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 32[$r1] = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r1] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %arrayidx = getelementptr inbounds <512 x i1>, <512 x i1>* %a, i64 2
  %0 = load <512 x i1>, <512 x i1>* %arrayidx, align 64
  %1 = tail call <1024 x i1> @llvm.kvx.buildfwm(<512 x i1> %0, <512 x i1> %0)
  %arrayidx2 = getelementptr inbounds <1024 x i1>, <1024 x i1>* %M, i64 1
  store <1024 x i1> %1, <1024 x i1>* %arrayidx2, align 128
  %arrayidx4 = getelementptr inbounds <512 x i1>, <512 x i1>* %a, i64 1
  %2 = load <512 x i1>, <512 x i1>* %arrayidx4, align 64
  %3 = tail call <1024 x i1> @llvm.kvx.buildfwm(<512 x i1> %0, <512 x i1> %2)
  store <1024 x i1> %3, <1024 x i1>* %M, align 128
  ret void
}

declare <1024 x i1> @llvm.kvx.buildfwm(<512 x i1>, <512 x i1>) #1

define void @buildfvw(<256 x i1>* nocapture readonly %a, <512 x i1>* nocapture %W) {
; CHECK-LABEL: buildfvw:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    lv $a0 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a1 = 64[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 32[$r1] = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r1] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %0 = load <256 x i1>, <256 x i1>* %a, align 32
  %arrayidx1 = getelementptr inbounds <256 x i1>, <256 x i1>* %a, i64 2
  %1 = load <256 x i1>, <256 x i1>* %arrayidx1, align 32
  %2 = tail call <512 x i1> @llvm.kvx.buildfvw(<256 x i1> %0, <256 x i1> %1)
  store <512 x i1> %2, <512 x i1>* %W, align 64
  ret void
}

declare <512 x i1> @llvm.kvx.buildfvw(<256 x i1>, <256 x i1>) #1
