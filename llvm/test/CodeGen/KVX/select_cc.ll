; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -O2 | FileCheck %s
target triple = "kvx-kalray-cos"

define i32 @f(i32 %a){
; CHECK-LABEL: f:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    copyw $r1 = $r0
; CHECK-NEXT:    make $r0 = 5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    cmoved.wlez $r1 ? $r0 = 7
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %cmp = icmp sgt i32 %a, 0
  %cond = select i1 %cmp, i32 5, i32 7
  ret i32 %cond
}

define i32 @f_select_cc_i32(i32 %c, i32 %c2, i32 %a, i32 %b){
; CHECK-LABEL: f_select_cc_i32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    compw.gt $r0 = $r0, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    cmoved.wnez $r0 ? $r3 = $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r0 = $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %cmp = icmp sgt i32 %c, %c2
  %cond = select i1 %cmp, i32 %a, i32 %b
  ret i32 %cond
}

define i32 @f_select_cc_i64(i64 %c, i64 %c2, i32 %a, i32 %b){
; CHECK-LABEL: f_select_cc_i64:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    compd.gt $r0 = $r0, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    cmoved.wnez $r0 ? $r3 = $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r0 = $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %cmp = icmp sgt i64 %c, %c2
  %cond = select i1 %cmp, i32 %a, i32 %b
  ret i32 %cond
}

define <4 x half> @f_Select32PAT(<4 x half> %x, <4 x half> %y){
; CHECK-LABEL: f_Select32PAT:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    extfz $r2 = $r0, 15, 0
; CHECK-NEXT:    extfz $r3 = $r1, 15, 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r4 = $r0, 31, 16
; CHECK-NEXT:    extfz $r5 = $r1, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    extfz $r6 = $r0, 47, 32
; CHECK-NEXT:    extfz $r7 = $r1, 47, 32
; CHECK-NEXT:    srld $r0 = $r0, 48
; CHECK-NEXT:    srld $r1 = $r1, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fcompnhq.olt $r8 = $r2, $r3
; CHECK-NEXT:    fcompnhq.olt $r9 = $r4, $r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fcompnhq.olt $r10 = $r6, $r7
; CHECK-NEXT:    fcompnhq.olt $r11 = $r0, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    cmoved.wnez $r8 ? $r2 = $r3
; CHECK-NEXT:    cmoved.wnez $r9 ? $r4 = $r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    cmoved.wnez $r10 ? $r6 = $r7
; CHECK-NEXT:    cmoved.wnez $r11 ? $r0 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r6 = $r0, 31, 16
; CHECK-NEXT:    insf $r2 = $r4, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r2 = $r6, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r0 = $r2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %vecext = extractelement <4 x half> %x, i32 0
  %vecext1 = extractelement <4 x half> %y, i32 0
  %cmp = fcmp olt half %vecext, %vecext1
  %cond = select i1 %cmp, half %vecext1, half %vecext
  %vecins = insertelement <4 x half> undef, half %cond, i32 0
  %vecext4 = extractelement <4 x half> %x, i32 1
  %vecext5 = extractelement <4 x half> %y, i32 1
  %cmp6 = fcmp olt half %vecext4, %vecext5
  %cond12 = select i1 %cmp6, half %vecext5, half %vecext4
  %vecins13 = insertelement <4 x half> %vecins, half %cond12, i32 1
  %vecext14 = extractelement <4 x half> %x, i32 2
  %vecext15 = extractelement <4 x half> %y, i32 2
  %cmp16 = fcmp olt half %vecext14, %vecext15
  %cond22 = select i1 %cmp16, half %vecext15, half %vecext14
  %vecins23 = insertelement <4 x half> %vecins13, half %cond22, i32 2
  %vecext24 = extractelement <4 x half> %x, i32 3
  %vecext25 = extractelement <4 x half> %y, i32 3
  %cmp26 = fcmp olt half %vecext24, %vecext25
  %cond32 = select i1 %cmp26, half %vecext25, half %vecext24
  %vecins33 = insertelement <4 x half> %vecins23, half %cond32, i32 3
  ret <4 x half> %vecins33
}

define half @f_Select64PAT(i64 %c, half %a, half %b){
; CHECK-LABEL: f_Select64PAT:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    cmoved.deqz $r0 ? $r2 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r0 = $r2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %cmp = icmp eq i64 %c, 0
  %cond = select i1 %cmp, half %a, half %b
  ret half %cond
}

define <2 x float> @f_select_cc_v2f32(i32 %c, i32 %c2, <2 x float> %a, <2 x float> %b){
; CHECK-LABEL: f_select_cc_v2f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    compw.gt $r0 = $r0, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyw $r1 = $r0
; CHECK-NEXT:    make $r0 = -1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    cmoved.weqz $r1 ? $r0 = 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r0 = $r0, 63, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    andnd $r1 = $r0, $r3
; CHECK-NEXT:    andd $r0 = $r2, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ord $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %cmp = icmp sgt i32 %c, %c2
  %cond = select i1 %cmp, <2 x float> %a, <2 x float> %b
  ret <2 x float> %cond
}

define void @test_select_vector_reg(<256 x i1> * %V, i1 %cc){
; CHECK-LABEL: test_select_vector_reg:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lv $a1 = 0[$r0]
; CHECK-NEXT:    sllw $r1 = $r1, 6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a0 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    alignv $a0 = $a0, $a1, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %v0 = load volatile <256 x i1>, <256 x i1>* %V, align 32
  %v1 = load volatile <256 x i1>, <256 x i1>* %V, align 32
  %v3 = select i1 %cc, <256 x i1> %v0, <256 x i1> %v1
  store volatile <256 x i1> %v3, <256 x i1>* %V, align 32
  ret void
}

define void @test_select_wide_reg(<512 x i1> * %V, i1 %cc){
; CHECK-LABEL: test_select_wide_reg:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lv $a0 = 32[$r0]
; CHECK-NEXT:    sllw $r1 = $r1, 6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a1 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a3 = 32[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a2 = 0[$r0]
; CHECK-NEXT:    alignv $a5 = $a3, $a0, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    alignv $a4 = $a2, $a1, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 32[$r0] = $a5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a4
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %v0 = load volatile <512 x i1>, <512 x i1>* %V, align 32
  %v1 = load volatile <512 x i1>, <512 x i1>* %V, align 32
  %v3 = select i1 %cc, <512 x i1> %v0, <512 x i1> %v1
  store volatile <512 x i1> %v3, <512 x i1>* %V, align 32
  ret void
}

; TODO: Wide and load matrix should be expanded
; before scheduling when possible, to allow lv
; and alignv on the same bundle
define void @test_select_matrix_reg(<1024 x i1> * %V, i1 %cc){
; CHECK-LABEL: test_select_matrix_reg:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lv $a0 = 96[$r0]
; CHECK-NEXT:    sllw $r1 = $r1, 6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a1 = 64[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a2 = 32[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a3 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a7 = 96[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a6 = 64[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a5 = 32[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a4 = 0[$r0]
; CHECK-NEXT:    alignv $a11 = $a7, $a0, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    alignv $a10 = $a6, $a1, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    alignv $a9 = $a5, $a2, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    alignv $a8 = $a4, $a3, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 96[$r0] = $a11
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 64[$r0] = $a10
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 32[$r0] = $a9
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r0] = $a8
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %v0 = load volatile <1024 x i1>, <1024 x i1>* %V, align 32
  %v1 = load volatile <1024 x i1>, <1024 x i1>* %V, align 32
  %v3 = select i1 %cc, <1024 x i1> %v0, <1024 x i1> %v1
  store volatile <1024 x i1> %v3, <1024 x i1>* %V, align 32
  ret void
}
