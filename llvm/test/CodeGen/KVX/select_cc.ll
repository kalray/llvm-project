; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -o - %s -O2 | FileCheck %s --check-prefixes=CHECK,CV1
; RUN: llc -mcpu=kv3-2 -o - %s -O2 | FileCheck %s --check-prefixes=CHECK,CV2
target triple = "kvx-kalray-cos"

define i32 @f(i32 %a){
; CHECK-LABEL: f:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    make $r0 = 5
; CHECK-NEXT:    copyw $r1 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    cmoved.wlez $r1 ? $r0 = 7
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %cmp = icmp sgt i32 %a, 0
  %cond = select i1 %cmp, i32 5, i32 7
  ret i32 %cond
}

define i32 @f_select_cc_i32(i32 %c, i32 %c2, i32 %a, i32 %b){
; CHECK-LABEL: f_select_cc_i32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    compw.gt $r0 = $r0, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    cmoved.wnez $r0 ? $r3 = $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r0 = $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %cmp = icmp sgt i32 %c, %c2
  %cond = select i1 %cmp, i32 %a, i32 %b
  ret i32 %cond
}

define i32 @f_select_cc_i64(i64 %c, i64 %c2, i32 %a, i32 %b){
; CHECK-LABEL: f_select_cc_i64:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    compd.gt $r0 = $r0, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    cmoved.wnez $r0 ? $r3 = $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r0 = $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %cmp = icmp sgt i64 %c, %c2
  %cond = select i1 %cmp, i32 %a, i32 %b
  ret i32 %cond
}

define <4 x half> @f_Select32PAT(<4 x half> %x, <4 x half> %y){
; CHECK-LABEL: f_Select32PAT:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    srlw $r3 = $r0, 16
; CHECK-NEXT:    srlw $r4 = $r1, 16
; CHECK-NEXT:    srld $r5 = $r0, 32
; CHECK-NEXT:    srld $r6 = $r1, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fcompnhq.olt $r2 = $r0, $r1
; CHECK-NEXT:    fcompnhq.olt $r7 = $r3, $r4
; CHECK-NEXT:    srld $r8 = $r0, 48
; CHECK-NEXT:    srld $r9 = $r1, 48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    andw $r2 = $r2, 1
; CHECK-NEXT:    andw $r7 = $r7, 1
; CHECK-NEXT:    fcompnhq.olt $r10 = $r5, $r6
; CHECK-NEXT:    fcompnhq.olt $r11 = $r8, $r9
; CHECK-NEXT:    ;;
; CHECK-NEXT:    cmoved.wnez $r2 ? $r0 = $r1
; CHECK-NEXT:    cmoved.wnez $r7 ? $r3 = $r4
; CHECK-NEXT:    andw $r10 = $r10, 1
; CHECK-NEXT:    andw $r11 = $r11, 1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    cmoved.wnez $r10 ? $r5 = $r6
; CHECK-NEXT:    cmoved.wnez $r11 ? $r8 = $r9
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r0 = $r3, 31, 16
; CHECK-NEXT:    insf $r5 = $r8, 31, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    insf $r0 = $r5, 63, 32
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %vecext = extractelement <4 x half> %x, i32 0
  %vecext1 = extractelement <4 x half> %y, i32 0
  %cmp = fcmp olt half %vecext, %vecext1
  %cond = select i1 %cmp, half %vecext1, half %vecext
  %vecins = insertelement <4 x half> undef, half %cond, i32 0
  %vecext4 = extractelement <4 x half> %x, i32 1
  %vecext5 = extractelement <4 x half> %y, i32 1
  %cmp6 = fcmp olt half %vecext4, %vecext5
  %cond12 = select i1 %cmp6, half %vecext5, half %vecext4
  %vecins13 = insertelement <4 x half> %vecins, half %cond12, i32 1
  %vecext14 = extractelement <4 x half> %x, i32 2
  %vecext15 = extractelement <4 x half> %y, i32 2
  %cmp16 = fcmp olt half %vecext14, %vecext15
  %cond22 = select i1 %cmp16, half %vecext15, half %vecext14
  %vecins23 = insertelement <4 x half> %vecins13, half %cond22, i32 2
  %vecext24 = extractelement <4 x half> %x, i32 3
  %vecext25 = extractelement <4 x half> %y, i32 3
  %cmp26 = fcmp olt half %vecext24, %vecext25
  %cond32 = select i1 %cmp26, half %vecext25, half %vecext24
  %vecins33 = insertelement <4 x half> %vecins23, half %cond32, i32 3
  ret <4 x half> %vecins33
}

define half @f_Select64PAT(i64 %c, half %a, half %b){
; CHECK-LABEL: f_Select64PAT:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    cmoved.deqz $r0 ? $r2 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r0 = $r2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %cmp = icmp eq i64 %c, 0
  %cond = select i1 %cmp, half %a, half %b
  ret half %cond
}

define <2 x float> @f_select_cc_v2f32(i32 %c, i32 %c2, <2 x float> %a, <2 x float> %b){
; CHECK-LABEL: f_select_cc_v2f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    compw.gt $r0 = $r0, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    cmoved.even $r0 ? $r2 = $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r0 = $r2
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %cmp = icmp sgt i32 %c, %c2
  %cond = select i1 %cmp, <2 x float> %a, <2 x float> %b
  ret <2 x float> %cond
}

define void @test_select_vector_reg(<256 x i1> * %V, i1 %cc){
; CV1-LABEL: test_select_vector_reg:
; CV1:       # %bb.0:
; CV1-NEXT:    lv $a1 = 0[$r0]
; CV1-NEXT:    andw $r1 = $r1, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    lv $a0 = 0[$r0]
; CV1-NEXT:    sllw $r1 = $r1, 6
; CV1-NEXT:    ;;
; CV1-NEXT:    alignv $a0 = $a0, $a1, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    sv 0[$r0] = $a0
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: test_select_vector_reg:
; CV2:       # %bb.0:
; CV2-NEXT:    xlo $a1 = 0[$r0]
; CV2-NEXT:    andw $r1 = $r1, 1
; CV2-NEXT:    ;;
; CV2-NEXT:    xlo $a0 = 0[$r0]
; CV2-NEXT:    sllw $r1 = $r1, 6
; CV2-NEXT:    ;;
; CV2-NEXT:    alignv $a0 = $a0, $a1, $r1
; CV2-NEXT:    ;;
; CV2-NEXT:    xso 0[$r0] = $a0
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %v0 = load volatile <256 x i1>, <256 x i1>* %V, align 32
  %v1 = load volatile <256 x i1>, <256 x i1>* %V, align 32
  %v3 = select i1 %cc, <256 x i1> %v0, <256 x i1> %v1
  store volatile <256 x i1> %v3, <256 x i1>* %V, align 32
  ret void
}

define void @test_select_wide_reg(<512 x i1> * %V, i1 %cc){
; CV1-LABEL: test_select_wide_reg:
; CV1:       # %bb.0:
; CV1-NEXT:    lv $a0 = 32[$r0]
; CV1-NEXT:    andw $r1 = $r1, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    lv $a1 = 0[$r0]
; CV1-NEXT:    sllw $r1 = $r1, 6
; CV1-NEXT:    ;;
; CV1-NEXT:    lv $a3 = 32[$r0]
; CV1-NEXT:    ;;
; CV1-NEXT:    lv $a2 = 0[$r0]
; CV1-NEXT:    ;;
; CV1-NEXT:    alignv $a3 = $a3, $a0, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    alignv $a0 = $a2, $a1, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    sv 32[$r0] = $a3
; CV1-NEXT:    ;;
; CV1-NEXT:    sv 0[$r0] = $a0
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: test_select_wide_reg:
; CV2:       # %bb.0:
; CV2-NEXT:    xlo $a0 = 32[$r0]
; CV2-NEXT:    andw $r1 = $r1, 1
; CV2-NEXT:    ;;
; CV2-NEXT:    xlo $a1 = 0[$r0]
; CV2-NEXT:    sllw $r1 = $r1, 6
; CV2-NEXT:    ;;
; CV2-NEXT:    xlo $a3 = 32[$r0]
; CV2-NEXT:    ;;
; CV2-NEXT:    xlo $a2 = 0[$r0]
; CV2-NEXT:    ;;
; CV2-NEXT:    alignv $a3 = $a3, $a0, $r1
; CV2-NEXT:    ;;
; CV2-NEXT:    alignv $a0 = $a2, $a1, $r1
; CV2-NEXT:    ;;
; CV2-NEXT:    xso 32[$r0] = $a3
; CV2-NEXT:    ;;
; CV2-NEXT:    xso 0[$r0] = $a0
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %v0 = load volatile <512 x i1>, <512 x i1>* %V, align 32
  %v1 = load volatile <512 x i1>, <512 x i1>* %V, align 32
  %v3 = select i1 %cc, <512 x i1> %v0, <512 x i1> %v1
  store volatile <512 x i1> %v3, <512 x i1>* %V, align 32
  ret void
}

; TODO: Wide and load matrix should be expanded
; before scheduling when possible, to allow lv
; and alignv on the same bundle
define void @test_select_matrix_reg(<1024 x i1> * %V, i1 %cc){
; CV1-LABEL: test_select_matrix_reg:
; CV1:       # %bb.0:
; CV1-NEXT:    lv $a0 = 96[$r0]
; CV1-NEXT:    andw $r1 = $r1, 1
; CV1-NEXT:    ;;
; CV1-NEXT:    lv $a1 = 64[$r0]
; CV1-NEXT:    sllw $r1 = $r1, 6
; CV1-NEXT:    ;;
; CV1-NEXT:    lv $a2 = 32[$r0]
; CV1-NEXT:    ;;
; CV1-NEXT:    lv $a3 = 0[$r0]
; CV1-NEXT:    ;;
; CV1-NEXT:    lv $a7 = 96[$r0]
; CV1-NEXT:    ;;
; CV1-NEXT:    lv $a6 = 64[$r0]
; CV1-NEXT:    ;;
; CV1-NEXT:    lv $a5 = 32[$r0]
; CV1-NEXT:    ;;
; CV1-NEXT:    lv $a0 = 0[$r0]
; CV1-NEXT:    alignv $a7 = $a7, $a0, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    alignv $a6 = $a6, $a1, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    alignv $a1 = $a5, $a2, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    alignv $a0 = $a0, $a3, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    sv 96[$r0] = $a7
; CV1-NEXT:    ;;
; CV1-NEXT:    sv 64[$r0] = $a6
; CV1-NEXT:    ;;
; CV1-NEXT:    sv 32[$r0] = $a1
; CV1-NEXT:    ;;
; CV1-NEXT:    sv 0[$r0] = $a0
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: test_select_matrix_reg:
; CV2:       # %bb.0:
; CV2-NEXT:    xlo $a0 = 96[$r0]
; CV2-NEXT:    andw $r1 = $r1, 1
; CV2-NEXT:    ;;
; CV2-NEXT:    xlo $a1 = 64[$r0]
; CV2-NEXT:    sllw $r1 = $r1, 6
; CV2-NEXT:    ;;
; CV2-NEXT:    xlo $a2 = 32[$r0]
; CV2-NEXT:    ;;
; CV2-NEXT:    xlo $a3 = 0[$r0]
; CV2-NEXT:    ;;
; CV2-NEXT:    xlo $a7 = 96[$r0]
; CV2-NEXT:    ;;
; CV2-NEXT:    xlo $a6 = 64[$r0]
; CV2-NEXT:    ;;
; CV2-NEXT:    xlo $a5 = 32[$r0]
; CV2-NEXT:    ;;
; CV2-NEXT:    xlo $a4 = 0[$r0]
; CV2-NEXT:    alignv $a7 = $a7, $a0, $r1
; CV2-NEXT:    ;;
; CV2-NEXT:    alignv $a6 = $a6, $a1, $r1
; CV2-NEXT:    ;;
; CV2-NEXT:    alignv $a1 = $a5, $a2, $r1
; CV2-NEXT:    ;;
; CV2-NEXT:    xso 96[$r0] = $a7
; CV2-NEXT:    alignv $a0 = $a4, $a3, $r1
; CV2-NEXT:    ;;
; CV2-NEXT:    xso 64[$r0] = $a6
; CV2-NEXT:    ;;
; CV2-NEXT:    xso 32[$r0] = $a1
; CV2-NEXT:    ;;
; CV2-NEXT:    xso 0[$r0] = $a0
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %v0 = load volatile <1024 x i1>, <1024 x i1>* %V, align 32
  %v1 = load volatile <1024 x i1>, <1024 x i1>* %V, align 32
  %v3 = select i1 %cc, <1024 x i1> %v0, <1024 x i1> %v1
  store volatile <1024 x i1> %v3, <1024 x i1>* %V, align 32
  ret void
}
