; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mcpu=kv3-2 -O2 -o - %s | FileCheck %s --check-prefixes=KV3_2
; RUN: clang -O2 -march=kv3-2 -c -o /dev/null %s

target triple = "kvx-kalray-cos"

define void @loadons(ptr noundef %a, i32 noundef %n, i32 noundef %i) {
; KV3_2-LABEL: loadons:
; KV3_2:       # %bb.0: # %entry
; KV3_2-NEXT:    mulw $r3 = $r1, 3
; KV3_2-NEXT:    sxwd $r4 = $r1
; KV3_2-NEXT:    sllw $r8 = $r1, 1
; KV3_2-NEXT:    addd $r12 = $r12, -96
; KV3_2-NEXT:    ;; # (end cycle 0)
; KV3_2-NEXT:    so 64[$r12] = $r28r29r30r31
; KV3_2-NEXT:    slld $r4 = $r4, 2
; KV3_2-NEXT:    mulw $r5 = $r1, 5
; KV3_2-NEXT:    sxwd $r11 = $r8
; KV3_2-NEXT:    ;; # (end cycle 1)
; KV3_2-NEXT:    so 32[$r12] = $r24r25r26r27
; KV3_2-NEXT:    mulw $r6 = $r1, 6
; KV3_2-NEXT:    slld $r11 = $r11, 2
; KV3_2-NEXT:    sxwd $r17 = $r3
; KV3_2-NEXT:    ;; # (end cycle 2)
; KV3_2-NEXT:    so 0[$r12] = $r20r21r22r23
; KV3_2-NEXT:    sllw $r7 = $r1, 2
; KV3_2-NEXT:    maxw $r10 = $r2, 0
; KV3_2-NEXT:    sxwd $r34 = $r5
; KV3_2-NEXT:    ;; # (end cycle 3)
; KV3_2-NEXT:    xlo $a2 = 0[$r0]
; KV3_2-NEXT:    zxwd $r10 = $r10
; KV3_2-NEXT:    sxwd $r16 = $r7
; KV3_2-NEXT:    make $r35 = 0
; KV3_2-NEXT:    ;; # (end cycle 4)
; KV3_2-NEXT:    xlo $a3 = $r4[$r0]
; KV3_2-NEXT:    mulw $r4 = $r1, 7
; KV3_2-NEXT:    sxwd $r33 = $r6
; KV3_2-NEXT:    slld $r45 = $r34, 2
; KV3_2-NEXT:    ;; # (end cycle 5)
; KV3_2-NEXT:    xlo $a0 = $r11[$r0]
; KV3_2-NEXT:    slld $r11 = $r17, 2
; KV3_2-NEXT:    make $r34 = 0
; KV3_2-NEXT:    slld $r44 = $r33, 2
; KV3_2-NEXT:    ;; # (end cycle 6)
; KV3_2-NEXT:    xlo $a1 = $r11[$r0]
; KV3_2-NEXT:    sxwd $r32 = $r4
; KV3_2-NEXT:    slld $r46 = $r16, 2
; KV3_2-NEXT:    slld $r47 = $r10, 5
; KV3_2-NEXT:    ;; # (end cycle 7)
; KV3_2-NEXT:    copyq $r32r33 = $r34, $r35
; KV3_2-NEXT:    addx4wd $r9 = $r1, 32
; KV3_2-NEXT:    addx4wd $r15 = $r8, 32
; KV3_2-NEXT:    slld $r39 = $r32, 2
; KV3_2-NEXT:    ;; # (end cycle 8)
; KV3_2-NEXT:    copyq $r16r17 = $r34, $r35
; KV3_2-NEXT:    addx4wd $r38 = $r3, 32
; KV3_2-NEXT:    make $r52 = 32
; KV3_2-NEXT:    xcopyx.zd $a2a3 = $a2a3
; KV3_2-NEXT:    ;; # (end cycle 9)
; KV3_2-NEXT:    copyq $r10r11 = $r34, $r35
; KV3_2-NEXT:    ;; # (end cycle 10)
; KV3_2-NEXT:    copyq $r36r37 = $r34, $r35
; KV3_2-NEXT:    ;; # (end cycle 11)
; KV3_2-NEXT:    copyq $r40r41 = $r34, $r35
; KV3_2-NEXT:    ;; # (end cycle 12)
; KV3_2-NEXT:    copyq $r42r43 = $r34, $r35
; KV3_2-NEXT:    xmovefo $r60r61r62r63 = $a3
; KV3_2-NEXT:    ;; # (end cycle 13)
; KV3_2-NEXT:    copyq $r48r49 = $r34, $r35
; KV3_2-NEXT:    xmovefo $r20r21r22r23 = $a2
; KV3_2-NEXT:    ;; # (end cycle 14)
; KV3_2-NEXT:    copyq $r50r51 = $r34, $r35
; KV3_2-NEXT:    ;; # (end cycle 15)
; KV3_2-NEXT:    copyq $r54r55 = $r34, $r35
; KV3_2-NEXT:    ;; # (end cycle 16)
; KV3_2-NEXT:    copyq $r56r57 = $r34, $r35
; KV3_2-NEXT:    compd.ne $r35 = $r47, $r34
; KV3_2-NEXT:    ;; # (end cycle 17)
; KV3_2-NEXT:    cb.even $r35 ? .LBB0_2
; KV3_2-NEXT:    ;;
; KV3_2-NEXT:  .LBB0_1: # %for.body
; KV3_2-NEXT:    # =>This Inner Loop Header: Depth=1
; KV3_2-NEXT:    fmma222w.nt $r56r57 = $r20r21, $r20r21
; KV3_2-NEXT:    addd $r34 = $r34, 32
; KV3_2-NEXT:    xcopyx.zd $a0a1 = $a0a1
; KV3_2-NEXT:    ;; # (end cycle 0)
; KV3_2-NEXT:    fmma222w.nt $r54r55 = $r22r23, $r20r21
; KV3_2-NEXT:    compd.ne $r35 = $r47, $r34
; KV3_2-NEXT:    ;; # (end cycle 1)
; KV3_2-NEXT:    fmma222w.nt $r50r51 = $r22r23, $r22r23
; KV3_2-NEXT:    ;; # (end cycle 2)
; KV3_2-NEXT:    fmma222w.nt $r48r49 = $r60r61, $r20r21
; KV3_2-NEXT:    ;; # (end cycle 3)
; KV3_2-NEXT:    fmma222w.nt $r42r43 = $r60r61, $r22r23
; KV3_2-NEXT:    ;; # (end cycle 4)
; KV3_2-NEXT:    fmma222w.nt $r40r41 = $r60r61, $r60r61
; KV3_2-NEXT:    ;; # (end cycle 5)
; KV3_2-NEXT:    fmma222w.nt $r36r37 = $r62r63, $r20r21
; KV3_2-NEXT:    ;; # (end cycle 6)
; KV3_2-NEXT:    fmma222w.nt $r32r33 = $r62r63, $r22r23
; KV3_2-NEXT:    xmovefo $r20r21r22r23 = $a1
; KV3_2-NEXT:    ;; # (end cycle 7)
; KV3_2-NEXT:    fmma222w.nt $r16r17 = $r62r63, $r60r61
; KV3_2-NEXT:    ;; # (end cycle 8)
; KV3_2-NEXT:    xlo $a0 = $r46[$r0]
; KV3_2-NEXT:    fmma222w.nt $r10r11 = $r62r63, $r62r63
; KV3_2-NEXT:    xmovefo $r60r61r62r63 = $a0
; KV3_2-NEXT:    addd $r46 = $r46, 36
; KV3_2-NEXT:    ;; # (end cycle 9)
; KV3_2-NEXT:    xlo $a1 = $r45[$r0]
; KV3_2-NEXT:    fmma222w.nt $r40r41 = $r20r21, $r20r21
; KV3_2-NEXT:    addd $r45 = $r45, 36
; KV3_2-NEXT:    ;; # (end cycle 10)
; KV3_2-NEXT:    fmma222w.nt $r56r57 = $r60r61, $r60r61
; KV3_2-NEXT:    ;; # (end cycle 12)
; KV3_2-NEXT:    fmma222w.nt $r54r55 = $r62r63, $r60r61
; KV3_2-NEXT:    ;; # (end cycle 13)
; KV3_2-NEXT:    fmma222w.nt $r50r51 = $r62r63, $r62r63
; KV3_2-NEXT:    xcopyx.zd $a0a1 = $a0a1
; KV3_2-NEXT:    ;; # (end cycle 14)
; KV3_2-NEXT:    fmma222w.nt $r48r49 = $r20r21, $r60r61
; KV3_2-NEXT:    ;; # (end cycle 15)
; KV3_2-NEXT:    fmma222w.nt $r42r43 = $r20r21, $r62r63
; KV3_2-NEXT:    ;; # (end cycle 16)
; KV3_2-NEXT:    fmma222w.nt $r36r37 = $r22r23, $r60r61
; KV3_2-NEXT:    ;; # (end cycle 17)
; KV3_2-NEXT:    xlo $a0 = $r44[$r0]
; KV3_2-NEXT:    fmma222w.nt $r32r33 = $r22r23, $r62r63
; KV3_2-NEXT:    xmovefo $r60r61r62r63 = $a0
; KV3_2-NEXT:    addd $r44 = $r44, 36
; KV3_2-NEXT:    ;; # (end cycle 18)
; KV3_2-NEXT:    fmma222w.nt $r16r17 = $r22r23, $r20r21
; KV3_2-NEXT:    ;; # (end cycle 19)
; KV3_2-NEXT:    xlo $a1 = $r39[$r0]
; KV3_2-NEXT:    fmma222w.nt $r10r11 = $r22r23, $r22r23
; KV3_2-NEXT:    xmovefo $r20r21r22r23 = $a1
; KV3_2-NEXT:    addd $r39 = $r39, 36
; KV3_2-NEXT:    ;; # (end cycle 20)
; KV3_2-NEXT:    xlo $a2 = $r52[$r0]
; KV3_2-NEXT:    fmma222w.nt $r56r57 = $r60r61, $r60r61
; KV3_2-NEXT:    addd $r52 = $r52, 36
; KV3_2-NEXT:    ;; # (end cycle 21)
; KV3_2-NEXT:    xlo $a3 = $r9[$r0]
; KV3_2-NEXT:    fmma222w.nt $r54r55 = $r62r63, $r60r61
; KV3_2-NEXT:    addd $r9 = $r9, 36
; KV3_2-NEXT:    ;; # (end cycle 22)
; KV3_2-NEXT:    fmma222w.nt $r50r51 = $r62r63, $r62r63
; KV3_2-NEXT:    ;; # (end cycle 23)
; KV3_2-NEXT:    fmma222w.nt $r48r49 = $r20r21, $r60r61
; KV3_2-NEXT:    xcopyx.zd $a0a1 = $a0a1
; KV3_2-NEXT:    ;; # (end cycle 24)
; KV3_2-NEXT:    fmma222w.nt $r42r43 = $r20r21, $r62r63
; KV3_2-NEXT:    ;; # (end cycle 25)
; KV3_2-NEXT:    fmma222w.nt $r40r41 = $r20r21, $r20r21
; KV3_2-NEXT:    xcopyx.zd $a2a3 = $a2a3
; KV3_2-NEXT:    ;; # (end cycle 26)
; KV3_2-NEXT:    fmma222w.nt $r36r37 = $r22r23, $r60r61
; KV3_2-NEXT:    ;; # (end cycle 27)
; KV3_2-NEXT:    xlo $a0 = $r15[$r0]
; KV3_2-NEXT:    fmma222w.nt $r32r33 = $r22r23, $r62r63
; KV3_2-NEXT:    xmovefo $r24r25r26r27 = $a0
; KV3_2-NEXT:    addd $r15 = $r15, 36
; KV3_2-NEXT:    ;; # (end cycle 28)
; KV3_2-NEXT:    xlo $a1 = $r38[$r0]
; KV3_2-NEXT:    fmma222w.nt $r16r17 = $r22r23, $r20r21
; KV3_2-NEXT:    xmovefo $r28r29r30r31 = $a1
; KV3_2-NEXT:    addd $r38 = $r38, 36
; KV3_2-NEXT:    ;; # (end cycle 29)
; KV3_2-NEXT:    fmma222w.nt $r10r11 = $r22r23, $r22r23
; KV3_2-NEXT:    xmovefo $r60r61r62r63 = $a3
; KV3_2-NEXT:    ;; # (end cycle 30)
; KV3_2-NEXT:    fmma222w.nt $r56r57 = $r24r25, $r24r25
; KV3_2-NEXT:    xmovefo $r20r21r22r23 = $a2
; KV3_2-NEXT:    ;; # (end cycle 31)
; KV3_2-NEXT:    fmma222w.nt $r54r55 = $r26r27, $r24r25
; KV3_2-NEXT:    ;; # (end cycle 32)
; KV3_2-NEXT:    fmma222w.nt $r50r51 = $r26r27, $r26r27
; KV3_2-NEXT:    ;; # (end cycle 33)
; KV3_2-NEXT:    fmma222w.nt $r48r49 = $r28r29, $r24r25
; KV3_2-NEXT:    ;; # (end cycle 34)
; KV3_2-NEXT:    fmma222w.nt $r42r43 = $r28r29, $r26r27
; KV3_2-NEXT:    ;; # (end cycle 35)
; KV3_2-NEXT:    fmma222w.nt $r40r41 = $r28r29, $r28r29
; KV3_2-NEXT:    ;; # (end cycle 36)
; KV3_2-NEXT:    fmma222w.nt $r36r37 = $r30r31, $r24r25
; KV3_2-NEXT:    ;; # (end cycle 37)
; KV3_2-NEXT:    fmma222w.nt $r32r33 = $r30r31, $r26r27
; KV3_2-NEXT:    ;; # (end cycle 38)
; KV3_2-NEXT:    fmma222w.nt $r16r17 = $r30r31, $r28r29
; KV3_2-NEXT:    ;; # (end cycle 39)
; KV3_2-NEXT:    fmma222w.nt $r10r11 = $r30r31, $r30r31
; KV3_2-NEXT:    cb.odd $r35 ? .LBB0_1
; KV3_2-NEXT:    ;; # (end cycle 40)
; KV3_2-NEXT:  .LBB0_2: # %for.cond.cleanup
; KV3_2-NEXT:    addw $r3 = $r3, $r2
; KV3_2-NEXT:    addw $r8 = $r8, $r2
; KV3_2-NEXT:    sxwd $r9 = $r2
; KV3_2-NEXT:    addw $r15 = $r2, $r1
; KV3_2-NEXT:    ;; # (end cycle 0)
; KV3_2-NEXT:    addw $r7 = $r7, $r2
; KV3_2-NEXT:    sxwd $r8 = $r8
; KV3_2-NEXT:    sq.xs $r9[$r0] = $r56r57
; KV3_2-NEXT:    sxwd $r9 = $r15
; KV3_2-NEXT:    ;; # (end cycle 1)
; KV3_2-NEXT:    sxwd $r3 = $r3
; KV3_2-NEXT:    addw $r5 = $r5, $r2
; KV3_2-NEXT:    sq.xs $r9[$r0] = $r54r55
; KV3_2-NEXT:    ;; # (end cycle 2)
; KV3_2-NEXT:    sq.xs $r8[$r0] = $r50r51
; KV3_2-NEXT:    ;; # (end cycle 3)
; KV3_2-NEXT:    sq.xs $r3[$r0] = $r48r49
; KV3_2-NEXT:    sxwd $r3 = $r7
; KV3_2-NEXT:    ;; # (end cycle 4)
; KV3_2-NEXT:    sq.xs $r3[$r0] = $r42r43
; KV3_2-NEXT:    sxwd $r3 = $r5
; KV3_2-NEXT:    addw $r5 = $r6, $r2
; KV3_2-NEXT:    ;; # (end cycle 5)
; KV3_2-NEXT:    sq.xs $r3[$r0] = $r40r41
; KV3_2-NEXT:    sxwd $r3 = $r5
; KV3_2-NEXT:    ;; # (end cycle 6)
; KV3_2-NEXT:    sq.xs $r3[$r0] = $r36r37
; KV3_2-NEXT:    addw $r3 = $r4, $r2
; KV3_2-NEXT:    addx8w $r4 = $r1, $r2
; KV3_2-NEXT:    ;; # (end cycle 7)
; KV3_2-NEXT:    sxwd $r1 = $r4
; KV3_2-NEXT:    maddw $r2 = $r1, 9
; KV3_2-NEXT:    sxwd $r3 = $r3
; KV3_2-NEXT:    ;; # (end cycle 8)
; KV3_2-NEXT:    sq.xs $r3[$r0] = $r32r33
; KV3_2-NEXT:    ;; # (end cycle 9)
; KV3_2-NEXT:    sq.xs $r1[$r0] = $r16r17
; KV3_2-NEXT:    sxwd $r1 = $r2
; KV3_2-NEXT:    ;; # (end cycle 10)
; KV3_2-NEXT:    sq.xs $r1[$r0] = $r10r11
; KV3_2-NEXT:    ;; # (end cycle 11)
; KV3_2-NEXT:    lo $r20r21r22r23 = 0[$r12]
; KV3_2-NEXT:    ;; # (end cycle 12)
; KV3_2-NEXT:    lo $r24r25r26r27 = 32[$r12]
; KV3_2-NEXT:    ;; # (end cycle 13)
; KV3_2-NEXT:    lo $r28r29r30r31 = 64[$r12]
; KV3_2-NEXT:    addd $r12 = $r12, 96
; KV3_2-NEXT:    ret
; KV3_2-NEXT:    ;; # (end cycle 14)
entry:
  %0 = tail call <256 x i1> @llvm.kvx.xload256(ptr %a, i32 0)
  %idxprom = sext i32 %n to i64
  %arrayidx1 = getelementptr inbounds float, ptr %a, i64 %idxprom
  %1 = tail call <256 x i1> @llvm.kvx.xload256(ptr %arrayidx1, i32 0)
  %mul2 = shl nsw i32 %n, 1
  %idxprom3 = sext i32 %mul2 to i64
  %arrayidx4 = getelementptr inbounds float, ptr %a, i64 %idxprom3
  %2 = tail call <256 x i1> @llvm.kvx.xload256(ptr %arrayidx4, i32 0)
  %mul5 = mul nsw i32 %n, 3
  %idxprom6 = sext i32 %mul5 to i64
  %arrayidx7 = getelementptr inbounds float, ptr %a, i64 %idxprom6
  %3 = tail call <256 x i1> @llvm.kvx.xload256(ptr %arrayidx7, i32 0)
  %4 = tail call <512 x i1> @llvm.kvx.cat.v512i1(<256 x i1> %0, <256 x i1> %1)
  %5 = tail call <512 x i1> @llvm.kvx.xcopyx(<512 x i1> %4, i32 1)
  %6 = tail call <256 x i1> @llvm.kvx.low.v256i1(<512 x i1> %5)
  %7 = tail call <4 x i64> @llvm.kvx.xmovefo256(<256 x i1> %6)
  %8 = tail call <256 x i1> @llvm.kvx.high.v256i1(<512 x i1> %5)
  %9 = tail call <4 x i64> @llvm.kvx.xmovefo256(<256 x i1> %8)
  %mul131 = shl nsw i32 %n, 2
  %mul135 = mul nsw i32 %n, 5
  %mul139 = mul nsw i32 %n, 6
  %mul143 = mul nsw i32 %n, 7
  %10 = sext i32 %mul143 to i64
  %11 = sext i32 %mul139 to i64
  %12 = sext i32 %mul135 to i64
  %13 = sext i32 %mul131 to i64
  %smax = tail call i32 @llvm.smax.i32(i32 %i, i32 0)
  %wide.trip.count = zext i32 %smax to i64
  br label %for.cond

for.cond:
  %indvars.iv = phi i64 [ %indvars.iv.next, %for.body ], [ 0, %entry ]
  %acc62.0 = phi <4 x float> [ %96, %for.body ], [ zeroinitializer, %entry ]
  %acc64.0 = phi <4 x float> [ %97, %for.body ], [ zeroinitializer, %entry ]
  %acc66.0 = phi <4 x float> [ %98, %for.body ], [ zeroinitializer, %entry ]
  %bloc.0 = phi ptr [ %next_bloc.0, %for.body ], [ %a, %entry ]
  %acc60.0 = phi <4 x float> [ %95, %for.body ], [ zeroinitializer, %entry ]
  %acc44.0 = phi <4 x float> [ %94, %for.body ], [ zeroinitializer, %entry ]
  %acc42.0 = phi <4 x float> [ %93, %for.body ], [ zeroinitializer, %entry ]
  %acc40.0 = phi <4 x float> [ %92, %for.body ], [ zeroinitializer, %entry ]
  %acc22.0 = phi <4 x float> [ %89, %for.body ], [ zeroinitializer, %entry ]
  %acc20.0 = phi <4 x float> [ %84, %for.body ], [ zeroinitializer, %entry ]
  %acc00.0 = phi <4 x float> [ %79, %for.body ], [ zeroinitializer, %entry ]
  %data1.sroa.0.0.in = phi <4 x i64> [ %91, %for.body ], [ %9, %entry ]
  %data0.sroa.0.0.in = phi <4 x i64> [ %86, %for.body ], [ %7, %entry ]
  %a3.0 = phi <256 x i1> [ %88, %for.body ], [ %3, %entry ]
  %a2.0 = phi <256 x i1> [ %83, %for.body ], [ %2, %entry ]
  %exitcond.not = icmp eq i64 %indvars.iv, %wide.trip.count
  br i1 %exitcond.not, label %for.cond.cleanup, label %for.body

for.cond.cleanup:
  %idxprom118 = sext i32 %i to i64
  %arrayidx119 = getelementptr inbounds <4 x float>, ptr %a, i64 %idxprom118
  store <4 x float> %acc00.0, ptr %arrayidx119
  %add120 = add nsw i32 %i, %n
  %idxprom121 = sext i32 %add120 to i64
  %arrayidx122 = getelementptr inbounds <4 x float>, ptr %a, i64 %idxprom121
  store <4 x float> %acc20.0, ptr %arrayidx122
  %add124 = add nsw i32 %mul2, %i
  %idxprom125 = sext i32 %add124 to i64
  %arrayidx126 = getelementptr inbounds <4 x float>, ptr %a, i64 %idxprom125
  store <4 x float> %acc22.0, ptr %arrayidx126
  %add128 = add nsw i32 %mul5, %i
  %idxprom129 = sext i32 %add128 to i64
  %arrayidx130 = getelementptr inbounds <4 x float>, ptr %a, i64 %idxprom129
  store <4 x float> %acc40.0, ptr %arrayidx130
  %add132 = add nsw i32 %mul131, %i
  %idxprom133 = sext i32 %add132 to i64
  %arrayidx134 = getelementptr inbounds <4 x float>, ptr %a, i64 %idxprom133
  store <4 x float> %acc42.0, ptr %arrayidx134
  %add136 = add nsw i32 %mul135, %i
  %idxprom137 = sext i32 %add136 to i64
  %arrayidx138 = getelementptr inbounds <4 x float>, ptr %a, i64 %idxprom137
  store <4 x float> %acc44.0, ptr %arrayidx138
  %add140 = add nsw i32 %mul139, %i
  %idxprom141 = sext i32 %add140 to i64
  %arrayidx142 = getelementptr inbounds <4 x float>, ptr %a, i64 %idxprom141
  store <4 x float> %acc60.0, ptr %arrayidx142
  %add144 = add nsw i32 %mul143, %i
  %idxprom145 = sext i32 %add144 to i64
  %arrayidx146 = getelementptr inbounds <4 x float>, ptr %a, i64 %idxprom145
  store <4 x float> %acc62.0, ptr %arrayidx146
  %mul147 = shl nsw i32 %n, 3
  %add148 = add nsw i32 %mul147, %i
  %idxprom149 = sext i32 %add148 to i64
  %arrayidx150 = getelementptr inbounds <4 x float>, ptr %a, i64 %idxprom149
  store <4 x float> %acc64.0, ptr %arrayidx150
  %mul151 = mul nsw i32 %n, 9
  %add152 = add nsw i32 %mul151, %i
  %idxprom153 = sext i32 %add152 to i64
  %arrayidx154 = getelementptr inbounds <4 x float>, ptr %a, i64 %idxprom153
  store <4 x float> %acc66.0, ptr %arrayidx154
  ret void

for.body:
  %data0.sroa.0.0 = bitcast <4 x i64> %data0.sroa.0.0.in to <8 x float>
  %data1.sroa.0.0 = bitcast <4 x i64> %data1.sroa.0.0.in to <8 x float>
  %next_bloc.0 = getelementptr inbounds float, ptr %bloc.0, i64 8
  %data0.sroa.0.0.vec.extract = shufflevector <8 x float> %data0.sroa.0.0, <8 x float> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %14 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data0.sroa.0.0.vec.extract, <4 x float> %data0.sroa.0.0.vec.extract, <4 x float> %acc00.0, i32 2, i32 7, i32 0)
  %15 = tail call <512 x i1> @llvm.kvx.cat.v512i1(<256 x i1> %a2.0, <256 x i1> %a3.0)
  %16 = tail call <512 x i1> @llvm.kvx.xcopyx(<512 x i1> %15, i32 1)
  %17 = add nsw i64 %indvars.iv, %13
  %arrayidx13 = getelementptr inbounds float, ptr %bloc.0, i64 %17
  %18 = tail call <256 x i1> @llvm.kvx.xload256(ptr %arrayidx13, i32 0)
  %data0.sroa.0.16.vec.extract = shufflevector <8 x float> %data0.sroa.0.0, <8 x float> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %19 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data0.sroa.0.16.vec.extract, <4 x float> %data0.sroa.0.0.vec.extract, <4 x float> %acc20.0, i32 2, i32 7, i32 0)
  %20 = tail call <256 x i1> @llvm.kvx.low.v256i1(<512 x i1> %16)
  %21 = tail call <4 x i64> @llvm.kvx.xmovefo256(<256 x i1> %20)
  %22 = bitcast <4 x i64> %21 to <8 x float>
  %23 = add nsw i64 %indvars.iv, %12
  %arrayidx19 = getelementptr inbounds float, ptr %bloc.0, i64 %23
  %24 = tail call <256 x i1> @llvm.kvx.xload256(ptr %arrayidx19, i32 0)
  %25 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data0.sroa.0.16.vec.extract, <4 x float> %data0.sroa.0.16.vec.extract, <4 x float> %acc22.0, i32 2, i32 7, i32 0)
  %26 = tail call <256 x i1> @llvm.kvx.high.v256i1(<512 x i1> %16)
  %27 = tail call <4 x i64> @llvm.kvx.xmovefo256(<256 x i1> %26)
  %28 = bitcast <4 x i64> %27 to <8 x float>
  %data1.sroa.0.0.vec.extract = shufflevector <8 x float> %data1.sroa.0.0, <8 x float> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %29 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data1.sroa.0.0.vec.extract, <4 x float> %data0.sroa.0.0.vec.extract, <4 x float> %acc40.0, i32 2, i32 7, i32 0)
  %30 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data1.sroa.0.0.vec.extract, <4 x float> %data0.sroa.0.16.vec.extract, <4 x float> %acc42.0, i32 2, i32 7, i32 0)
  %31 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data1.sroa.0.0.vec.extract, <4 x float> %data1.sroa.0.0.vec.extract, <4 x float> %acc44.0, i32 2, i32 7, i32 0)
  %data1.sroa.0.16.vec.extract = shufflevector <8 x float> %data1.sroa.0.0, <8 x float> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %32 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data1.sroa.0.16.vec.extract, <4 x float> %data0.sroa.0.0.vec.extract, <4 x float> %acc60.0, i32 2, i32 7, i32 0)
  %33 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data1.sroa.0.16.vec.extract, <4 x float> %data0.sroa.0.16.vec.extract, <4 x float> %acc62.0, i32 2, i32 7, i32 0)
  %34 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data1.sroa.0.16.vec.extract, <4 x float> %data1.sroa.0.0.vec.extract, <4 x float> %acc64.0, i32 2, i32 7, i32 0)
  %35 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data1.sroa.0.16.vec.extract, <4 x float> %data1.sroa.0.16.vec.extract, <4 x float> %acc66.0, i32 2, i32 7, i32 0)
  %data2.sroa.0.0.vec.extract = shufflevector <8 x float> %22, <8 x float> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %36 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data2.sroa.0.0.vec.extract, <4 x float> %data2.sroa.0.0.vec.extract, <4 x float> %14, i32 2, i32 7, i32 0)
  %37 = tail call <512 x i1> @llvm.kvx.cat.v512i1(<256 x i1> %18, <256 x i1> %24)
  %38 = tail call <512 x i1> @llvm.kvx.xcopyx(<512 x i1> %37, i32 1)
  %39 = add nsw i64 %indvars.iv, %11
  %arrayidx41 = getelementptr inbounds float, ptr %bloc.0, i64 %39
  %40 = tail call <256 x i1> @llvm.kvx.xload256(ptr %arrayidx41, i32 0)
  %data2.sroa.0.16.vec.extract = shufflevector <8 x float> %22, <8 x float> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %41 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data2.sroa.0.16.vec.extract, <4 x float> %data2.sroa.0.0.vec.extract, <4 x float> %19, i32 2, i32 7, i32 0)
  %42 = tail call <256 x i1> @llvm.kvx.low.v256i1(<512 x i1> %38)
  %43 = tail call <4 x i64> @llvm.kvx.xmovefo256(<256 x i1> %42)
  %44 = bitcast <4 x i64> %43 to <8 x float>
  %45 = add nsw i64 %indvars.iv, %10
  %arrayidx47 = getelementptr inbounds float, ptr %bloc.0, i64 %45
  %46 = tail call <256 x i1> @llvm.kvx.xload256(ptr %arrayidx47, i32 0)
  %47 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data2.sroa.0.16.vec.extract, <4 x float> %data2.sroa.0.16.vec.extract, <4 x float> %25, i32 2, i32 7, i32 0)
  %48 = tail call <256 x i1> @llvm.kvx.high.v256i1(<512 x i1> %38)
  %49 = tail call <4 x i64> @llvm.kvx.xmovefo256(<256 x i1> %48)
  %50 = bitcast <4 x i64> %49 to <8 x float>
  %data3.sroa.0.0.vec.extract = shufflevector <8 x float> %28, <8 x float> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %51 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data3.sroa.0.0.vec.extract, <4 x float> %data2.sroa.0.0.vec.extract, <4 x float> %29, i32 2, i32 7, i32 0)
  %52 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data3.sroa.0.0.vec.extract, <4 x float> %data2.sroa.0.16.vec.extract, <4 x float> %30, i32 2, i32 7, i32 0)
  %53 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data3.sroa.0.0.vec.extract, <4 x float> %data3.sroa.0.0.vec.extract, <4 x float> %31, i32 2, i32 7, i32 0)
  %data3.sroa.0.16.vec.extract = shufflevector <8 x float> %28, <8 x float> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %54 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data3.sroa.0.16.vec.extract, <4 x float> %data2.sroa.0.0.vec.extract, <4 x float> %32, i32 2, i32 7, i32 0)
  %55 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data3.sroa.0.16.vec.extract, <4 x float> %data2.sroa.0.16.vec.extract, <4 x float> %33, i32 2, i32 7, i32 0)
  %56 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data3.sroa.0.16.vec.extract, <4 x float> %data3.sroa.0.0.vec.extract, <4 x float> %34, i32 2, i32 7, i32 0)
  %57 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data3.sroa.0.16.vec.extract, <4 x float> %data3.sroa.0.16.vec.extract, <4 x float> %35, i32 2, i32 7, i32 0)
  %data4.sroa.0.0.vec.extract = shufflevector <8 x float> %44, <8 x float> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %58 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data4.sroa.0.0.vec.extract, <4 x float> %data4.sroa.0.0.vec.extract, <4 x float> %36, i32 2, i32 7, i32 0)
  %59 = tail call <512 x i1> @llvm.kvx.cat.v512i1(<256 x i1> %40, <256 x i1> %46)
  %60 = tail call <512 x i1> @llvm.kvx.xcopyx(<512 x i1> %59, i32 1)
  %arrayidx67 = getelementptr inbounds float, ptr %next_bloc.0, i64 %indvars.iv
  %61 = tail call <256 x i1> @llvm.kvx.xload256(ptr nonnull %arrayidx67, i32 0)
  %data4.sroa.0.16.vec.extract = shufflevector <8 x float> %44, <8 x float> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %62 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data4.sroa.0.16.vec.extract, <4 x float> %data4.sroa.0.0.vec.extract, <4 x float> %41, i32 2, i32 7, i32 0)
  %63 = tail call <256 x i1> @llvm.kvx.low.v256i1(<512 x i1> %60)
  %64 = tail call <4 x i64> @llvm.kvx.xmovefo256(<256 x i1> %63)
  %65 = bitcast <4 x i64> %64 to <8 x float>
  %66 = add nsw i64 %indvars.iv, %idxprom
  %arrayidx73 = getelementptr inbounds float, ptr %next_bloc.0, i64 %66
  %67 = tail call <256 x i1> @llvm.kvx.xload256(ptr nonnull %arrayidx73, i32 0)
  %68 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data4.sroa.0.16.vec.extract, <4 x float> %data4.sroa.0.16.vec.extract, <4 x float> %47, i32 2, i32 7, i32 0)
  %69 = tail call <256 x i1> @llvm.kvx.high.v256i1(<512 x i1> %60)
  %70 = tail call <4 x i64> @llvm.kvx.xmovefo256(<256 x i1> %69)
  %71 = bitcast <4 x i64> %70 to <8 x float>
  %data5.sroa.0.0.vec.extract = shufflevector <8 x float> %50, <8 x float> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %72 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data5.sroa.0.0.vec.extract, <4 x float> %data4.sroa.0.0.vec.extract, <4 x float> %51, i32 2, i32 7, i32 0)
  %73 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data5.sroa.0.0.vec.extract, <4 x float> %data4.sroa.0.16.vec.extract, <4 x float> %52, i32 2, i32 7, i32 0)
  %74 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data5.sroa.0.0.vec.extract, <4 x float> %data5.sroa.0.0.vec.extract, <4 x float> %53, i32 2, i32 7, i32 0)
  %data5.sroa.0.16.vec.extract = shufflevector <8 x float> %50, <8 x float> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %75 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data5.sroa.0.16.vec.extract, <4 x float> %data4.sroa.0.0.vec.extract, <4 x float> %54, i32 2, i32 7, i32 0)
  %76 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data5.sroa.0.16.vec.extract, <4 x float> %data4.sroa.0.16.vec.extract, <4 x float> %55, i32 2, i32 7, i32 0)
  %77 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data5.sroa.0.16.vec.extract, <4 x float> %data5.sroa.0.0.vec.extract, <4 x float> %56, i32 2, i32 7, i32 0)
  %78 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data5.sroa.0.16.vec.extract, <4 x float> %data5.sroa.0.16.vec.extract, <4 x float> %57, i32 2, i32 7, i32 0)
  %data6.sroa.0.0.vec.extract = shufflevector <8 x float> %65, <8 x float> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %79 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data6.sroa.0.0.vec.extract, <4 x float> %data6.sroa.0.0.vec.extract, <4 x float> %58, i32 2, i32 7, i32 0)
  %80 = tail call <512 x i1> @llvm.kvx.cat.v512i1(<256 x i1> %61, <256 x i1> %67)
  %81 = tail call <512 x i1> @llvm.kvx.xcopyx(<512 x i1> %80, i32 1)
  %82 = add nsw i64 %indvars.iv, %idxprom3
  %arrayidx95 = getelementptr inbounds float, ptr %next_bloc.0, i64 %82
  %83 = tail call <256 x i1> @llvm.kvx.xload256(ptr nonnull %arrayidx95, i32 0)
  %data6.sroa.0.16.vec.extract = shufflevector <8 x float> %65, <8 x float> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %84 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data6.sroa.0.16.vec.extract, <4 x float> %data6.sroa.0.0.vec.extract, <4 x float> %62, i32 2, i32 7, i32 0)
  %85 = tail call <256 x i1> @llvm.kvx.low.v256i1(<512 x i1> %81)
  %86 = tail call <4 x i64> @llvm.kvx.xmovefo256(<256 x i1> %85)
  %87 = add nsw i64 %indvars.iv, %idxprom6
  %arrayidx101 = getelementptr inbounds float, ptr %next_bloc.0, i64 %87
  %88 = tail call <256 x i1> @llvm.kvx.xload256(ptr nonnull %arrayidx101, i32 0)
  %89 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data6.sroa.0.16.vec.extract, <4 x float> %data6.sroa.0.16.vec.extract, <4 x float> %68, i32 2, i32 7, i32 0)
  %90 = tail call <256 x i1> @llvm.kvx.high.v256i1(<512 x i1> %81)
  %91 = tail call <4 x i64> @llvm.kvx.xmovefo256(<256 x i1> %90)
  %data7.sroa.0.0.vec.extract = shufflevector <8 x float> %71, <8 x float> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %92 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data7.sroa.0.0.vec.extract, <4 x float> %data6.sroa.0.0.vec.extract, <4 x float> %72, i32 2, i32 7, i32 0)
  %93 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data7.sroa.0.0.vec.extract, <4 x float> %data6.sroa.0.16.vec.extract, <4 x float> %73, i32 2, i32 7, i32 0)
  %94 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data7.sroa.0.0.vec.extract, <4 x float> %data7.sroa.0.0.vec.extract, <4 x float> %74, i32 2, i32 7, i32 0)
  %data7.sroa.0.16.vec.extract = shufflevector <8 x float> %71, <8 x float> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %95 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data7.sroa.0.16.vec.extract, <4 x float> %data6.sroa.0.0.vec.extract, <4 x float> %75, i32 2, i32 7, i32 0)
  %96 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data7.sroa.0.16.vec.extract, <4 x float> %data6.sroa.0.16.vec.extract, <4 x float> %76, i32 2, i32 7, i32 0)
  %97 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data7.sroa.0.16.vec.extract, <4 x float> %data7.sroa.0.0.vec.extract, <4 x float> %77, i32 2, i32 7, i32 0)
  %98 = tail call <4 x float> @llvm.kvx.fmma222w(<4 x float> %data7.sroa.0.16.vec.extract, <4 x float> %data7.sroa.0.16.vec.extract, <4 x float> %78, i32 2, i32 7, i32 0)
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  br label %for.cond
}

declare <256 x i1> @llvm.kvx.xload256(ptr, i32)

declare <512 x i1> @llvm.kvx.xcopyx(<512 x i1>, i32)

declare <512 x i1> @llvm.kvx.cat.v512i1(<256 x i1>, <256 x i1>)

declare <4 x i64> @llvm.kvx.xmovefo256(<256 x i1>)

declare <256 x i1> @llvm.kvx.low.v256i1(<512 x i1>)

declare <256 x i1> @llvm.kvx.high.v256i1(<512 x i1>)

declare <4 x float> @llvm.kvx.fmma222w(<4 x float>, <4 x float>, <4 x float>, i32, i32, i32)

declare i32 @llvm.smax.i32(i32, i32)

