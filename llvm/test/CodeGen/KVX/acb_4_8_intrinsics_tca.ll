; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -O3 < %s | FileCheck %s
target triple = "kvx-kalray-cos"

define <4 x i64> @test_tca_builtins(i64 %0, i64 %1, i64 %2, i64 %3, <256 x i1>* %4, <512 x i1>* %5, <1024 x i1>* %6) {
; CHECK-LABEL: test_tca_builtins:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lv $a0 = 0[$r4]
; CHECK-NEXT:    copyd $r7 = $r0
; CHECK-NEXT:    make $r8 = 0
; CHECK-NEXT:    make $r9 = 1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    movetq $a0_hi = $r8, $r9
; CHECK-NEXT:    make $r0 = 4
; CHECK-NEXT:    make $r10 = 2
; CHECK-NEXT:    make $r11 = 3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r4] = $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a0 = 0[$r4]
; CHECK-NEXT:    movetq $a1_lo = $r11, $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    movetq $a0_lo = $r11, $r10
; CHECK-NEXT:    movetq $a1_hi = $r9, $r10
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r4] = $a0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 32[$r4] = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a5 = 0[$r4]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a3 = 96[$r6]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a2 = 64[$r6]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a1 = 32[$r6]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a0 = 0[$r6]
; CHECK-NEXT:    movetq $a4_lo = $r8, $r9
; CHECK-NEXT:    movetq $a4_hi = $r10, $r11
; CHECK-NEXT:    addd $r8 = $r4, 128
; CHECK-NEXT:    ;;
; CHECK-NEXT:    alignv $a5 = $a4, $a5, 16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    convdhv0.rn.sat $a4_lo = $a0a1a2a3
; CHECK-NEXT:    aligno $r0r1r2r3 = $a4, $a5, 1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a7 = 32[$r5]
; CHECK-NEXT:    convdhv1.ru.satu $a4_hi = $a0a1a2a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv $a6 = 0[$r5]
; CHECK-NEXT:    convwbv0.ru.sat $a5_x = $a0a1a2a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    convwbv1.ru.sat $a5_y = $a0a1a2a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    convwbv2.ru.sat $a5_z = $a0a1a2a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyv $a8 = $a4
; CHECK-NEXT:    convwbv3.ru.sat $a5_t = $a0a1a2a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmma242hw0 $a8_lo = $a6a7, $a8, $a5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyv $a10 = $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmma242hw1 $a10_hi = $a6a7, $a4, $a10
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyv $a4 = $a10
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmma242hw2 $a4_lo = $a6a7, $a4, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyv $a8 = $a4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmma242hw3 $a8_hi = $a6a7, $a10, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmma242hw0 $a10_lo = $a6a7, $a4, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmma242hw1 $a10_hi = $a6a7, $a4, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmma242hw2 $a11_lo = $a6a7, $a4, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fmma242hw3 $a11_hi = $a6a7, $a4, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma444hbd0 $a0a1a2a3 = $a0a1a2a3, $a8, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma444hbd1 $a0a1a2a3 = $a0a1a2a3, $a8, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma444hbd0 $a0a1a2a3 = $a0a1a2a3, $a10, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma444hbd1 $a0a1a2a3 = $a0a1a2a3, $a11, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma444hd $a0a1a2a3 = $a0a1a2a3, $a8, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma444suhbd0 $a0a1a2a3 = $a0a1a2a3, $a8, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma444suhbd1 $a0a1a2a3 = $a0a1a2a3, $a8, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma444suhbd0 $a0a1a2a3 = $a0a1a2a3, $a10, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma444suhbd1 $a0a1a2a3 = $a0a1a2a3, $a11, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma444suhd $a0a1a2a3 = $a0a1a2a3, $a8, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma444uhbd0 $a0a1a2a3 = $a0a1a2a3, $a8, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma444uhbd1 $a0a1a2a3 = $a0a1a2a3, $a8, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma444uhbd0 $a0a1a2a3 = $a0a1a2a3, $a10, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma444uhbd1 $a0a1a2a3 = $a0a1a2a3, $a11, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma444uhd $a0a1a2a3 = $a0a1a2a3, $a8, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma444ushbd0 $a0a1a2a3 = $a0a1a2a3, $a8, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma444ushbd1 $a0a1a2a3 = $a0a1a2a3, $a8, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma444ushbd0 $a0a1a2a3 = $a0a1a2a3, $a10, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma444ushbd1 $a0a1a2a3 = $a0a1a2a3, $a11, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma444ushd $a0a1a2a3 = $a0a1a2a3, $a8, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma484bw $a6a7 = $a10a11, $a8, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma484subw $a6a7 = $a6a7, $a8, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma484ubw $a6a7 = $a6a7, $a8, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mma484usbw $a6a7 = $a6a7, $a8, $a8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    mt44d $a0a1a2a3 = $a0a1a2a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fscalewv $a4 = $a4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fnarrowwhv.rn.s $a5 = $a6a7
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fscalewv.rn.relu $a4 = $a4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    movetq $a4_lo = $r0, $r1
; CHECK-NEXT:    movetq $a4_hi = $r2, $r3
; CHECK-NEXT:    movefo $r0r1r2r3 = $a4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fscalewv.relu $a4 = $a4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv.s $a4 = 0[$r4]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r4 = $r4, 32
; CHECK-NEXT:    addd $r8 = $r4, 160
; CHECK-NEXT:    lv.c3.s $a0a1a2a3 = 0[$r8]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lv.c2.odd $r7 ? $a0a1a2a3 = [$r8]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv.even $r9 ? [$r4] = $a4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 32[$r5] = $a7
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r5] = $a6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 96[$r6] = $a3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 64[$r6] = $a2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 32[$r6] = $a1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sv 0[$r6] = $a0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %8 = load volatile <256 x i1>, <256 x i1>* %4, align 32
  %9 = tail call <256 x i1> @llvm.kvx.xmovetohi(<256 x i1> %8, i64 0, i64 1)
  store volatile <256 x i1> %9, <256 x i1>* %4, align 32
  %10 = load volatile <256 x i1>, <256 x i1>* %4, align 32
  %11 = tail call <256 x i1> @llvm.kvx.xmovetolo(<256 x i1> %10, i64 3, i64 2)
  store volatile <256 x i1> %11, <256 x i1>* %4, align 32
  %12 = tail call <256 x i1> @llvm.kvx.xmoveto(i64 1, i64 2, i64 3, i64 4)
  %13 = getelementptr inbounds <256 x i1>, <256 x i1>* %4, i64 1
  store volatile <256 x i1> %12, <256 x i1>* %13, align 32
  %14 = tail call <256 x i1> @llvm.kvx.xmoveoto(<4 x i64> <i64 0, i64 1, i64 2, i64 3>)
  %15 = load volatile <256 x i1>, <256 x i1>* %4, align 32
  %16 = tail call <256 x i1> @llvm.kvx.alignv(<256 x i1> %14, <256 x i1> %15, i64 16)
  %17 = tail call <4 x i64> @llvm.kvx.alignov(<256 x i1> %14, <256 x i1> %16, i64 1)
  %18 = load volatile <1024 x i1>, <1024 x i1>* %6, align 32
  %19 = tail call <256 x i1> @llvm.kvx.xconvdhv0(<256 x i1> %14, <1024 x i1> %18, i32 0, i32 0)
  %20 = tail call <256 x i1> @llvm.kvx.xconvdhv1(<256 x i1> %19, <1024 x i1> %18, i32 1, i32 1)
  %21 = load volatile <512 x i1>, <512 x i1>* %5, align 32
  %22 = tail call <256 x i1> @llvm.kvx.xconvwbv(<1024 x i1> %18, i32 1, i32 0)
  %23 = tail call <256 x i1> @llvm.kvx.xfmma242hw0(<256 x i1> %20, <512 x i1> %21, <256 x i1> %20, <256 x i1> %22)
  %24 = tail call <256 x i1> @llvm.kvx.xfmma242hw1(<256 x i1> %23, <512 x i1> %21, <256 x i1> %20, <256 x i1> %23)
  %25 = tail call <256 x i1> @llvm.kvx.xfmma242hw2(<256 x i1> %24, <512 x i1> %21, <256 x i1> %24, <256 x i1> %23)
  %26 = tail call <256 x i1> @llvm.kvx.xfmma242hw3(<256 x i1> %25, <512 x i1> %21, <256 x i1> %24, <256 x i1> %25)
  %27 = tail call <512 x i1> @llvm.kvx.xfmma444hw(<512 x i1> %21, <256 x i1> %25, <256 x i1> %26)
  %28 = tail call <1024 x i1> @llvm.kvx.xmma444hbd0(<1024 x i1> %18, <256 x i1> %26, <256 x i1> %26)
  %29 = tail call <1024 x i1> @llvm.kvx.xmma444hbd1(<1024 x i1> %28, <256 x i1> %26, <256 x i1> %26)
  %30 = tail call <1024 x i1> @llvm.kvx.xmma484hbd(<1024 x i1> %29, <512 x i1> %27, <256 x i1> %26)
  %31 = tail call <1024 x i1> @llvm.kvx.xmma444hd(<1024 x i1> %30, <256 x i1> %26, <256 x i1> %26)
  %32 = tail call <1024 x i1> @llvm.kvx.xmma444suhbd0(<1024 x i1> %31, <256 x i1> %26, <256 x i1> %26)
  %33 = tail call <1024 x i1> @llvm.kvx.xmma444suhbd1(<1024 x i1> %32, <256 x i1> %26, <256 x i1> %26)
  %34 = tail call <1024 x i1> @llvm.kvx.xmma484suhbd(<1024 x i1> %33, <512 x i1> %27, <256 x i1> %26)
  %35 = tail call <1024 x i1> @llvm.kvx.xmma444suhd(<1024 x i1> %34, <256 x i1> %26, <256 x i1> %26)
  %36 = tail call <1024 x i1> @llvm.kvx.xmma444uhbd0(<1024 x i1> %35, <256 x i1> %26, <256 x i1> %26)
  %37 = tail call <1024 x i1> @llvm.kvx.xmma444uhbd1(<1024 x i1> %36, <256 x i1> %26, <256 x i1> %26)
  %38 = tail call <1024 x i1> @llvm.kvx.xmma484uhbd(<1024 x i1> %37, <512 x i1> %27, <256 x i1> %26)
  %39 = tail call <1024 x i1> @llvm.kvx.xmma444uhd(<1024 x i1> %38, <256 x i1> %26, <256 x i1> %26)
  %40 = tail call <1024 x i1> @llvm.kvx.xmma444ushbd0(<1024 x i1> %39, <256 x i1> %26, <256 x i1> %26)
  %41 = tail call <1024 x i1> @llvm.kvx.xmma444ushbd1(<1024 x i1> %40, <256 x i1> %26, <256 x i1> %26)
  %42 = tail call <1024 x i1> @llvm.kvx.xmma484ushbd(<1024 x i1> %41, <512 x i1> %27, <256 x i1> %26)
  %43 = tail call <1024 x i1> @llvm.kvx.xmma444ushd(<1024 x i1> %42, <256 x i1> %26, <256 x i1> %26)
  %44 = tail call <512 x i1> @llvm.kvx.xmma484bw(<512 x i1> %27, <256 x i1> %26, <256 x i1> %26)
  %45 = tail call <512 x i1> @llvm.kvx.xmma484subw(<512 x i1> %44, <256 x i1> %26, <256 x i1> %26)
  %46 = tail call <512 x i1> @llvm.kvx.xmma484ubw(<512 x i1> %45, <256 x i1> %26, <256 x i1> %26)
  %47 = tail call <512 x i1> @llvm.kvx.xmma484usbw(<512 x i1> %46, <256 x i1> %26, <256 x i1> %26)
  %48 = tail call <1024 x i1> @llvm.kvx.xmt44d(<1024 x i1> %43)
  %49 = tail call <256 x i1> @llvm.kvx.xfscalewv(<256 x i1> %25, i32 7, i32 0, i32 0)
  %50 = tail call <256 x i1> @llvm.kvx.xfnarrowwhv(<512 x i1> %47, i32 0, i32 1)
  %51 = tail call <256 x i1> @llvm.kvx.xfscalewv(<256 x i1> %49, i32 0, i32 0, i32 1)
  %52 = tail call { <4 x i64>, <256 x i1> } @llvm.kvx.xswapvo(<4 x i64> %17, <256 x i1> %51)
  %53 = extractvalue { <4 x i64>, <256 x i1> } %52, 1
  %54 = extractvalue { <4 x i64>, <256 x i1> } %52, 0
  %55 = tail call <256 x i1> @llvm.kvx.xfscalewv(<256 x i1> %53, i32 7, i32 0, i32 1)
  %56 = addrspacecast <256 x i1>* %4 to <256 x i1> addrspace(258)*
  %57 = load <256 x i1>, <256 x i1> addrspace(258)* %56, align 32
  %58 = getelementptr inbounds <256 x i1>, <256 x i1>* %4, i64 4
  %59 = bitcast <256 x i1>* %58 to i8*
  %60 = tail call <1024 x i1> @llvm.kvx.xload1024q(<1024 x i1> %48, i8* nonnull %59, i32 3, i32 1)
  %61 = getelementptr inbounds <256 x i1>, <256 x i1>* %4, i64 5
  %62 = bitcast <256 x i1>* %61 to i8*
  %63 = tail call <1024 x i1> @llvm.kvx.xloadc1024q(<1024 x i1> %60, i8* nonnull %62, i64 %0, i32 2, i32 0, i32 6)
  store <256 x i1> %57, <256 x i1>* %4, align 32
  %64 = bitcast <256 x i1>* %13 to i8*
  tail call void @llvm.kvx.sv.cond(i8* nonnull %64, <256 x i1> %57, i64 1, i32 7)
  store volatile <512 x i1> %47, <512 x i1>* %5, align 32
  store volatile <1024 x i1> %63, <1024 x i1>* %6, align 32
  ret <4 x i64> %54
}

declare <256 x i1> @llvm.kvx.xmovetohi(<256 x i1>, i64, i64)

declare <256 x i1> @llvm.kvx.xmovetolo(<256 x i1>, i64, i64)

declare <256 x i1> @llvm.kvx.xmoveto(i64, i64, i64, i64)

declare <256 x i1> @llvm.kvx.xmoveoto(<4 x i64>)

declare <256 x i1> @llvm.kvx.alignv(<256 x i1>, <256 x i1>, i64)

declare <4 x i64> @llvm.kvx.alignov(<256 x i1>, <256 x i1>, i64)

declare <256 x i1> @llvm.kvx.xconvdhv0(<256 x i1>, <1024 x i1>, i32, i32)

declare <256 x i1> @llvm.kvx.xconvdhv1(<256 x i1>, <1024 x i1>, i32, i32)

declare <256 x i1> @llvm.kvx.xconvwbv(<1024 x i1>, i32, i32)

declare <256 x i1> @llvm.kvx.xfmma242hw0(<256 x i1>, <512 x i1>, <256 x i1>, <256 x i1>)

declare <256 x i1> @llvm.kvx.xfmma242hw1(<256 x i1>, <512 x i1>, <256 x i1>, <256 x i1>)

declare <256 x i1> @llvm.kvx.xfmma242hw2(<256 x i1>, <512 x i1>, <256 x i1>, <256 x i1>)

declare <256 x i1> @llvm.kvx.xfmma242hw3(<256 x i1>, <512 x i1>, <256 x i1>, <256 x i1>)

declare <512 x i1> @llvm.kvx.xfmma444hw(<512 x i1>, <256 x i1>, <256 x i1>)

declare <1024 x i1> @llvm.kvx.xmma444hbd0(<1024 x i1>, <256 x i1>, <256 x i1>)

declare <1024 x i1> @llvm.kvx.xmma444hbd1(<1024 x i1>, <256 x i1>, <256 x i1>)

declare <1024 x i1> @llvm.kvx.xmma484hbd(<1024 x i1>, <512 x i1>, <256 x i1>)

declare <1024 x i1> @llvm.kvx.xmma444hd(<1024 x i1>, <256 x i1>, <256 x i1>)

declare <1024 x i1> @llvm.kvx.xmma444suhbd0(<1024 x i1>, <256 x i1>, <256 x i1>)

declare <1024 x i1> @llvm.kvx.xmma444suhbd1(<1024 x i1>, <256 x i1>, <256 x i1>)

declare <1024 x i1> @llvm.kvx.xmma484suhbd(<1024 x i1>, <512 x i1>, <256 x i1>)

declare <1024 x i1> @llvm.kvx.xmma444suhd(<1024 x i1>, <256 x i1>, <256 x i1>)

declare <1024 x i1> @llvm.kvx.xmma444uhbd0(<1024 x i1>, <256 x i1>, <256 x i1>)

declare <1024 x i1> @llvm.kvx.xmma444uhbd1(<1024 x i1>, <256 x i1>, <256 x i1>)

declare <1024 x i1> @llvm.kvx.xmma484uhbd(<1024 x i1>, <512 x i1>, <256 x i1>)

declare <1024 x i1> @llvm.kvx.xmma444uhd(<1024 x i1>, <256 x i1>, <256 x i1>)

declare <1024 x i1> @llvm.kvx.xmma444ushbd0(<1024 x i1>, <256 x i1>, <256 x i1>)

declare <1024 x i1> @llvm.kvx.xmma444ushbd1(<1024 x i1>, <256 x i1>, <256 x i1>)

declare <1024 x i1> @llvm.kvx.xmma484ushbd(<1024 x i1>, <512 x i1>, <256 x i1>)

declare <1024 x i1> @llvm.kvx.xmma444ushd(<1024 x i1>, <256 x i1>, <256 x i1>)

declare <512 x i1> @llvm.kvx.xmma484bw(<512 x i1>, <256 x i1>, <256 x i1>)

declare <512 x i1> @llvm.kvx.xmma484subw(<512 x i1>, <256 x i1>, <256 x i1>)

declare <512 x i1> @llvm.kvx.xmma484ubw(<512 x i1>, <256 x i1>, <256 x i1>)

declare <512 x i1> @llvm.kvx.xmma484usbw(<512 x i1>, <256 x i1>, <256 x i1>)

declare <1024 x i1> @llvm.kvx.xmt44d(<1024 x i1>)

declare <256 x i1> @llvm.kvx.xfscalewv(<256 x i1>, i32, i32, i32)

declare <256 x i1> @llvm.kvx.xfnarrowwhv(<512 x i1>, i32, i32)

declare { <4 x i64>, <256 x i1> } @llvm.kvx.xswapvo(<4 x i64>, <256 x i1>)

declare <1024 x i1> @llvm.kvx.xload1024q(<1024 x i1>, i8*, i32, i32)

declare <1024 x i1> @llvm.kvx.xloadc1024q(<1024 x i1>, i8*, i64, i32, i32, i32)

declare void @llvm.kvx.sv.cond(i8*, <256 x i1>, i64, i32)

