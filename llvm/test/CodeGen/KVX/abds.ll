; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mcpu=kv3-1 -O2 -o - %s | FileCheck %s --check-prefixes=V1
; RUN: llc -mcpu=kv3-2 -O2 -o - %s | FileCheck %s --check-prefixes=V2
target triple = "kvx-kalray-cos"

declare i64 @llvm.abs.i64(i64, i1 immarg)

declare <8 x i8> @llvm.abs.v8i8(<8 x i8>, i1)
declare <8 x i8> @llvm.ssub.sat.v8i8(<8 x i8>, <8 x i8>)

declare <4 x i8> @llvm.abs.v4i8(<4 x i8>, i1)
declare <4 x i8> @llvm.ssub.sat.v4i8(<4 x i8>, <4 x i8>)

declare <2 x i8> @llvm.abs.v2i8(<2 x i8>, i1)
declare <2 x i8> @llvm.ssub.sat.v2i8(<2 x i8>, <2 x i8>)

declare <4 x i16> @llvm.abs.v4i16(<4 x i16>, i1)
declare <4 x i16> @llvm.ssub.sat.v4i16(<4 x i16>, <4 x i16>)

declare <2 x i16> @llvm.abs.v2i16(<2 x i16>, i1)
declare <2 x i16> @llvm.ssub.sat.v2i16(<2 x i16>, <2 x i16>)

declare <2 x i32> @llvm.abs.v2i32(<2 x i32>, i1)
declare <2 x i32> @llvm.ssub.sat.v2i32(<2 x i32>, <2 x i32>)

define i32 @ABDSWrr(i32 %0, i32 %1) {
; V1-LABEL: ABDSWrr:
; V1:       # %bb.0:
; V1-NEXT:    sxwd $r0 = $r0
; V1-NEXT:    sxwd $r1 = $r1
; V1-NEXT:    ;;
; V1-NEXT:    abdd $r0 = $r0, $r1
; V1-NEXT:    ;;
; V1-NEXT:    minud $r0 = $r0, 0x7fffffff
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: ABDSWrr:
; V2:       # %bb.0:
; V2-NEXT:    abdsw $r0 = $r0, $r1
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = sext i32 %1 to i64
  %4 = sext i32 %0 to i64
  %5 = sub nsw i64 %3, %4
  %6 = tail call i64 @llvm.abs.i64(i64 %5, i1 true)
  %7 = icmp ult i64 %6, 2147483647
  %8 = select i1 %7, i64 %6, i64 2147483647
  %9 = trunc i64 %8 to i32
  ret i32 %9
}

define i32 @ABDSWri(i32 %0) {
; V1-LABEL: ABDSWri:
; V1:       # %bb.0:
; V1-NEXT:    sxwd $r0 = $r0
; V1-NEXT:    ;;
; V1-NEXT:    abdd $r0 = $r0, 0x3039
; V1-NEXT:    ;;
; V1-NEXT:    minud $r0 = $r0, 0x7fffffff
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: ABDSWri:
; V2:       # %bb.0:
; V2-NEXT:    abdsw $r0 = $r0, 0x3039
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %2 = sext i32 %0 to i64
  %3 = sub nsw i64 12345, %2
  %4 = tail call i64 @llvm.abs.i64(i64 %3, i1 true)
  %5 = icmp ult i64 %4, 2147483647
  %6 = select i1 %5, i64 %4, i64 2147483647
  %7 = trunc i64 %6 to i32
  ret i32 %7
}


define i64 @abdsd_builtins(i64 %0, i64 %1) {
; V1-LABEL: abdsd_builtins:
; V1:       # %bb.0:
; V1-NEXT:    sbfsd $r0 = $r1, $r0
; V1-NEXT:    ;;
; V1-NEXT:    absd $r0 = $r0
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: abdsd_builtins:
; V2:       # %bb.0:
; V2-NEXT:    abdsd $r0 = $r1, $r0
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call i64 @llvm.ssub.sat.i64(i64 %0, i64 %1)
  %4 = tail call i64 @llvm.abs.i64(i64 %3, i1 false)
  ret i64 %4
}

declare i64 @llvm.ssub.sat.i64(i64, i64)

define i32 @abdsw_builtins(i32 %0, i32 %1) {
; V1-LABEL: abdsw_builtins:
; V1:       # %bb.0:
; V1-NEXT:    sbfsw $r0 = $r1, $r0
; V1-NEXT:    ;;
; V1-NEXT:    absw $r0 = $r0
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: abdsw_builtins:
; V2:       # %bb.0:
; V2-NEXT:    abdsw $r0 = $r1, $r0
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call i32 @llvm.ssub.sat.i32(i32 %0, i32 %1)
  %4 = tail call i32 @llvm.abs.i32(i32 %3, i1 false)
  ret i32 %4
}

declare i32 @llvm.ssub.sat.i32(i32, i32)

declare i32 @llvm.abs.i32(i32, i1 immarg)


define <8 x i8> @v8i8abds_rr(<8 x i8> %0, <8 x i8> %1) #0 {
; V1-LABEL: v8i8abds_rr:
; V1:       # %bb.0:
; V1-NEXT:    sbmm8 $r2 = $r0, 0x4000200004000100
; V1-NEXT:    sbmm8 $r3 = $r1, 0x4000200004000100
; V1-NEXT:    ;;
; V1-NEXT:    andd $r0 = $r0, 0xff00ff00ff00ff00
; V1-NEXT:    andd $r1 = $r1, 0xff00ff00ff00ff00
; V1-NEXT:    sbfshq $r2 = $r3, $r2
; V1-NEXT:    ;;
; V1-NEXT:    sbfshq $r0 = $r1, $r0
; V1-NEXT:    srlhqs $r1 = $r2, 8
; V1-NEXT:    ;;
; V1-NEXT:    andd $r0 = $r0, 0xff00ff00ff00ff00
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r0, $r1
; V1-NEXT:    ;;
; V1-NEXT:    sxlbhq $r0 = $r0
; V1-NEXT:    sxmbhq $r1 = $r0
; V1-NEXT:    ;;
; V1-NEXT:    abshq $r0 = $r0
; V1-NEXT:    abshq $r1 = $r1
; V1-NEXT:    ;;
; V1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; V1-NEXT:    sbmm8 $r1 = $r1, 0x40100401
; V1-NEXT:    ;;
; V1-NEXT:    insf $r0 = $r1, 32, 63
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: v8i8abds_rr:
; V2:       # %bb.0:
; V2-NEXT:    abdsbo $r0 = $r1, $r0
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <8 x i8> @llvm.ssub.sat.v8i8(<8 x i8> %0, <8 x i8> %1)
  %4 = tail call <8 x i8> @llvm.abs.v8i8(<8 x i8> %3, i1 false)
  ret <8 x i8> %4
}

define <8 x i8> @v8i8abds_ri_(<8 x i8> %0) #0 {
; V1-LABEL: v8i8abds_ri_:
; V1:       # %bb.0:
; V1-NEXT:    make $r1 = 0x1004ff01
; V1-NEXT:    sbmm8 $r2 = $r0, 0x4000200004000100
; V1-NEXT:    ;;
; V1-NEXT:    andd $r0 = $r0, 0xff00ff00ff00ff00
; V1-NEXT:    sbmm8 $r3 = $r1, 0x4000200004000100
; V1-NEXT:    ;;
; V1-NEXT:    andd $r1 = $r1, 0xff00ff00ff00ff00
; V1-NEXT:    sbfshq $r2 = $r3, $r2
; V1-NEXT:    ;;
; V1-NEXT:    sbfshq $r0 = $r1, $r0
; V1-NEXT:    srlhqs $r1 = $r2, 8
; V1-NEXT:    ;;
; V1-NEXT:    andd $r0 = $r0, 0xff00ff00ff00ff00
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r0, $r1
; V1-NEXT:    ;;
; V1-NEXT:    sxlbhq $r0 = $r0
; V1-NEXT:    sxmbhq $r1 = $r0
; V1-NEXT:    ;;
; V1-NEXT:    abshq $r0 = $r0
; V1-NEXT:    abshq $r1 = $r1
; V1-NEXT:    ;;
; V1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; V1-NEXT:    sbmm8 $r1 = $r1, 0x40100401
; V1-NEXT:    ;;
; V1-NEXT:    insf $r0 = $r1, 32, 63
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: v8i8abds_ri_:
; V2:       # %bb.0:
; V2-NEXT:    abdsbo $r0 = $r0, 0x1004ff01
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %2 = tail call <8 x i8> @llvm.ssub.sat.v8i8(<8 x i8> %0, <8 x i8> <i8 1, i8 -1, i8 4, i8 16, i8 0, i8 0, i8 0, i8 0>)
  %3 = tail call <8 x i8> @llvm.abs.v8i8(<8 x i8> %2, i1 false)
  ret <8 x i8> %3
}

define <8 x i8> @v8i8abds_ri_at(<8 x i8> %0) #0 {
; V1-LABEL: v8i8abds_ri_at:
; V1:       # %bb.0:
; V1-NEXT:    make $r1 = 0x1004ff011004ff01
; V1-NEXT:    sbmm8 $r2 = $r0, 0x4000200004000100
; V1-NEXT:    ;;
; V1-NEXT:    andd $r0 = $r0, 0xff00ff00ff00ff00
; V1-NEXT:    sbmm8 $r3 = $r1, 0x4000200004000100
; V1-NEXT:    ;;
; V1-NEXT:    andd $r1 = $r1, 0xff00ff00ff00ff00
; V1-NEXT:    sbfshq $r2 = $r3, $r2
; V1-NEXT:    ;;
; V1-NEXT:    sbfshq $r0 = $r1, $r0
; V1-NEXT:    srlhqs $r1 = $r2, 8
; V1-NEXT:    ;;
; V1-NEXT:    andd $r0 = $r0, 0xff00ff00ff00ff00
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r0, $r1
; V1-NEXT:    ;;
; V1-NEXT:    sxlbhq $r0 = $r0
; V1-NEXT:    sxmbhq $r1 = $r0
; V1-NEXT:    ;;
; V1-NEXT:    abshq $r0 = $r0
; V1-NEXT:    abshq $r1 = $r1
; V1-NEXT:    ;;
; V1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; V1-NEXT:    sbmm8 $r1 = $r1, 0x40100401
; V1-NEXT:    ;;
; V1-NEXT:    insf $r0 = $r1, 32, 63
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: v8i8abds_ri_at:
; V2:       # %bb.0:
; V2-NEXT:    abdsbo.@ $r0 = $r0, 0x1004ff01
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %2 = tail call <8 x i8> @llvm.ssub.sat.v8i8(<8 x i8> %0, <8 x i8> <i8 1, i8 -1, i8 4, i8 16, i8 1, i8 -1, i8 4, i8 16>)
  %3 = tail call <8 x i8> @llvm.abs.v8i8(<8 x i8> %2, i1 false)
  ret <8 x i8> %3
}

define <8 x i8> @v8i8abds_ri_at_2(<8 x i8> %0) #0 {
; V1-LABEL: v8i8abds_ri_at_2:
; V1:       # %bb.0:
; V1-NEXT:    make $r1 = 0x1004ff011004ff01
; V1-NEXT:    sbmm8 $r2 = $r0, 0x4000200004000100
; V1-NEXT:    ;;
; V1-NEXT:    andd $r1 = $r1, 0xff00ff00ff00ff00
; V1-NEXT:    sbmm8 $r3 = $r1, 0x4000200004000100
; V1-NEXT:    ;;
; V1-NEXT:    andd $r0 = $r0, 0xff00ff00ff00ff00
; V1-NEXT:    sbfshq $r2 = $r2, $r3
; V1-NEXT:    ;;
; V1-NEXT:    sbfshq $r0 = $r0, $r1
; V1-NEXT:    srlhqs $r1 = $r2, 8
; V1-NEXT:    ;;
; V1-NEXT:    andd $r0 = $r0, 0xff00ff00ff00ff00
; V1-NEXT:    ;;
; V1-NEXT:    ord $r0 = $r0, $r1
; V1-NEXT:    ;;
; V1-NEXT:    sxlbhq $r0 = $r0
; V1-NEXT:    sxmbhq $r1 = $r0
; V1-NEXT:    ;;
; V1-NEXT:    abshq $r0 = $r0
; V1-NEXT:    abshq $r1 = $r1
; V1-NEXT:    ;;
; V1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; V1-NEXT:    sbmm8 $r1 = $r1, 0x40100401
; V1-NEXT:    ;;
; V1-NEXT:    insf $r0 = $r1, 32, 63
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: v8i8abds_ri_at_2:
; V2:       # %bb.0:
; V2-NEXT:    abdsbo.@ $r0 = $r0, 0x1004ff01
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %2 = tail call <8 x i8> @llvm.ssub.sat.v8i8(<8 x i8> <i8 1, i8 -1, i8 4, i8 16, i8 1, i8 -1, i8 4, i8 16>, <8 x i8> %0)
  %3 = tail call <8 x i8> @llvm.abs.v8i8(<8 x i8> %2, i1 false)
  ret <8 x i8> %3
}

define <4 x i8> @v4i8abds_rr(<4 x i8> %0, <4 x i8> %1) {
; V1-LABEL: v4i8abds_rr:
; V1:       # %bb.0:
; V1-NEXT:    srlw $r2 = $r0, 24
; V1-NEXT:    srlw $r3 = $r1, 24
; V1-NEXT:    extfz $r4 = $r0, 23, 16
; V1-NEXT:    extfz $r5 = $r1, 23, 16
; V1-NEXT:    ;;
; V1-NEXT:    sllw $r2 = $r2, 24
; V1-NEXT:    sllw $r3 = $r3, 24
; V1-NEXT:    sllw $r4 = $r4, 24
; V1-NEXT:    sllw $r5 = $r5, 24
; V1-NEXT:    ;;
; V1-NEXT:    sbfsw $r2 = $r3, $r2
; V1-NEXT:    sbfsw $r3 = $r5, $r4
; V1-NEXT:    ;;
; V1-NEXT:    zxbd $r0 = $r0
; V1-NEXT:    zxbd $r1 = $r1
; V1-NEXT:    extfz $r4 = $r0, 15, 8
; V1-NEXT:    extfz $r5 = $r1, 15, 8
; V1-NEXT:    ;;
; V1-NEXT:    sllw $r0 = $r0, 24
; V1-NEXT:    sllw $r1 = $r1, 24
; V1-NEXT:    sllw $r4 = $r4, 24
; V1-NEXT:    sllw $r5 = $r5, 24
; V1-NEXT:    ;;
; V1-NEXT:    sbfsw $r0 = $r1, $r0
; V1-NEXT:    sraw $r1 = $r2, 24
; V1-NEXT:    sraw $r2 = $r3, 24
; V1-NEXT:    sbfsw $r4 = $r5, $r4
; V1-NEXT:    ;;
; V1-NEXT:    sraw $r0 = $r0, 24
; V1-NEXT:    insf $r2 = $r1, 15, 8
; V1-NEXT:    sraw $r3 = $r4, 24
; V1-NEXT:    ;;
; V1-NEXT:    insf $r0 = $r3, 15, 8
; V1-NEXT:    ;;
; V1-NEXT:    insf $r0 = $r2, 31, 16
; V1-NEXT:    ;;
; V1-NEXT:    sxlbhq $r0 = $r0
; V1-NEXT:    ;;
; V1-NEXT:    abshq $r0 = $r0
; V1-NEXT:    ;;
; V1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: v4i8abds_rr:
; V2:       # %bb.0:
; V2-NEXT:    abdsbo $r0 = $r0, $r1
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x i8> @llvm.ssub.sat.v4i8(<4 x i8> %0, <4 x i8> %1)
  %4 = tail call <4 x i8> @llvm.abs.v4i8(<4 x i8> %3, i1 false)
  ret <4 x i8> %4
}

define <4 x i8> @v4i8abds_ri_(<4 x i8> %0) {
; V1-LABEL: v4i8abds_ri_:
; V1:       # %bb.0:
; V1-NEXT:    srlw $r1 = $r0, 24
; V1-NEXT:    make $r2 = 0x10000000
; V1-NEXT:    extfz $r3 = $r0, 23, 16
; V1-NEXT:    make $r4 = 0x4000000
; V1-NEXT:    ;;
; V1-NEXT:    zxbd $r0 = $r0
; V1-NEXT:    sllw $r1 = $r1, 24
; V1-NEXT:    sllw $r3 = $r3, 24
; V1-NEXT:    extfz $r5 = $r0, 15, 8
; V1-NEXT:    ;;
; V1-NEXT:    sbfsw $r1 = $r2, $r1
; V1-NEXT:    sbfsw $r2 = $r4, $r3
; V1-NEXT:    make $r3 = 0xff000000
; V1-NEXT:    sllw $r5 = $r5, 24
; V1-NEXT:    ;;
; V1-NEXT:    sllw $r0 = $r0, 24
; V1-NEXT:    sraw $r1 = $r1, 24
; V1-NEXT:    sbfsw $r3 = $r3, $r5
; V1-NEXT:    make $r4 = 0x1000000
; V1-NEXT:    ;;
; V1-NEXT:    sbfsw $r0 = $r4, $r0
; V1-NEXT:    sraw $r2 = $r2, 24
; V1-NEXT:    sraw $r3 = $r3, 24
; V1-NEXT:    ;;
; V1-NEXT:    sraw $r0 = $r0, 24
; V1-NEXT:    insf $r2 = $r1, 15, 8
; V1-NEXT:    ;;
; V1-NEXT:    insf $r0 = $r3, 15, 8
; V1-NEXT:    ;;
; V1-NEXT:    insf $r0 = $r2, 31, 16
; V1-NEXT:    ;;
; V1-NEXT:    sxlbhq $r0 = $r0
; V1-NEXT:    ;;
; V1-NEXT:    abshq $r0 = $r0
; V1-NEXT:    ;;
; V1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: v4i8abds_ri_:
; V2:       # %bb.0:
; V2-NEXT:    abdsbo $r0 = $r0, 0x1004ff01
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %2 = tail call <4 x i8> @llvm.ssub.sat.v4i8(<4 x i8> %0, <4 x i8> <i8 1, i8 -1, i8 4, i8 16>)
  %3 = tail call <4 x i8> @llvm.abs.v4i8(<4 x i8> %2, i1 false)
  ret <4 x i8> %3
}

define <4 x i8> @v4i8abds_ri_2(<4 x i8> %0) {
; V1-LABEL: v4i8abds_ri_2:
; V1:       # %bb.0:
; V1-NEXT:    zxbd $r0 = $r0
; V1-NEXT:    srlw $r1 = $r0, 24
; V1-NEXT:    extfz $r2 = $r0, 23, 16
; V1-NEXT:    extfz $r3 = $r0, 15, 8
; V1-NEXT:    ;;
; V1-NEXT:    sllw $r0 = $r0, 24
; V1-NEXT:    sllw $r1 = $r1, 24
; V1-NEXT:    sllw $r2 = $r2, 24
; V1-NEXT:    sllw $r3 = $r3, 24
; V1-NEXT:    ;;
; V1-NEXT:    sbfsw $r1 = $r1, 0x10000000
; V1-NEXT:    sbfsw $r2 = $r2, 0x4000000
; V1-NEXT:    ;;
; V1-NEXT:    sbfsw $r0 = $r0, 0x1000000
; V1-NEXT:    sraw $r1 = $r1, 24
; V1-NEXT:    sraw $r2 = $r2, 24
; V1-NEXT:    sbfsw $r3 = $r3, 0xff000000
; V1-NEXT:    ;;
; V1-NEXT:    sraw $r0 = $r0, 24
; V1-NEXT:    insf $r2 = $r1, 15, 8
; V1-NEXT:    sraw $r3 = $r3, 24
; V1-NEXT:    ;;
; V1-NEXT:    insf $r0 = $r3, 15, 8
; V1-NEXT:    ;;
; V1-NEXT:    insf $r0 = $r2, 31, 16
; V1-NEXT:    ;;
; V1-NEXT:    sxlbhq $r0 = $r0
; V1-NEXT:    ;;
; V1-NEXT:    abshq $r0 = $r0
; V1-NEXT:    ;;
; V1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: v4i8abds_ri_2:
; V2:       # %bb.0:
; V2-NEXT:    abdsbo $r0 = $r0, 0x1004ff01
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %2 = tail call <4 x i8> @llvm.ssub.sat.v4i8(<4 x i8> <i8 1, i8 -1, i8 4, i8 16>, <4 x i8> %0)
  %3 = tail call <4 x i8> @llvm.abs.v4i8(<4 x i8> %2, i1 false)
  ret <4 x i8> %3
}

define <2 x i8> @v2i8abds_rr(<2 x i8> %0, <2 x i8> %1) {
; V1-LABEL: v2i8abds_rr:
; V1:       # %bb.0:
; V1-NEXT:    zxbd $r0 = $r0
; V1-NEXT:    zxbd $r1 = $r1
; V1-NEXT:    extfz $r2 = $r0, 15, 8
; V1-NEXT:    extfz $r3 = $r1, 15, 8
; V1-NEXT:    ;;
; V1-NEXT:    sllw $r0 = $r0, 24
; V1-NEXT:    sllw $r1 = $r1, 24
; V1-NEXT:    sllw $r2 = $r2, 24
; V1-NEXT:    sllw $r3 = $r3, 24
; V1-NEXT:    ;;
; V1-NEXT:    sbfsw $r0 = $r1, $r0
; V1-NEXT:    sbfsw $r2 = $r3, $r2
; V1-NEXT:    ;;
; V1-NEXT:    sraw $r0 = $r0, 24
; V1-NEXT:    sraw $r1 = $r2, 24
; V1-NEXT:    ;;
; V1-NEXT:    insf $r0 = $r1, 15, 8
; V1-NEXT:    ;;
; V1-NEXT:    sxlbhq $r0 = $r0
; V1-NEXT:    ;;
; V1-NEXT:    abshq $r0 = $r0
; V1-NEXT:    ;;
; V1-NEXT:    sbmm8 $r0 = $r0, 0x401
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: v2i8abds_rr:
; V2:       # %bb.0:
; V2-NEXT:    abdsbo $r0 = $r0, $r1
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <2 x i8> @llvm.ssub.sat.v2i8(<2 x i8> %0, <2 x i8> %1)
  %4 = tail call <2 x i8> @llvm.abs.v2i8(<2 x i8> %3, i1 false)
  ret <2 x i8> %4
}

define <2 x i8> @v2i8abds_ri_(<2 x i8> %0) {
; V1-LABEL: v2i8abds_ri_:
; V1:       # %bb.0:
; V1-NEXT:    zxbd $r0 = $r0
; V1-NEXT:    extfz $r1 = $r0, 15, 8
; V1-NEXT:    make $r2 = 0xff000000
; V1-NEXT:    make $r3 = 0x1000000
; V1-NEXT:    ;;
; V1-NEXT:    sllw $r0 = $r0, 24
; V1-NEXT:    sllw $r1 = $r1, 24
; V1-NEXT:    ;;
; V1-NEXT:    sbfsw $r0 = $r3, $r0
; V1-NEXT:    sbfsw $r1 = $r2, $r1
; V1-NEXT:    ;;
; V1-NEXT:    sraw $r0 = $r0, 24
; V1-NEXT:    sraw $r1 = $r1, 24
; V1-NEXT:    ;;
; V1-NEXT:    insf $r0 = $r1, 15, 8
; V1-NEXT:    ;;
; V1-NEXT:    sxlbhq $r0 = $r0
; V1-NEXT:    ;;
; V1-NEXT:    abshq $r0 = $r0
; V1-NEXT:    ;;
; V1-NEXT:    sbmm8 $r0 = $r0, 0x401
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: v2i8abds_ri_:
; V2:       # %bb.0:
; V2-NEXT:    abdsbo $r0 = $r0, -255
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %2 = tail call <2 x i8> @llvm.ssub.sat.v2i8(<2 x i8> %0, <2 x i8> <i8 1, i8 -1>)
  %3 = tail call <2 x i8> @llvm.abs.v2i8(<2 x i8> %2, i1 false)
  ret <2 x i8> %3
}

define <2 x i8> @v2i8abds_ri_at(<2 x i8> %0) {
; V1-LABEL: v2i8abds_ri_at:
; V1:       # %bb.0:
; V1-NEXT:    zxbd $r0 = $r0
; V1-NEXT:    extfz $r1 = $r0, 15, 8
; V1-NEXT:    make $r2 = 0xff000000
; V1-NEXT:    make $r3 = 0x1000000
; V1-NEXT:    ;;
; V1-NEXT:    sllw $r0 = $r0, 24
; V1-NEXT:    sllw $r1 = $r1, 24
; V1-NEXT:    ;;
; V1-NEXT:    sbfsw $r0 = $r3, $r0
; V1-NEXT:    sbfsw $r1 = $r2, $r1
; V1-NEXT:    ;;
; V1-NEXT:    sraw $r0 = $r0, 24
; V1-NEXT:    sraw $r1 = $r1, 24
; V1-NEXT:    ;;
; V1-NEXT:    insf $r0 = $r1, 15, 8
; V1-NEXT:    ;;
; V1-NEXT:    sxlbhq $r0 = $r0
; V1-NEXT:    ;;
; V1-NEXT:    abshq $r0 = $r0
; V1-NEXT:    ;;
; V1-NEXT:    sbmm8 $r0 = $r0, 0x401
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: v2i8abds_ri_at:
; V2:       # %bb.0:
; V2-NEXT:    abdsbo $r0 = $r0, -255
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %2 = tail call <2 x i8> @llvm.ssub.sat.v2i8(<2 x i8> %0, <2 x i8> <i8 1, i8 -1>)
  %3 = tail call <2 x i8> @llvm.abs.v2i8(<2 x i8> %2, i1 false)
  ret <2 x i8> %3
}

define <2 x i32> @v2i32abds_ri_at(<2 x i32> %0) {
; V1-LABEL: v2i32abds_ri_at:
; V1:       # %bb.0:
; V1-NEXT:    make $r1 = -1
; V1-NEXT:    ;;
; V1-NEXT:    sbfswp $r0 = $r1, $r0
; V1-NEXT:    ;;
; V1-NEXT:    abswp $r0 = $r0
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: v2i32abds_ri_at:
; V2:       # %bb.0:
; V2-NEXT:    abdswp.@ $r0 = $r0, -1
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %2 = tail call <2 x i32> @llvm.ssub.sat.v2i32(<2 x i32> %0, <2 x i32> <i32 -1, i32 -1>)
  %3 = tail call <2 x i32> @llvm.abs.v2i32(<2 x i32> %2, i1 false)
  ret <2 x i32> %3
}

define <2 x i32> @v2i32abds_ri_at_2(<2 x i32> %0) {
; V1-LABEL: v2i32abds_ri_at_2:
; V1:       # %bb.0:
; V1-NEXT:    sbfswp.@ $r0 = $r0, -1
; V1-NEXT:    ;;
; V1-NEXT:    abswp $r0 = $r0
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: v2i32abds_ri_at_2:
; V2:       # %bb.0:
; V2-NEXT:    abdswp.@ $r0 = $r0, -1
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %2 = tail call <2 x i32> @llvm.ssub.sat.v2i32(<2 x i32> <i32 -1, i32 -1>, <2 x i32> %0)
  %3 = tail call <2 x i32> @llvm.abs.v2i32(<2 x i32> %2, i1 false)
  ret <2 x i32> %3
}

define <2 x i32> @v2i32abds_ri_(<2 x i32> %0) {
; V1-LABEL: v2i32abds_ri_:
; V1:       # %bb.0:
; V1-NEXT:    make $r1 = 0xffffffff
; V1-NEXT:    ;;
; V1-NEXT:    sbfswp $r0 = $r1, $r0
; V1-NEXT:    ;;
; V1-NEXT:    abswp $r0 = $r0
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: v2i32abds_ri_:
; V2:       # %bb.0:
; V2-NEXT:    abdswp $r0 = $r0, 0xffffffff
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %2 = tail call <2 x i32> @llvm.ssub.sat.v2i32(<2 x i32> %0, <2 x i32> <i32 -1, i32 0>)
  %3 = tail call <2 x i32> @llvm.abs.v2i32(<2 x i32> %2, i1 false)
  ret <2 x i32> %3
}

define <2 x i32> @v2i32abds_ri_2(<2 x i32> %0) {
; V1-LABEL: v2i32abds_ri_2:
; V1:       # %bb.0:
; V1-NEXT:    sbfswp $r0 = $r0, -1
; V1-NEXT:    ;;
; V1-NEXT:    abswp $r0 = $r0
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: v2i32abds_ri_2:
; V2:       # %bb.0:
; V2-NEXT:    abdswp $r0 = $r0, 0xffffffff
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %2 = tail call <2 x i32> @llvm.ssub.sat.v2i32(<2 x i32> <i32 -1, i32 0>, <2 x i32> %0)
  %3 = tail call <2 x i32> @llvm.abs.v2i32(<2 x i32> %2, i1 false)
  ret <2 x i32> %3
}

define <2 x i32> @v2i32abds_rr(<2 x i32> %0, <2 x i32> %1) {
; V1-LABEL: v2i32abds_rr:
; V1:       # %bb.0:
; V1-NEXT:    sbfswp $r0 = $r1, $r0
; V1-NEXT:    ;;
; V1-NEXT:    abswp $r0 = $r0
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: v2i32abds_rr:
; V2:       # %bb.0:
; V2-NEXT:    abdswp $r0 = $r1, $r0
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <2 x i32> @llvm.ssub.sat.v2i32(<2 x i32> %0, <2 x i32> %1)
  %4 = tail call <2 x i32> @llvm.abs.v2i32(<2 x i32> %3, i1 false)
  ret <2 x i32> %4
}

define i64 @abdsd(i64 %0, i64 %1) {
; V1-LABEL: abdsd:
; V1:       # %bb.0:
; V1-NEXT:    sbfsd $r0 = $r1, $r0
; V1-NEXT:    ;;
; V1-NEXT:    absd $r0 = $r0
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: abdsd:
; V2:       # %bb.0:
; V2-NEXT:    abdsd $r0 = $r1, $r0
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call i64 @llvm.ssub.sat.i64(i64 %0, i64 %1)
  %4 = tail call i64 @llvm.abs.i64(i64 %3, i1 false)
  ret i64 %4
}

define i64 @abdsd_ri10(i64 %0, i64 %1) {
; V1-LABEL: abdsd_ri10:
; V1:       # %bb.0:
; V1-NEXT:    sbfsd $r0 = $r0, 171
; V1-NEXT:    ;;
; V1-NEXT:    absd $r0 = $r0
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: abdsd_ri10:
; V2:       # %bb.0:
; V2-NEXT:    abdsd $r0 = $r0, 171
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call i64 @llvm.ssub.sat.i64(i64 171, i64 %0)
  %4 = tail call i64 @llvm.abs.i64(i64 %3, i1 false)
  ret i64 %4
}

define i64 @abdsd_ri_37(i64 %0, i64 %1) {
; V1-LABEL: abdsd_ri_37:
; V1:       # %bb.0:
; V1-NEXT:    sbfsd $r0 = $r0, 0x1deadbeef
; V1-NEXT:    ;;
; V1-NEXT:    absd $r0 = $r0
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: abdsd_ri_37:
; V2:       # %bb.0:
; V2-NEXT:    make $r1 = 0x1deadbeef
; V2-NEXT:    ;;
; V2-NEXT:    abdsd $r0 = $r0, $r1
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call i64 @llvm.ssub.sat.i64(i64 8030895855, i64 %0)
  %4 = tail call i64 @llvm.abs.i64(i64 %3, i1 false)
  ret i64 %4
}

define i64 @abdsd_ri64(i64 %0, i64 %1) {
; V1-LABEL: abdsd_ri64:
; V1:       # %bb.0:
; V1-NEXT:    sbfsd $r0 = $r0, 0x1dadbeefdeadbeef
; V1-NEXT:    ;;
; V1-NEXT:    absd $r0 = $r0
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: abdsd_ri64:
; V2:       # %bb.0:
; V2-NEXT:    make $r1 = 0x1dadbeefdeadbeef
; V2-NEXT:    ;;
; V2-NEXT:    abdsd $r0 = $r0, $r1
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call i64 @llvm.ssub.sat.i64(i64 2138575335513243375, i64 %0)
  %4 = tail call i64 @llvm.abs.i64(i64 %3, i1 false)
  ret i64 %4
}

define i64 @abdsd_ri(i64 %0, i64 %1) {
; V1-LABEL: abdsd_ri:
; V1:       # %bb.0:
; V1-NEXT:    sbfsd $r0 = $r0, 0xdeadbeef
; V1-NEXT:    ;;
; V1-NEXT:    absd $r0 = $r0
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: abdsd_ri:
; V2:       # %bb.0:
; V2-NEXT:    abdsd $r0 = $r0, 0xdeadbeef
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call i64 @llvm.ssub.sat.i64(i64 3735928559, i64 %0)
  %4 = tail call i64 @llvm.abs.i64(i64 %3, i1 false)
  ret i64 %4
}

define i64 @abdsd_ri_at(i64 %0, i64 %1) {
; V1-LABEL: abdsd_ri_at:
; V1:       # %bb.0:
; V1-NEXT:    sbfsd $r0 = $r0, 0xdeadbeefdeadbeef
; V1-NEXT:    ;;
; V1-NEXT:    absd $r0 = $r0
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: abdsd_ri_at:
; V2:       # %bb.0:
; V2-NEXT:    abdsd.@ $r0 = $r0, 0xdeadbeef
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call i64 @llvm.ssub.sat.i64(i64 -2401053088876216593, i64 %0)
  %4 = tail call i64 @llvm.abs.i64(i64 %3, i1 false)
  ret i64 %4
}


define <4 x i16> @v4i16abds_rr(<4 x i16> %0, <4 x i16> %1) {
; V1-LABEL: v4i16abds_rr:
; V1:       # %bb.0:
; V1-NEXT:    sbfshq $r0 = $r1, $r0
; V1-NEXT:    ;;
; V1-NEXT:    abshq $r0 = $r0
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: v4i16abds_rr:
; V2:       # %bb.0:
; V2-NEXT:    abdshq $r0 = $r1, $r0
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <4 x i16> @llvm.ssub.sat.v4i16(<4 x i16> %0, <4 x i16> %1)
  %4 = tail call <4 x i16> @llvm.abs.v4i16(<4 x i16> %3, i1 false)
  ret <4 x i16> %4
}

define <4 x i16> @v4i16abds_ri_(<4 x i16> %0) {
; V1-LABEL: v4i16abds_ri_:
; V1:       # %bb.0:
; V1-NEXT:    make $r1 = 0xffff0001
; V1-NEXT:    ;;
; V1-NEXT:    sbfshq $r0 = $r1, $r0
; V1-NEXT:    ;;
; V1-NEXT:    abshq $r0 = $r0
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: v4i16abds_ri_:
; V2:       # %bb.0:
; V2-NEXT:    abdshq $r0 = $r0, 0xffff0001
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %2 = tail call <4 x i16> @llvm.ssub.sat.v4i16(<4 x i16> %0, <4 x i16> <i16 1, i16 -1, i16 0, i16 0>)
  %3 = tail call <4 x i16> @llvm.abs.v4i16(<4 x i16> %2, i1 false)
  ret <4 x i16> %3
}

define <4 x i16> @v4i16abds_ri_at(<4 x i16> %0) {
; V1-LABEL: v4i16abds_ri_at:
; V1:       # %bb.0:
; V1-NEXT:    sbfshq.@ $r0 = $r0, 0x100004
; V1-NEXT:    ;;
; V1-NEXT:    abshq $r0 = $r0
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: v4i16abds_ri_at:
; V2:       # %bb.0:
; V2-NEXT:    abdshq.@ $r0 = $r0, 0x100004
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %2 = tail call <4 x i16> @llvm.ssub.sat.v4i16(<4 x i16> <i16 4, i16 16, i16 4, i16 16>, <4 x i16> %0)
  %3 = tail call <4 x i16> @llvm.abs.v4i16(<4 x i16> %2, i1 false)
  ret <4 x i16> %3
}

define <2 x i16> @v2i16abds_rr(<2 x i16> %0, <2 x i16> %1) {
; V1-LABEL: v2i16abds_rr:
; V1:       # %bb.0:
; V1-NEXT:    sbfshq $r0 = $r1, $r0
; V1-NEXT:    ;;
; V1-NEXT:    abshq $r0 = $r0
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: v2i16abds_rr:
; V2:       # %bb.0:
; V2-NEXT:    abdshq $r0 = $r0, $r1
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %3 = tail call <2 x i16> @llvm.ssub.sat.v2i16(<2 x i16> %0, <2 x i16> %1)
  %4 = tail call <2 x i16> @llvm.abs.v2i16(<2 x i16> %3, i1 false)
  ret <2 x i16> %4
}

define <2 x i16> @v2i16abds_ri_(<2 x i16> %0) {
; V1-LABEL: v2i16abds_ri_:
; V1:       # %bb.0:
; V1-NEXT:    make $r1 = 0xffffffffffff0001
; V1-NEXT:    ;;
; V1-NEXT:    sbfshq $r0 = $r1, $r0
; V1-NEXT:    ;;
; V1-NEXT:    abshq $r0 = $r0
; V1-NEXT:    ret
; V1-NEXT:    ;;
;
; V2-LABEL: v2i16abds_ri_:
; V2:       # %bb.0:
; V2-NEXT:    abdshq $r0 = $r0, 0xffff0001
; V2-NEXT:    ret
; V2-NEXT:    ;;
  %2 = tail call <2 x i16> @llvm.ssub.sat.v2i16(<2 x i16> %0, <2 x i16> <i16 1, i16 -1>)
  %3 = tail call <2 x i16> @llvm.abs.v2i16(<2 x i16> %2, i1 false)
  ret <2 x i16> %3
}
