; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -O2 -disable-kvx-loadstore-packing=false | FileCheck %s
target triple = "kvx-kalray-cos"

@v = common global [50 x i64] zeroinitializer, align 8

define i64 @f_1_nopack(){
; CHECK-LABEL: f_1_nopack:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -32
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    .cfi_register 67, 16
; CHECK-NEXT:    sd 24[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -8
; CHECK-NEXT:    sd 16[$r12] = $r18
; CHECK-NEXT:    make $r0 = v
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 18, -16
; CHECK-NEXT:    ld $r18 = 0[$r0]
; CHECK-NEXT:    call foo
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r0 = $r18
; CHECK-NEXT:    ld $r18 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 24[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %0 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 0), align 8
  tail call void bitcast (void (...)* @foo to void ()*)() #2
  ret i64 %0
}

declare void @foo(...) #1

define i64 @f_2_pairpack(){
; CHECK-LABEL: f_2_pairpack:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -32
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    .cfi_register 67, 16
; CHECK-NEXT:    sd 24[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -8
; CHECK-NEXT:    sd 16[$r12] = $r18
; CHECK-NEXT:    make $r0 = v
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 18, -16
; CHECK-NEXT:    lq $r0r1 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r18 = $r1, $r0
; CHECK-NEXT:    call foo
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r0 = $r18
; CHECK-NEXT:    ld $r18 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 24[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %0 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 0), align 8
  %1 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 1), align 8
  tail call void bitcast (void (...)* @foo to void ()*)() #2
  %add = add nsw i64 %1, %0
  ret i64 %add
}

define i64 @f_3_pairpack(){
; CHECK-LABEL: f_3_pairpack:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -32
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    .cfi_register 67, 16
; CHECK-NEXT:    sd 24[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -8
; CHECK-NEXT:    sd 16[$r12] = $r18
; CHECK-NEXT:    make $r0 = v
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 18, -16
; CHECK-NEXT:    lq $r2r3 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r0 = 16[$r0]
; CHECK-NEXT:    addd $r1 = $r3, $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r18 = $r1, $r0
; CHECK-NEXT:    call foo
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r0 = $r18
; CHECK-NEXT:    ld $r18 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 24[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %0 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 0), align 8
  %1 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 1), align 8
  %2 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 2), align 8
  tail call void bitcast (void (...)* @foo to void ()*)() #2
  %add = add nsw i64 %1, %0
  %add1 = add nsw i64 %add, %2
  ret i64 %add1
}

define i64 @f_4_quadpack(){
; CHECK-LABEL: f_4_quadpack:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -64
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 64
; CHECK-NEXT:    .cfi_register 67, 16
; CHECK-NEXT:    sd 56[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -8
; CHECK-NEXT:    so 24[$r12] = $r20r21r22r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 20, -16
; CHECK-NEXT:    .cfi_offset 21, -24
; CHECK-NEXT:    .cfi_offset 22, -32
; CHECK-NEXT:    .cfi_offset 23, -40
; CHECK-NEXT:    sd 16[$r12] = $r18
; CHECK-NEXT:    make $r0 = v
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 18, -48
; CHECK-NEXT:    lo $r20r21r22r23 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r0 = $r21, $r20
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r18 = $r0, $r22
; CHECK-NEXT:    call foo
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r0 = $r18, $r23
; CHECK-NEXT:    ld $r18 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r20r21r22r23 = 24[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 56[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 64
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %0 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 0), align 8
  %1 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 1), align 8
  %2 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 2), align 8
  %3 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 3), align 8
  tail call void bitcast (void (...)* @foo to void ()*)() #2
  %add = add nsw i64 %1, %0
  %add1 = add nsw i64 %add, %2
  %add2 = add nsw i64 %add1, %3
  ret i64 %add2
}

define i64 @f_5_quadpack(){
; CHECK-LABEL: f_5_quadpack:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -32
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    .cfi_register 67, 16
; CHECK-NEXT:    sd 24[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -8
; CHECK-NEXT:    sq 8[$r12] = $r18r19
; CHECK-NEXT:    make $r0 = v
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 18, -16
; CHECK-NEXT:    .cfi_offset 19, -24
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r18 = 32[$r0]
; CHECK-NEXT:    addd $r1 = $r5, $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r1 = $r1, $r6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r19 = $r1, $r7
; CHECK-NEXT:    call foo
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r0 = $r19, $r18
; CHECK-NEXT:    lq $r18r19 = 8[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 24[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %0 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 0), align 8
  %1 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 1), align 8
  %2 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 2), align 8
  %3 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 3), align 8
  %4 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 4), align 8
  tail call void bitcast (void (...)* @foo to void ()*)() #2
  %add = add nsw i64 %1, %0
  %add1 = add nsw i64 %add, %2
  %add2 = add nsw i64 %add1, %3
  %add3 = add nsw i64 %add2, %4
  ret i64 %add3
}

define i64 @f_6_1quad1pairpack(){
; CHECK-LABEL: f_6_1quad1pairpack:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -32
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    .cfi_register 67, 16
; CHECK-NEXT:    sd 24[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -8
; CHECK-NEXT:    sq 8[$r12] = $r20r21
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 20, -16
; CHECK-NEXT:    .cfi_offset 21, -24
; CHECK-NEXT:    sd 0[$r12] = $r18
; CHECK-NEXT:    make $r0 = v
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 18, -32
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lq $r20r21 = 32[$r0]
; CHECK-NEXT:    addd $r1 = $r5, $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r1 = $r1, $r6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r18 = $r1, $r7
; CHECK-NEXT:    call foo
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r0 = $r18, $r20
; CHECK-NEXT:    ld $r18 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r0 = $r0, $r21
; CHECK-NEXT:    lq $r20r21 = 8[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 24[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %0 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 0), align 8
  %1 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 1), align 8
  %2 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 2), align 8
  %3 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 3), align 8
  %4 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 4), align 8
  %5 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 5), align 8
  tail call void bitcast (void (...)* @foo to void ()*)() #2
  %add = add nsw i64 %1, %0
  %add1 = add nsw i64 %add, %2
  %add2 = add nsw i64 %add1, %3
  %add3 = add nsw i64 %add2, %4
  %add4 = add nsw i64 %add3, %5
  ret i64 %add4
}

define i64 @f_14_3quad1pairpack(){
; CHECK-LABEL: f_14_3quad1pairpack:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -64
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 64
; CHECK-NEXT:    .cfi_register 67, 16
; CHECK-NEXT:    sd 56[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -8
; CHECK-NEXT:    sq 40[$r12] = $r24r25
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 24, -16
; CHECK-NEXT:    .cfi_offset 25, -24
; CHECK-NEXT:    so 8[$r12] = $r20r21r22r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 20, -32
; CHECK-NEXT:    .cfi_offset 21, -40
; CHECK-NEXT:    .cfi_offset 22, -48
; CHECK-NEXT:    .cfi_offset 23, -56
; CHECK-NEXT:    sd 0[$r12] = $r18
; CHECK-NEXT:    make $r0 = v
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 18, -64
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r8r9r10r11 = 32[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r20r21r22r23 = 64[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lq $r24r25 = 96[$r0]
; CHECK-NEXT:    addd $r1 = $r5, $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r1 = $r1, $r6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r1 = $r1, $r7
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r1 = $r1, $r8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r1 = $r1, $r9
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r1 = $r1, $r10
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r18 = $r1, $r11
; CHECK-NEXT:    call foo
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r0 = $r18, $r20
; CHECK-NEXT:    ld $r18 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r0 = $r0, $r21
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r0 = $r0, $r22
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r0 = $r0, $r23
; CHECK-NEXT:    lo $r20r21r22r23 = 8[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r0 = $r0, $r24
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r0 = $r0, $r25
; CHECK-NEXT:    lq $r24r25 = 40[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 56[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 64
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %0 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 0), align 8
  %1 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 1), align 8
  %2 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 2), align 8
  %3 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 3), align 8
  %4 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 4), align 8
  %5 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 5), align 8
  %6 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 6), align 8
  %7 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 7), align 8
  %8 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 8), align 8
  %9 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 9), align 8
  %10 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 10), align 8
  %11 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 11), align 8
  %12 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 12), align 8
  %13 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 13), align 8
  tail call void bitcast (void (...)* @foo to void ()*)() #2
  %add = add nsw i64 %1, %0
  %add1 = add nsw i64 %add, %2
  %add2 = add nsw i64 %add1, %3
  %add3 = add nsw i64 %add2, %4
  %add4 = add nsw i64 %add3, %5
  %add5 = add nsw i64 %add4, %6
  %add6 = add nsw i64 %add5, %7
  %add7 = add nsw i64 %add6, %8
  %add8 = add nsw i64 %add7, %9
  %add9 = add nsw i64 %add8, %10
  %add10 = add nsw i64 %add9, %11
  %add11 = add nsw i64 %add10, %12
  %add12 = add nsw i64 %add11, %13
  ret i64 %add12
}

define i64 @f_15_3quad1pairpack(){
; CHECK-LABEL: f_15_3quad1pairpack:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -64
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 64
; CHECK-NEXT:    .cfi_register 67, 16
; CHECK-NEXT:    sd 56[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -8
; CHECK-NEXT:    sd 48[$r12] = $r24
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 24, -16
; CHECK-NEXT:    so 16[$r12] = $r20r21r22r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 20, -24
; CHECK-NEXT:    .cfi_offset 21, -32
; CHECK-NEXT:    .cfi_offset 22, -40
; CHECK-NEXT:    .cfi_offset 23, -48
; CHECK-NEXT:    sq 0[$r12] = $r18r19
; CHECK-NEXT:    make $r0 = v
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 18, -56
; CHECK-NEXT:    .cfi_offset 19, -64
; CHECK-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r8r9r10r11 = 32[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r20r21r22r23 = 64[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lq $r18r19 = 96[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r24 = 112[$r0]
; CHECK-NEXT:    addd $r1 = $r5, $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r1 = $r1, $r6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r1 = $r1, $r7
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r1 = $r1, $r8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r1 = $r1, $r9
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r1 = $r1, $r10
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r1 = $r1, $r11
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r20 = $r1, $r20
; CHECK-NEXT:    call foo
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r0 = $r20, $r21
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r0 = $r0, $r22
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r0 = $r0, $r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r0 = $r0, $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r0 = $r0, $r19
; CHECK-NEXT:    lq $r18r19 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r20r21r22r23 = 16[$r12]
; CHECK-NEXT:    addd $r0 = $r0, $r24
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r24 = 48[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 56[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 64
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %0 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 0), align 8
  %1 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 1), align 8
  %2 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 2), align 8
  %3 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 3), align 8
  %4 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 4), align 8
  %5 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 5), align 8
  %6 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 6), align 8
  %7 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 7), align 8
  %8 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 8), align 8
  %9 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 9), align 8
  %10 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 10), align 8
  %11 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 11), align 8
  %12 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 12), align 8
  %13 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 13), align 8
  %14 = load i64, i64* getelementptr inbounds ([50 x i64], [50 x i64]* @v, i64 0, i64 14), align 8
  tail call void bitcast (void (...)* @foo to void ()*)() #2
  %add = add nsw i64 %1, %0
  %add1 = add nsw i64 %add, %2
  %add2 = add nsw i64 %add1, %3
  %add3 = add nsw i64 %add2, %4
  %add4 = add nsw i64 %add3, %5
  %add5 = add nsw i64 %add4, %6
  %add6 = add nsw i64 %add5, %7
  %add7 = add nsw i64 %add6, %8
  %add8 = add nsw i64 %add7, %9
  %add9 = add nsw i64 %add8, %10
  %add10 = add nsw i64 %add9, %11
  %add11 = add nsw i64 %add10, %12
  %add12 = add nsw i64 %add11, %13
  %add13 = add nsw i64 %add12, %14
  ret i64 %add13
}

