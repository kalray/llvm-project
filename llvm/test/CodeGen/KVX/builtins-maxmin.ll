; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mcpu=kv3-1 -o - %s -O2 | FileCheck %s --check-prefixes=CHECK,CV1
; RUN: llc -mcpu=kv3-2 -o - %s -O2 | FileCheck %s --check-prefixes=CHECK,CV2
; RUN: clang -O2 -march=kv3-1 -c -o /dev/null %s
; RUN: clang -O2 -march=kv3-2 -c -o /dev/null %s

target triple = "kvx-kalray-cos"

define <8 x i8> @maxbo(<8 x i8> %0, <8 x i8> %1) {
; CV1-LABEL: maxbo:
; CV1:       # %bb.0:
; CV1-NEXT:    sxmbhq $r2 = $r1
; CV1-NEXT:    sxmbhq $r3 = $r0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r4 = $r2, 47, 32
; CV1-NEXT:    extfs $r5 = $r3, 47, 32
; CV1-NEXT:    srad $r8 = $r2, 48
; CV1-NEXT:    srad $r9 = $r3, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    maxw $r4 = $r5, $r4
; CV1-NEXT:    extfs $r6 = $r2, 31, 16
; CV1-NEXT:    extfs $r7 = $r3, 31, 16
; CV1-NEXT:    maxw $r8 = $r9, $r8
; CV1-NEXT:    ;;
; CV1-NEXT:    sxhd $r2 = $r2
; CV1-NEXT:    sxhd $r3 = $r3
; CV1-NEXT:    maxw $r5 = $r7, $r6
; CV1-NEXT:    ;;
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    maxw $r2 = $r3, $r2
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r5, 15, 8
; CV1-NEXT:    insf $r4 = $r8, 15, 8
; CV1-NEXT:    srad $r8 = $r1, 48
; CV1-NEXT:    srad $r9 = $r0, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r3 = $r1, 47, 32
; CV1-NEXT:    extfs $r5 = $r0, 47, 32
; CV1-NEXT:    maxw $r8 = $r9, $r8
; CV1-NEXT:    ;;
; CV1-NEXT:    maxw $r3 = $r5, $r3
; CV1-NEXT:    extfs $r6 = $r1, 31, 16
; CV1-NEXT:    extfs $r7 = $r0, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    sxhd $r0 = $r0
; CV1-NEXT:    sxhd $r1 = $r1
; CV1-NEXT:    maxw $r5 = $r7, $r6
; CV1-NEXT:    ;;
; CV1-NEXT:    maxw $r0 = $r0, $r1
; CV1-NEXT:    insf $r2 = $r4, 31, 16
; CV1-NEXT:    insf $r3 = $r8, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r5, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r3, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r2, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: maxbo:
; CV2:       # %bb.0:
; CV2-NEXT:    maxbo $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = tail call <8 x i8> @llvm.smax.v8i8(<8 x i8> %0, <8 x i8> %1)
  ret <8 x i8> %3
}

declare <8 x i8> @llvm.smax.v8i8(<8 x i8>, <8 x i8>)

define <2 x i8> @maxbp(<2 x i8> %0, <2 x i8> %1) {
; CV1-LABEL: maxbp:
; CV1:       # %bb.0:
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x401
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: maxbp:
; CV2:       # %bb.0:
; CV2-NEXT:    maxbo $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = tail call <2 x i8> @llvm.smax.v2i8(<2 x i8> %0, <2 x i8> %1)
  ret <2 x i8> %3
}

declare <2 x i8> @llvm.smax.v2i8(<2 x i8>, <2 x i8>)

define <4 x i8> @maxbq(<4 x i8> %0, <4 x i8> %1) {
; CV1-LABEL: maxbq:
; CV1:       # %bb.0:
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: maxbq:
; CV2:       # %bb.0:
; CV2-NEXT:    maxbo $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = tail call <4 x i8> @llvm.smax.v4i8(<4 x i8> %0, <4 x i8> %1)
  ret <4 x i8> %3
}

declare <4 x i8> @llvm.smax.v4i8(<4 x i8>, <4 x i8>)

define <32 x i8> @maxbv(<32 x i8> %0, <32 x i8> %1) {
; CV1-LABEL: maxbv:
; CV1:       # %bb.0:
; CV1-NEXT:    sxmbhq $r8 = $r4
; CV1-NEXT:    sxmbhq $r9 = $r0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r10 = $r8, 47, 32
; CV1-NEXT:    extfs $r11 = $r9, 47, 32
; CV1-NEXT:    srad $r17 = $r8, 48
; CV1-NEXT:    srad $r32 = $r9, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    maxw $r10 = $r11, $r10
; CV1-NEXT:    extfs $r15 = $r8, 31, 16
; CV1-NEXT:    extfs $r16 = $r9, 31, 16
; CV1-NEXT:    maxw $r17 = $r32, $r17
; CV1-NEXT:    ;;
; CV1-NEXT:    sxhd $r8 = $r8
; CV1-NEXT:    sxhd $r9 = $r9
; CV1-NEXT:    maxw $r11 = $r16, $r15
; CV1-NEXT:    ;;
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxlbhq $r4 = $r4
; CV1-NEXT:    maxw $r8 = $r9, $r8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r8 = $r11, 15, 8
; CV1-NEXT:    insf $r10 = $r17, 15, 8
; CV1-NEXT:    srad $r17 = $r4, 48
; CV1-NEXT:    srad $r32 = $r0, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r9 = $r4, 47, 32
; CV1-NEXT:    extfs $r11 = $r0, 47, 32
; CV1-NEXT:    maxw $r17 = $r32, $r17
; CV1-NEXT:    ;;
; CV1-NEXT:    maxw $r9 = $r11, $r9
; CV1-NEXT:    extfs $r15 = $r4, 31, 16
; CV1-NEXT:    extfs $r16 = $r0, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    sxhd $r0 = $r0
; CV1-NEXT:    sxhd $r4 = $r4
; CV1-NEXT:    maxw $r11 = $r16, $r15
; CV1-NEXT:    ;;
; CV1-NEXT:    maxw $r0 = $r0, $r4
; CV1-NEXT:    insf $r8 = $r10, 31, 16
; CV1-NEXT:    insf $r9 = $r17, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r11, 15, 8
; CV1-NEXT:    sxmbhq $r4 = $r5
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r9, 31, 16
; CV1-NEXT:    extfs $r9 = $r4, 47, 32
; CV1-NEXT:    srad $r16 = $r4, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r8, 63, 32
; CV1-NEXT:    sxmbhq $r8 = $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r10 = $r8, 47, 32
; CV1-NEXT:    extfs $r11 = $r4, 31, 16
; CV1-NEXT:    srad $r17 = $r8, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    sxhd $r4 = $r4
; CV1-NEXT:    maxw $r9 = $r10, $r9
; CV1-NEXT:    extfs $r15 = $r8, 31, 16
; CV1-NEXT:    maxw $r16 = $r17, $r16
; CV1-NEXT:    ;;
; CV1-NEXT:    sxlbhq $r5 = $r5
; CV1-NEXT:    sxhd $r8 = $r8
; CV1-NEXT:    maxw $r10 = $r15, $r11
; CV1-NEXT:    ;;
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    maxw $r4 = $r8, $r4
; CV1-NEXT:    insf $r9 = $r16, 15, 8
; CV1-NEXT:    srad $r16 = $r5, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r4 = $r10, 15, 8
; CV1-NEXT:    extfs $r8 = $r5, 47, 32
; CV1-NEXT:    srad $r17 = $r1, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r10 = $r1, 47, 32
; CV1-NEXT:    extfs $r11 = $r5, 31, 16
; CV1-NEXT:    maxw $r16 = $r17, $r16
; CV1-NEXT:    ;;
; CV1-NEXT:    sxhd $r5 = $r5
; CV1-NEXT:    maxw $r8 = $r10, $r8
; CV1-NEXT:    extfs $r15 = $r1, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    sxhd $r1 = $r1
; CV1-NEXT:    insf $r8 = $r16, 15, 8
; CV1-NEXT:    maxw $r10 = $r15, $r11
; CV1-NEXT:    ;;
; CV1-NEXT:    maxw $r1 = $r1, $r5
; CV1-NEXT:    insf $r4 = $r9, 31, 16
; CV1-NEXT:    sxmbhq $r5 = $r6
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r10, 15, 8
; CV1-NEXT:    extfs $r9 = $r5, 47, 32
; CV1-NEXT:    srad $r16 = $r5, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r8, 31, 16
; CV1-NEXT:    sxmbhq $r8 = $r2
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r10 = $r8, 47, 32
; CV1-NEXT:    extfs $r11 = $r5, 31, 16
; CV1-NEXT:    srad $r17 = $r8, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    sxhd $r5 = $r5
; CV1-NEXT:    maxw $r9 = $r10, $r9
; CV1-NEXT:    extfs $r15 = $r8, 31, 16
; CV1-NEXT:    maxw $r16 = $r17, $r16
; CV1-NEXT:    ;;
; CV1-NEXT:    sxhd $r8 = $r8
; CV1-NEXT:    insf $r9 = $r16, 15, 8
; CV1-NEXT:    maxw $r10 = $r15, $r11
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r4, 63, 32
; CV1-NEXT:    sxlbhq $r4 = $r6
; CV1-NEXT:    maxw $r5 = $r8, $r5
; CV1-NEXT:    ;;
; CV1-NEXT:    sxlbhq $r2 = $r2
; CV1-NEXT:    insf $r5 = $r10, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r5 = $r9, 31, 16
; CV1-NEXT:    extfs $r6 = $r4, 47, 32
; CV1-NEXT:    srad $r9 = $r4, 48
; CV1-NEXT:    srad $r10 = $r2, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r8 = $r2, 47, 32
; CV1-NEXT:    maxw $r9 = $r10, $r9
; CV1-NEXT:    extfs $r11 = $r4, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    sxhd $r4 = $r4
; CV1-NEXT:    maxw $r6 = $r8, $r6
; CV1-NEXT:    extfs $r15 = $r2, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    sxhd $r2 = $r2
; CV1-NEXT:    maxw $r8 = $r15, $r11
; CV1-NEXT:    sxmbhq $r10 = $r3
; CV1-NEXT:    ;;
; CV1-NEXT:    maxw $r2 = $r2, $r4
; CV1-NEXT:    sxmbhq $r4 = $r7
; CV1-NEXT:    srad $r15 = $r10, 48
; CV1-NEXT:    extfs $r17 = $r10, 47, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r6 = $r9, 15, 8
; CV1-NEXT:    srad $r11 = $r4, 48
; CV1-NEXT:    extfs $r16 = $r4, 47, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r8, 15, 8
; CV1-NEXT:    sxlbhq $r7 = $r7
; CV1-NEXT:    maxw $r8 = $r15, $r11
; CV1-NEXT:    maxw $r9 = $r17, $r16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r6, 31, 16
; CV1-NEXT:    insf $r9 = $r8, 15, 8
; CV1-NEXT:    srad $r15 = $r7, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r6 = $r4, 31, 16
; CV1-NEXT:    extfs $r8 = $r10, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    sxhd $r4 = $r4
; CV1-NEXT:    maxw $r6 = $r8, $r6
; CV1-NEXT:    sxhd $r10 = $r10
; CV1-NEXT:    ;;
; CV1-NEXT:    sxlbhq $r3 = $r3
; CV1-NEXT:    maxw $r4 = $r10, $r4
; CV1-NEXT:    extfs $r8 = $r7, 47, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r11 = $r3, 47, 32
; CV1-NEXT:    srad $r16 = $r3, 48
; CV1-NEXT:    extfs $r17 = $r7, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    sxhd $r7 = $r7
; CV1-NEXT:    maxw $r10 = $r16, $r15
; CV1-NEXT:    extfs $r32 = $r3, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    sxhd $r3 = $r3
; CV1-NEXT:    insf $r4 = $r6, 15, 8
; CV1-NEXT:    maxw $r6 = $r11, $r8
; CV1-NEXT:    maxw $r8 = $r32, $r17
; CV1-NEXT:    ;;
; CV1-NEXT:    maxw $r3 = $r3, $r7
; CV1-NEXT:    insf $r4 = $r9, 31, 16
; CV1-NEXT:    insf $r6 = $r10, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r5, 63, 32
; CV1-NEXT:    insf $r3 = $r8, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r3 = $r6, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r3 = $r4, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: maxbv:
; CV2:       # %bb.0:
; CV2-NEXT:    maxbo $r0 = $r0, $r4
; CV2-NEXT:    maxbo $r1 = $r1, $r5
; CV2-NEXT:    maxbo $r2 = $r2, $r6
; CV2-NEXT:    maxbo $r3 = $r3, $r7
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %4 = shufflevector <32 x i8> %1, <32 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5 = tail call <8 x i8> @llvm.smax.v8i8(<8 x i8> %3, <8 x i8> %4)
  %6 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %7 = shufflevector <32 x i8> %1, <32 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %8 = tail call <8 x i8> @llvm.smax.v8i8(<8 x i8> %6, <8 x i8> %7)
  %9 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %10 = shufflevector <32 x i8> %1, <32 x i8> undef, <8 x i32> <i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %11 = tail call <8 x i8> @llvm.smax.v8i8(<8 x i8> %9, <8 x i8> %10)
  %12 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  %13 = shufflevector <32 x i8> %1, <32 x i8> undef, <8 x i32> <i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  %14 = tail call <8 x i8> @llvm.smax.v8i8(<8 x i8> %12, <8 x i8> %13)
  %15 = shufflevector <8 x i8> %5, <8 x i8> %8, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %16 = shufflevector <8 x i8> %11, <8 x i8> %14, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %17 = shufflevector <16 x i8> %15, <16 x i8> %16, <32 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  ret <32 x i8> %17
}

define <16 x i8> @maxbx(<16 x i8> %0, <16 x i8> %1) {
; CV1-LABEL: maxbx:
; CV1:       # %bb.0:
; CV1-NEXT:    sxmbhq $r4 = $r2
; CV1-NEXT:    sxmbhq $r5 = $r0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r6 = $r4, 47, 32
; CV1-NEXT:    extfs $r7 = $r5, 47, 32
; CV1-NEXT:    srad $r10 = $r4, 48
; CV1-NEXT:    srad $r11 = $r5, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    maxw $r6 = $r7, $r6
; CV1-NEXT:    extfs $r8 = $r4, 31, 16
; CV1-NEXT:    extfs $r9 = $r5, 31, 16
; CV1-NEXT:    maxw $r10 = $r11, $r10
; CV1-NEXT:    ;;
; CV1-NEXT:    sxhd $r4 = $r4
; CV1-NEXT:    sxhd $r5 = $r5
; CV1-NEXT:    maxw $r7 = $r9, $r8
; CV1-NEXT:    ;;
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxlbhq $r2 = $r2
; CV1-NEXT:    maxw $r4 = $r5, $r4
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r4 = $r7, 15, 8
; CV1-NEXT:    insf $r6 = $r10, 15, 8
; CV1-NEXT:    srad $r10 = $r2, 48
; CV1-NEXT:    srad $r11 = $r0, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r5 = $r2, 47, 32
; CV1-NEXT:    extfs $r7 = $r0, 47, 32
; CV1-NEXT:    maxw $r10 = $r11, $r10
; CV1-NEXT:    ;;
; CV1-NEXT:    maxw $r5 = $r7, $r5
; CV1-NEXT:    extfs $r8 = $r2, 31, 16
; CV1-NEXT:    extfs $r9 = $r0, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    sxhd $r0 = $r0
; CV1-NEXT:    sxhd $r2 = $r2
; CV1-NEXT:    maxw $r7 = $r9, $r8
; CV1-NEXT:    ;;
; CV1-NEXT:    maxw $r0 = $r0, $r2
; CV1-NEXT:    sxmbhq $r2 = $r3
; CV1-NEXT:    insf $r5 = $r10, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r7, 15, 8
; CV1-NEXT:    insf $r4 = $r6, 31, 16
; CV1-NEXT:    srad $r10 = $r2, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r5, 31, 16
; CV1-NEXT:    sxmbhq $r5 = $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r6 = $r2, 47, 32
; CV1-NEXT:    extfs $r7 = $r5, 47, 32
; CV1-NEXT:    srad $r11 = $r5, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    maxw $r6 = $r7, $r6
; CV1-NEXT:    extfs $r8 = $r2, 31, 16
; CV1-NEXT:    extfs $r9 = $r5, 31, 16
; CV1-NEXT:    maxw $r10 = $r11, $r10
; CV1-NEXT:    ;;
; CV1-NEXT:    sxhd $r2 = $r2
; CV1-NEXT:    sxhd $r5 = $r5
; CV1-NEXT:    maxw $r7 = $r9, $r8
; CV1-NEXT:    ;;
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    maxw $r2 = $r5, $r2
; CV1-NEXT:    sxlbhq $r3 = $r3
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r7, 15, 8
; CV1-NEXT:    insf $r6 = $r10, 15, 8
; CV1-NEXT:    srad $r10 = $r3, 48
; CV1-NEXT:    srad $r11 = $r1, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r5 = $r3, 47, 32
; CV1-NEXT:    extfs $r7 = $r1, 47, 32
; CV1-NEXT:    maxw $r10 = $r11, $r10
; CV1-NEXT:    ;;
; CV1-NEXT:    maxw $r5 = $r7, $r5
; CV1-NEXT:    extfs $r8 = $r3, 31, 16
; CV1-NEXT:    extfs $r9 = $r1, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    sxhd $r1 = $r1
; CV1-NEXT:    sxhd $r3 = $r3
; CV1-NEXT:    maxw $r7 = $r9, $r8
; CV1-NEXT:    ;;
; CV1-NEXT:    maxw $r1 = $r1, $r3
; CV1-NEXT:    insf $r2 = $r6, 31, 16
; CV1-NEXT:    insf $r5 = $r10, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r4, 63, 32
; CV1-NEXT:    insf $r1 = $r7, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r5, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r2, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: maxbx:
; CV2:       # %bb.0:
; CV2-NEXT:    maxbo $r0 = $r0, $r2
; CV2-NEXT:    maxbo $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = shufflevector <16 x i8> %0, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %4 = shufflevector <16 x i8> %1, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5 = tail call <8 x i8> @llvm.smax.v8i8(<8 x i8> %3, <8 x i8> %4)
  %6 = shufflevector <16 x i8> %0, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %7 = shufflevector <16 x i8> %1, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %8 = tail call <8 x i8> @llvm.smax.v8i8(<8 x i8> %6, <8 x i8> %7)
  %9 = shufflevector <8 x i8> %5, <8 x i8> %8, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  ret <16 x i8> %9
}

define i64 @maxd(i64 %0, i64 %1) {
; CHECK-LABEL: maxd:
; CHECK:       # %bb.0:
; CHECK-NEXT:    maxd $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = tail call i64 @llvm.smax.i64(i64 %0, i64 %1)
  ret i64 %3
}

declare i64 @llvm.smax.i64(i64, i64)

define <2 x i64> @maxdp(<2 x i64> %0, <2 x i64> %1) {
; CHECK-LABEL: maxdp:
; CHECK:       # %bb.0:
; CHECK-NEXT:    maxd $r0 = $r0, $r2
; CHECK-NEXT:    maxd $r1 = $r1, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = extractelement <2 x i64> %0, i64 0
  %4 = extractelement <2 x i64> %1, i64 0
  %5 = tail call i64 @llvm.smax.i64(i64 %3, i64 %4)
  %6 = extractelement <2 x i64> %0, i64 1
  %7 = extractelement <2 x i64> %1, i64 1
  %8 = tail call i64 @llvm.smax.i64(i64 %6, i64 %7)
  %9 = insertelement <2 x i64> undef, i64 %5, i32 0
  %10 = insertelement <2 x i64> %9, i64 %8, i32 1
  ret <2 x i64> %10
}

define <4 x i64> @maxdq(<4 x i64> %0, <4 x i64> %1) {
; CHECK-LABEL: maxdq:
; CHECK:       # %bb.0:
; CHECK-NEXT:    maxd $r0 = $r0, $r4
; CHECK-NEXT:    maxd $r1 = $r1, $r5
; CHECK-NEXT:    maxd $r2 = $r2, $r6
; CHECK-NEXT:    maxd $r3 = $r3, $r7
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = extractelement <4 x i64> %0, i64 0
  %4 = extractelement <4 x i64> %1, i64 0
  %5 = tail call i64 @llvm.smax.i64(i64 %3, i64 %4)
  %6 = extractelement <4 x i64> %0, i64 1
  %7 = extractelement <4 x i64> %1, i64 1
  %8 = tail call i64 @llvm.smax.i64(i64 %6, i64 %7)
  %9 = extractelement <4 x i64> %0, i64 2
  %10 = extractelement <4 x i64> %1, i64 2
  %11 = tail call i64 @llvm.smax.i64(i64 %9, i64 %10)
  %12 = extractelement <4 x i64> %0, i64 3
  %13 = extractelement <4 x i64> %1, i64 3
  %14 = tail call i64 @llvm.smax.i64(i64 %12, i64 %13)
  %15 = insertelement <4 x i64> undef, i64 %5, i32 0
  %16 = insertelement <4 x i64> %15, i64 %8, i32 1
  %17 = insertelement <4 x i64> %16, i64 %11, i32 2
  %18 = insertelement <4 x i64> %17, i64 %14, i32 3
  ret <4 x i64> %18
}

define <8 x i16> @maxho(<8 x i16> %0, <8 x i16> %1) {
; CHECK-LABEL: maxho:
; CHECK:       # %bb.0:
; CHECK-NEXT:    maxhq $r0 = $r0, $r2
; CHECK-NEXT:    maxhq $r1 = $r1, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4 = shufflevector <8 x i16> %1, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5 = tail call <4 x i16> @llvm.smax.v4i16(<4 x i16> %3, <4 x i16> %4)
  %6 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7 = shufflevector <8 x i16> %1, <8 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8 = tail call <4 x i16> @llvm.smax.v4i16(<4 x i16> %6, <4 x i16> %7)
  %9 = shufflevector <4 x i16> %5, <4 x i16> %8, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i16> %9
}

declare <4 x i16> @llvm.smax.v4i16(<4 x i16>, <4 x i16>)

define <2 x i16> @maxhp(<2 x i16> %0, <2 x i16> %1) {
; CHECK-LABEL: maxhp:
; CHECK:       # %bb.0:
; CHECK-NEXT:    maxhq $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = tail call <2 x i16> @llvm.smax.v2i16(<2 x i16> %0, <2 x i16> %1)
  ret <2 x i16> %3
}

declare <2 x i16> @llvm.smax.v2i16(<2 x i16>, <2 x i16>)

define <4 x i16> @maxhq(<4 x i16> %0, <4 x i16> %1) {
; CHECK-LABEL: maxhq:
; CHECK:       # %bb.0:
; CHECK-NEXT:    maxhq $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = tail call <4 x i16> @llvm.smax.v4i16(<4 x i16> %0, <4 x i16> %1)
  ret <4 x i16> %3
}

define <16 x i16> @maxhx(<16 x i16> %0, <16 x i16> %1) {
; CHECK-LABEL: maxhx:
; CHECK:       # %bb.0:
; CHECK-NEXT:    maxhq $r0 = $r0, $r4
; CHECK-NEXT:    maxhq $r1 = $r1, $r5
; CHECK-NEXT:    maxhq $r2 = $r2, $r6
; CHECK-NEXT:    maxhq $r3 = $r3, $r7
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4 = shufflevector <16 x i16> %1, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5 = tail call <4 x i16> @llvm.smax.v4i16(<4 x i16> %3, <4 x i16> %4)
  %6 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7 = shufflevector <16 x i16> %1, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8 = tail call <4 x i16> @llvm.smax.v4i16(<4 x i16> %6, <4 x i16> %7)
  %9 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %10 = shufflevector <16 x i16> %1, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %11 = tail call <4 x i16> @llvm.smax.v4i16(<4 x i16> %9, <4 x i16> %10)
  %12 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %13 = shufflevector <16 x i16> %1, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %14 = tail call <4 x i16> @llvm.smax.v4i16(<4 x i16> %12, <4 x i16> %13)
  %15 = shufflevector <4 x i16> %5, <4 x i16> %8, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16 = shufflevector <4 x i16> %11, <4 x i16> %14, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %17 = shufflevector <8 x i16> %15, <8 x i16> %16, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  ret <16 x i16> %17
}

define <8 x i8> @maxubo(<8 x i8> %0, <8 x i8> %1) {
; CV1-LABEL: maxubo:
; CV1:       # %bb.0:
; CV1-NEXT:    sbmm8 $r2 = $r1, 0x80004000200010
; CV1-NEXT:    sbmm8 $r3 = $r0, 0x80004000200010
; CV1-NEXT:    ;;
; CV1-NEXT:    srld $r4 = $r2, 48
; CV1-NEXT:    srld $r5 = $r3, 48
; CV1-NEXT:    extfz $r6 = $r2, 47, 32
; CV1-NEXT:    extfz $r7 = $r3, 47, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r2 = $r2
; CV1-NEXT:    zxhd $r3 = $r3
; CV1-NEXT:    srlw $r8 = $r2, 16
; CV1-NEXT:    srlw $r9 = $r3, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r4 = $r4
; CV1-NEXT:    zxhd $r5 = $r5
; CV1-NEXT:    ;;
; CV1-NEXT:    maxuw $r4 = $r5, $r4
; CV1-NEXT:    zxhd $r6 = $r6
; CV1-NEXT:    zxhd $r7 = $r7
; CV1-NEXT:    ;;
; CV1-NEXT:    maxuw $r5 = $r7, $r6
; CV1-NEXT:    zxhd $r8 = $r8
; CV1-NEXT:    zxhd $r9 = $r9
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r2 = $r2
; CV1-NEXT:    zxhd $r3 = $r3
; CV1-NEXT:    maxuw $r6 = $r9, $r8
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x8000400020001
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x8000400020001
; CV1-NEXT:    maxuw $r2 = $r3, $r2
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r6, 15, 8
; CV1-NEXT:    srld $r3 = $r1, 48
; CV1-NEXT:    srld $r4 = $r0, 48
; CV1-NEXT:    insf $r5 = $r4, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    extfz $r6 = $r1, 47, 32
; CV1-NEXT:    extfz $r7 = $r0, 47, 32
; CV1-NEXT:    srlw $r8 = $r1, 16
; CV1-NEXT:    srlw $r9 = $r0, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    zxhd $r1 = $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r3 = $r3
; CV1-NEXT:    zxhd $r4 = $r4
; CV1-NEXT:    ;;
; CV1-NEXT:    maxuw $r3 = $r4, $r3
; CV1-NEXT:    zxhd $r6 = $r6
; CV1-NEXT:    zxhd $r7 = $r7
; CV1-NEXT:    ;;
; CV1-NEXT:    maxuw $r4 = $r7, $r6
; CV1-NEXT:    zxhd $r8 = $r8
; CV1-NEXT:    zxhd $r9 = $r9
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    zxhd $r1 = $r1
; CV1-NEXT:    maxuw $r6 = $r9, $r8
; CV1-NEXT:    ;;
; CV1-NEXT:    maxuw $r0 = $r0, $r1
; CV1-NEXT:    insf $r2 = $r5, 31, 16
; CV1-NEXT:    insf $r4 = $r3, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r6, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r4, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r2, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: maxubo:
; CV2:       # %bb.0:
; CV2-NEXT:    maxubo $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = tail call <8 x i8> @llvm.umax.v8i8(<8 x i8> %0, <8 x i8> %1)
  ret <8 x i8> %3
}

declare <8 x i8> @llvm.umax.v8i8(<8 x i8>, <8 x i8>)

define <2 x i8> @maxubp(<2 x i8> %0, <2 x i8> %1) {
; CV1-LABEL: maxubp:
; CV1:       # %bb.0:
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x20001
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x20001
; CV1-NEXT:    ;;
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x401
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: maxubp:
; CV2:       # %bb.0:
; CV2-NEXT:    maxubo $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = tail call <2 x i8> @llvm.umax.v2i8(<2 x i8> %0, <2 x i8> %1)
  ret <2 x i8> %3
}

declare <2 x i8> @llvm.umax.v2i8(<2 x i8>, <2 x i8>)

define <4 x i8> @maxubq(<4 x i8> %0, <4 x i8> %1) {
; CV1-LABEL: maxubq:
; CV1:       # %bb.0:
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x8000400020001
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x8000400020001
; CV1-NEXT:    ;;
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: maxubq:
; CV2:       # %bb.0:
; CV2-NEXT:    maxubo $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = tail call <4 x i8> @llvm.umax.v4i8(<4 x i8> %0, <4 x i8> %1)
  ret <4 x i8> %3
}

declare <4 x i8> @llvm.umax.v4i8(<4 x i8>, <4 x i8>)

define <32 x i8> @maxubv(<32 x i8> %0, <32 x i8> %1) {
; CV1-LABEL: maxubv:
; CV1:       # %bb.0:
; CV1-NEXT:    sbmm8 $r8 = $r4, 0x80004000200010
; CV1-NEXT:    sbmm8 $r9 = $r0, 0x80004000200010
; CV1-NEXT:    ;;
; CV1-NEXT:    srld $r10 = $r8, 48
; CV1-NEXT:    srld $r11 = $r9, 48
; CV1-NEXT:    extfz $r15 = $r8, 47, 32
; CV1-NEXT:    extfz $r16 = $r9, 47, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r8 = $r8
; CV1-NEXT:    zxhd $r9 = $r9
; CV1-NEXT:    srlw $r17 = $r8, 16
; CV1-NEXT:    srlw $r32 = $r9, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r10 = $r10
; CV1-NEXT:    zxhd $r11 = $r11
; CV1-NEXT:    ;;
; CV1-NEXT:    maxuw $r10 = $r11, $r10
; CV1-NEXT:    zxhd $r15 = $r15
; CV1-NEXT:    zxhd $r16 = $r16
; CV1-NEXT:    ;;
; CV1-NEXT:    maxuw $r11 = $r16, $r15
; CV1-NEXT:    zxhd $r17 = $r17
; CV1-NEXT:    zxhd $r32 = $r32
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r8 = $r8
; CV1-NEXT:    zxhd $r9 = $r9
; CV1-NEXT:    maxuw $r15 = $r32, $r17
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x8000400020001
; CV1-NEXT:    sbmm8 $r4 = $r4, 0x8000400020001
; CV1-NEXT:    maxuw $r8 = $r9, $r8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r8 = $r15, 15, 8
; CV1-NEXT:    srld $r9 = $r4, 48
; CV1-NEXT:    srld $r10 = $r0, 48
; CV1-NEXT:    insf $r11 = $r10, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    extfz $r15 = $r4, 47, 32
; CV1-NEXT:    extfz $r16 = $r0, 47, 32
; CV1-NEXT:    srlw $r17 = $r4, 16
; CV1-NEXT:    srlw $r32 = $r0, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    zxhd $r4 = $r4
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r9 = $r9
; CV1-NEXT:    zxhd $r10 = $r10
; CV1-NEXT:    ;;
; CV1-NEXT:    maxuw $r9 = $r10, $r9
; CV1-NEXT:    zxhd $r15 = $r15
; CV1-NEXT:    zxhd $r16 = $r16
; CV1-NEXT:    ;;
; CV1-NEXT:    maxuw $r10 = $r16, $r15
; CV1-NEXT:    zxhd $r17 = $r17
; CV1-NEXT:    zxhd $r32 = $r32
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    zxhd $r4 = $r4
; CV1-NEXT:    maxuw $r15 = $r32, $r17
; CV1-NEXT:    ;;
; CV1-NEXT:    maxuw $r0 = $r0, $r4
; CV1-NEXT:    insf $r8 = $r11, 31, 16
; CV1-NEXT:    insf $r10 = $r9, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r15, 15, 8
; CV1-NEXT:    sbmm8 $r4 = $r5, 0x80004000200010
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r10, 31, 16
; CV1-NEXT:    sbmm8 $r9 = $r1, 0x80004000200010
; CV1-NEXT:    srlw $r16 = $r4, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r8, 63, 32
; CV1-NEXT:    srld $r8 = $r4, 48
; CV1-NEXT:    srld $r10 = $r9, 48
; CV1-NEXT:    extfz $r11 = $r4, 47, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r4 = $r4
; CV1-NEXT:    extfz $r15 = $r9, 47, 32
; CV1-NEXT:    srlw $r17 = $r9, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r8 = $r8
; CV1-NEXT:    zxhd $r9 = $r9
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r10 = $r10
; CV1-NEXT:    zxhd $r11 = $r11
; CV1-NEXT:    ;;
; CV1-NEXT:    maxuw $r8 = $r10, $r8
; CV1-NEXT:    zxhd $r15 = $r15
; CV1-NEXT:    zxhd $r16 = $r16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r4 = $r4
; CV1-NEXT:    maxuw $r10 = $r15, $r11
; CV1-NEXT:    zxhd $r17 = $r17
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r5 = $r5, 0x8000400020001
; CV1-NEXT:    zxhd $r9 = $r9
; CV1-NEXT:    maxuw $r11 = $r17, $r16
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x8000400020001
; CV1-NEXT:    maxuw $r4 = $r9, $r4
; CV1-NEXT:    srld $r8 = $r5, 48
; CV1-NEXT:    insf $r10 = $r8, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r4 = $r11, 15, 8
; CV1-NEXT:    srld $r9 = $r1, 48
; CV1-NEXT:    extfz $r11 = $r5, 47, 32
; CV1-NEXT:    srlw $r16 = $r5, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r5 = $r5
; CV1-NEXT:    extfz $r15 = $r1, 47, 32
; CV1-NEXT:    srlw $r17 = $r1, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r1 = $r1
; CV1-NEXT:    zxhd $r8 = $r8
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r9 = $r9
; CV1-NEXT:    zxhd $r11 = $r11
; CV1-NEXT:    ;;
; CV1-NEXT:    maxuw $r8 = $r9, $r8
; CV1-NEXT:    zxhd $r15 = $r15
; CV1-NEXT:    zxhd $r16 = $r16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r5 = $r5
; CV1-NEXT:    maxuw $r9 = $r15, $r11
; CV1-NEXT:    zxhd $r17 = $r17
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r1 = $r1
; CV1-NEXT:    insf $r9 = $r8, 15, 8
; CV1-NEXT:    maxuw $r11 = $r17, $r16
; CV1-NEXT:    ;;
; CV1-NEXT:    maxuw $r1 = $r1, $r5
; CV1-NEXT:    sbmm8 $r5 = $r6, 0x80004000200010
; CV1-NEXT:    sbmm8 $r8 = $r2, 0x80004000200010
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r11, 15, 8
; CV1-NEXT:    insf $r4 = $r10, 31, 16
; CV1-NEXT:    srld $r10 = $r8, 48
; CV1-NEXT:    srlw $r16 = $r5, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r9, 31, 16
; CV1-NEXT:    srld $r9 = $r5, 48
; CV1-NEXT:    extfz $r11 = $r5, 47, 32
; CV1-NEXT:    srlw $r17 = $r8, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r5 = $r5
; CV1-NEXT:    extfz $r15 = $r8, 47, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r8 = $r8
; CV1-NEXT:    zxhd $r9 = $r9
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r10 = $r10
; CV1-NEXT:    zxhd $r11 = $r11
; CV1-NEXT:    ;;
; CV1-NEXT:    maxuw $r9 = $r10, $r9
; CV1-NEXT:    zxhd $r15 = $r15
; CV1-NEXT:    zxhd $r16 = $r16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r5 = $r5
; CV1-NEXT:    maxuw $r10 = $r15, $r11
; CV1-NEXT:    zxhd $r17 = $r17
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r8 = $r8
; CV1-NEXT:    insf $r10 = $r9, 15, 8
; CV1-NEXT:    maxuw $r11 = $r17, $r16
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r2 = $r2, 0x8000400020001
; CV1-NEXT:    maxuw $r5 = $r8, $r5
; CV1-NEXT:    sbmm8 $r6 = $r6, 0x8000400020001
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r4, 63, 32
; CV1-NEXT:    srld $r4 = $r6, 48
; CV1-NEXT:    insf $r5 = $r11, 15, 8
; CV1-NEXT:    srld $r8 = $r2, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r5 = $r10, 31, 16
; CV1-NEXT:    extfz $r9 = $r6, 47, 32
; CV1-NEXT:    srlw $r11 = $r6, 16
; CV1-NEXT:    srlw $r15 = $r2, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r4 = $r4
; CV1-NEXT:    extfz $r10 = $r2, 47, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r8 = $r8
; CV1-NEXT:    zxhd $r9 = $r9
; CV1-NEXT:    ;;
; CV1-NEXT:    maxuw $r4 = $r8, $r4
; CV1-NEXT:    zxhd $r6 = $r6
; CV1-NEXT:    zxhd $r10 = $r10
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r2 = $r2
; CV1-NEXT:    maxuw $r8 = $r10, $r9
; CV1-NEXT:    sbmm8 $r16 = $r7, 0x80004000200010
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r11 = $r11
; CV1-NEXT:    sbmm8 $r17 = $r3, 0x80004000200010
; CV1-NEXT:    srld $r32 = $r16, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r6 = $r6
; CV1-NEXT:    zxhd $r15 = $r15
; CV1-NEXT:    srld $r33 = $r17, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r2 = $r2
; CV1-NEXT:    extfz $r9 = $r16, 47, 32
; CV1-NEXT:    maxuw $r11 = $r15, $r11
; CV1-NEXT:    ;;
; CV1-NEXT:    maxuw $r2 = $r2, $r6
; CV1-NEXT:    extfz $r10 = $r17, 47, 32
; CV1-NEXT:    zxhd $r32 = $r32
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r6 = $r9
; CV1-NEXT:    zxhd $r33 = $r33
; CV1-NEXT:    ;;
; CV1-NEXT:    maxuw $r4 = $r33, $r32
; CV1-NEXT:    insf $r8 = $r4, 15, 8
; CV1-NEXT:    zxhd $r9 = $r10
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r11, 15, 8
; CV1-NEXT:    maxuw $r6 = $r9, $r6
; CV1-NEXT:    zxhd $r9 = $r16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r8, 31, 16
; CV1-NEXT:    srlw $r4 = $r16, 16
; CV1-NEXT:    insf $r6 = $r4, 15, 8
; CV1-NEXT:    srlw $r8 = $r17, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r7 = $r7, 0x8000400020001
; CV1-NEXT:    zxhd $r10 = $r17
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r3 = $r3, 0x8000400020001
; CV1-NEXT:    zxhd $r4 = $r4
; CV1-NEXT:    srld $r11 = $r7, 48
; CV1-NEXT:    srlw $r32 = $r7, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r8 = $r8
; CV1-NEXT:    zxhd $r9 = $r9
; CV1-NEXT:    srld $r15 = $r3, 48
; CV1-NEXT:    srlw $r33 = $r3, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    maxuw $r4 = $r8, $r4
; CV1-NEXT:    zxhd $r10 = $r10
; CV1-NEXT:    extfz $r16 = $r7, 47, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r7 = $r7
; CV1-NEXT:    maxuw $r8 = $r10, $r9
; CV1-NEXT:    extfz $r17 = $r3, 47, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r3 = $r3
; CV1-NEXT:    zxhd $r11 = $r11
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r15 = $r15
; CV1-NEXT:    zxhd $r16 = $r16
; CV1-NEXT:    ;;
; CV1-NEXT:    maxuw $r9 = $r15, $r11
; CV1-NEXT:    zxhd $r17 = $r17
; CV1-NEXT:    zxhd $r32 = $r32
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r7 = $r7
; CV1-NEXT:    zxhd $r33 = $r33
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r3 = $r3
; CV1-NEXT:    maxuw $r4 = $r17, $r16
; CV1-NEXT:    insf $r8 = $r4, 15, 8
; CV1-NEXT:    maxuw $r10 = $r33, $r32
; CV1-NEXT:    ;;
; CV1-NEXT:    maxuw $r3 = $r3, $r7
; CV1-NEXT:    insf $r4 = $r9, 15, 8
; CV1-NEXT:    insf $r8 = $r6, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r5, 63, 32
; CV1-NEXT:    insf $r3 = $r10, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r3 = $r4, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r3 = $r8, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: maxubv:
; CV2:       # %bb.0:
; CV2-NEXT:    maxubo $r0 = $r0, $r4
; CV2-NEXT:    maxubo $r1 = $r1, $r5
; CV2-NEXT:    maxubo $r2 = $r2, $r6
; CV2-NEXT:    maxubo $r3 = $r3, $r7
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %4 = shufflevector <32 x i8> %1, <32 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5 = tail call <8 x i8> @llvm.umax.v8i8(<8 x i8> %3, <8 x i8> %4)
  %6 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %7 = shufflevector <32 x i8> %1, <32 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %8 = tail call <8 x i8> @llvm.umax.v8i8(<8 x i8> %6, <8 x i8> %7)
  %9 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %10 = shufflevector <32 x i8> %1, <32 x i8> undef, <8 x i32> <i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %11 = tail call <8 x i8> @llvm.umax.v8i8(<8 x i8> %9, <8 x i8> %10)
  %12 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  %13 = shufflevector <32 x i8> %1, <32 x i8> undef, <8 x i32> <i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  %14 = tail call <8 x i8> @llvm.umax.v8i8(<8 x i8> %12, <8 x i8> %13)
  %15 = shufflevector <8 x i8> %5, <8 x i8> %8, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %16 = shufflevector <8 x i8> %11, <8 x i8> %14, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %17 = shufflevector <16 x i8> %15, <16 x i8> %16, <32 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  ret <32 x i8> %17
}

define <16 x i8> @maxubx(<16 x i8> %0, <16 x i8> %1) {
; CV1-LABEL: maxubx:
; CV1:       # %bb.0:
; CV1-NEXT:    sbmm8 $r4 = $r2, 0x80004000200010
; CV1-NEXT:    sbmm8 $r5 = $r0, 0x80004000200010
; CV1-NEXT:    ;;
; CV1-NEXT:    srld $r6 = $r4, 48
; CV1-NEXT:    srld $r7 = $r5, 48
; CV1-NEXT:    extfz $r8 = $r4, 47, 32
; CV1-NEXT:    extfz $r9 = $r5, 47, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r4 = $r4
; CV1-NEXT:    zxhd $r5 = $r5
; CV1-NEXT:    srlw $r10 = $r4, 16
; CV1-NEXT:    srlw $r11 = $r5, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r6 = $r6
; CV1-NEXT:    zxhd $r7 = $r7
; CV1-NEXT:    ;;
; CV1-NEXT:    maxuw $r6 = $r7, $r6
; CV1-NEXT:    zxhd $r8 = $r8
; CV1-NEXT:    zxhd $r9 = $r9
; CV1-NEXT:    ;;
; CV1-NEXT:    maxuw $r7 = $r9, $r8
; CV1-NEXT:    zxhd $r10 = $r10
; CV1-NEXT:    zxhd $r11 = $r11
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r4 = $r4
; CV1-NEXT:    zxhd $r5 = $r5
; CV1-NEXT:    maxuw $r8 = $r11, $r10
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x8000400020001
; CV1-NEXT:    sbmm8 $r2 = $r2, 0x8000400020001
; CV1-NEXT:    maxuw $r4 = $r5, $r4
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r4 = $r8, 15, 8
; CV1-NEXT:    srld $r5 = $r2, 48
; CV1-NEXT:    srld $r6 = $r0, 48
; CV1-NEXT:    insf $r7 = $r6, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    extfz $r8 = $r2, 47, 32
; CV1-NEXT:    extfz $r9 = $r0, 47, 32
; CV1-NEXT:    srlw $r10 = $r2, 16
; CV1-NEXT:    srlw $r11 = $r0, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    zxhd $r2 = $r2
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r5 = $r5
; CV1-NEXT:    zxhd $r6 = $r6
; CV1-NEXT:    ;;
; CV1-NEXT:    maxuw $r5 = $r6, $r5
; CV1-NEXT:    zxhd $r8 = $r8
; CV1-NEXT:    zxhd $r9 = $r9
; CV1-NEXT:    ;;
; CV1-NEXT:    maxuw $r6 = $r9, $r8
; CV1-NEXT:    zxhd $r10 = $r10
; CV1-NEXT:    zxhd $r11 = $r11
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    zxhd $r2 = $r2
; CV1-NEXT:    maxuw $r8 = $r11, $r10
; CV1-NEXT:    ;;
; CV1-NEXT:    maxuw $r0 = $r0, $r2
; CV1-NEXT:    sbmm8 $r2 = $r3, 0x80004000200010
; CV1-NEXT:    insf $r6 = $r5, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r8, 15, 8
; CV1-NEXT:    sbmm8 $r5 = $r1, 0x80004000200010
; CV1-NEXT:    srlw $r10 = $r2, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r6, 31, 16
; CV1-NEXT:    insf $r4 = $r7, 31, 16
; CV1-NEXT:    srld $r6 = $r2, 48
; CV1-NEXT:    srld $r7 = $r5, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    extfz $r8 = $r2, 47, 32
; CV1-NEXT:    extfz $r9 = $r5, 47, 32
; CV1-NEXT:    srlw $r11 = $r5, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r2 = $r2
; CV1-NEXT:    zxhd $r5 = $r5
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r6 = $r6
; CV1-NEXT:    zxhd $r7 = $r7
; CV1-NEXT:    ;;
; CV1-NEXT:    maxuw $r6 = $r7, $r6
; CV1-NEXT:    zxhd $r8 = $r8
; CV1-NEXT:    zxhd $r9 = $r9
; CV1-NEXT:    ;;
; CV1-NEXT:    maxuw $r7 = $r9, $r8
; CV1-NEXT:    zxhd $r10 = $r10
; CV1-NEXT:    zxhd $r11 = $r11
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r2 = $r2
; CV1-NEXT:    zxhd $r5 = $r5
; CV1-NEXT:    maxuw $r8 = $r11, $r10
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x8000400020001
; CV1-NEXT:    maxuw $r2 = $r5, $r2
; CV1-NEXT:    sbmm8 $r3 = $r3, 0x8000400020001
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r8, 15, 8
; CV1-NEXT:    srld $r5 = $r3, 48
; CV1-NEXT:    srld $r6 = $r1, 48
; CV1-NEXT:    insf $r7 = $r6, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    extfz $r8 = $r3, 47, 32
; CV1-NEXT:    extfz $r9 = $r1, 47, 32
; CV1-NEXT:    srlw $r10 = $r3, 16
; CV1-NEXT:    srlw $r11 = $r1, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r1 = $r1
; CV1-NEXT:    zxhd $r3 = $r3
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r5 = $r5
; CV1-NEXT:    zxhd $r6 = $r6
; CV1-NEXT:    ;;
; CV1-NEXT:    maxuw $r5 = $r6, $r5
; CV1-NEXT:    zxhd $r8 = $r8
; CV1-NEXT:    zxhd $r9 = $r9
; CV1-NEXT:    ;;
; CV1-NEXT:    maxuw $r6 = $r9, $r8
; CV1-NEXT:    zxhd $r10 = $r10
; CV1-NEXT:    zxhd $r11 = $r11
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r1 = $r1
; CV1-NEXT:    zxhd $r3 = $r3
; CV1-NEXT:    maxuw $r8 = $r11, $r10
; CV1-NEXT:    ;;
; CV1-NEXT:    maxuw $r1 = $r1, $r3
; CV1-NEXT:    insf $r2 = $r7, 31, 16
; CV1-NEXT:    insf $r6 = $r5, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r4, 63, 32
; CV1-NEXT:    insf $r1 = $r8, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r6, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r2, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: maxubx:
; CV2:       # %bb.0:
; CV2-NEXT:    maxubo $r0 = $r0, $r2
; CV2-NEXT:    maxubo $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = shufflevector <16 x i8> %0, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %4 = shufflevector <16 x i8> %1, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5 = tail call <8 x i8> @llvm.umax.v8i8(<8 x i8> %3, <8 x i8> %4)
  %6 = shufflevector <16 x i8> %0, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %7 = shufflevector <16 x i8> %1, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %8 = tail call <8 x i8> @llvm.umax.v8i8(<8 x i8> %6, <8 x i8> %7)
  %9 = shufflevector <8 x i8> %5, <8 x i8> %8, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  ret <16 x i8> %9
}

define i64 @maxud(i64 %0, i64 %1) {
; CHECK-LABEL: maxud:
; CHECK:       # %bb.0:
; CHECK-NEXT:    maxud $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = tail call i64 @llvm.umax.i64(i64 %0, i64 %1)
  ret i64 %3
}

declare i64 @llvm.umax.i64(i64, i64)

define <2 x i64> @maxudp(<2 x i64> %0, <2 x i64> %1) {
; CHECK-LABEL: maxudp:
; CHECK:       # %bb.0:
; CHECK-NEXT:    maxud $r0 = $r0, $r2
; CHECK-NEXT:    maxud $r1 = $r1, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = extractelement <2 x i64> %0, i64 0
  %4 = extractelement <2 x i64> %1, i64 0
  %5 = tail call i64 @llvm.umax.i64(i64 %3, i64 %4)
  %6 = extractelement <2 x i64> %0, i64 1
  %7 = extractelement <2 x i64> %1, i64 1
  %8 = tail call i64 @llvm.umax.i64(i64 %6, i64 %7)
  %9 = insertelement <2 x i64> undef, i64 %5, i32 0
  %10 = insertelement <2 x i64> %9, i64 %8, i32 1
  ret <2 x i64> %10
}

define <4 x i64> @maxudq(<4 x i64> %0, <4 x i64> %1) {
; CHECK-LABEL: maxudq:
; CHECK:       # %bb.0:
; CHECK-NEXT:    maxud $r0 = $r0, $r4
; CHECK-NEXT:    maxud $r1 = $r1, $r5
; CHECK-NEXT:    maxud $r2 = $r2, $r6
; CHECK-NEXT:    maxud $r3 = $r3, $r7
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = extractelement <4 x i64> %0, i64 0
  %4 = extractelement <4 x i64> %1, i64 0
  %5 = tail call i64 @llvm.umax.i64(i64 %3, i64 %4)
  %6 = extractelement <4 x i64> %0, i64 1
  %7 = extractelement <4 x i64> %1, i64 1
  %8 = tail call i64 @llvm.umax.i64(i64 %6, i64 %7)
  %9 = extractelement <4 x i64> %0, i64 2
  %10 = extractelement <4 x i64> %1, i64 2
  %11 = tail call i64 @llvm.umax.i64(i64 %9, i64 %10)
  %12 = extractelement <4 x i64> %0, i64 3
  %13 = extractelement <4 x i64> %1, i64 3
  %14 = tail call i64 @llvm.umax.i64(i64 %12, i64 %13)
  %15 = insertelement <4 x i64> undef, i64 %5, i32 0
  %16 = insertelement <4 x i64> %15, i64 %8, i32 1
  %17 = insertelement <4 x i64> %16, i64 %11, i32 2
  %18 = insertelement <4 x i64> %17, i64 %14, i32 3
  ret <4 x i64> %18
}

define <8 x i16> @maxuho(<8 x i16> %0, <8 x i16> %1) {
; CHECK-LABEL: maxuho:
; CHECK:       # %bb.0:
; CHECK-NEXT:    maxuhq $r0 = $r0, $r2
; CHECK-NEXT:    maxuhq $r1 = $r1, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4 = shufflevector <8 x i16> %1, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5 = tail call <4 x i16> @llvm.umax.v4i16(<4 x i16> %3, <4 x i16> %4)
  %6 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7 = shufflevector <8 x i16> %1, <8 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8 = tail call <4 x i16> @llvm.umax.v4i16(<4 x i16> %6, <4 x i16> %7)
  %9 = shufflevector <4 x i16> %5, <4 x i16> %8, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i16> %9
}

declare <4 x i16> @llvm.umax.v4i16(<4 x i16>, <4 x i16>)

define <2 x i16> @maxuhp(<2 x i16> %0, <2 x i16> %1) {
; CHECK-LABEL: maxuhp:
; CHECK:       # %bb.0:
; CHECK-NEXT:    maxuhq $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = tail call <2 x i16> @llvm.umax.v2i16(<2 x i16> %0, <2 x i16> %1)
  ret <2 x i16> %3
}

declare <2 x i16> @llvm.umax.v2i16(<2 x i16>, <2 x i16>)

define <4 x i16> @maxuhq(<4 x i16> %0, <4 x i16> %1) {
; CHECK-LABEL: maxuhq:
; CHECK:       # %bb.0:
; CHECK-NEXT:    maxuhq $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = tail call <4 x i16> @llvm.umax.v4i16(<4 x i16> %0, <4 x i16> %1)
  ret <4 x i16> %3
}

define <16 x i16> @maxuhx(<16 x i16> %0, <16 x i16> %1) {
; CHECK-LABEL: maxuhx:
; CHECK:       # %bb.0:
; CHECK-NEXT:    maxuhq $r0 = $r0, $r4
; CHECK-NEXT:    maxuhq $r1 = $r1, $r5
; CHECK-NEXT:    maxuhq $r2 = $r2, $r6
; CHECK-NEXT:    maxuhq $r3 = $r3, $r7
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4 = shufflevector <16 x i16> %1, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5 = tail call <4 x i16> @llvm.umax.v4i16(<4 x i16> %3, <4 x i16> %4)
  %6 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7 = shufflevector <16 x i16> %1, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8 = tail call <4 x i16> @llvm.umax.v4i16(<4 x i16> %6, <4 x i16> %7)
  %9 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %10 = shufflevector <16 x i16> %1, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %11 = tail call <4 x i16> @llvm.umax.v4i16(<4 x i16> %9, <4 x i16> %10)
  %12 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %13 = shufflevector <16 x i16> %1, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %14 = tail call <4 x i16> @llvm.umax.v4i16(<4 x i16> %12, <4 x i16> %13)
  %15 = shufflevector <4 x i16> %5, <4 x i16> %8, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16 = shufflevector <4 x i16> %11, <4 x i16> %14, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %17 = shufflevector <8 x i16> %15, <8 x i16> %16, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  ret <16 x i16> %17
}

define i32 @maxuw(i32 %0, i32 %1) {
; CHECK-LABEL: maxuw:
; CHECK:       # %bb.0:
; CHECK-NEXT:    maxuw $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = tail call i32 @llvm.umax.i32(i32 %0, i32 %1)
  ret i32 %3
}

declare i32 @llvm.umax.i32(i32, i32)

define <8 x i32> @maxuwo(<8 x i32> %0, <8 x i32> %1) {
; CHECK-LABEL: maxuwo:
; CHECK:       # %bb.0:
; CHECK-NEXT:    maxuwp $r0 = $r0, $r4
; CHECK-NEXT:    maxuwp $r1 = $r1, $r5
; CHECK-NEXT:    maxuwp $r2 = $r2, $r6
; CHECK-NEXT:    maxuwp $r3 = $r3, $r7
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 0, i32 1>
  %4 = shufflevector <8 x i32> %1, <8 x i32> undef, <2 x i32> <i32 0, i32 1>
  %5 = tail call <2 x i32> @llvm.umax.v2i32(<2 x i32> %3, <2 x i32> %4)
  %6 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 2, i32 3>
  %7 = shufflevector <8 x i32> %1, <8 x i32> undef, <2 x i32> <i32 2, i32 3>
  %8 = tail call <2 x i32> @llvm.umax.v2i32(<2 x i32> %6, <2 x i32> %7)
  %9 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 4, i32 5>
  %10 = shufflevector <8 x i32> %1, <8 x i32> undef, <2 x i32> <i32 4, i32 5>
  %11 = tail call <2 x i32> @llvm.umax.v2i32(<2 x i32> %9, <2 x i32> %10)
  %12 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 6, i32 7>
  %13 = shufflevector <8 x i32> %1, <8 x i32> undef, <2 x i32> <i32 6, i32 7>
  %14 = tail call <2 x i32> @llvm.umax.v2i32(<2 x i32> %12, <2 x i32> %13)
  %15 = shufflevector <2 x i32> %5, <2 x i32> %8, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %16 = shufflevector <2 x i32> %11, <2 x i32> %14, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %17 = shufflevector <4 x i32> %15, <4 x i32> %16, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i32> %17
}

declare <2 x i32> @llvm.umax.v2i32(<2 x i32>, <2 x i32>)

define <2 x i32> @maxuwp(<2 x i32> %0, <2 x i32> %1) {
; CHECK-LABEL: maxuwp:
; CHECK:       # %bb.0:
; CHECK-NEXT:    maxuwp $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = tail call <2 x i32> @llvm.umax.v2i32(<2 x i32> %0, <2 x i32> %1)
  ret <2 x i32> %3
}

define <4 x i32> @maxuwq(<4 x i32> %0, <4 x i32> %1) {
; CHECK-LABEL: maxuwq:
; CHECK:       # %bb.0:
; CHECK-NEXT:    maxuwp $r0 = $r0, $r2
; CHECK-NEXT:    maxuwp $r1 = $r1, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 0, i32 1>
  %4 = shufflevector <4 x i32> %1, <4 x i32> undef, <2 x i32> <i32 0, i32 1>
  %5 = tail call <2 x i32> @llvm.umax.v2i32(<2 x i32> %3, <2 x i32> %4)
  %6 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 2, i32 3>
  %7 = shufflevector <4 x i32> %1, <4 x i32> undef, <2 x i32> <i32 2, i32 3>
  %8 = tail call <2 x i32> @llvm.umax.v2i32(<2 x i32> %6, <2 x i32> %7)
  %9 = shufflevector <2 x i32> %5, <2 x i32> %8, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  ret <4 x i32> %9
}

define i32 @maxw(i32 %0, i32 %1) {
; CHECK-LABEL: maxw:
; CHECK:       # %bb.0:
; CHECK-NEXT:    maxw $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = tail call i32 @llvm.smax.i32(i32 %0, i32 %1)
  ret i32 %3
}

declare i32 @llvm.smax.i32(i32, i32)

define <8 x i32> @maxwo(<8 x i32> %0, <8 x i32> %1) {
; CHECK-LABEL: maxwo:
; CHECK:       # %bb.0:
; CHECK-NEXT:    maxwp $r0 = $r0, $r4
; CHECK-NEXT:    maxwp $r1 = $r1, $r5
; CHECK-NEXT:    maxwp $r2 = $r2, $r6
; CHECK-NEXT:    maxwp $r3 = $r3, $r7
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 0, i32 1>
  %4 = shufflevector <8 x i32> %1, <8 x i32> undef, <2 x i32> <i32 0, i32 1>
  %5 = tail call <2 x i32> @llvm.smax.v2i32(<2 x i32> %3, <2 x i32> %4)
  %6 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 2, i32 3>
  %7 = shufflevector <8 x i32> %1, <8 x i32> undef, <2 x i32> <i32 2, i32 3>
  %8 = tail call <2 x i32> @llvm.smax.v2i32(<2 x i32> %6, <2 x i32> %7)
  %9 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 4, i32 5>
  %10 = shufflevector <8 x i32> %1, <8 x i32> undef, <2 x i32> <i32 4, i32 5>
  %11 = tail call <2 x i32> @llvm.smax.v2i32(<2 x i32> %9, <2 x i32> %10)
  %12 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 6, i32 7>
  %13 = shufflevector <8 x i32> %1, <8 x i32> undef, <2 x i32> <i32 6, i32 7>
  %14 = tail call <2 x i32> @llvm.smax.v2i32(<2 x i32> %12, <2 x i32> %13)
  %15 = shufflevector <2 x i32> %5, <2 x i32> %8, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %16 = shufflevector <2 x i32> %11, <2 x i32> %14, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %17 = shufflevector <4 x i32> %15, <4 x i32> %16, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i32> %17
}

declare <2 x i32> @llvm.smax.v2i32(<2 x i32>, <2 x i32>)

define <2 x i32> @maxwp(<2 x i32> %0, <2 x i32> %1) {
; CHECK-LABEL: maxwp:
; CHECK:       # %bb.0:
; CHECK-NEXT:    maxwp $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = tail call <2 x i32> @llvm.smax.v2i32(<2 x i32> %0, <2 x i32> %1)
  ret <2 x i32> %3
}

define <4 x i32> @maxwq(<4 x i32> %0, <4 x i32> %1) {
; CHECK-LABEL: maxwq:
; CHECK:       # %bb.0:
; CHECK-NEXT:    maxwp $r0 = $r0, $r2
; CHECK-NEXT:    maxwp $r1 = $r1, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 0, i32 1>
  %4 = shufflevector <4 x i32> %1, <4 x i32> undef, <2 x i32> <i32 0, i32 1>
  %5 = tail call <2 x i32> @llvm.smax.v2i32(<2 x i32> %3, <2 x i32> %4)
  %6 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 2, i32 3>
  %7 = shufflevector <4 x i32> %1, <4 x i32> undef, <2 x i32> <i32 2, i32 3>
  %8 = tail call <2 x i32> @llvm.smax.v2i32(<2 x i32> %6, <2 x i32> %7)
  %9 = shufflevector <2 x i32> %5, <2 x i32> %8, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  ret <4 x i32> %9
}

define <8 x i8> @minbo(<8 x i8> %0, <8 x i8> %1) {
; CV1-LABEL: minbo:
; CV1:       # %bb.0:
; CV1-NEXT:    sxmbhq $r2 = $r1
; CV1-NEXT:    sxmbhq $r3 = $r0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r4 = $r2, 47, 32
; CV1-NEXT:    extfs $r5 = $r3, 47, 32
; CV1-NEXT:    srad $r8 = $r2, 48
; CV1-NEXT:    srad $r9 = $r3, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    minw $r4 = $r5, $r4
; CV1-NEXT:    extfs $r6 = $r2, 31, 16
; CV1-NEXT:    extfs $r7 = $r3, 31, 16
; CV1-NEXT:    minw $r8 = $r9, $r8
; CV1-NEXT:    ;;
; CV1-NEXT:    sxhd $r2 = $r2
; CV1-NEXT:    sxhd $r3 = $r3
; CV1-NEXT:    minw $r5 = $r7, $r6
; CV1-NEXT:    ;;
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    minw $r2 = $r3, $r2
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r5, 15, 8
; CV1-NEXT:    insf $r4 = $r8, 15, 8
; CV1-NEXT:    srad $r8 = $r1, 48
; CV1-NEXT:    srad $r9 = $r0, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r3 = $r1, 47, 32
; CV1-NEXT:    extfs $r5 = $r0, 47, 32
; CV1-NEXT:    minw $r8 = $r9, $r8
; CV1-NEXT:    ;;
; CV1-NEXT:    minw $r3 = $r5, $r3
; CV1-NEXT:    extfs $r6 = $r1, 31, 16
; CV1-NEXT:    extfs $r7 = $r0, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    sxhd $r0 = $r0
; CV1-NEXT:    sxhd $r1 = $r1
; CV1-NEXT:    minw $r5 = $r7, $r6
; CV1-NEXT:    ;;
; CV1-NEXT:    minw $r0 = $r0, $r1
; CV1-NEXT:    insf $r2 = $r4, 31, 16
; CV1-NEXT:    insf $r3 = $r8, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r5, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r3, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r2, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: minbo:
; CV2:       # %bb.0:
; CV2-NEXT:    minbo $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = tail call <8 x i8> @llvm.smin.v8i8(<8 x i8> %0, <8 x i8> %1)
  ret <8 x i8> %3
}

declare <8 x i8> @llvm.smin.v8i8(<8 x i8>, <8 x i8>)

define <2 x i8> @minbp(<2 x i8> %0, <2 x i8> %1) {
; CV1-LABEL: minbp:
; CV1:       # %bb.0:
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x401
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: minbp:
; CV2:       # %bb.0:
; CV2-NEXT:    minbo $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = tail call <2 x i8> @llvm.smin.v2i8(<2 x i8> %0, <2 x i8> %1)
  ret <2 x i8> %3
}

declare <2 x i8> @llvm.smin.v2i8(<2 x i8>, <2 x i8>)

define <4 x i8> @minbq(<4 x i8> %0, <4 x i8> %1) {
; CV1-LABEL: minbq:
; CV1:       # %bb.0:
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: minbq:
; CV2:       # %bb.0:
; CV2-NEXT:    minbo $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = tail call <4 x i8> @llvm.smin.v4i8(<4 x i8> %0, <4 x i8> %1)
  ret <4 x i8> %3
}

declare <4 x i8> @llvm.smin.v4i8(<4 x i8>, <4 x i8>)

define <32 x i8> @minbv(<32 x i8> %0, <32 x i8> %1) {
; CV1-LABEL: minbv:
; CV1:       # %bb.0:
; CV1-NEXT:    sxmbhq $r8 = $r4
; CV1-NEXT:    sxmbhq $r9 = $r0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r10 = $r8, 47, 32
; CV1-NEXT:    extfs $r11 = $r9, 47, 32
; CV1-NEXT:    srad $r17 = $r8, 48
; CV1-NEXT:    srad $r32 = $r9, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    minw $r10 = $r11, $r10
; CV1-NEXT:    extfs $r15 = $r8, 31, 16
; CV1-NEXT:    extfs $r16 = $r9, 31, 16
; CV1-NEXT:    minw $r17 = $r32, $r17
; CV1-NEXT:    ;;
; CV1-NEXT:    sxhd $r8 = $r8
; CV1-NEXT:    sxhd $r9 = $r9
; CV1-NEXT:    minw $r11 = $r16, $r15
; CV1-NEXT:    ;;
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxlbhq $r4 = $r4
; CV1-NEXT:    minw $r8 = $r9, $r8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r8 = $r11, 15, 8
; CV1-NEXT:    insf $r10 = $r17, 15, 8
; CV1-NEXT:    srad $r17 = $r4, 48
; CV1-NEXT:    srad $r32 = $r0, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r9 = $r4, 47, 32
; CV1-NEXT:    extfs $r11 = $r0, 47, 32
; CV1-NEXT:    minw $r17 = $r32, $r17
; CV1-NEXT:    ;;
; CV1-NEXT:    minw $r9 = $r11, $r9
; CV1-NEXT:    extfs $r15 = $r4, 31, 16
; CV1-NEXT:    extfs $r16 = $r0, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    sxhd $r0 = $r0
; CV1-NEXT:    sxhd $r4 = $r4
; CV1-NEXT:    minw $r11 = $r16, $r15
; CV1-NEXT:    ;;
; CV1-NEXT:    minw $r0 = $r0, $r4
; CV1-NEXT:    insf $r8 = $r10, 31, 16
; CV1-NEXT:    insf $r9 = $r17, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r11, 15, 8
; CV1-NEXT:    sxmbhq $r4 = $r5
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r9, 31, 16
; CV1-NEXT:    extfs $r9 = $r4, 47, 32
; CV1-NEXT:    srad $r16 = $r4, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r8, 63, 32
; CV1-NEXT:    sxmbhq $r8 = $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r10 = $r8, 47, 32
; CV1-NEXT:    extfs $r11 = $r4, 31, 16
; CV1-NEXT:    srad $r17 = $r8, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    sxhd $r4 = $r4
; CV1-NEXT:    minw $r9 = $r10, $r9
; CV1-NEXT:    extfs $r15 = $r8, 31, 16
; CV1-NEXT:    minw $r16 = $r17, $r16
; CV1-NEXT:    ;;
; CV1-NEXT:    sxlbhq $r5 = $r5
; CV1-NEXT:    sxhd $r8 = $r8
; CV1-NEXT:    minw $r10 = $r15, $r11
; CV1-NEXT:    ;;
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    minw $r4 = $r8, $r4
; CV1-NEXT:    insf $r9 = $r16, 15, 8
; CV1-NEXT:    srad $r16 = $r5, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r4 = $r10, 15, 8
; CV1-NEXT:    extfs $r8 = $r5, 47, 32
; CV1-NEXT:    srad $r17 = $r1, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r10 = $r1, 47, 32
; CV1-NEXT:    extfs $r11 = $r5, 31, 16
; CV1-NEXT:    minw $r16 = $r17, $r16
; CV1-NEXT:    ;;
; CV1-NEXT:    sxhd $r5 = $r5
; CV1-NEXT:    minw $r8 = $r10, $r8
; CV1-NEXT:    extfs $r15 = $r1, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    sxhd $r1 = $r1
; CV1-NEXT:    insf $r8 = $r16, 15, 8
; CV1-NEXT:    minw $r10 = $r15, $r11
; CV1-NEXT:    ;;
; CV1-NEXT:    minw $r1 = $r1, $r5
; CV1-NEXT:    insf $r4 = $r9, 31, 16
; CV1-NEXT:    sxmbhq $r5 = $r6
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r10, 15, 8
; CV1-NEXT:    extfs $r9 = $r5, 47, 32
; CV1-NEXT:    srad $r16 = $r5, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r8, 31, 16
; CV1-NEXT:    sxmbhq $r8 = $r2
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r10 = $r8, 47, 32
; CV1-NEXT:    extfs $r11 = $r5, 31, 16
; CV1-NEXT:    srad $r17 = $r8, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    sxhd $r5 = $r5
; CV1-NEXT:    minw $r9 = $r10, $r9
; CV1-NEXT:    extfs $r15 = $r8, 31, 16
; CV1-NEXT:    minw $r16 = $r17, $r16
; CV1-NEXT:    ;;
; CV1-NEXT:    sxhd $r8 = $r8
; CV1-NEXT:    insf $r9 = $r16, 15, 8
; CV1-NEXT:    minw $r10 = $r15, $r11
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r4, 63, 32
; CV1-NEXT:    sxlbhq $r4 = $r6
; CV1-NEXT:    minw $r5 = $r8, $r5
; CV1-NEXT:    ;;
; CV1-NEXT:    sxlbhq $r2 = $r2
; CV1-NEXT:    insf $r5 = $r10, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r5 = $r9, 31, 16
; CV1-NEXT:    extfs $r6 = $r4, 47, 32
; CV1-NEXT:    srad $r9 = $r4, 48
; CV1-NEXT:    srad $r10 = $r2, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r8 = $r2, 47, 32
; CV1-NEXT:    minw $r9 = $r10, $r9
; CV1-NEXT:    extfs $r11 = $r4, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    sxhd $r4 = $r4
; CV1-NEXT:    minw $r6 = $r8, $r6
; CV1-NEXT:    extfs $r15 = $r2, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    sxhd $r2 = $r2
; CV1-NEXT:    minw $r8 = $r15, $r11
; CV1-NEXT:    sxmbhq $r10 = $r3
; CV1-NEXT:    ;;
; CV1-NEXT:    minw $r2 = $r2, $r4
; CV1-NEXT:    sxmbhq $r4 = $r7
; CV1-NEXT:    srad $r15 = $r10, 48
; CV1-NEXT:    extfs $r17 = $r10, 47, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r6 = $r9, 15, 8
; CV1-NEXT:    srad $r11 = $r4, 48
; CV1-NEXT:    extfs $r16 = $r4, 47, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r8, 15, 8
; CV1-NEXT:    sxlbhq $r7 = $r7
; CV1-NEXT:    minw $r8 = $r15, $r11
; CV1-NEXT:    minw $r9 = $r17, $r16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r6, 31, 16
; CV1-NEXT:    insf $r9 = $r8, 15, 8
; CV1-NEXT:    srad $r15 = $r7, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r6 = $r4, 31, 16
; CV1-NEXT:    extfs $r8 = $r10, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    sxhd $r4 = $r4
; CV1-NEXT:    minw $r6 = $r8, $r6
; CV1-NEXT:    sxhd $r10 = $r10
; CV1-NEXT:    ;;
; CV1-NEXT:    sxlbhq $r3 = $r3
; CV1-NEXT:    minw $r4 = $r10, $r4
; CV1-NEXT:    extfs $r8 = $r7, 47, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r11 = $r3, 47, 32
; CV1-NEXT:    srad $r16 = $r3, 48
; CV1-NEXT:    extfs $r17 = $r7, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    sxhd $r7 = $r7
; CV1-NEXT:    minw $r10 = $r16, $r15
; CV1-NEXT:    extfs $r32 = $r3, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    sxhd $r3 = $r3
; CV1-NEXT:    insf $r4 = $r6, 15, 8
; CV1-NEXT:    minw $r6 = $r11, $r8
; CV1-NEXT:    minw $r8 = $r32, $r17
; CV1-NEXT:    ;;
; CV1-NEXT:    minw $r3 = $r3, $r7
; CV1-NEXT:    insf $r4 = $r9, 31, 16
; CV1-NEXT:    insf $r6 = $r10, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r5, 63, 32
; CV1-NEXT:    insf $r3 = $r8, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r3 = $r6, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r3 = $r4, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: minbv:
; CV2:       # %bb.0:
; CV2-NEXT:    minbo $r0 = $r0, $r4
; CV2-NEXT:    minbo $r1 = $r1, $r5
; CV2-NEXT:    minbo $r2 = $r2, $r6
; CV2-NEXT:    minbo $r3 = $r3, $r7
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %4 = shufflevector <32 x i8> %1, <32 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5 = tail call <8 x i8> @llvm.smin.v8i8(<8 x i8> %3, <8 x i8> %4)
  %6 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %7 = shufflevector <32 x i8> %1, <32 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %8 = tail call <8 x i8> @llvm.smin.v8i8(<8 x i8> %6, <8 x i8> %7)
  %9 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %10 = shufflevector <32 x i8> %1, <32 x i8> undef, <8 x i32> <i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %11 = tail call <8 x i8> @llvm.smin.v8i8(<8 x i8> %9, <8 x i8> %10)
  %12 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  %13 = shufflevector <32 x i8> %1, <32 x i8> undef, <8 x i32> <i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  %14 = tail call <8 x i8> @llvm.smin.v8i8(<8 x i8> %12, <8 x i8> %13)
  %15 = shufflevector <8 x i8> %5, <8 x i8> %8, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %16 = shufflevector <8 x i8> %11, <8 x i8> %14, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %17 = shufflevector <16 x i8> %15, <16 x i8> %16, <32 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  ret <32 x i8> %17
}

define <16 x i8> @minbx(<16 x i8> %0, <16 x i8> %1) {
; CV1-LABEL: minbx:
; CV1:       # %bb.0:
; CV1-NEXT:    sxmbhq $r4 = $r2
; CV1-NEXT:    sxmbhq $r5 = $r0
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r6 = $r4, 47, 32
; CV1-NEXT:    extfs $r7 = $r5, 47, 32
; CV1-NEXT:    srad $r10 = $r4, 48
; CV1-NEXT:    srad $r11 = $r5, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    minw $r6 = $r7, $r6
; CV1-NEXT:    extfs $r8 = $r4, 31, 16
; CV1-NEXT:    extfs $r9 = $r5, 31, 16
; CV1-NEXT:    minw $r10 = $r11, $r10
; CV1-NEXT:    ;;
; CV1-NEXT:    sxhd $r4 = $r4
; CV1-NEXT:    sxhd $r5 = $r5
; CV1-NEXT:    minw $r7 = $r9, $r8
; CV1-NEXT:    ;;
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxlbhq $r2 = $r2
; CV1-NEXT:    minw $r4 = $r5, $r4
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r4 = $r7, 15, 8
; CV1-NEXT:    insf $r6 = $r10, 15, 8
; CV1-NEXT:    srad $r10 = $r2, 48
; CV1-NEXT:    srad $r11 = $r0, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r5 = $r2, 47, 32
; CV1-NEXT:    extfs $r7 = $r0, 47, 32
; CV1-NEXT:    minw $r10 = $r11, $r10
; CV1-NEXT:    ;;
; CV1-NEXT:    minw $r5 = $r7, $r5
; CV1-NEXT:    extfs $r8 = $r2, 31, 16
; CV1-NEXT:    extfs $r9 = $r0, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    sxhd $r0 = $r0
; CV1-NEXT:    sxhd $r2 = $r2
; CV1-NEXT:    minw $r7 = $r9, $r8
; CV1-NEXT:    ;;
; CV1-NEXT:    minw $r0 = $r0, $r2
; CV1-NEXT:    sxmbhq $r2 = $r3
; CV1-NEXT:    insf $r5 = $r10, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r7, 15, 8
; CV1-NEXT:    insf $r4 = $r6, 31, 16
; CV1-NEXT:    srad $r10 = $r2, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r5, 31, 16
; CV1-NEXT:    sxmbhq $r5 = $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r6 = $r2, 47, 32
; CV1-NEXT:    extfs $r7 = $r5, 47, 32
; CV1-NEXT:    srad $r11 = $r5, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    minw $r6 = $r7, $r6
; CV1-NEXT:    extfs $r8 = $r2, 31, 16
; CV1-NEXT:    extfs $r9 = $r5, 31, 16
; CV1-NEXT:    minw $r10 = $r11, $r10
; CV1-NEXT:    ;;
; CV1-NEXT:    sxhd $r2 = $r2
; CV1-NEXT:    sxhd $r5 = $r5
; CV1-NEXT:    minw $r7 = $r9, $r8
; CV1-NEXT:    ;;
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    minw $r2 = $r5, $r2
; CV1-NEXT:    sxlbhq $r3 = $r3
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r7, 15, 8
; CV1-NEXT:    insf $r6 = $r10, 15, 8
; CV1-NEXT:    srad $r10 = $r3, 48
; CV1-NEXT:    srad $r11 = $r1, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    extfs $r5 = $r3, 47, 32
; CV1-NEXT:    extfs $r7 = $r1, 47, 32
; CV1-NEXT:    minw $r10 = $r11, $r10
; CV1-NEXT:    ;;
; CV1-NEXT:    minw $r5 = $r7, $r5
; CV1-NEXT:    extfs $r8 = $r3, 31, 16
; CV1-NEXT:    extfs $r9 = $r1, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    sxhd $r1 = $r1
; CV1-NEXT:    sxhd $r3 = $r3
; CV1-NEXT:    minw $r7 = $r9, $r8
; CV1-NEXT:    ;;
; CV1-NEXT:    minw $r1 = $r1, $r3
; CV1-NEXT:    insf $r2 = $r6, 31, 16
; CV1-NEXT:    insf $r5 = $r10, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r4, 63, 32
; CV1-NEXT:    insf $r1 = $r7, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r5, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r2, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: minbx:
; CV2:       # %bb.0:
; CV2-NEXT:    minbo $r0 = $r0, $r2
; CV2-NEXT:    minbo $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = shufflevector <16 x i8> %0, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %4 = shufflevector <16 x i8> %1, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5 = tail call <8 x i8> @llvm.smin.v8i8(<8 x i8> %3, <8 x i8> %4)
  %6 = shufflevector <16 x i8> %0, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %7 = shufflevector <16 x i8> %1, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %8 = tail call <8 x i8> @llvm.smin.v8i8(<8 x i8> %6, <8 x i8> %7)
  %9 = shufflevector <8 x i8> %5, <8 x i8> %8, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  ret <16 x i8> %9
}

define i64 @mind(i64 %0, i64 %1) {
; CHECK-LABEL: mind:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mind $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = tail call i64 @llvm.smin.i64(i64 %0, i64 %1)
  ret i64 %3
}

declare i64 @llvm.smin.i64(i64, i64)

define <2 x i64> @mindp(<2 x i64> %0, <2 x i64> %1) {
; CHECK-LABEL: mindp:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mind $r0 = $r0, $r2
; CHECK-NEXT:    mind $r1 = $r1, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = extractelement <2 x i64> %0, i64 0
  %4 = extractelement <2 x i64> %1, i64 0
  %5 = tail call i64 @llvm.smin.i64(i64 %3, i64 %4)
  %6 = extractelement <2 x i64> %0, i64 1
  %7 = extractelement <2 x i64> %1, i64 1
  %8 = tail call i64 @llvm.smin.i64(i64 %6, i64 %7)
  %9 = insertelement <2 x i64> undef, i64 %5, i32 0
  %10 = insertelement <2 x i64> %9, i64 %8, i32 1
  ret <2 x i64> %10
}

define <4 x i64> @mindq(<4 x i64> %0, <4 x i64> %1) {
; CHECK-LABEL: mindq:
; CHECK:       # %bb.0:
; CHECK-NEXT:    mind $r0 = $r0, $r4
; CHECK-NEXT:    mind $r1 = $r1, $r5
; CHECK-NEXT:    mind $r2 = $r2, $r6
; CHECK-NEXT:    mind $r3 = $r3, $r7
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = extractelement <4 x i64> %0, i64 0
  %4 = extractelement <4 x i64> %1, i64 0
  %5 = tail call i64 @llvm.smin.i64(i64 %3, i64 %4)
  %6 = extractelement <4 x i64> %0, i64 1
  %7 = extractelement <4 x i64> %1, i64 1
  %8 = tail call i64 @llvm.smin.i64(i64 %6, i64 %7)
  %9 = extractelement <4 x i64> %0, i64 2
  %10 = extractelement <4 x i64> %1, i64 2
  %11 = tail call i64 @llvm.smin.i64(i64 %9, i64 %10)
  %12 = extractelement <4 x i64> %0, i64 3
  %13 = extractelement <4 x i64> %1, i64 3
  %14 = tail call i64 @llvm.smin.i64(i64 %12, i64 %13)
  %15 = insertelement <4 x i64> undef, i64 %5, i32 0
  %16 = insertelement <4 x i64> %15, i64 %8, i32 1
  %17 = insertelement <4 x i64> %16, i64 %11, i32 2
  %18 = insertelement <4 x i64> %17, i64 %14, i32 3
  ret <4 x i64> %18
}

define <8 x i16> @minho(<8 x i16> %0, <8 x i16> %1) {
; CHECK-LABEL: minho:
; CHECK:       # %bb.0:
; CHECK-NEXT:    minhq $r0 = $r0, $r2
; CHECK-NEXT:    minhq $r1 = $r1, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4 = shufflevector <8 x i16> %1, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5 = tail call <4 x i16> @llvm.smin.v4i16(<4 x i16> %3, <4 x i16> %4)
  %6 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7 = shufflevector <8 x i16> %1, <8 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8 = tail call <4 x i16> @llvm.smin.v4i16(<4 x i16> %6, <4 x i16> %7)
  %9 = shufflevector <4 x i16> %5, <4 x i16> %8, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i16> %9
}

declare <4 x i16> @llvm.smin.v4i16(<4 x i16>, <4 x i16>)

define <2 x i16> @minhp(<2 x i16> %0, <2 x i16> %1) {
; CHECK-LABEL: minhp:
; CHECK:       # %bb.0:
; CHECK-NEXT:    minhq $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = tail call <2 x i16> @llvm.smin.v2i16(<2 x i16> %0, <2 x i16> %1)
  ret <2 x i16> %3
}

declare <2 x i16> @llvm.smin.v2i16(<2 x i16>, <2 x i16>)

define <4 x i16> @minhq(<4 x i16> %0, <4 x i16> %1) {
; CHECK-LABEL: minhq:
; CHECK:       # %bb.0:
; CHECK-NEXT:    minhq $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = tail call <4 x i16> @llvm.smin.v4i16(<4 x i16> %0, <4 x i16> %1)
  ret <4 x i16> %3
}

define <16 x i16> @minhx(<16 x i16> %0, <16 x i16> %1) {
; CHECK-LABEL: minhx:
; CHECK:       # %bb.0:
; CHECK-NEXT:    minhq $r0 = $r0, $r4
; CHECK-NEXT:    minhq $r1 = $r1, $r5
; CHECK-NEXT:    minhq $r2 = $r2, $r6
; CHECK-NEXT:    minhq $r3 = $r3, $r7
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4 = shufflevector <16 x i16> %1, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5 = tail call <4 x i16> @llvm.smin.v4i16(<4 x i16> %3, <4 x i16> %4)
  %6 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7 = shufflevector <16 x i16> %1, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8 = tail call <4 x i16> @llvm.smin.v4i16(<4 x i16> %6, <4 x i16> %7)
  %9 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %10 = shufflevector <16 x i16> %1, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %11 = tail call <4 x i16> @llvm.smin.v4i16(<4 x i16> %9, <4 x i16> %10)
  %12 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %13 = shufflevector <16 x i16> %1, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %14 = tail call <4 x i16> @llvm.smin.v4i16(<4 x i16> %12, <4 x i16> %13)
  %15 = shufflevector <4 x i16> %5, <4 x i16> %8, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16 = shufflevector <4 x i16> %11, <4 x i16> %14, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %17 = shufflevector <8 x i16> %15, <8 x i16> %16, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  ret <16 x i16> %17
}

define <8 x i8> @minubo(<8 x i8> %0, <8 x i8> %1) {
; CV1-LABEL: minubo:
; CV1:       # %bb.0:
; CV1-NEXT:    sbmm8 $r2 = $r1, 0x80004000200010
; CV1-NEXT:    sbmm8 $r3 = $r0, 0x80004000200010
; CV1-NEXT:    ;;
; CV1-NEXT:    srld $r4 = $r2, 48
; CV1-NEXT:    srld $r5 = $r3, 48
; CV1-NEXT:    extfz $r6 = $r2, 47, 32
; CV1-NEXT:    extfz $r7 = $r3, 47, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r2 = $r2
; CV1-NEXT:    zxhd $r3 = $r3
; CV1-NEXT:    srlw $r8 = $r2, 16
; CV1-NEXT:    srlw $r9 = $r3, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r4 = $r4
; CV1-NEXT:    zxhd $r5 = $r5
; CV1-NEXT:    ;;
; CV1-NEXT:    minuw $r4 = $r5, $r4
; CV1-NEXT:    zxhd $r6 = $r6
; CV1-NEXT:    zxhd $r7 = $r7
; CV1-NEXT:    ;;
; CV1-NEXT:    minuw $r5 = $r7, $r6
; CV1-NEXT:    zxhd $r8 = $r8
; CV1-NEXT:    zxhd $r9 = $r9
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r2 = $r2
; CV1-NEXT:    zxhd $r3 = $r3
; CV1-NEXT:    minuw $r6 = $r9, $r8
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x8000400020001
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x8000400020001
; CV1-NEXT:    minuw $r2 = $r3, $r2
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r6, 15, 8
; CV1-NEXT:    srld $r3 = $r1, 48
; CV1-NEXT:    srld $r4 = $r0, 48
; CV1-NEXT:    insf $r5 = $r4, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    extfz $r6 = $r1, 47, 32
; CV1-NEXT:    extfz $r7 = $r0, 47, 32
; CV1-NEXT:    srlw $r8 = $r1, 16
; CV1-NEXT:    srlw $r9 = $r0, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    zxhd $r1 = $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r3 = $r3
; CV1-NEXT:    zxhd $r4 = $r4
; CV1-NEXT:    ;;
; CV1-NEXT:    minuw $r3 = $r4, $r3
; CV1-NEXT:    zxhd $r6 = $r6
; CV1-NEXT:    zxhd $r7 = $r7
; CV1-NEXT:    ;;
; CV1-NEXT:    minuw $r4 = $r7, $r6
; CV1-NEXT:    zxhd $r8 = $r8
; CV1-NEXT:    zxhd $r9 = $r9
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    zxhd $r1 = $r1
; CV1-NEXT:    minuw $r6 = $r9, $r8
; CV1-NEXT:    ;;
; CV1-NEXT:    minuw $r0 = $r0, $r1
; CV1-NEXT:    insf $r2 = $r5, 31, 16
; CV1-NEXT:    insf $r4 = $r3, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r6, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r4, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r2, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: minubo:
; CV2:       # %bb.0:
; CV2-NEXT:    minubo $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = tail call <8 x i8> @llvm.umin.v8i8(<8 x i8> %0, <8 x i8> %1)
  ret <8 x i8> %3
}

declare <8 x i8> @llvm.umin.v8i8(<8 x i8>, <8 x i8>)

define <2 x i8> @minubp(<2 x i8> %0, <2 x i8> %1) {
; CV1-LABEL: minubp:
; CV1:       # %bb.0:
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x20001
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x20001
; CV1-NEXT:    ;;
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x401
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: minubp:
; CV2:       # %bb.0:
; CV2-NEXT:    minubo $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = tail call <2 x i8> @llvm.umin.v2i8(<2 x i8> %0, <2 x i8> %1)
  ret <2 x i8> %3
}

declare <2 x i8> @llvm.umin.v2i8(<2 x i8>, <2 x i8>)

define <4 x i8> @minubq(<4 x i8> %0, <4 x i8> %1) {
; CV1-LABEL: minubq:
; CV1:       # %bb.0:
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x8000400020001
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x8000400020001
; CV1-NEXT:    ;;
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: minubq:
; CV2:       # %bb.0:
; CV2-NEXT:    minubo $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = tail call <4 x i8> @llvm.umin.v4i8(<4 x i8> %0, <4 x i8> %1)
  ret <4 x i8> %3
}

declare <4 x i8> @llvm.umin.v4i8(<4 x i8>, <4 x i8>)

define <32 x i8> @minubv(<32 x i8> %0, <32 x i8> %1) {
; CV1-LABEL: minubv:
; CV1:       # %bb.0:
; CV1-NEXT:    sbmm8 $r8 = $r4, 0x80004000200010
; CV1-NEXT:    sbmm8 $r9 = $r0, 0x80004000200010
; CV1-NEXT:    ;;
; CV1-NEXT:    srld $r10 = $r8, 48
; CV1-NEXT:    srld $r11 = $r9, 48
; CV1-NEXT:    extfz $r15 = $r8, 47, 32
; CV1-NEXT:    extfz $r16 = $r9, 47, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r8 = $r8
; CV1-NEXT:    zxhd $r9 = $r9
; CV1-NEXT:    srlw $r17 = $r8, 16
; CV1-NEXT:    srlw $r32 = $r9, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r10 = $r10
; CV1-NEXT:    zxhd $r11 = $r11
; CV1-NEXT:    ;;
; CV1-NEXT:    minuw $r10 = $r11, $r10
; CV1-NEXT:    zxhd $r15 = $r15
; CV1-NEXT:    zxhd $r16 = $r16
; CV1-NEXT:    ;;
; CV1-NEXT:    minuw $r11 = $r16, $r15
; CV1-NEXT:    zxhd $r17 = $r17
; CV1-NEXT:    zxhd $r32 = $r32
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r8 = $r8
; CV1-NEXT:    zxhd $r9 = $r9
; CV1-NEXT:    minuw $r15 = $r32, $r17
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x8000400020001
; CV1-NEXT:    sbmm8 $r4 = $r4, 0x8000400020001
; CV1-NEXT:    minuw $r8 = $r9, $r8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r8 = $r15, 15, 8
; CV1-NEXT:    srld $r9 = $r4, 48
; CV1-NEXT:    srld $r10 = $r0, 48
; CV1-NEXT:    insf $r11 = $r10, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    extfz $r15 = $r4, 47, 32
; CV1-NEXT:    extfz $r16 = $r0, 47, 32
; CV1-NEXT:    srlw $r17 = $r4, 16
; CV1-NEXT:    srlw $r32 = $r0, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    zxhd $r4 = $r4
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r9 = $r9
; CV1-NEXT:    zxhd $r10 = $r10
; CV1-NEXT:    ;;
; CV1-NEXT:    minuw $r9 = $r10, $r9
; CV1-NEXT:    zxhd $r15 = $r15
; CV1-NEXT:    zxhd $r16 = $r16
; CV1-NEXT:    ;;
; CV1-NEXT:    minuw $r10 = $r16, $r15
; CV1-NEXT:    zxhd $r17 = $r17
; CV1-NEXT:    zxhd $r32 = $r32
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    zxhd $r4 = $r4
; CV1-NEXT:    minuw $r15 = $r32, $r17
; CV1-NEXT:    ;;
; CV1-NEXT:    minuw $r0 = $r0, $r4
; CV1-NEXT:    insf $r8 = $r11, 31, 16
; CV1-NEXT:    insf $r10 = $r9, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r15, 15, 8
; CV1-NEXT:    sbmm8 $r4 = $r5, 0x80004000200010
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r10, 31, 16
; CV1-NEXT:    sbmm8 $r9 = $r1, 0x80004000200010
; CV1-NEXT:    srlw $r16 = $r4, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r8, 63, 32
; CV1-NEXT:    srld $r8 = $r4, 48
; CV1-NEXT:    srld $r10 = $r9, 48
; CV1-NEXT:    extfz $r11 = $r4, 47, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r4 = $r4
; CV1-NEXT:    extfz $r15 = $r9, 47, 32
; CV1-NEXT:    srlw $r17 = $r9, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r8 = $r8
; CV1-NEXT:    zxhd $r9 = $r9
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r10 = $r10
; CV1-NEXT:    zxhd $r11 = $r11
; CV1-NEXT:    ;;
; CV1-NEXT:    minuw $r8 = $r10, $r8
; CV1-NEXT:    zxhd $r15 = $r15
; CV1-NEXT:    zxhd $r16 = $r16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r4 = $r4
; CV1-NEXT:    minuw $r10 = $r15, $r11
; CV1-NEXT:    zxhd $r17 = $r17
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r5 = $r5, 0x8000400020001
; CV1-NEXT:    zxhd $r9 = $r9
; CV1-NEXT:    minuw $r11 = $r17, $r16
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x8000400020001
; CV1-NEXT:    minuw $r4 = $r9, $r4
; CV1-NEXT:    srld $r8 = $r5, 48
; CV1-NEXT:    insf $r10 = $r8, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r4 = $r11, 15, 8
; CV1-NEXT:    srld $r9 = $r1, 48
; CV1-NEXT:    extfz $r11 = $r5, 47, 32
; CV1-NEXT:    srlw $r16 = $r5, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r5 = $r5
; CV1-NEXT:    extfz $r15 = $r1, 47, 32
; CV1-NEXT:    srlw $r17 = $r1, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r1 = $r1
; CV1-NEXT:    zxhd $r8 = $r8
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r9 = $r9
; CV1-NEXT:    zxhd $r11 = $r11
; CV1-NEXT:    ;;
; CV1-NEXT:    minuw $r8 = $r9, $r8
; CV1-NEXT:    zxhd $r15 = $r15
; CV1-NEXT:    zxhd $r16 = $r16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r5 = $r5
; CV1-NEXT:    minuw $r9 = $r15, $r11
; CV1-NEXT:    zxhd $r17 = $r17
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r1 = $r1
; CV1-NEXT:    insf $r9 = $r8, 15, 8
; CV1-NEXT:    minuw $r11 = $r17, $r16
; CV1-NEXT:    ;;
; CV1-NEXT:    minuw $r1 = $r1, $r5
; CV1-NEXT:    sbmm8 $r5 = $r6, 0x80004000200010
; CV1-NEXT:    sbmm8 $r8 = $r2, 0x80004000200010
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r11, 15, 8
; CV1-NEXT:    insf $r4 = $r10, 31, 16
; CV1-NEXT:    srld $r10 = $r8, 48
; CV1-NEXT:    srlw $r16 = $r5, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r9, 31, 16
; CV1-NEXT:    srld $r9 = $r5, 48
; CV1-NEXT:    extfz $r11 = $r5, 47, 32
; CV1-NEXT:    srlw $r17 = $r8, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r5 = $r5
; CV1-NEXT:    extfz $r15 = $r8, 47, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r8 = $r8
; CV1-NEXT:    zxhd $r9 = $r9
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r10 = $r10
; CV1-NEXT:    zxhd $r11 = $r11
; CV1-NEXT:    ;;
; CV1-NEXT:    minuw $r9 = $r10, $r9
; CV1-NEXT:    zxhd $r15 = $r15
; CV1-NEXT:    zxhd $r16 = $r16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r5 = $r5
; CV1-NEXT:    minuw $r10 = $r15, $r11
; CV1-NEXT:    zxhd $r17 = $r17
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r8 = $r8
; CV1-NEXT:    insf $r10 = $r9, 15, 8
; CV1-NEXT:    minuw $r11 = $r17, $r16
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r2 = $r2, 0x8000400020001
; CV1-NEXT:    minuw $r5 = $r8, $r5
; CV1-NEXT:    sbmm8 $r6 = $r6, 0x8000400020001
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r4, 63, 32
; CV1-NEXT:    srld $r4 = $r6, 48
; CV1-NEXT:    insf $r5 = $r11, 15, 8
; CV1-NEXT:    srld $r8 = $r2, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r5 = $r10, 31, 16
; CV1-NEXT:    extfz $r9 = $r6, 47, 32
; CV1-NEXT:    srlw $r11 = $r6, 16
; CV1-NEXT:    srlw $r15 = $r2, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r4 = $r4
; CV1-NEXT:    extfz $r10 = $r2, 47, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r8 = $r8
; CV1-NEXT:    zxhd $r9 = $r9
; CV1-NEXT:    ;;
; CV1-NEXT:    minuw $r4 = $r8, $r4
; CV1-NEXT:    zxhd $r6 = $r6
; CV1-NEXT:    zxhd $r10 = $r10
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r2 = $r2
; CV1-NEXT:    minuw $r8 = $r10, $r9
; CV1-NEXT:    sbmm8 $r16 = $r7, 0x80004000200010
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r11 = $r11
; CV1-NEXT:    sbmm8 $r17 = $r3, 0x80004000200010
; CV1-NEXT:    srld $r32 = $r16, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r6 = $r6
; CV1-NEXT:    zxhd $r15 = $r15
; CV1-NEXT:    srld $r33 = $r17, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r2 = $r2
; CV1-NEXT:    extfz $r9 = $r16, 47, 32
; CV1-NEXT:    minuw $r11 = $r15, $r11
; CV1-NEXT:    ;;
; CV1-NEXT:    minuw $r2 = $r2, $r6
; CV1-NEXT:    extfz $r10 = $r17, 47, 32
; CV1-NEXT:    zxhd $r32 = $r32
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r6 = $r9
; CV1-NEXT:    zxhd $r33 = $r33
; CV1-NEXT:    ;;
; CV1-NEXT:    minuw $r4 = $r33, $r32
; CV1-NEXT:    insf $r8 = $r4, 15, 8
; CV1-NEXT:    zxhd $r9 = $r10
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r11, 15, 8
; CV1-NEXT:    minuw $r6 = $r9, $r6
; CV1-NEXT:    zxhd $r9 = $r16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r8, 31, 16
; CV1-NEXT:    srlw $r4 = $r16, 16
; CV1-NEXT:    insf $r6 = $r4, 15, 8
; CV1-NEXT:    srlw $r8 = $r17, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r7 = $r7, 0x8000400020001
; CV1-NEXT:    zxhd $r10 = $r17
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r3 = $r3, 0x8000400020001
; CV1-NEXT:    zxhd $r4 = $r4
; CV1-NEXT:    srld $r11 = $r7, 48
; CV1-NEXT:    srlw $r32 = $r7, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r8 = $r8
; CV1-NEXT:    zxhd $r9 = $r9
; CV1-NEXT:    srld $r15 = $r3, 48
; CV1-NEXT:    srlw $r33 = $r3, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    minuw $r4 = $r8, $r4
; CV1-NEXT:    zxhd $r10 = $r10
; CV1-NEXT:    extfz $r16 = $r7, 47, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r7 = $r7
; CV1-NEXT:    minuw $r8 = $r10, $r9
; CV1-NEXT:    extfz $r17 = $r3, 47, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r3 = $r3
; CV1-NEXT:    zxhd $r11 = $r11
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r15 = $r15
; CV1-NEXT:    zxhd $r16 = $r16
; CV1-NEXT:    ;;
; CV1-NEXT:    minuw $r9 = $r15, $r11
; CV1-NEXT:    zxhd $r17 = $r17
; CV1-NEXT:    zxhd $r32 = $r32
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r7 = $r7
; CV1-NEXT:    zxhd $r33 = $r33
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r3 = $r3
; CV1-NEXT:    minuw $r4 = $r17, $r16
; CV1-NEXT:    insf $r8 = $r4, 15, 8
; CV1-NEXT:    minuw $r10 = $r33, $r32
; CV1-NEXT:    ;;
; CV1-NEXT:    minuw $r3 = $r3, $r7
; CV1-NEXT:    insf $r4 = $r9, 15, 8
; CV1-NEXT:    insf $r8 = $r6, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r5, 63, 32
; CV1-NEXT:    insf $r3 = $r10, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r3 = $r4, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r3 = $r8, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: minubv:
; CV2:       # %bb.0:
; CV2-NEXT:    minubo $r0 = $r0, $r4
; CV2-NEXT:    minubo $r1 = $r1, $r5
; CV2-NEXT:    minubo $r2 = $r2, $r6
; CV2-NEXT:    minubo $r3 = $r3, $r7
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %4 = shufflevector <32 x i8> %1, <32 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5 = tail call <8 x i8> @llvm.umin.v8i8(<8 x i8> %3, <8 x i8> %4)
  %6 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %7 = shufflevector <32 x i8> %1, <32 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %8 = tail call <8 x i8> @llvm.umin.v8i8(<8 x i8> %6, <8 x i8> %7)
  %9 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %10 = shufflevector <32 x i8> %1, <32 x i8> undef, <8 x i32> <i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %11 = tail call <8 x i8> @llvm.umin.v8i8(<8 x i8> %9, <8 x i8> %10)
  %12 = shufflevector <32 x i8> %0, <32 x i8> undef, <8 x i32> <i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  %13 = shufflevector <32 x i8> %1, <32 x i8> undef, <8 x i32> <i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  %14 = tail call <8 x i8> @llvm.umin.v8i8(<8 x i8> %12, <8 x i8> %13)
  %15 = shufflevector <8 x i8> %5, <8 x i8> %8, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %16 = shufflevector <8 x i8> %11, <8 x i8> %14, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %17 = shufflevector <16 x i8> %15, <16 x i8> %16, <32 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  ret <32 x i8> %17
}

define <16 x i8> @minubx(<16 x i8> %0, <16 x i8> %1) {
; CV1-LABEL: minubx:
; CV1:       # %bb.0:
; CV1-NEXT:    sbmm8 $r4 = $r2, 0x80004000200010
; CV1-NEXT:    sbmm8 $r5 = $r0, 0x80004000200010
; CV1-NEXT:    ;;
; CV1-NEXT:    srld $r6 = $r4, 48
; CV1-NEXT:    srld $r7 = $r5, 48
; CV1-NEXT:    extfz $r8 = $r4, 47, 32
; CV1-NEXT:    extfz $r9 = $r5, 47, 32
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r4 = $r4
; CV1-NEXT:    zxhd $r5 = $r5
; CV1-NEXT:    srlw $r10 = $r4, 16
; CV1-NEXT:    srlw $r11 = $r5, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r6 = $r6
; CV1-NEXT:    zxhd $r7 = $r7
; CV1-NEXT:    ;;
; CV1-NEXT:    minuw $r6 = $r7, $r6
; CV1-NEXT:    zxhd $r8 = $r8
; CV1-NEXT:    zxhd $r9 = $r9
; CV1-NEXT:    ;;
; CV1-NEXT:    minuw $r7 = $r9, $r8
; CV1-NEXT:    zxhd $r10 = $r10
; CV1-NEXT:    zxhd $r11 = $r11
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r4 = $r4
; CV1-NEXT:    zxhd $r5 = $r5
; CV1-NEXT:    minuw $r8 = $r11, $r10
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x8000400020001
; CV1-NEXT:    sbmm8 $r2 = $r2, 0x8000400020001
; CV1-NEXT:    minuw $r4 = $r5, $r4
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r4 = $r8, 15, 8
; CV1-NEXT:    srld $r5 = $r2, 48
; CV1-NEXT:    srld $r6 = $r0, 48
; CV1-NEXT:    insf $r7 = $r6, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    extfz $r8 = $r2, 47, 32
; CV1-NEXT:    extfz $r9 = $r0, 47, 32
; CV1-NEXT:    srlw $r10 = $r2, 16
; CV1-NEXT:    srlw $r11 = $r0, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    zxhd $r2 = $r2
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r5 = $r5
; CV1-NEXT:    zxhd $r6 = $r6
; CV1-NEXT:    ;;
; CV1-NEXT:    minuw $r5 = $r6, $r5
; CV1-NEXT:    zxhd $r8 = $r8
; CV1-NEXT:    zxhd $r9 = $r9
; CV1-NEXT:    ;;
; CV1-NEXT:    minuw $r6 = $r9, $r8
; CV1-NEXT:    zxhd $r10 = $r10
; CV1-NEXT:    zxhd $r11 = $r11
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    zxhd $r2 = $r2
; CV1-NEXT:    minuw $r8 = $r11, $r10
; CV1-NEXT:    ;;
; CV1-NEXT:    minuw $r0 = $r0, $r2
; CV1-NEXT:    sbmm8 $r2 = $r3, 0x80004000200010
; CV1-NEXT:    insf $r6 = $r5, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r8, 15, 8
; CV1-NEXT:    sbmm8 $r5 = $r1, 0x80004000200010
; CV1-NEXT:    srlw $r10 = $r2, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r6, 31, 16
; CV1-NEXT:    insf $r4 = $r7, 31, 16
; CV1-NEXT:    srld $r6 = $r2, 48
; CV1-NEXT:    srld $r7 = $r5, 48
; CV1-NEXT:    ;;
; CV1-NEXT:    extfz $r8 = $r2, 47, 32
; CV1-NEXT:    extfz $r9 = $r5, 47, 32
; CV1-NEXT:    srlw $r11 = $r5, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r2 = $r2
; CV1-NEXT:    zxhd $r5 = $r5
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r6 = $r6
; CV1-NEXT:    zxhd $r7 = $r7
; CV1-NEXT:    ;;
; CV1-NEXT:    minuw $r6 = $r7, $r6
; CV1-NEXT:    zxhd $r8 = $r8
; CV1-NEXT:    zxhd $r9 = $r9
; CV1-NEXT:    ;;
; CV1-NEXT:    minuw $r7 = $r9, $r8
; CV1-NEXT:    zxhd $r10 = $r10
; CV1-NEXT:    zxhd $r11 = $r11
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r2 = $r2
; CV1-NEXT:    zxhd $r5 = $r5
; CV1-NEXT:    minuw $r8 = $r11, $r10
; CV1-NEXT:    ;;
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x8000400020001
; CV1-NEXT:    minuw $r2 = $r5, $r2
; CV1-NEXT:    sbmm8 $r3 = $r3, 0x8000400020001
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r2 = $r8, 15, 8
; CV1-NEXT:    srld $r5 = $r3, 48
; CV1-NEXT:    srld $r6 = $r1, 48
; CV1-NEXT:    insf $r7 = $r6, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    extfz $r8 = $r3, 47, 32
; CV1-NEXT:    extfz $r9 = $r1, 47, 32
; CV1-NEXT:    srlw $r10 = $r3, 16
; CV1-NEXT:    srlw $r11 = $r1, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r1 = $r1
; CV1-NEXT:    zxhd $r3 = $r3
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r5 = $r5
; CV1-NEXT:    zxhd $r6 = $r6
; CV1-NEXT:    ;;
; CV1-NEXT:    minuw $r5 = $r6, $r5
; CV1-NEXT:    zxhd $r8 = $r8
; CV1-NEXT:    zxhd $r9 = $r9
; CV1-NEXT:    ;;
; CV1-NEXT:    minuw $r6 = $r9, $r8
; CV1-NEXT:    zxhd $r10 = $r10
; CV1-NEXT:    zxhd $r11 = $r11
; CV1-NEXT:    ;;
; CV1-NEXT:    zxhd $r1 = $r1
; CV1-NEXT:    zxhd $r3 = $r3
; CV1-NEXT:    minuw $r8 = $r11, $r10
; CV1-NEXT:    ;;
; CV1-NEXT:    minuw $r1 = $r1, $r3
; CV1-NEXT:    insf $r2 = $r7, 31, 16
; CV1-NEXT:    insf $r6 = $r5, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r0 = $r4, 63, 32
; CV1-NEXT:    insf $r1 = $r8, 15, 8
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r6, 31, 16
; CV1-NEXT:    ;;
; CV1-NEXT:    insf $r1 = $r2, 63, 32
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: minubx:
; CV2:       # %bb.0:
; CV2-NEXT:    minubo $r0 = $r0, $r2
; CV2-NEXT:    minubo $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %3 = shufflevector <16 x i8> %0, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %4 = shufflevector <16 x i8> %1, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %5 = tail call <8 x i8> @llvm.umin.v8i8(<8 x i8> %3, <8 x i8> %4)
  %6 = shufflevector <16 x i8> %0, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %7 = shufflevector <16 x i8> %1, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %8 = tail call <8 x i8> @llvm.umin.v8i8(<8 x i8> %6, <8 x i8> %7)
  %9 = shufflevector <8 x i8> %5, <8 x i8> %8, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  ret <16 x i8> %9
}

define i64 @minud(i64 %0, i64 %1) {
; CHECK-LABEL: minud:
; CHECK:       # %bb.0:
; CHECK-NEXT:    minud $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = tail call i64 @llvm.umin.i64(i64 %0, i64 %1)
  ret i64 %3
}

declare i64 @llvm.umin.i64(i64, i64)

define <2 x i64> @minudp(<2 x i64> %0, <2 x i64> %1) {
; CHECK-LABEL: minudp:
; CHECK:       # %bb.0:
; CHECK-NEXT:    minud $r0 = $r0, $r2
; CHECK-NEXT:    minud $r1 = $r1, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = extractelement <2 x i64> %0, i64 0
  %4 = extractelement <2 x i64> %1, i64 0
  %5 = tail call i64 @llvm.umin.i64(i64 %3, i64 %4)
  %6 = extractelement <2 x i64> %0, i64 1
  %7 = extractelement <2 x i64> %1, i64 1
  %8 = tail call i64 @llvm.umin.i64(i64 %6, i64 %7)
  %9 = insertelement <2 x i64> undef, i64 %5, i32 0
  %10 = insertelement <2 x i64> %9, i64 %8, i32 1
  ret <2 x i64> %10
}

define <4 x i64> @minudq(<4 x i64> %0, <4 x i64> %1) {
; CHECK-LABEL: minudq:
; CHECK:       # %bb.0:
; CHECK-NEXT:    minud $r0 = $r0, $r4
; CHECK-NEXT:    minud $r1 = $r1, $r5
; CHECK-NEXT:    minud $r2 = $r2, $r6
; CHECK-NEXT:    minud $r3 = $r3, $r7
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = extractelement <4 x i64> %0, i64 0
  %4 = extractelement <4 x i64> %1, i64 0
  %5 = tail call i64 @llvm.umin.i64(i64 %3, i64 %4)
  %6 = extractelement <4 x i64> %0, i64 1
  %7 = extractelement <4 x i64> %1, i64 1
  %8 = tail call i64 @llvm.umin.i64(i64 %6, i64 %7)
  %9 = extractelement <4 x i64> %0, i64 2
  %10 = extractelement <4 x i64> %1, i64 2
  %11 = tail call i64 @llvm.umin.i64(i64 %9, i64 %10)
  %12 = extractelement <4 x i64> %0, i64 3
  %13 = extractelement <4 x i64> %1, i64 3
  %14 = tail call i64 @llvm.umin.i64(i64 %12, i64 %13)
  %15 = insertelement <4 x i64> undef, i64 %5, i32 0
  %16 = insertelement <4 x i64> %15, i64 %8, i32 1
  %17 = insertelement <4 x i64> %16, i64 %11, i32 2
  %18 = insertelement <4 x i64> %17, i64 %14, i32 3
  ret <4 x i64> %18
}

define <8 x i16> @minuho(<8 x i16> %0, <8 x i16> %1) {
; CHECK-LABEL: minuho:
; CHECK:       # %bb.0:
; CHECK-NEXT:    minuhq $r0 = $r0, $r2
; CHECK-NEXT:    minuhq $r1 = $r1, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4 = shufflevector <8 x i16> %1, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5 = tail call <4 x i16> @llvm.umin.v4i16(<4 x i16> %3, <4 x i16> %4)
  %6 = shufflevector <8 x i16> %0, <8 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7 = shufflevector <8 x i16> %1, <8 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8 = tail call <4 x i16> @llvm.umin.v4i16(<4 x i16> %6, <4 x i16> %7)
  %9 = shufflevector <4 x i16> %5, <4 x i16> %8, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i16> %9
}

declare <4 x i16> @llvm.umin.v4i16(<4 x i16>, <4 x i16>)

define <2 x i16> @minuhp(<2 x i16> %0, <2 x i16> %1) {
; CHECK-LABEL: minuhp:
; CHECK:       # %bb.0:
; CHECK-NEXT:    minuhq $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = tail call <2 x i16> @llvm.umin.v2i16(<2 x i16> %0, <2 x i16> %1)
  ret <2 x i16> %3
}

declare <2 x i16> @llvm.umin.v2i16(<2 x i16>, <2 x i16>)

define <4 x i16> @minuhq(<4 x i16> %0, <4 x i16> %1) {
; CHECK-LABEL: minuhq:
; CHECK:       # %bb.0:
; CHECK-NEXT:    minuhq $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = tail call <4 x i16> @llvm.umin.v4i16(<4 x i16> %0, <4 x i16> %1)
  ret <4 x i16> %3
}

define <16 x i16> @minuhx(<16 x i16> %0, <16 x i16> %1) {
; CHECK-LABEL: minuhx:
; CHECK:       # %bb.0:
; CHECK-NEXT:    minuhq $r0 = $r0, $r4
; CHECK-NEXT:    minuhq $r1 = $r1, $r5
; CHECK-NEXT:    minuhq $r2 = $r2, $r6
; CHECK-NEXT:    minuhq $r3 = $r3, $r7
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %4 = shufflevector <16 x i16> %1, <16 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5 = tail call <4 x i16> @llvm.umin.v4i16(<4 x i16> %3, <4 x i16> %4)
  %6 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %7 = shufflevector <16 x i16> %1, <16 x i16> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8 = tail call <4 x i16> @llvm.umin.v4i16(<4 x i16> %6, <4 x i16> %7)
  %9 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %10 = shufflevector <16 x i16> %1, <16 x i16> undef, <4 x i32> <i32 8, i32 9, i32 10, i32 11>
  %11 = tail call <4 x i16> @llvm.umin.v4i16(<4 x i16> %9, <4 x i16> %10)
  %12 = shufflevector <16 x i16> %0, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %13 = shufflevector <16 x i16> %1, <16 x i16> undef, <4 x i32> <i32 12, i32 13, i32 14, i32 15>
  %14 = tail call <4 x i16> @llvm.umin.v4i16(<4 x i16> %12, <4 x i16> %13)
  %15 = shufflevector <4 x i16> %5, <4 x i16> %8, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %16 = shufflevector <4 x i16> %11, <4 x i16> %14, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %17 = shufflevector <8 x i16> %15, <8 x i16> %16, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  ret <16 x i16> %17
}

define i32 @minuw(i32 %0, i32 %1) {
; CHECK-LABEL: minuw:
; CHECK:       # %bb.0:
; CHECK-NEXT:    minuw $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = tail call i32 @llvm.umin.i32(i32 %0, i32 %1)
  ret i32 %3
}

declare i32 @llvm.umin.i32(i32, i32)

define <8 x i32> @minuwo(<8 x i32> %0, <8 x i32> %1) {
; CHECK-LABEL: minuwo:
; CHECK:       # %bb.0:
; CHECK-NEXT:    minuwp $r0 = $r0, $r4
; CHECK-NEXT:    minuwp $r1 = $r1, $r5
; CHECK-NEXT:    minuwp $r2 = $r2, $r6
; CHECK-NEXT:    minuwp $r3 = $r3, $r7
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 0, i32 1>
  %4 = shufflevector <8 x i32> %1, <8 x i32> undef, <2 x i32> <i32 0, i32 1>
  %5 = tail call <2 x i32> @llvm.umin.v2i32(<2 x i32> %3, <2 x i32> %4)
  %6 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 2, i32 3>
  %7 = shufflevector <8 x i32> %1, <8 x i32> undef, <2 x i32> <i32 2, i32 3>
  %8 = tail call <2 x i32> @llvm.umin.v2i32(<2 x i32> %6, <2 x i32> %7)
  %9 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 4, i32 5>
  %10 = shufflevector <8 x i32> %1, <8 x i32> undef, <2 x i32> <i32 4, i32 5>
  %11 = tail call <2 x i32> @llvm.umin.v2i32(<2 x i32> %9, <2 x i32> %10)
  %12 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 6, i32 7>
  %13 = shufflevector <8 x i32> %1, <8 x i32> undef, <2 x i32> <i32 6, i32 7>
  %14 = tail call <2 x i32> @llvm.umin.v2i32(<2 x i32> %12, <2 x i32> %13)
  %15 = shufflevector <2 x i32> %5, <2 x i32> %8, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %16 = shufflevector <2 x i32> %11, <2 x i32> %14, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %17 = shufflevector <4 x i32> %15, <4 x i32> %16, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i32> %17
}

declare <2 x i32> @llvm.umin.v2i32(<2 x i32>, <2 x i32>)

define <2 x i32> @minuwp(<2 x i32> %0, <2 x i32> %1) {
; CHECK-LABEL: minuwp:
; CHECK:       # %bb.0:
; CHECK-NEXT:    minuwp $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = tail call <2 x i32> @llvm.umin.v2i32(<2 x i32> %0, <2 x i32> %1)
  ret <2 x i32> %3
}

define <4 x i32> @minuwq(<4 x i32> %0, <4 x i32> %1) {
; CHECK-LABEL: minuwq:
; CHECK:       # %bb.0:
; CHECK-NEXT:    minuwp $r0 = $r0, $r2
; CHECK-NEXT:    minuwp $r1 = $r1, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 0, i32 1>
  %4 = shufflevector <4 x i32> %1, <4 x i32> undef, <2 x i32> <i32 0, i32 1>
  %5 = tail call <2 x i32> @llvm.umin.v2i32(<2 x i32> %3, <2 x i32> %4)
  %6 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 2, i32 3>
  %7 = shufflevector <4 x i32> %1, <4 x i32> undef, <2 x i32> <i32 2, i32 3>
  %8 = tail call <2 x i32> @llvm.umin.v2i32(<2 x i32> %6, <2 x i32> %7)
  %9 = shufflevector <2 x i32> %5, <2 x i32> %8, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  ret <4 x i32> %9
}

define i32 @minw(i32 %0, i32 %1) {
; CHECK-LABEL: minw:
; CHECK:       # %bb.0:
; CHECK-NEXT:    minw $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = tail call i32 @llvm.smin.i32(i32 %0, i32 %1)
  ret i32 %3
}

declare i32 @llvm.smin.i32(i32, i32)

define <8 x i32> @minwo(<8 x i32> %0, <8 x i32> %1) {
; CHECK-LABEL: minwo:
; CHECK:       # %bb.0:
; CHECK-NEXT:    minwp $r0 = $r0, $r4
; CHECK-NEXT:    minwp $r1 = $r1, $r5
; CHECK-NEXT:    minwp $r2 = $r2, $r6
; CHECK-NEXT:    minwp $r3 = $r3, $r7
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 0, i32 1>
  %4 = shufflevector <8 x i32> %1, <8 x i32> undef, <2 x i32> <i32 0, i32 1>
  %5 = tail call <2 x i32> @llvm.smin.v2i32(<2 x i32> %3, <2 x i32> %4)
  %6 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 2, i32 3>
  %7 = shufflevector <8 x i32> %1, <8 x i32> undef, <2 x i32> <i32 2, i32 3>
  %8 = tail call <2 x i32> @llvm.smin.v2i32(<2 x i32> %6, <2 x i32> %7)
  %9 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 4, i32 5>
  %10 = shufflevector <8 x i32> %1, <8 x i32> undef, <2 x i32> <i32 4, i32 5>
  %11 = tail call <2 x i32> @llvm.smin.v2i32(<2 x i32> %9, <2 x i32> %10)
  %12 = shufflevector <8 x i32> %0, <8 x i32> undef, <2 x i32> <i32 6, i32 7>
  %13 = shufflevector <8 x i32> %1, <8 x i32> undef, <2 x i32> <i32 6, i32 7>
  %14 = tail call <2 x i32> @llvm.smin.v2i32(<2 x i32> %12, <2 x i32> %13)
  %15 = shufflevector <2 x i32> %5, <2 x i32> %8, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %16 = shufflevector <2 x i32> %11, <2 x i32> %14, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %17 = shufflevector <4 x i32> %15, <4 x i32> %16, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  ret <8 x i32> %17
}

declare <2 x i32> @llvm.smin.v2i32(<2 x i32>, <2 x i32>)

define <2 x i32> @minwp(<2 x i32> %0, <2 x i32> %1) {
; CHECK-LABEL: minwp:
; CHECK:       # %bb.0:
; CHECK-NEXT:    minwp $r0 = $r0, $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = tail call <2 x i32> @llvm.smin.v2i32(<2 x i32> %0, <2 x i32> %1)
  ret <2 x i32> %3
}

define <4 x i32> @minwq(<4 x i32> %0, <4 x i32> %1) {
; CHECK-LABEL: minwq:
; CHECK:       # %bb.0:
; CHECK-NEXT:    minwp $r0 = $r0, $r2
; CHECK-NEXT:    minwp $r1 = $r1, $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 0, i32 1>
  %4 = shufflevector <4 x i32> %1, <4 x i32> undef, <2 x i32> <i32 0, i32 1>
  %5 = tail call <2 x i32> @llvm.smin.v2i32(<2 x i32> %3, <2 x i32> %4)
  %6 = shufflevector <4 x i32> %0, <4 x i32> undef, <2 x i32> <i32 2, i32 3>
  %7 = shufflevector <4 x i32> %1, <4 x i32> undef, <2 x i32> <i32 2, i32 3>
  %8 = tail call <2 x i32> @llvm.smin.v2i32(<2 x i32> %6, <2 x i32> %7)
  %9 = shufflevector <2 x i32> %5, <2 x i32> %8, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  ret <4 x i32> %9
}

