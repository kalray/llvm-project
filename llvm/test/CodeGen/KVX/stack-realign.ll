; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -O2 -o - %s | FileCheck %s
target triple = "kvx-kalray-cos"

@g1 = common global i32 0, align 4
@g2 = common global i32 0, align 4
@g3 = common global i32 0, align 4
@g4 = common global i32 0, align 4
@g5 = common global i32 0, align 4
@g6 = common global i32 0, align 4
@g7 = common global i32 0, align 4
@g8 = common global i32 0, align 4
@g9 = common global i32 0, align 4
@g10 = common global i32 0, align 4
@g11 = common global i32 0, align 4
@g12 = common global i32 0, align 4
@g13 = common global i32 0, align 4
@g14 = common global i32 0, align 4
@g15 = common global i32 0, align 4
@g16 = common global i32 0, align 4
@g17 = common global i32 0, align 4
@g18 = common global i32 0, align 4
@g19 = common global i32 0, align 4
@g20 = common global i32 0, align 4
@g21 = common global i32 0, align 4
@g22 = common global i32 0, align 4
@g23 = common global i32 0, align 4
@g24 = common global i32 0, align 4
@g25 = common global i32 0, align 4
@g26 = common global i32 0, align 4
@g27 = common global i32 0, align 4
@g28 = common global i32 0, align 4
@g29 = common global i32 0, align 4
@g30 = common global i32 0, align 4
@g31 = common global i32 0, align 4
@g32 = common global i32 0, align 4
@g33 = common global i32 0, align 4
@g34 = common global i32 0, align 4
@g35 = common global i32 0, align 4
@g36 = common global i32 0, align 4
@g37 = common global i32 0, align 4
@g38 = common global i32 0, align 4
@g39 = common global i32 0, align 4
@g40 = common global i32 0, align 4
@g41 = common global i32 0, align 4
@g42 = common global i32 0, align 4
@g43 = common global i32 0, align 4
@g44 = common global i32 0, align 4
@g45 = common global i32 0, align 4
@g46 = common global i32 0, align 4
@g47 = common global i32 0, align 4
@g48 = common global i32 0, align 4
@g49 = common global i32 0, align 4
@g50 = common global i32 0, align 4
@g51 = common global i32 0, align 4
@g52 = common global i32 0, align 4
@g53 = common global i32 0, align 4
@g54 = common global i32 0, align 4
@g55 = common global i32 0, align 4
@g56 = common global i32 0, align 4
@g57 = common global i32 0, align 4
@g58 = common global i32 0, align 4
@g59 = common global i32 0, align 4
@g60 = common global i32 0, align 4
@g61 = common global i32 0, align 4
@g62 = common global i32 0, align 4
@g63 = common global i32 0, align 4
@g64 = common global i32 0, align 4
@g65 = common global i32 0, align 4

define i32 @stackrealign1(){
; CHECK-LABEL: stackrealign1:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -256
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 256
; CHECK-NEXT:    .cfi_register 67, 16
; CHECK-NEXT:    sd 248[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -8
; CHECK-NEXT:    sd 240[$r12] = $r14
; CHECK-NEXT:    addd $r14 = $r12, 240
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 14, -16
; CHECK-NEXT:    .cfi_def_cfa 14, 16
; CHECK-NEXT:    sd 232[$r12] = $r31
; CHECK-NEXT:    andd $r31 = $r12, -128
; CHECK-NEXT:    make $r0 = 7
; CHECK-NEXT:    make $r1 = 0x4d2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 31, -24
; CHECK-NEXT:    sw 228[$r31] = $r0
; CHECK-NEXT:    addd $r0 = $r31, 228
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 128[$r31] = $r1
; CHECK-NEXT:    addd $r1 = $r31, 128
; CHECK-NEXT:    call other
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r12 = $r14, -240
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r31 = 232[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r14 = 240[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 248[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 256
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %c = alloca i32, align 4
  %i = alloca i32, align 128
  %0 = bitcast i32* %c to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #3
  store i32 7, i32* %c, align 4
  %1 = bitcast i32* %i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #3
  store i32 1234, i32* %i, align 128
  %call = call i32 @other(i32* nonnull %c, i32* nonnull %i) #3
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #3
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #3
  ret i32 %call
}

declare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #1

declare i32 @other(i32*, i32*) #2

declare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #1

define i32 @stackrealign2(i32 %n){
; CHECK-LABEL: stackrealign2:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -256
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 256
; CHECK-NEXT:    .cfi_register 67, 16
; CHECK-NEXT:    sd 248[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -8
; CHECK-NEXT:    sd 240[$r12] = $r14
; CHECK-NEXT:    addd $r14 = $r12, 240
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 14, -16
; CHECK-NEXT:    .cfi_def_cfa 14, 16
; CHECK-NEXT:    sd 232[$r12] = $r31
; CHECK-NEXT:    copyd $r1 = $r0
; CHECK-NEXT:    andd $r31 = $r12, -128
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 31, -24
; CHECK-NEXT:    sxwd $r0 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addx4d $r0 = $r0, 31
; CHECK-NEXT:    ;;
; CHECK-NEXT:    andd $r0 = $r0, -32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfd $r0 = $r0, $r12
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r12 = $r0
; CHECK-NEXT:    cb.wlez $r1 ? .LBB1_12
; CHECK-NEXT:    ;;
; CHECK-NEXT:  # %bb.1: # %for.body.preheader
; CHECK-NEXT:    zxwd $r4 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r2 = $r4, -1
; CHECK-NEXT:    andd $r3 = $r4, 7
; CHECK-NEXT:    ;;
; CHECK-NEXT:    compd.ltu $r5 = $r2, 7
; CHECK-NEXT:    make $r2 = 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    cb.odd $r5 ? .LBB1_4
; CHECK-NEXT:    ;;
; CHECK-NEXT:  # %bb.2: # %for.body.preheader.new
; CHECK-NEXT:    sbfd $r4 = $r3, $r4
; CHECK-NEXT:    addw $r5 = $r1, -7
; CHECK-NEXT:    make $r2 = 0
; CHECK-NEXT:    copyd $r6 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:  .LBB1_3: # %for.body
; CHECK-NEXT:    # =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    addw $r7 = $r5, 7
; CHECK-NEXT:    addw $r8 = $r5, 6
; CHECK-NEXT:    addd $r2 = $r2, 8
; CHECK-NEXT:    addw $r9 = $r5, 3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 0[$r6] = $r7
; CHECK-NEXT:    addw $r7 = $r5, 5
; CHECK-NEXT:    addw $r10 = $r5, 2
; CHECK-NEXT:    addw $r11 = $r5, 1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 4[$r6] = $r8
; CHECK-NEXT:    addw $r8 = $r5, 4
; CHECK-NEXT:    compd.eq $r15 = $r4, $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 8[$r6] = $r7
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 12[$r6] = $r8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 16[$r6] = $r9
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 20[$r6] = $r10
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 24[$r6] = $r11
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 28[$r6] = $r5
; CHECK-NEXT:    addw $r5 = $r5, -8
; CHECK-NEXT:    addd $r6 = $r6, 32
; CHECK-NEXT:    cb.even $r15 ? .LBB1_3
; CHECK-NEXT:    ;;
; CHECK-NEXT:  .LBB1_4: # %for.cond.cleanup.loopexit.unr-lcssa
; CHECK-NEXT:    cb.deqz $r3 ? .LBB1_12
; CHECK-NEXT:    ;;
; CHECK-NEXT:  # %bb.5: # %for.body.epil
; CHECK-NEXT:    zxwd $r4 = $r2
; CHECK-NEXT:    compd.ne $r5 = $r3, 1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfw $r4 = $r4, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw.xs $r2[$r0] = $r4
; CHECK-NEXT:    cb.even $r5 ? .LBB1_12
; CHECK-NEXT:    ;;
; CHECK-NEXT:  # %bb.6: # %for.body.epil.1
; CHECK-NEXT:    addd $r4 = $r2, 1
; CHECK-NEXT:    compd.eq $r6 = $r3, 2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r5 = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfw $r5 = $r5, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw.xs $r4[$r0] = $r5
; CHECK-NEXT:    cb.odd $r6 ? .LBB1_12
; CHECK-NEXT:    ;;
; CHECK-NEXT:  # %bb.7: # %for.body.epil.2
; CHECK-NEXT:    addd $r4 = $r2, 2
; CHECK-NEXT:    compd.eq $r6 = $r3, 3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r5 = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfw $r5 = $r5, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw.xs $r4[$r0] = $r5
; CHECK-NEXT:    cb.odd $r6 ? .LBB1_12
; CHECK-NEXT:    ;;
; CHECK-NEXT:  # %bb.8: # %for.body.epil.3
; CHECK-NEXT:    addd $r4 = $r2, 3
; CHECK-NEXT:    compd.eq $r6 = $r3, 4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r5 = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfw $r5 = $r5, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw.xs $r4[$r0] = $r5
; CHECK-NEXT:    cb.odd $r6 ? .LBB1_12
; CHECK-NEXT:    ;;
; CHECK-NEXT:  # %bb.9: # %for.body.epil.4
; CHECK-NEXT:    addd $r4 = $r2, 4
; CHECK-NEXT:    compd.eq $r6 = $r3, 5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r5 = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfw $r5 = $r5, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw.xs $r4[$r0] = $r5
; CHECK-NEXT:    cb.odd $r6 ? .LBB1_12
; CHECK-NEXT:    ;;
; CHECK-NEXT:  # %bb.10: # %for.body.epil.5
; CHECK-NEXT:    addd $r4 = $r2, 5
; CHECK-NEXT:    compd.eq $r3 = $r3, 6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r5 = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfw $r5 = $r5, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw.xs $r4[$r0] = $r5
; CHECK-NEXT:    cb.odd $r3 ? .LBB1_12
; CHECK-NEXT:    ;;
; CHECK-NEXT:  # %bb.11: # %for.body.epil.6
; CHECK-NEXT:    addd $r2 = $r2, 6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r3 = $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfw $r3 = $r3, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw.xs $r2[$r0] = $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:  .LBB1_12: # %for.cond.cleanup
; CHECK-NEXT:    make $r2 = 0x4d2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 128[$r31] = $r2
; CHECK-NEXT:    addd $r2 = $r31, 128
; CHECK-NEXT:    call other2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r12 = $r14, -240
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r31 = 232[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r14 = 240[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 248[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 256
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %i = alloca i32, align 128
  %conv = sext i32 %n to i64
  %0 = alloca i32, i64 %conv, align 8
  %cmp11 = icmp sgt i32 %n, 0
  br i1 %cmp11, label %for.body.preheader, label %for.cond.cleanup

for.body.preheader:                               ; preds = %entry
  %wide.trip.count = zext i32 %n to i64
  %1 = add nsw i64 %wide.trip.count, -1
  %xtraiter = and i64 %wide.trip.count, 7
  %2 = icmp ult i64 %1, 7
  br i1 %2, label %for.cond.cleanup.loopexit.unr-lcssa, label %for.body.preheader.new

for.body.preheader.new:                           ; preds = %for.body.preheader
  %unroll_iter = sub nsw i64 %wide.trip.count, %xtraiter
  br label %for.body

for.cond.cleanup.loopexit.unr-lcssa:              ; preds = %for.body, %for.body.preheader
  %indvars.iv.unr = phi i64 [ 0, %for.body.preheader ], [ %indvars.iv.next.7, %for.body ]
  %lcmp.mod = icmp eq i64 %xtraiter, 0
  br i1 %lcmp.mod, label %for.cond.cleanup, label %for.body.epil

for.body.epil:                                    ; preds = %for.cond.cleanup.loopexit.unr-lcssa
  %3 = trunc i64 %indvars.iv.unr to i32
  %sub.epil = sub nsw i32 %n, %3
  %arrayidx.epil = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.unr
  store i32 %sub.epil, i32* %arrayidx.epil, align 4
  %indvars.iv.next.epil = add nuw nsw i64 %indvars.iv.unr, 1
  %epil.iter.cmp = icmp eq i64 %xtraiter, 1
  br i1 %epil.iter.cmp, label %for.cond.cleanup, label %for.body.epil.1

for.cond.cleanup:                                 ; preds = %for.cond.cleanup.loopexit.unr-lcssa, %for.body.epil.5, %for.body.epil.4, %for.body.epil.3, %for.body.epil.2, %for.body.epil.1, %for.body.epil, %for.body.epil.6, %entry
  %4 = bitcast i32* %i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %4) #3
  store i32 1234, i32* %i, align 128
  %call = call i32 @other2(i32* nonnull %0, i32 %n, i32* nonnull %i) #3
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %4) #3
  ret i32 %call

for.body:                                         ; preds = %for.body, %for.body.preheader.new
  %indvars.iv = phi i64 [ 0, %for.body.preheader.new ], [ %indvars.iv.next.7, %for.body ]
  %niter = phi i64 [ %unroll_iter, %for.body.preheader.new ], [ %niter.nsub.7, %for.body ]
  %5 = trunc i64 %indvars.iv to i32
  %sub = sub nsw i32 %n, %5
  %arrayidx = getelementptr inbounds i32, i32* %0, i64 %indvars.iv
  store i32 %sub, i32* %arrayidx, align 8
  %indvars.iv.next = or i64 %indvars.iv, 1
  %6 = trunc i64 %indvars.iv.next to i32
  %sub.1 = sub nsw i32 %n, %6
  %arrayidx.1 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next
  store i32 %sub.1, i32* %arrayidx.1, align 4
  %indvars.iv.next.1 = or i64 %indvars.iv, 2
  %7 = trunc i64 %indvars.iv.next.1 to i32
  %sub.2 = sub nsw i32 %n, %7
  %arrayidx.2 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.1
  store i32 %sub.2, i32* %arrayidx.2, align 8
  %indvars.iv.next.2 = or i64 %indvars.iv, 3
  %8 = trunc i64 %indvars.iv.next.2 to i32
  %sub.3 = sub nsw i32 %n, %8
  %arrayidx.3 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.2
  store i32 %sub.3, i32* %arrayidx.3, align 4
  %indvars.iv.next.3 = or i64 %indvars.iv, 4
  %9 = trunc i64 %indvars.iv.next.3 to i32
  %sub.4 = sub nsw i32 %n, %9
  %arrayidx.4 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.3
  store i32 %sub.4, i32* %arrayidx.4, align 8
  %indvars.iv.next.4 = or i64 %indvars.iv, 5
  %10 = trunc i64 %indvars.iv.next.4 to i32
  %sub.5 = sub nsw i32 %n, %10
  %arrayidx.5 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.4
  store i32 %sub.5, i32* %arrayidx.5, align 4
  %indvars.iv.next.5 = or i64 %indvars.iv, 6
  %11 = trunc i64 %indvars.iv.next.5 to i32
  %sub.6 = sub nsw i32 %n, %11
  %arrayidx.6 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.5
  store i32 %sub.6, i32* %arrayidx.6, align 8
  %indvars.iv.next.6 = or i64 %indvars.iv, 7
  %12 = trunc i64 %indvars.iv.next.6 to i32
  %sub.7 = sub nsw i32 %n, %12
  %arrayidx.7 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.6
  store i32 %sub.7, i32* %arrayidx.7, align 4
  %indvars.iv.next.7 = add nuw nsw i64 %indvars.iv, 8
  %niter.nsub.7 = add i64 %niter, -8
  %niter.ncmp.7 = icmp eq i64 %niter.nsub.7, 0
  br i1 %niter.ncmp.7, label %for.cond.cleanup.loopexit.unr-lcssa, label %for.body

for.body.epil.1:                                  ; preds = %for.body.epil
  %13 = trunc i64 %indvars.iv.next.epil to i32
  %sub.epil.1 = sub nsw i32 %n, %13
  %arrayidx.epil.1 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.epil
  store i32 %sub.epil.1, i32* %arrayidx.epil.1, align 4
  %indvars.iv.next.epil.1 = add nuw nsw i64 %indvars.iv.unr, 2
  %epil.iter.cmp.1 = icmp eq i64 %xtraiter, 2
  br i1 %epil.iter.cmp.1, label %for.cond.cleanup, label %for.body.epil.2

for.body.epil.2:                                  ; preds = %for.body.epil.1
  %14 = trunc i64 %indvars.iv.next.epil.1 to i32
  %sub.epil.2 = sub nsw i32 %n, %14
  %arrayidx.epil.2 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.epil.1
  store i32 %sub.epil.2, i32* %arrayidx.epil.2, align 4
  %indvars.iv.next.epil.2 = add nuw nsw i64 %indvars.iv.unr, 3
  %epil.iter.cmp.2 = icmp eq i64 %xtraiter, 3
  br i1 %epil.iter.cmp.2, label %for.cond.cleanup, label %for.body.epil.3

for.body.epil.3:                                  ; preds = %for.body.epil.2
  %15 = trunc i64 %indvars.iv.next.epil.2 to i32
  %sub.epil.3 = sub nsw i32 %n, %15
  %arrayidx.epil.3 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.epil.2
  store i32 %sub.epil.3, i32* %arrayidx.epil.3, align 4
  %indvars.iv.next.epil.3 = add nuw nsw i64 %indvars.iv.unr, 4
  %epil.iter.cmp.3 = icmp eq i64 %xtraiter, 4
  br i1 %epil.iter.cmp.3, label %for.cond.cleanup, label %for.body.epil.4

for.body.epil.4:                                  ; preds = %for.body.epil.3
  %16 = trunc i64 %indvars.iv.next.epil.3 to i32
  %sub.epil.4 = sub nsw i32 %n, %16
  %arrayidx.epil.4 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.epil.3
  store i32 %sub.epil.4, i32* %arrayidx.epil.4, align 4
  %indvars.iv.next.epil.4 = add nuw nsw i64 %indvars.iv.unr, 5
  %epil.iter.cmp.4 = icmp eq i64 %xtraiter, 5
  br i1 %epil.iter.cmp.4, label %for.cond.cleanup, label %for.body.epil.5

for.body.epil.5:                                  ; preds = %for.body.epil.4
  %17 = trunc i64 %indvars.iv.next.epil.4 to i32
  %sub.epil.5 = sub nsw i32 %n, %17
  %arrayidx.epil.5 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.epil.4
  store i32 %sub.epil.5, i32* %arrayidx.epil.5, align 4
  %indvars.iv.next.epil.5 = add nuw nsw i64 %indvars.iv.unr, 6
  %epil.iter.cmp.5 = icmp eq i64 %xtraiter, 6
  br i1 %epil.iter.cmp.5, label %for.cond.cleanup, label %for.body.epil.6

for.body.epil.6:                                  ; preds = %for.body.epil.5
  %18 = trunc i64 %indvars.iv.next.epil.5 to i32
  %sub.epil.6 = sub nsw i32 %n, %18
  %arrayidx.epil.6 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.epil.5
  store i32 %sub.epil.6, i32* %arrayidx.epil.6, align 4
  br label %for.cond.cleanup
}

declare i32 @other2(i32*, i32, i32*) #2

define i64 @stackrealign3(i32 %x){
; CHECK-LABEL: stackrealign3:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -640
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 640
; CHECK-NEXT:    .cfi_register 67, 16
; CHECK-NEXT:    sd 632[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -8
; CHECK-NEXT:    sd 624[$r12] = $r14
; CHECK-NEXT:    addd $r14 = $r12, 624
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 14, -16
; CHECK-NEXT:    .cfi_def_cfa 14, 16
; CHECK-NEXT:    sd 616[$r12] = $r31
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 31, -24
; CHECK-NEXT:    sd 608[$r12] = $r18
; CHECK-NEXT:    andd $r31 = $r12, -128
; CHECK-NEXT:    copyd $r18 = $r0
; CHECK-NEXT:    make $r4 = 4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 18, -32
; CHECK-NEXT:    make $r8 = 8
; CHECK-NEXT:    make $r32 = 12
; CHECK-NEXT:    make $r36 = 16
; CHECK-NEXT:    make $r5 = 3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r9 = 7
; CHECK-NEXT:    make $r33 = 11
; CHECK-NEXT:    make $r37 = 15
; CHECK-NEXT:    make $r6 = 2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r7 = 1
; CHECK-NEXT:    make $r10 = 6
; CHECK-NEXT:    make $r34 = 10
; CHECK-NEXT:    make $r38 = 14
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r11 = 5
; CHECK-NEXT:    make $r35 = 9
; CHECK-NEXT:    make $r39 = 13
; CHECK-NEXT:    addd $r15 = $r31, 128
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r0 = $r31, 380
; CHECK-NEXT:    addd $r1 = $r31, 384
; CHECK-NEXT:    so 480[$r31] = $r4r5r6r7
; CHECK-NEXT:    ;;
; CHECK-NEXT:    so 448[$r31] = $r8r9r10r11
; CHECK-NEXT:    ;;
; CHECK-NEXT:    so 416[$r31] = $r32r33r34r35
; CHECK-NEXT:    ;;
; CHECK-NEXT:    so 384[$r31] = $r36r37r38r39
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 380[$r31] = $r18
; CHECK-NEXT:    call otherv
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r0 = 128[$r31]
; CHECK-NEXT:    sxwd $r1 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r0 = $r0, $r1
; CHECK-NEXT:    addd $r12 = $r14, -624
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r18 = 608[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r31 = 616[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r14 = 624[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 632[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 640
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %i = alloca <16 x i64>, align 128
  %a = alloca i32, align 4
  %tmp = alloca <16 x i64>, align 128
  %0 = bitcast <16 x i64>* %i to i8*
  call void @llvm.lifetime.start.p0i8(i64 128, i8* nonnull %0) #3
  store <16 x i64> <i64 16, i64 15, i64 14, i64 13, i64 12, i64 11, i64 10, i64 9, i64 8, i64 7, i64 6, i64 5, i64 4, i64 3, i64 2, i64 1>, <16 x i64>* %i, align 128
  %1 = bitcast i32* %a to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #3
  store i32 %x, i32* %a, align 4
  call void @otherv(<16 x i64>* nonnull sret(<16 x i64>) %tmp, i32* nonnull %a, <16 x i64>* nonnull %i) #3
  %2 = load <16 x i64>, <16 x i64>* %tmp, align 128
  %vecext = extractelement <16 x i64> %2, i32 0
  %conv = sext i32 %x to i64
  %add = add nsw i64 %vecext, %conv
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #3
  call void @llvm.lifetime.end.p0i8(i64 128, i8* nonnull %0) #3
  ret i64 %add
}

declare void @otherv(<16 x i64>* sret(<16 x i64>), i32*, <16 x i64>*) #2

define i32 @stackrealign4(i32 %n){
; CHECK-LABEL: stackrealign4:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -512
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 512
; CHECK-NEXT:    .cfi_register 67, 16
; CHECK-NEXT:    sd 504[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -8
; CHECK-NEXT:    sd 496[$r12] = $r14
; CHECK-NEXT:    addd $r14 = $r12, 496
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 14, -16
; CHECK-NEXT:    .cfi_def_cfa 14, 16
; CHECK-NEXT:    so 464[$r12] = $r28r29r30r31
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 28, -24
; CHECK-NEXT:    .cfi_offset 29, -32
; CHECK-NEXT:    .cfi_offset 30, -40
; CHECK-NEXT:    .cfi_offset 31, -48
; CHECK-NEXT:    so 432[$r12] = $r24r25r26r27
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 24, -56
; CHECK-NEXT:    .cfi_offset 25, -64
; CHECK-NEXT:    .cfi_offset 26, -72
; CHECK-NEXT:    .cfi_offset 27, -80
; CHECK-NEXT:    so 400[$r12] = $r20r21r22r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 20, -88
; CHECK-NEXT:    .cfi_offset 21, -96
; CHECK-NEXT:    .cfi_offset 22, -104
; CHECK-NEXT:    .cfi_offset 23, -112
; CHECK-NEXT:    sq 384[$r12] = $r18r19
; CHECK-NEXT:    copyd $r11 = $r0
; CHECK-NEXT:    andd $r31 = $r12, -128
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 18, -120
; CHECK-NEXT:    .cfi_offset 19, -128
; CHECK-NEXT:    sxwd $r0 = $r11
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addx4d $r0 = $r0, 31
; CHECK-NEXT:    ;;
; CHECK-NEXT:    andd $r0 = $r0, -32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfd $r15 = $r0, $r12
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r12 = $r15
; CHECK-NEXT:    cb.wlez $r11 ? .LBB3_12
; CHECK-NEXT:    ;;
; CHECK-NEXT:  # %bb.1: # %for.body.preheader
; CHECK-NEXT:    zxwd $r4 = $r11
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r2 = $r4, -1
; CHECK-NEXT:    andd $r3 = $r4, 7
; CHECK-NEXT:    ;;
; CHECK-NEXT:    compd.ltu $r5 = $r2, 7
; CHECK-NEXT:    make $r2 = 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    cb.odd $r5 ? .LBB3_4
; CHECK-NEXT:    ;;
; CHECK-NEXT:  # %bb.2: # %for.body.preheader.new
; CHECK-NEXT:    sbfd $r4 = $r3, $r4
; CHECK-NEXT:    addw $r5 = $r11, -7
; CHECK-NEXT:    make $r2 = 0
; CHECK-NEXT:    copyd $r6 = $r15
; CHECK-NEXT:    ;;
; CHECK-NEXT:  .LBB3_3: # %for.body
; CHECK-NEXT:    # =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    addw $r0 = $r5, 7
; CHECK-NEXT:    addw $r1 = $r5, 6
; CHECK-NEXT:    addd $r2 = $r2, 8
; CHECK-NEXT:    addw $r7 = $r5, 3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 0[$r6] = $r0
; CHECK-NEXT:    addw $r0 = $r5, 5
; CHECK-NEXT:    addw $r8 = $r5, 2
; CHECK-NEXT:    addw $r9 = $r5, 1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 4[$r6] = $r1
; CHECK-NEXT:    addw $r1 = $r5, 4
; CHECK-NEXT:    compd.eq $r10 = $r4, $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 8[$r6] = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 12[$r6] = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 16[$r6] = $r7
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 20[$r6] = $r8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 24[$r6] = $r9
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 28[$r6] = $r5
; CHECK-NEXT:    addw $r5 = $r5, -8
; CHECK-NEXT:    addd $r6 = $r6, 32
; CHECK-NEXT:    cb.even $r10 ? .LBB3_3
; CHECK-NEXT:    ;;
; CHECK-NEXT:  .LBB3_4: # %for.cond.cleanup.loopexit.unr-lcssa
; CHECK-NEXT:    cb.deqz $r3 ? .LBB3_12
; CHECK-NEXT:    ;;
; CHECK-NEXT:  # %bb.5: # %for.body.epil
; CHECK-NEXT:    zxwd $r4 = $r2
; CHECK-NEXT:    compd.ne $r5 = $r3, 1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfw $r4 = $r4, $r11
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw.xs $r2[$r15] = $r4
; CHECK-NEXT:    cb.even $r5 ? .LBB3_12
; CHECK-NEXT:    ;;
; CHECK-NEXT:  # %bb.6: # %for.body.epil.1
; CHECK-NEXT:    addd $r0 = $r2, 1
; CHECK-NEXT:    compd.eq $r4 = $r3, 2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r1 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfw $r1 = $r1, $r11
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw.xs $r0[$r15] = $r1
; CHECK-NEXT:    cb.odd $r4 ? .LBB3_12
; CHECK-NEXT:    ;;
; CHECK-NEXT:  # %bb.7: # %for.body.epil.2
; CHECK-NEXT:    addd $r0 = $r2, 2
; CHECK-NEXT:    compd.eq $r4 = $r3, 3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r1 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfw $r1 = $r1, $r11
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw.xs $r0[$r15] = $r1
; CHECK-NEXT:    cb.odd $r4 ? .LBB3_12
; CHECK-NEXT:    ;;
; CHECK-NEXT:  # %bb.8: # %for.body.epil.3
; CHECK-NEXT:    addd $r0 = $r2, 3
; CHECK-NEXT:    compd.eq $r4 = $r3, 4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r1 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfw $r1 = $r1, $r11
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw.xs $r0[$r15] = $r1
; CHECK-NEXT:    cb.odd $r4 ? .LBB3_12
; CHECK-NEXT:    ;;
; CHECK-NEXT:  # %bb.9: # %for.body.epil.4
; CHECK-NEXT:    addd $r0 = $r2, 4
; CHECK-NEXT:    compd.eq $r4 = $r3, 5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r1 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfw $r1 = $r1, $r11
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw.xs $r0[$r15] = $r1
; CHECK-NEXT:    cb.odd $r4 ? .LBB3_12
; CHECK-NEXT:    ;;
; CHECK-NEXT:  # %bb.10: # %for.body.epil.5
; CHECK-NEXT:    addd $r0 = $r2, 5
; CHECK-NEXT:    compd.eq $r3 = $r3, 6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r1 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfw $r1 = $r1, $r11
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw.xs $r0[$r15] = $r1
; CHECK-NEXT:    cb.odd $r3 ? .LBB3_12
; CHECK-NEXT:    ;;
; CHECK-NEXT:  # %bb.11: # %for.body.epil.6
; CHECK-NEXT:    addd $r0 = $r2, 6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r1 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfw $r1 = $r1, $r11
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw.xs $r0[$r15] = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:  .LBB3_12: # %for.cond.cleanup
; CHECK-NEXT:    make $r2 = 0x4d2
; CHECK-NEXT:    make $r3 = g1
; CHECK-NEXT:    make $r5 = g3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r6 = g4
; CHECK-NEXT:    sw 256[$r31] = $r2
; CHECK-NEXT:    make $r2 = g2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r7 = g5
; CHECK-NEXT:    make $r8 = g6
; CHECK-NEXT:    lwz $r0 = 0[$r3]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r9 = g7
; CHECK-NEXT:    make $r10 = g8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r16 = g11
; CHECK-NEXT:    make $r17 = g12
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r32 = g13
; CHECK-NEXT:    make $r33 = g14
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r34 = g15
; CHECK-NEXT:    make $r35 = g16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r36 = g17
; CHECK-NEXT:    make $r37 = g19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r38 = g20
; CHECK-NEXT:    make $r39 = g21
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r40 = g22
; CHECK-NEXT:    make $r41 = g23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r42 = g24
; CHECK-NEXT:    sd 248[$r31] = $r0
; CHECK-NEXT:    make $r43 = g25
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r44 = g26
; CHECK-NEXT:    make $r45 = g27
; CHECK-NEXT:    sd 200[$r31] = $r11
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r11 = g9
; CHECK-NEXT:    make $r46 = g28
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r47 = g29
; CHECK-NEXT:    lwz $r0 = 0[$r2]
; CHECK-NEXT:    make $r2 = g10
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r48 = g30
; CHECK-NEXT:    make $r49 = g31
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r50 = g32
; CHECK-NEXT:    make $r51 = g33
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r52 = g34
; CHECK-NEXT:    make $r53 = g35
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r54 = g36
; CHECK-NEXT:    make $r55 = g37
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r56 = g38
; CHECK-NEXT:    make $r57 = g39
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r58 = g40
; CHECK-NEXT:    make $r59 = g41
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r60 = g42
; CHECK-NEXT:    make $r61 = g43
; CHECK-NEXT:    sd 224[$r31] = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r62 = g44
; CHECK-NEXT:    make $r63 = g45
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r18 = g46
; CHECK-NEXT:    lwz $r0 = 0[$r5]
; CHECK-NEXT:    make $r19 = g47
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r20 = g48
; CHECK-NEXT:    make $r21 = g49
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r22 = g50
; CHECK-NEXT:    make $r23 = g51
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r24 = g52
; CHECK-NEXT:    make $r25 = g53
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r26 = g54
; CHECK-NEXT:    make $r27 = g55
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r28 = g56
; CHECK-NEXT:    make $r29 = g57
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r30 = g58
; CHECK-NEXT:    make $r5 = g59
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r4 = g61
; CHECK-NEXT:    make $r3 = g62
; CHECK-NEXT:    sd 216[$r31] = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r1 = g64
; CHECK-NEXT:    lwz $r0 = 0[$r6]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 208[$r31] = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r0 = 0[$r7]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 232[$r31] = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r0 = 0[$r8]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 240[$r31] = $r0
; CHECK-NEXT:    make $r0 = g65
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r8 = 0[$r2]
; CHECK-NEXT:    make $r2 = g18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r7 = 0[$r2]
; CHECK-NEXT:    make $r2 = g63
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r9 = 0[$r9]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r10 = 0[$r10]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r11 = 0[$r11]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r16 = 0[$r16]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r17 = 0[$r17]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r32 = 0[$r32]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r33 = 0[$r33]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r34 = 0[$r34]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r35 = 0[$r35]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r36 = 0[$r36]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r37 = 0[$r37]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r38 = 0[$r38]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r39 = 0[$r39]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r40 = 0[$r40]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r41 = 0[$r41]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r42 = 0[$r42]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r43 = 0[$r43]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r44 = 0[$r44]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r45 = 0[$r45]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r46 = 0[$r46]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r47 = 0[$r47]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r48 = 0[$r48]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r49 = 0[$r49]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r50 = 0[$r50]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r51 = 0[$r51]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r52 = 0[$r52]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r53 = 0[$r53]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r54 = 0[$r54]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r55 = 0[$r55]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r56 = 0[$r56]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r57 = 0[$r57]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r58 = 0[$r58]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r59 = 0[$r59]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r60 = 0[$r60]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r61 = 0[$r61]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r62 = 0[$r62]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r63 = 0[$r63]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r18 = 0[$r18]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r19 = 0[$r19]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r20 = 0[$r20]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r21 = 0[$r21]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r22 = 0[$r22]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r23 = 0[$r23]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r24 = 0[$r24]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r25 = 0[$r25]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r26 = 0[$r26]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r27 = 0[$r27]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r28 = 0[$r28]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r29 = 0[$r29]
; CHECK-NEXT:    copyd $r6 = $r15
; CHECK-NEXT:    make $r15 = g60
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r30 = 0[$r30]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r5 = 0[$r5]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r15 = 0[$r15]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r4 = 0[$r4]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r3 = 0[$r3]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r2 = 0[$r2]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r1 = 0[$r1]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r0 = 0[$r0]
; CHECK-NEXT:    addd $r12 = $r12, -448
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 440[$r12] = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 432[$r12] = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 424[$r12] = $r2
; CHECK-NEXT:    addd $r2 = $r31, 256
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 416[$r12] = $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 408[$r12] = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 400[$r12] = $r15
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 392[$r12] = $r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 384[$r12] = $r30
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 376[$r12] = $r29
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 368[$r12] = $r28
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 360[$r12] = $r27
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 352[$r12] = $r26
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 344[$r12] = $r25
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 336[$r12] = $r24
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 328[$r12] = $r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 320[$r12] = $r22
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 312[$r12] = $r21
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 304[$r12] = $r20
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 296[$r12] = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 288[$r12] = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 280[$r12] = $r63
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 272[$r12] = $r62
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 264[$r12] = $r61
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 256[$r12] = $r60
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 248[$r12] = $r59
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 240[$r12] = $r58
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 232[$r12] = $r57
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 224[$r12] = $r56
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 216[$r12] = $r55
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 208[$r12] = $r54
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 200[$r12] = $r53
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 192[$r12] = $r52
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 184[$r12] = $r51
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 176[$r12] = $r50
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 168[$r12] = $r49
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 160[$r12] = $r48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 152[$r12] = $r47
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 144[$r12] = $r46
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 136[$r12] = $r45
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 128[$r12] = $r44
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 120[$r12] = $r43
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 112[$r12] = $r42
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 104[$r12] = $r41
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 96[$r12] = $r40
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 88[$r12] = $r39
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 80[$r12] = $r38
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 72[$r12] = $r37
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 64[$r12] = $r7
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 56[$r12] = $r36
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 48[$r12] = $r35
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 40[$r12] = $r34
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 32[$r12] = $r33
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 24[$r12] = $r32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 16[$r12] = $r17
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 8[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 0[$r12] = $r8
; CHECK-NEXT:    copyd $r0 = $r6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r1 = 200[$r31]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r3 = 248[$r31]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r4 = 224[$r31]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r5 = 216[$r31]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r6 = 208[$r31]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r7 = 232[$r31]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r8 = 240[$r31]
; CHECK-NEXT:    call other4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r12 = $r12, 448
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r12 = $r14, -496
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lq $r18r19 = 384[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r20r21r22r23 = 400[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r24r25r26r27 = 432[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r28r29r30r31 = 464[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r14 = 496[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 504[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 512
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %i = alloca i32, align 128
  %conv = sext i32 %n to i64
  %0 = alloca i32, i64 %conv, align 8
  %cmp11 = icmp sgt i32 %n, 0
  br i1 %cmp11, label %for.body.preheader, label %for.cond.cleanup

for.body.preheader:                               ; preds = %entry
  %wide.trip.count = zext i32 %n to i64
  %1 = add nsw i64 %wide.trip.count, -1
  %xtraiter = and i64 %wide.trip.count, 7
  %2 = icmp ult i64 %1, 7
  br i1 %2, label %for.cond.cleanup.loopexit.unr-lcssa, label %for.body.preheader.new

for.body.preheader.new:                           ; preds = %for.body.preheader
  %unroll_iter = sub nsw i64 %wide.trip.count, %xtraiter
  br label %for.body

for.cond.cleanup.loopexit.unr-lcssa:              ; preds = %for.body, %for.body.preheader
  %indvars.iv.unr = phi i64 [ 0, %for.body.preheader ], [ %indvars.iv.next.7, %for.body ]
  %lcmp.mod = icmp eq i64 %xtraiter, 0
  br i1 %lcmp.mod, label %for.cond.cleanup, label %for.body.epil

for.body.epil:                                    ; preds = %for.cond.cleanup.loopexit.unr-lcssa
  %3 = trunc i64 %indvars.iv.unr to i32
  %sub.epil = sub nsw i32 %n, %3
  %arrayidx.epil = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.unr
  store i32 %sub.epil, i32* %arrayidx.epil, align 4
  %indvars.iv.next.epil = add nuw nsw i64 %indvars.iv.unr, 1
  %epil.iter.cmp = icmp eq i64 %xtraiter, 1
  br i1 %epil.iter.cmp, label %for.cond.cleanup, label %for.body.epil.1

for.cond.cleanup:                                 ; preds = %for.cond.cleanup.loopexit.unr-lcssa, %for.body.epil.5, %for.body.epil.4, %for.body.epil.3, %for.body.epil.2, %for.body.epil.1, %for.body.epil, %for.body.epil.6, %entry
  %4 = bitcast i32* %i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %4) #3
  store i32 1234, i32* %i, align 128
  %5 = load i32, i32* @g1, align 4
  %6 = load i32, i32* @g2, align 4
  %7 = load i32, i32* @g3, align 4
  %8 = load i32, i32* @g4, align 4
  %9 = load i32, i32* @g5, align 4
  %10 = load i32, i32* @g6, align 4
  %11 = load i32, i32* @g7, align 4
  %12 = load i32, i32* @g8, align 4
  %13 = load i32, i32* @g9, align 4
  %14 = load i32, i32* @g10, align 4
  %15 = load i32, i32* @g11, align 4
  %16 = load i32, i32* @g12, align 4
  %17 = load i32, i32* @g13, align 4
  %18 = load i32, i32* @g14, align 4
  %19 = load i32, i32* @g15, align 4
  %20 = load i32, i32* @g16, align 4
  %21 = load i32, i32* @g17, align 4
  %22 = load i32, i32* @g18, align 4
  %23 = load i32, i32* @g19, align 4
  %24 = load i32, i32* @g20, align 4
  %25 = load i32, i32* @g21, align 4
  %26 = load i32, i32* @g22, align 4
  %27 = load i32, i32* @g23, align 4
  %28 = load i32, i32* @g24, align 4
  %29 = load i32, i32* @g25, align 4
  %30 = load i32, i32* @g26, align 4
  %31 = load i32, i32* @g27, align 4
  %32 = load i32, i32* @g28, align 4
  %33 = load i32, i32* @g29, align 4
  %34 = load i32, i32* @g30, align 4
  %35 = load i32, i32* @g31, align 4
  %36 = load i32, i32* @g32, align 4
  %37 = load i32, i32* @g33, align 4
  %38 = load i32, i32* @g34, align 4
  %39 = load i32, i32* @g35, align 4
  %40 = load i32, i32* @g36, align 4
  %41 = load i32, i32* @g37, align 4
  %42 = load i32, i32* @g38, align 4
  %43 = load i32, i32* @g39, align 4
  %44 = load i32, i32* @g40, align 4
  %45 = load i32, i32* @g41, align 4
  %46 = load i32, i32* @g42, align 4
  %47 = load i32, i32* @g43, align 4
  %48 = load i32, i32* @g44, align 4
  %49 = load i32, i32* @g45, align 4
  %50 = load i32, i32* @g46, align 4
  %51 = load i32, i32* @g47, align 4
  %52 = load i32, i32* @g48, align 4
  %53 = load i32, i32* @g49, align 4
  %54 = load i32, i32* @g50, align 4
  %55 = load i32, i32* @g51, align 4
  %56 = load i32, i32* @g52, align 4
  %57 = load i32, i32* @g53, align 4
  %58 = load i32, i32* @g54, align 4
  %59 = load i32, i32* @g55, align 4
  %60 = load i32, i32* @g56, align 4
  %61 = load i32, i32* @g57, align 4
  %62 = load i32, i32* @g58, align 4
  %63 = load i32, i32* @g59, align 4
  %64 = load i32, i32* @g60, align 4
  %65 = load i32, i32* @g61, align 4
  %66 = load i32, i32* @g62, align 4
  %67 = load i32, i32* @g63, align 4
  %68 = load i32, i32* @g64, align 4
  %69 = load i32, i32* @g65, align 4
  %call = call i32 @other4(i32* nonnull %0, i32 %n, i32* nonnull %i, i32 %5, i32 %6, i32 %7, i32 %8, i32 %9, i32 %10, i32 %11, i32 %12, i32 %13, i32 %14, i32 %15, i32 %16, i32 %17, i32 %18, i32 %19, i32 %20, i32 %21, i32 %22, i32 %23, i32 %24, i32 %25, i32 %26, i32 %27, i32 %28, i32 %29, i32 %30, i32 %31, i32 %32, i32 %33, i32 %34, i32 %35, i32 %36, i32 %37, i32 %38, i32 %39, i32 %40, i32 %41, i32 %42, i32 %43, i32 %44, i32 %45, i32 %46, i32 %47, i32 %48, i32 %49, i32 %50, i32 %51, i32 %52, i32 %53, i32 %54, i32 %55, i32 %56, i32 %57, i32 %58, i32 %59, i32 %60, i32 %61, i32 %62, i32 %63, i32 %64, i32 %65, i32 %66, i32 %67, i32 %68, i32 %69) #3
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %4) #3
  ret i32 %call

for.body:                                         ; preds = %for.body, %for.body.preheader.new
  %indvars.iv = phi i64 [ 0, %for.body.preheader.new ], [ %indvars.iv.next.7, %for.body ]
  %niter = phi i64 [ %unroll_iter, %for.body.preheader.new ], [ %niter.nsub.7, %for.body ]
  %70 = trunc i64 %indvars.iv to i32
  %sub = sub nsw i32 %n, %70
  %arrayidx = getelementptr inbounds i32, i32* %0, i64 %indvars.iv
  store i32 %sub, i32* %arrayidx, align 8
  %indvars.iv.next = or i64 %indvars.iv, 1
  %71 = trunc i64 %indvars.iv.next to i32
  %sub.1 = sub nsw i32 %n, %71
  %arrayidx.1 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next
  store i32 %sub.1, i32* %arrayidx.1, align 4
  %indvars.iv.next.1 = or i64 %indvars.iv, 2
  %72 = trunc i64 %indvars.iv.next.1 to i32
  %sub.2 = sub nsw i32 %n, %72
  %arrayidx.2 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.1
  store i32 %sub.2, i32* %arrayidx.2, align 8
  %indvars.iv.next.2 = or i64 %indvars.iv, 3
  %73 = trunc i64 %indvars.iv.next.2 to i32
  %sub.3 = sub nsw i32 %n, %73
  %arrayidx.3 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.2
  store i32 %sub.3, i32* %arrayidx.3, align 4
  %indvars.iv.next.3 = or i64 %indvars.iv, 4
  %74 = trunc i64 %indvars.iv.next.3 to i32
  %sub.4 = sub nsw i32 %n, %74
  %arrayidx.4 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.3
  store i32 %sub.4, i32* %arrayidx.4, align 8
  %indvars.iv.next.4 = or i64 %indvars.iv, 5
  %75 = trunc i64 %indvars.iv.next.4 to i32
  %sub.5 = sub nsw i32 %n, %75
  %arrayidx.5 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.4
  store i32 %sub.5, i32* %arrayidx.5, align 4
  %indvars.iv.next.5 = or i64 %indvars.iv, 6
  %76 = trunc i64 %indvars.iv.next.5 to i32
  %sub.6 = sub nsw i32 %n, %76
  %arrayidx.6 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.5
  store i32 %sub.6, i32* %arrayidx.6, align 8
  %indvars.iv.next.6 = or i64 %indvars.iv, 7
  %77 = trunc i64 %indvars.iv.next.6 to i32
  %sub.7 = sub nsw i32 %n, %77
  %arrayidx.7 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.6
  store i32 %sub.7, i32* %arrayidx.7, align 4
  %indvars.iv.next.7 = add nuw nsw i64 %indvars.iv, 8
  %niter.nsub.7 = add i64 %niter, -8
  %niter.ncmp.7 = icmp eq i64 %niter.nsub.7, 0
  br i1 %niter.ncmp.7, label %for.cond.cleanup.loopexit.unr-lcssa, label %for.body

for.body.epil.1:                                  ; preds = %for.body.epil
  %78 = trunc i64 %indvars.iv.next.epil to i32
  %sub.epil.1 = sub nsw i32 %n, %78
  %arrayidx.epil.1 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.epil
  store i32 %sub.epil.1, i32* %arrayidx.epil.1, align 4
  %indvars.iv.next.epil.1 = add nuw nsw i64 %indvars.iv.unr, 2
  %epil.iter.cmp.1 = icmp eq i64 %xtraiter, 2
  br i1 %epil.iter.cmp.1, label %for.cond.cleanup, label %for.body.epil.2

for.body.epil.2:                                  ; preds = %for.body.epil.1
  %79 = trunc i64 %indvars.iv.next.epil.1 to i32
  %sub.epil.2 = sub nsw i32 %n, %79
  %arrayidx.epil.2 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.epil.1
  store i32 %sub.epil.2, i32* %arrayidx.epil.2, align 4
  %indvars.iv.next.epil.2 = add nuw nsw i64 %indvars.iv.unr, 3
  %epil.iter.cmp.2 = icmp eq i64 %xtraiter, 3
  br i1 %epil.iter.cmp.2, label %for.cond.cleanup, label %for.body.epil.3

for.body.epil.3:                                  ; preds = %for.body.epil.2
  %80 = trunc i64 %indvars.iv.next.epil.2 to i32
  %sub.epil.3 = sub nsw i32 %n, %80
  %arrayidx.epil.3 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.epil.2
  store i32 %sub.epil.3, i32* %arrayidx.epil.3, align 4
  %indvars.iv.next.epil.3 = add nuw nsw i64 %indvars.iv.unr, 4
  %epil.iter.cmp.3 = icmp eq i64 %xtraiter, 4
  br i1 %epil.iter.cmp.3, label %for.cond.cleanup, label %for.body.epil.4

for.body.epil.4:                                  ; preds = %for.body.epil.3
  %81 = trunc i64 %indvars.iv.next.epil.3 to i32
  %sub.epil.4 = sub nsw i32 %n, %81
  %arrayidx.epil.4 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.epil.3
  store i32 %sub.epil.4, i32* %arrayidx.epil.4, align 4
  %indvars.iv.next.epil.4 = add nuw nsw i64 %indvars.iv.unr, 5
  %epil.iter.cmp.4 = icmp eq i64 %xtraiter, 5
  br i1 %epil.iter.cmp.4, label %for.cond.cleanup, label %for.body.epil.5

for.body.epil.5:                                  ; preds = %for.body.epil.4
  %82 = trunc i64 %indvars.iv.next.epil.4 to i32
  %sub.epil.5 = sub nsw i32 %n, %82
  %arrayidx.epil.5 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.epil.4
  store i32 %sub.epil.5, i32* %arrayidx.epil.5, align 4
  %indvars.iv.next.epil.5 = add nuw nsw i64 %indvars.iv.unr, 6
  %epil.iter.cmp.5 = icmp eq i64 %xtraiter, 6
  br i1 %epil.iter.cmp.5, label %for.cond.cleanup, label %for.body.epil.6

for.body.epil.6:                                  ; preds = %for.body.epil.5
  %83 = trunc i64 %indvars.iv.next.epil.5 to i32
  %sub.epil.6 = sub nsw i32 %n, %83
  %arrayidx.epil.6 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.epil.5
  store i32 %sub.epil.6, i32* %arrayidx.epil.6, align 4
  br label %for.cond.cleanup
}

declare i32 @other4(i32*, i32, i32*, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) #2

