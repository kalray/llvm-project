; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -O2 | FileCheck %s
target triple = "kvx-kalray-cos"

@g1 = common global i32 0, align 4
@g2 = common global i32 0, align 4
@g3 = common global i32 0, align 4
@g4 = common global i32 0, align 4
@g5 = common global i32 0, align 4
@g6 = common global i32 0, align 4
@g7 = common global i32 0, align 4
@g8 = common global i32 0, align 4
@g9 = common global i32 0, align 4
@g10 = common global i32 0, align 4
@g11 = common global i32 0, align 4
@g12 = common global i32 0, align 4
@g13 = common global i32 0, align 4
@g14 = common global i32 0, align 4
@g15 = common global i32 0, align 4
@g16 = common global i32 0, align 4
@g17 = common global i32 0, align 4
@g18 = common global i32 0, align 4
@g19 = common global i32 0, align 4
@g20 = common global i32 0, align 4
@g21 = common global i32 0, align 4
@g22 = common global i32 0, align 4
@g23 = common global i32 0, align 4
@g24 = common global i32 0, align 4
@g25 = common global i32 0, align 4
@g26 = common global i32 0, align 4
@g27 = common global i32 0, align 4
@g28 = common global i32 0, align 4
@g29 = common global i32 0, align 4
@g30 = common global i32 0, align 4
@g31 = common global i32 0, align 4
@g32 = common global i32 0, align 4
@g33 = common global i32 0, align 4
@g34 = common global i32 0, align 4
@g35 = common global i32 0, align 4
@g36 = common global i32 0, align 4
@g37 = common global i32 0, align 4
@g38 = common global i32 0, align 4
@g39 = common global i32 0, align 4
@g40 = common global i32 0, align 4
@g41 = common global i32 0, align 4
@g42 = common global i32 0, align 4
@g43 = common global i32 0, align 4
@g44 = common global i32 0, align 4
@g45 = common global i32 0, align 4
@g46 = common global i32 0, align 4
@g47 = common global i32 0, align 4
@g48 = common global i32 0, align 4
@g49 = common global i32 0, align 4
@g50 = common global i32 0, align 4
@g51 = common global i32 0, align 4
@g52 = common global i32 0, align 4
@g53 = common global i32 0, align 4
@g54 = common global i32 0, align 4
@g55 = common global i32 0, align 4
@g56 = common global i32 0, align 4
@g57 = common global i32 0, align 4
@g58 = common global i32 0, align 4
@g59 = common global i32 0, align 4
@g60 = common global i32 0, align 4
@g61 = common global i32 0, align 4
@g62 = common global i32 0, align 4
@g63 = common global i32 0, align 4
@g64 = common global i32 0, align 4
@g65 = common global i32 0, align 4

define i32 @stackrealign1(){
; CHECK-LABEL: stackrealign1:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -384
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 384
; CHECK-NEXT:    copyd $r32 = $r12
; CHECK-NEXT:    addd $r12 = $r12, 96
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_register 32
; CHECK-NEXT:    andd $r12 = $r12, -128
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 8[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_escape 0x10, 0x43, 0x02, 0x7c, 0x08
; CHECK-NEXT:    sd 0[$r12] = $r14
; CHECK-NEXT:    copyd $r14 = $r32
; CHECK-NEXT:    make $r0 = 7
; CHECK-NEXT:    addd $r1 = $r12, 128
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_escape 0x10, 0x0e, 0x02, 0x7c, 0x00
; CHECK-NEXT:    .cfi_def_cfa_register 14
; CHECK-NEXT:    sw 16[$r12] = $r0
; CHECK-NEXT:    make $r0 = 0x4d2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 128[$r12] = $r0
; CHECK-NEXT:    addd $r0 = $r12, 16
; CHECK-NEXT:    call other
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r32 = $r14
; CHECK-NEXT:    ld $r16 = 8[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_register 32
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    ld $r14 = 0[$r12]
; CHECK-NEXT:    copyd $r12 = $r32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_restore 67
; CHECK-NEXT:    .cfi_def_cfa_register 12
; CHECK-NEXT:    addd $r12 = $r12, 384
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %c = alloca i32, align 4
  %i = alloca i32, align 128
  %0 = bitcast i32* %c to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #3
  store i32 7, i32* %c, align 4
  %1 = bitcast i32* %i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #3
  store i32 1234, i32* %i, align 128
  %call = call i32 @other(i32* nonnull %c, i32* nonnull %i) #3
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #3
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #3
  ret i32 %call
}

declare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #1

declare i32 @other(i32*, i32*) #2

declare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #1

define i32 @stackrealign2(i32 %n){
; CHECK-LABEL: stackrealign2:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -384
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 384
; CHECK-NEXT:    copyd $r32 = $r12
; CHECK-NEXT:    addd $r12 = $r12, 96
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_register 32
; CHECK-NEXT:    andd $r12 = $r12, -128
; CHECK-NEXT:    copyd $r33 = $r14
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r14 = $r12
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 16[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_escape 0x10, 0x43, 0x02, 0x7e, 0x10
; CHECK-NEXT:    sd 8[$r12] = $r33
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_escape 0x10, 0x0e, 0x02, 0x7e, 0x08
; CHECK-NEXT:    sd 0[$r12] = $r31
; CHECK-NEXT:    copyd $r31 = $r32
; CHECK-NEXT:    copyd $r1 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_escape 0x10, 0x1f, 0x02, 0x7e, 0x00
; CHECK-NEXT:    .cfi_def_cfa_register 31
; CHECK-NEXT:    sxwd $r0 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addx4d $r0 = $r0, 31
; CHECK-NEXT:    ;;
; CHECK-NEXT:    andd $r0 = $r0, -32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfd $r0 = $r0, $r12
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r12 = $r0
; CHECK-NEXT:    cb.wlez $r1 ? .LBB1_12
; CHECK-NEXT:    ;;
; CHECK-NEXT:  # %bb.1: # %for.body.preheader
; CHECK-NEXT:    zxwd $r4 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r3 = $r4, -1
; CHECK-NEXT:    andd $r2 = $r4, 7
; CHECK-NEXT:    ;;
; CHECK-NEXT:    compd.ltu $r5 = $r3, 7
; CHECK-NEXT:    make $r3 = 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    cb.odd $r5 ? .LBB1_4
; CHECK-NEXT:    ;;
; CHECK-NEXT:  # %bb.2: # %for.body.preheader.new
; CHECK-NEXT:    sbfd $r4 = $r2, $r4
; CHECK-NEXT:    addw $r5 = $r1, -7
; CHECK-NEXT:    addd $r6 = $r0, 16
; CHECK-NEXT:    make $r3 = 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:  .LBB1_3: # %for.body
; CHECK-NEXT:    # =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    addw $r7 = $r5, 7
; CHECK-NEXT:    addd $r3 = $r3, 8
; CHECK-NEXT:    sw 12[$r6] = $r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw -16[$r6] = $r7
; CHECK-NEXT:    addw $r7 = $r5, 6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw -12[$r6] = $r7
; CHECK-NEXT:    addw $r7 = $r5, 5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw -8[$r6] = $r7
; CHECK-NEXT:    addw $r7 = $r5, 4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw -4[$r6] = $r7
; CHECK-NEXT:    addw $r7 = $r5, 3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 0[$r6] = $r7
; CHECK-NEXT:    addw $r7 = $r5, 2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 4[$r6] = $r7
; CHECK-NEXT:    addw $r7 = $r5, 1
; CHECK-NEXT:    addw $r5 = $r5, -8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 8[$r6] = $r7
; CHECK-NEXT:    compd.eq $r7 = $r4, $r3
; CHECK-NEXT:    addd $r6 = $r6, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    cb.even $r7 ? .LBB1_3
; CHECK-NEXT:    ;;
; CHECK-NEXT:  .LBB1_4: # %for.cond.cleanup.loopexit.unr-lcssa
; CHECK-NEXT:    cb.deqz $r2 ? .LBB1_12
; CHECK-NEXT:    ;;
; CHECK-NEXT:  # %bb.5: # %for.body.epil
; CHECK-NEXT:    zxwd $r4 = $r3
; CHECK-NEXT:    compd.ne $r5 = $r2, 1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfw $r4 = $r4, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw.xs $r3[$r0] = $r4
; CHECK-NEXT:    cb.even $r5 ? .LBB1_12
; CHECK-NEXT:    ;;
; CHECK-NEXT:  # %bb.6: # %for.body.epil.1
; CHECK-NEXT:    addd $r4 = $r3, 1
; CHECK-NEXT:    compd.eq $r6 = $r2, 2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r5 = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfw $r5 = $r5, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw.xs $r4[$r0] = $r5
; CHECK-NEXT:    cb.odd $r6 ? .LBB1_12
; CHECK-NEXT:    ;;
; CHECK-NEXT:  # %bb.7: # %for.body.epil.2
; CHECK-NEXT:    addd $r4 = $r3, 2
; CHECK-NEXT:    compd.eq $r6 = $r2, 3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r5 = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfw $r5 = $r5, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw.xs $r4[$r0] = $r5
; CHECK-NEXT:    cb.odd $r6 ? .LBB1_12
; CHECK-NEXT:    ;;
; CHECK-NEXT:  # %bb.8: # %for.body.epil.3
; CHECK-NEXT:    addd $r4 = $r3, 3
; CHECK-NEXT:    compd.eq $r6 = $r2, 4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r5 = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfw $r5 = $r5, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw.xs $r4[$r0] = $r5
; CHECK-NEXT:    cb.odd $r6 ? .LBB1_12
; CHECK-NEXT:    ;;
; CHECK-NEXT:  # %bb.9: # %for.body.epil.4
; CHECK-NEXT:    addd $r4 = $r3, 4
; CHECK-NEXT:    compd.eq $r6 = $r2, 5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r5 = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfw $r5 = $r5, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw.xs $r4[$r0] = $r5
; CHECK-NEXT:    cb.odd $r6 ? .LBB1_12
; CHECK-NEXT:    ;;
; CHECK-NEXT:  # %bb.10: # %for.body.epil.5
; CHECK-NEXT:    addd $r4 = $r3, 5
; CHECK-NEXT:    compd.eq $r2 = $r2, 6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r5 = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfw $r5 = $r5, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw.xs $r4[$r0] = $r5
; CHECK-NEXT:    cb.odd $r2 ? .LBB1_12
; CHECK-NEXT:    ;;
; CHECK-NEXT:  # %bb.11: # %for.body.epil.6
; CHECK-NEXT:    addd $r2 = $r3, 6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r3 = $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfw $r3 = $r3, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw.xs $r2[$r0] = $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:  .LBB1_12: # %for.cond.cleanup
; CHECK-NEXT:    make $r2 = 0x4d2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 128[$r14] = $r2
; CHECK-NEXT:    addd $r2 = $r14, 128
; CHECK-NEXT:    call other2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r32 = $r31
; CHECK-NEXT:    copyd $r12 = $r14
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_register 32
; CHECK-NEXT:    ld $r31 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    ld $r14 = 8[$r12]
; CHECK-NEXT:    copyd $r12 = $r32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_restore 67
; CHECK-NEXT:    .cfi_def_cfa_register 12
; CHECK-NEXT:    addd $r12 = $r12, 384
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %i = alloca i32, align 128
  %conv = sext i32 %n to i64
  %0 = alloca i32, i64 %conv, align 8
  %cmp11 = icmp sgt i32 %n, 0
  br i1 %cmp11, label %for.body.preheader, label %for.cond.cleanup

for.body.preheader:                               ; preds = %entry
  %wide.trip.count = zext i32 %n to i64
  %1 = add nsw i64 %wide.trip.count, -1
  %xtraiter = and i64 %wide.trip.count, 7
  %2 = icmp ult i64 %1, 7
  br i1 %2, label %for.cond.cleanup.loopexit.unr-lcssa, label %for.body.preheader.new

for.body.preheader.new:                           ; preds = %for.body.preheader
  %unroll_iter = sub nsw i64 %wide.trip.count, %xtraiter
  br label %for.body

for.cond.cleanup.loopexit.unr-lcssa:              ; preds = %for.body, %for.body.preheader
  %indvars.iv.unr = phi i64 [ 0, %for.body.preheader ], [ %indvars.iv.next.7, %for.body ]
  %lcmp.mod = icmp eq i64 %xtraiter, 0
  br i1 %lcmp.mod, label %for.cond.cleanup, label %for.body.epil

for.body.epil:                                    ; preds = %for.cond.cleanup.loopexit.unr-lcssa
  %3 = trunc i64 %indvars.iv.unr to i32
  %sub.epil = sub nsw i32 %n, %3
  %arrayidx.epil = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.unr
  store i32 %sub.epil, i32* %arrayidx.epil, align 4
  %indvars.iv.next.epil = add nuw nsw i64 %indvars.iv.unr, 1
  %epil.iter.cmp = icmp eq i64 %xtraiter, 1
  br i1 %epil.iter.cmp, label %for.cond.cleanup, label %for.body.epil.1

for.cond.cleanup:                                 ; preds = %for.cond.cleanup.loopexit.unr-lcssa, %for.body.epil.5, %for.body.epil.4, %for.body.epil.3, %for.body.epil.2, %for.body.epil.1, %for.body.epil, %for.body.epil.6, %entry
  %4 = bitcast i32* %i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %4) #3
  store i32 1234, i32* %i, align 128
  %call = call i32 @other2(i32* nonnull %0, i32 %n, i32* nonnull %i) #3
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %4) #3
  ret i32 %call

for.body:                                         ; preds = %for.body, %for.body.preheader.new
  %indvars.iv = phi i64 [ 0, %for.body.preheader.new ], [ %indvars.iv.next.7, %for.body ]
  %niter = phi i64 [ %unroll_iter, %for.body.preheader.new ], [ %niter.nsub.7, %for.body ]
  %5 = trunc i64 %indvars.iv to i32
  %sub = sub nsw i32 %n, %5
  %arrayidx = getelementptr inbounds i32, i32* %0, i64 %indvars.iv
  store i32 %sub, i32* %arrayidx, align 8
  %indvars.iv.next = or i64 %indvars.iv, 1
  %6 = trunc i64 %indvars.iv.next to i32
  %sub.1 = sub nsw i32 %n, %6
  %arrayidx.1 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next
  store i32 %sub.1, i32* %arrayidx.1, align 4
  %indvars.iv.next.1 = or i64 %indvars.iv, 2
  %7 = trunc i64 %indvars.iv.next.1 to i32
  %sub.2 = sub nsw i32 %n, %7
  %arrayidx.2 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.1
  store i32 %sub.2, i32* %arrayidx.2, align 8
  %indvars.iv.next.2 = or i64 %indvars.iv, 3
  %8 = trunc i64 %indvars.iv.next.2 to i32
  %sub.3 = sub nsw i32 %n, %8
  %arrayidx.3 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.2
  store i32 %sub.3, i32* %arrayidx.3, align 4
  %indvars.iv.next.3 = or i64 %indvars.iv, 4
  %9 = trunc i64 %indvars.iv.next.3 to i32
  %sub.4 = sub nsw i32 %n, %9
  %arrayidx.4 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.3
  store i32 %sub.4, i32* %arrayidx.4, align 8
  %indvars.iv.next.4 = or i64 %indvars.iv, 5
  %10 = trunc i64 %indvars.iv.next.4 to i32
  %sub.5 = sub nsw i32 %n, %10
  %arrayidx.5 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.4
  store i32 %sub.5, i32* %arrayidx.5, align 4
  %indvars.iv.next.5 = or i64 %indvars.iv, 6
  %11 = trunc i64 %indvars.iv.next.5 to i32
  %sub.6 = sub nsw i32 %n, %11
  %arrayidx.6 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.5
  store i32 %sub.6, i32* %arrayidx.6, align 8
  %indvars.iv.next.6 = or i64 %indvars.iv, 7
  %12 = trunc i64 %indvars.iv.next.6 to i32
  %sub.7 = sub nsw i32 %n, %12
  %arrayidx.7 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.6
  store i32 %sub.7, i32* %arrayidx.7, align 4
  %indvars.iv.next.7 = add nuw nsw i64 %indvars.iv, 8
  %niter.nsub.7 = add i64 %niter, -8
  %niter.ncmp.7 = icmp eq i64 %niter.nsub.7, 0
  br i1 %niter.ncmp.7, label %for.cond.cleanup.loopexit.unr-lcssa, label %for.body

for.body.epil.1:                                  ; preds = %for.body.epil
  %13 = trunc i64 %indvars.iv.next.epil to i32
  %sub.epil.1 = sub nsw i32 %n, %13
  %arrayidx.epil.1 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.epil
  store i32 %sub.epil.1, i32* %arrayidx.epil.1, align 4
  %indvars.iv.next.epil.1 = add nuw nsw i64 %indvars.iv.unr, 2
  %epil.iter.cmp.1 = icmp eq i64 %xtraiter, 2
  br i1 %epil.iter.cmp.1, label %for.cond.cleanup, label %for.body.epil.2

for.body.epil.2:                                  ; preds = %for.body.epil.1
  %14 = trunc i64 %indvars.iv.next.epil.1 to i32
  %sub.epil.2 = sub nsw i32 %n, %14
  %arrayidx.epil.2 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.epil.1
  store i32 %sub.epil.2, i32* %arrayidx.epil.2, align 4
  %indvars.iv.next.epil.2 = add nuw nsw i64 %indvars.iv.unr, 3
  %epil.iter.cmp.2 = icmp eq i64 %xtraiter, 3
  br i1 %epil.iter.cmp.2, label %for.cond.cleanup, label %for.body.epil.3

for.body.epil.3:                                  ; preds = %for.body.epil.2
  %15 = trunc i64 %indvars.iv.next.epil.2 to i32
  %sub.epil.3 = sub nsw i32 %n, %15
  %arrayidx.epil.3 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.epil.2
  store i32 %sub.epil.3, i32* %arrayidx.epil.3, align 4
  %indvars.iv.next.epil.3 = add nuw nsw i64 %indvars.iv.unr, 4
  %epil.iter.cmp.3 = icmp eq i64 %xtraiter, 4
  br i1 %epil.iter.cmp.3, label %for.cond.cleanup, label %for.body.epil.4

for.body.epil.4:                                  ; preds = %for.body.epil.3
  %16 = trunc i64 %indvars.iv.next.epil.3 to i32
  %sub.epil.4 = sub nsw i32 %n, %16
  %arrayidx.epil.4 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.epil.3
  store i32 %sub.epil.4, i32* %arrayidx.epil.4, align 4
  %indvars.iv.next.epil.4 = add nuw nsw i64 %indvars.iv.unr, 5
  %epil.iter.cmp.4 = icmp eq i64 %xtraiter, 5
  br i1 %epil.iter.cmp.4, label %for.cond.cleanup, label %for.body.epil.5

for.body.epil.5:                                  ; preds = %for.body.epil.4
  %17 = trunc i64 %indvars.iv.next.epil.4 to i32
  %sub.epil.5 = sub nsw i32 %n, %17
  %arrayidx.epil.5 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.epil.4
  store i32 %sub.epil.5, i32* %arrayidx.epil.5, align 4
  %indvars.iv.next.epil.5 = add nuw nsw i64 %indvars.iv.unr, 6
  %epil.iter.cmp.5 = icmp eq i64 %xtraiter, 6
  br i1 %epil.iter.cmp.5, label %for.cond.cleanup, label %for.body.epil.6

for.body.epil.6:                                  ; preds = %for.body.epil.5
  %18 = trunc i64 %indvars.iv.next.epil.5 to i32
  %sub.epil.6 = sub nsw i32 %n, %18
  %arrayidx.epil.6 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.epil.5
  store i32 %sub.epil.6, i32* %arrayidx.epil.6, align 4
  br label %for.cond.cleanup
}

declare i32 @other2(i32*, i32, i32*) #2

define i64 @stackrealign3(i32 %x){
; CHECK-LABEL: stackrealign3:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -640
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 640
; CHECK-NEXT:    copyd $r32 = $r12
; CHECK-NEXT:    addd $r12 = $r12, 96
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_register 32
; CHECK-NEXT:    andd $r12 = $r12, -128
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 16[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_escape 0x10, 0x43, 0x02, 0x7c, 0x10
; CHECK-NEXT:    sd 8[$r12] = $r14
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_escape 0x10, 0x0e, 0x02, 0x7c, 0x08
; CHECK-NEXT:    sd 0[$r12] = $r18
; CHECK-NEXT:    copyd $r14 = $r32
; CHECK-NEXT:    copyd $r18 = $r0
; CHECK-NEXT:    make $r0 = 4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_escape 0x10, 0x12, 0x02, 0x7c, 0x00
; CHECK-NEXT:    .cfi_def_cfa_register 14
; CHECK-NEXT:    addd $r15 = $r12, 384
; CHECK-NEXT:    make $r1 = 3
; CHECK-NEXT:    make $r2 = 2
; CHECK-NEXT:    make $r3 = 1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    so 224[$r12] = $r0r1r2r3
; CHECK-NEXT:    make $r0 = 8
; CHECK-NEXT:    make $r1 = 7
; CHECK-NEXT:    make $r2 = 6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r3 = 5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    so 192[$r12] = $r0r1r2r3
; CHECK-NEXT:    make $r0 = 12
; CHECK-NEXT:    make $r1 = 11
; CHECK-NEXT:    make $r2 = 10
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r3 = 9
; CHECK-NEXT:    ;;
; CHECK-NEXT:    so 160[$r12] = $r0r1r2r3
; CHECK-NEXT:    make $r0 = 16
; CHECK-NEXT:    make $r1 = 15
; CHECK-NEXT:    make $r2 = 14
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r3 = 13
; CHECK-NEXT:    sw 256[$r12] = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    so 128[$r12] = $r0r1r2r3
; CHECK-NEXT:    addd $r0 = $r12, 256
; CHECK-NEXT:    addd $r1 = $r12, 128
; CHECK-NEXT:    call otherv
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r0 = 384[$r12]
; CHECK-NEXT:    sxwd $r1 = $r18
; CHECK-NEXT:    copyd $r32 = $r14
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r0 = $r0, $r1
; CHECK-NEXT:    ld $r18 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_register 32
; CHECK-NEXT:    ld $r16 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    ld $r14 = 8[$r12]
; CHECK-NEXT:    copyd $r12 = $r32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_restore 67
; CHECK-NEXT:    .cfi_def_cfa_register 12
; CHECK-NEXT:    addd $r12 = $r12, 640
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %i = alloca <16 x i64>, align 128
  %a = alloca i32, align 4
  %tmp = alloca <16 x i64>, align 128
  %0 = bitcast <16 x i64>* %i to i8*
  call void @llvm.lifetime.start.p0i8(i64 128, i8* nonnull %0) #3
  store <16 x i64> <i64 16, i64 15, i64 14, i64 13, i64 12, i64 11, i64 10, i64 9, i64 8, i64 7, i64 6, i64 5, i64 4, i64 3, i64 2, i64 1>, <16 x i64>* %i, align 128
  %1 = bitcast i32* %a to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #3
  store i32 %x, i32* %a, align 4
  call void @otherv(<16 x i64>* nonnull sret %tmp, i32* nonnull %a, <16 x i64>* nonnull %i) #3
  %2 = load <16 x i64>, <16 x i64>* %tmp, align 128
  %vecext = extractelement <16 x i64> %2, i32 0
  %conv = sext i32 %x to i64
  %add = add nsw i64 %vecext, %conv
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #3
  call void @llvm.lifetime.end.p0i8(i64 128, i8* nonnull %0) #3
  ret i64 %add
}

declare void @otherv(<16 x i64>* sret, i32*, <16 x i64>*) #2

define i32 @stackrealign4(i32 %n){
; CHECK-LABEL: stackrealign4:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addd $r12 = $r12, -384
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 384
; CHECK-NEXT:    copyd $r32 = $r12
; CHECK-NEXT:    addd $r12 = $r12, 96
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_register 32
; CHECK-NEXT:    andd $r12 = $r12, -128
; CHECK-NEXT:    copyd $r33 = $r14
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r14 = $r12
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 120[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_escape 0x10, 0x43, 0x03, 0x7e, 0xf8, 0x00
; CHECK-NEXT:    sd 112[$r12] = $r33
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_escape 0x10, 0x0e, 0x03, 0x7e, 0xf0, 0x00
; CHECK-NEXT:    so 80[$r12] = $r28r29r30r31
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_escape 0x10, 0x1f, 0x03, 0x7e, 0xe8, 0x00
; CHECK-NEXT:    .cfi_escape 0x10, 0x1e, 0x03, 0x7e, 0xe0, 0x00
; CHECK-NEXT:    .cfi_escape 0x10, 0x1d, 0x03, 0x7e, 0xd8, 0x00
; CHECK-NEXT:    .cfi_escape 0x10, 0x1c, 0x03, 0x7e, 0xd0, 0x00
; CHECK-NEXT:    so 48[$r12] = $r24r25r26r27
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_escape 0x10, 0x1b, 0x03, 0x7e, 0xc8, 0x00
; CHECK-NEXT:    .cfi_escape 0x10, 0x1a, 0x03, 0x7e, 0xc0, 0x00
; CHECK-NEXT:    .cfi_escape 0x10, 0x19, 0x02, 0x7e, 0x38
; CHECK-NEXT:    .cfi_escape 0x10, 0x18, 0x02, 0x7e, 0x30
; CHECK-NEXT:    so 16[$r12] = $r20r21r22r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_escape 0x10, 0x17, 0x02, 0x7e, 0x28
; CHECK-NEXT:    .cfi_escape 0x10, 0x16, 0x02, 0x7e, 0x20
; CHECK-NEXT:    .cfi_escape 0x10, 0x15, 0x02, 0x7e, 0x18
; CHECK-NEXT:    .cfi_escape 0x10, 0x14, 0x02, 0x7e, 0x10
; CHECK-NEXT:    sq 0[$r12] = $r18r19
; CHECK-NEXT:    copyd $r31 = $r32
; CHECK-NEXT:    copyd $r1 = $r0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_escape 0x10, 0x13, 0x02, 0x7e, 0x08
; CHECK-NEXT:    .cfi_escape 0x10, 0x12, 0x02, 0x7e, 0x00
; CHECK-NEXT:    .cfi_def_cfa_register 31
; CHECK-NEXT:    sxwd $r0 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addx4d $r0 = $r0, 31
; CHECK-NEXT:    ;;
; CHECK-NEXT:    andd $r0 = $r0, -32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfd $r0 = $r0, $r12
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r12 = $r0
; CHECK-NEXT:    cb.wlez $r1 ? .LBB3_12
; CHECK-NEXT:    ;;
; CHECK-NEXT:  # %bb.1: # %for.body.preheader
; CHECK-NEXT:    zxwd $r4 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r3 = $r4, -1
; CHECK-NEXT:    andd $r2 = $r4, 7
; CHECK-NEXT:    ;;
; CHECK-NEXT:    compd.ltu $r5 = $r3, 7
; CHECK-NEXT:    make $r3 = 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:    cb.odd $r5 ? .LBB3_4
; CHECK-NEXT:    ;;
; CHECK-NEXT:  # %bb.2: # %for.body.preheader.new
; CHECK-NEXT:    sbfd $r4 = $r2, $r4
; CHECK-NEXT:    addw $r5 = $r1, -7
; CHECK-NEXT:    addd $r6 = $r0, 16
; CHECK-NEXT:    make $r3 = 0
; CHECK-NEXT:    ;;
; CHECK-NEXT:  .LBB3_3: # %for.body
; CHECK-NEXT:    # =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    addw $r7 = $r5, 7
; CHECK-NEXT:    addd $r3 = $r3, 8
; CHECK-NEXT:    sw 12[$r6] = $r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw -16[$r6] = $r7
; CHECK-NEXT:    addw $r7 = $r5, 6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw -12[$r6] = $r7
; CHECK-NEXT:    addw $r7 = $r5, 5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw -8[$r6] = $r7
; CHECK-NEXT:    addw $r7 = $r5, 4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw -4[$r6] = $r7
; CHECK-NEXT:    addw $r7 = $r5, 3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 0[$r6] = $r7
; CHECK-NEXT:    addw $r7 = $r5, 2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 4[$r6] = $r7
; CHECK-NEXT:    addw $r7 = $r5, 1
; CHECK-NEXT:    addw $r5 = $r5, -8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 8[$r6] = $r7
; CHECK-NEXT:    compd.eq $r7 = $r4, $r3
; CHECK-NEXT:    addd $r6 = $r6, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    cb.even $r7 ? .LBB3_3
; CHECK-NEXT:    ;;
; CHECK-NEXT:  .LBB3_4: # %for.cond.cleanup.loopexit.unr-lcssa
; CHECK-NEXT:    cb.deqz $r2 ? .LBB3_12
; CHECK-NEXT:    ;;
; CHECK-NEXT:  # %bb.5: # %for.body.epil
; CHECK-NEXT:    zxwd $r4 = $r3
; CHECK-NEXT:    compd.ne $r5 = $r2, 1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfw $r4 = $r4, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw.xs $r3[$r0] = $r4
; CHECK-NEXT:    cb.even $r5 ? .LBB3_12
; CHECK-NEXT:    ;;
; CHECK-NEXT:  # %bb.6: # %for.body.epil.1
; CHECK-NEXT:    addd $r4 = $r3, 1
; CHECK-NEXT:    compd.eq $r6 = $r2, 2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r5 = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfw $r5 = $r5, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw.xs $r4[$r0] = $r5
; CHECK-NEXT:    cb.odd $r6 ? .LBB3_12
; CHECK-NEXT:    ;;
; CHECK-NEXT:  # %bb.7: # %for.body.epil.2
; CHECK-NEXT:    addd $r4 = $r3, 2
; CHECK-NEXT:    compd.eq $r6 = $r2, 3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r5 = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfw $r5 = $r5, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw.xs $r4[$r0] = $r5
; CHECK-NEXT:    cb.odd $r6 ? .LBB3_12
; CHECK-NEXT:    ;;
; CHECK-NEXT:  # %bb.8: # %for.body.epil.3
; CHECK-NEXT:    addd $r4 = $r3, 3
; CHECK-NEXT:    compd.eq $r6 = $r2, 4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r5 = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfw $r5 = $r5, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw.xs $r4[$r0] = $r5
; CHECK-NEXT:    cb.odd $r6 ? .LBB3_12
; CHECK-NEXT:    ;;
; CHECK-NEXT:  # %bb.9: # %for.body.epil.4
; CHECK-NEXT:    addd $r4 = $r3, 4
; CHECK-NEXT:    compd.eq $r6 = $r2, 5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r5 = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfw $r5 = $r5, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw.xs $r4[$r0] = $r5
; CHECK-NEXT:    cb.odd $r6 ? .LBB3_12
; CHECK-NEXT:    ;;
; CHECK-NEXT:  # %bb.10: # %for.body.epil.5
; CHECK-NEXT:    addd $r4 = $r3, 5
; CHECK-NEXT:    compd.eq $r2 = $r2, 6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r5 = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfw $r5 = $r5, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw.xs $r4[$r0] = $r5
; CHECK-NEXT:    cb.odd $r2 ? .LBB3_12
; CHECK-NEXT:    ;;
; CHECK-NEXT:  # %bb.11: # %for.body.epil.6
; CHECK-NEXT:    addd $r2 = $r3, 6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    zxwd $r3 = $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfw $r3 = $r3, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw.xs $r2[$r0] = $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:  .LBB3_12: # %for.cond.cleanup
; CHECK-NEXT:    make $r2 = g1
; CHECK-NEXT:    make $r41 = 0x4d2
; CHECK-NEXT:    make $r16 = g12
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r17 = g13
; CHECK-NEXT:    lwz $r2 = 0[$r2]
; CHECK-NEXT:    make $r32 = g14
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r33 = g15
; CHECK-NEXT:    make $r34 = g16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r35 = g17
; CHECK-NEXT:    make $r36 = g18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r37 = g19
; CHECK-NEXT:    make $r38 = g20
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r39 = g21
; CHECK-NEXT:    make $r40 = g22
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r42 = g23
; CHECK-NEXT:    make $r43 = g24
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r44 = g25
; CHECK-NEXT:    make $r45 = g26
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r46 = g27
; CHECK-NEXT:    make $r47 = g28
; CHECK-NEXT:    sd 136[$r14] = $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r2 = g2
; CHECK-NEXT:    make $r48 = g29
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r49 = g30
; CHECK-NEXT:    make $r50 = g31
; CHECK-NEXT:    lwz $r2 = 0[$r2]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r51 = g32
; CHECK-NEXT:    make $r52 = g33
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r53 = g34
; CHECK-NEXT:    make $r54 = g35
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r55 = g36
; CHECK-NEXT:    make $r56 = g37
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r57 = g38
; CHECK-NEXT:    make $r58 = g39
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r59 = g40
; CHECK-NEXT:    make $r60 = g41
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r61 = g42
; CHECK-NEXT:    make $r62 = g43
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r63 = g44
; CHECK-NEXT:    make $r18 = g45
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r19 = g46
; CHECK-NEXT:    sd 144[$r14] = $r2
; CHECK-NEXT:    make $r2 = g3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r20 = g47
; CHECK-NEXT:    make $r21 = g48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r22 = g49
; CHECK-NEXT:    lwz $r2 = 0[$r2]
; CHECK-NEXT:    make $r23 = g50
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r24 = g51
; CHECK-NEXT:    make $r25 = g52
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r26 = g53
; CHECK-NEXT:    make $r27 = g54
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r28 = g55
; CHECK-NEXT:    make $r29 = g56
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r30 = g57
; CHECK-NEXT:    make $r15 = g58
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r3 = g59
; CHECK-NEXT:    make $r4 = g60
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r5 = g61
; CHECK-NEXT:    make $r6 = g62
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r7 = g63
; CHECK-NEXT:    make $r8 = g64
; CHECK-NEXT:    sd 152[$r14] = $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    make $r2 = g4
; CHECK-NEXT:    make $r9 = g65
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r2 = 0[$r2]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 160[$r14] = $r2
; CHECK-NEXT:    make $r2 = g5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r2 = 0[$r2]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 168[$r14] = $r2
; CHECK-NEXT:    make $r2 = g6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r2 = 0[$r2]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 176[$r14] = $r2
; CHECK-NEXT:    make $r2 = g7
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r2 = 0[$r2]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 184[$r14] = $r2
; CHECK-NEXT:    make $r2 = g8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r2 = 0[$r2]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 192[$r14] = $r2
; CHECK-NEXT:    make $r2 = g9
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 128[$r14] = $r41
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r11 = 0[$r2]
; CHECK-NEXT:    make $r2 = g10
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r10 = 0[$r2]
; CHECK-NEXT:    make $r2 = g11
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r2 = 0[$r2]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r16 = 0[$r16]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r17 = 0[$r17]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r32 = 0[$r32]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r33 = 0[$r33]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r34 = 0[$r34]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r35 = 0[$r35]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r36 = 0[$r36]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r37 = 0[$r37]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r38 = 0[$r38]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r39 = 0[$r39]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r40 = 0[$r40]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r42 = 0[$r42]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r43 = 0[$r43]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r44 = 0[$r44]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r45 = 0[$r45]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r46 = 0[$r46]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r47 = 0[$r47]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r48 = 0[$r48]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r49 = 0[$r49]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r50 = 0[$r50]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r51 = 0[$r51]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r52 = 0[$r52]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r53 = 0[$r53]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r54 = 0[$r54]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r55 = 0[$r55]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r56 = 0[$r56]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r57 = 0[$r57]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r58 = 0[$r58]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r59 = 0[$r59]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r60 = 0[$r60]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r61 = 0[$r61]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r62 = 0[$r62]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r63 = 0[$r63]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r18 = 0[$r18]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r19 = 0[$r19]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r20 = 0[$r20]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r21 = 0[$r21]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r22 = 0[$r22]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r23 = 0[$r23]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r24 = 0[$r24]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r25 = 0[$r25]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r26 = 0[$r26]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r27 = 0[$r27]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r28 = 0[$r28]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r29 = 0[$r29]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r30 = 0[$r30]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r15 = 0[$r15]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r3 = 0[$r3]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r4 = 0[$r4]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r5 = 0[$r5]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r6 = 0[$r6]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r7 = 0[$r7]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r8 = 0[$r8]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz $r9 = 0[$r9]
; CHECK-NEXT:    addd $r12 = $r12, -448
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 440[$r12] = $r9
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 432[$r12] = $r8
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 424[$r12] = $r7
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 416[$r12] = $r6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 408[$r12] = $r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 400[$r12] = $r4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 392[$r12] = $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 384[$r12] = $r15
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 376[$r12] = $r30
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 368[$r12] = $r29
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 360[$r12] = $r28
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 352[$r12] = $r27
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 344[$r12] = $r26
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 336[$r12] = $r25
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 328[$r12] = $r24
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 320[$r12] = $r23
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 312[$r12] = $r22
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 304[$r12] = $r21
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 296[$r12] = $r20
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 288[$r12] = $r19
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 280[$r12] = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 272[$r12] = $r63
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 264[$r12] = $r62
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 256[$r12] = $r61
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 248[$r12] = $r60
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 240[$r12] = $r59
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 232[$r12] = $r58
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 224[$r12] = $r57
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 216[$r12] = $r56
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 208[$r12] = $r55
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 200[$r12] = $r54
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 192[$r12] = $r53
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 184[$r12] = $r52
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 176[$r12] = $r51
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 168[$r12] = $r50
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 160[$r12] = $r49
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 152[$r12] = $r48
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 144[$r12] = $r47
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 136[$r12] = $r46
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 128[$r12] = $r45
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 120[$r12] = $r44
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 112[$r12] = $r43
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 104[$r12] = $r42
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 96[$r12] = $r40
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 88[$r12] = $r39
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 80[$r12] = $r38
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 72[$r12] = $r37
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 64[$r12] = $r36
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 56[$r12] = $r35
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 48[$r12] = $r34
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 40[$r12] = $r33
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 32[$r12] = $r32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 24[$r12] = $r17
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 16[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 8[$r12] = $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sw 0[$r12] = $r10
; CHECK-NEXT:    addd $r2 = $r14, 128
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r3 = 136[$r14]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r4 = 144[$r14]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r5 = 152[$r14]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r6 = 160[$r14]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r7 = 168[$r14]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r8 = 176[$r14]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r9 = 184[$r14]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r10 = 192[$r14]
; CHECK-NEXT:    call other4
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r12 = $r12, 448
; CHECK-NEXT:    copyd $r32 = $r31
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_register 32
; CHECK-NEXT:    copyd $r12 = $r14
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lq $r18r19 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r20r21r22r23 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r24r25r26r27 = 48[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lo $r28r29r30r31 = 80[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 120[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    ld $r14 = 112[$r12]
; CHECK-NEXT:    copyd $r12 = $r32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_restore 67
; CHECK-NEXT:    .cfi_def_cfa_register 12
; CHECK-NEXT:    addd $r12 = $r12, 384
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  %i = alloca i32, align 128
  %conv = sext i32 %n to i64
  %0 = alloca i32, i64 %conv, align 8
  %cmp11 = icmp sgt i32 %n, 0
  br i1 %cmp11, label %for.body.preheader, label %for.cond.cleanup

for.body.preheader:                               ; preds = %entry
  %wide.trip.count = zext i32 %n to i64
  %1 = add nsw i64 %wide.trip.count, -1
  %xtraiter = and i64 %wide.trip.count, 7
  %2 = icmp ult i64 %1, 7
  br i1 %2, label %for.cond.cleanup.loopexit.unr-lcssa, label %for.body.preheader.new

for.body.preheader.new:                           ; preds = %for.body.preheader
  %unroll_iter = sub nsw i64 %wide.trip.count, %xtraiter
  br label %for.body

for.cond.cleanup.loopexit.unr-lcssa:              ; preds = %for.body, %for.body.preheader
  %indvars.iv.unr = phi i64 [ 0, %for.body.preheader ], [ %indvars.iv.next.7, %for.body ]
  %lcmp.mod = icmp eq i64 %xtraiter, 0
  br i1 %lcmp.mod, label %for.cond.cleanup, label %for.body.epil

for.body.epil:                                    ; preds = %for.cond.cleanup.loopexit.unr-lcssa
  %3 = trunc i64 %indvars.iv.unr to i32
  %sub.epil = sub nsw i32 %n, %3
  %arrayidx.epil = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.unr
  store i32 %sub.epil, i32* %arrayidx.epil, align 4
  %indvars.iv.next.epil = add nuw nsw i64 %indvars.iv.unr, 1
  %epil.iter.cmp = icmp eq i64 %xtraiter, 1
  br i1 %epil.iter.cmp, label %for.cond.cleanup, label %for.body.epil.1

for.cond.cleanup:                                 ; preds = %for.cond.cleanup.loopexit.unr-lcssa, %for.body.epil.5, %for.body.epil.4, %for.body.epil.3, %for.body.epil.2, %for.body.epil.1, %for.body.epil, %for.body.epil.6, %entry
  %4 = bitcast i32* %i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %4) #3
  store i32 1234, i32* %i, align 128
  %5 = load i32, i32* @g1, align 4
  %6 = load i32, i32* @g2, align 4
  %7 = load i32, i32* @g3, align 4
  %8 = load i32, i32* @g4, align 4
  %9 = load i32, i32* @g5, align 4
  %10 = load i32, i32* @g6, align 4
  %11 = load i32, i32* @g7, align 4
  %12 = load i32, i32* @g8, align 4
  %13 = load i32, i32* @g9, align 4
  %14 = load i32, i32* @g10, align 4
  %15 = load i32, i32* @g11, align 4
  %16 = load i32, i32* @g12, align 4
  %17 = load i32, i32* @g13, align 4
  %18 = load i32, i32* @g14, align 4
  %19 = load i32, i32* @g15, align 4
  %20 = load i32, i32* @g16, align 4
  %21 = load i32, i32* @g17, align 4
  %22 = load i32, i32* @g18, align 4
  %23 = load i32, i32* @g19, align 4
  %24 = load i32, i32* @g20, align 4
  %25 = load i32, i32* @g21, align 4
  %26 = load i32, i32* @g22, align 4
  %27 = load i32, i32* @g23, align 4
  %28 = load i32, i32* @g24, align 4
  %29 = load i32, i32* @g25, align 4
  %30 = load i32, i32* @g26, align 4
  %31 = load i32, i32* @g27, align 4
  %32 = load i32, i32* @g28, align 4
  %33 = load i32, i32* @g29, align 4
  %34 = load i32, i32* @g30, align 4
  %35 = load i32, i32* @g31, align 4
  %36 = load i32, i32* @g32, align 4
  %37 = load i32, i32* @g33, align 4
  %38 = load i32, i32* @g34, align 4
  %39 = load i32, i32* @g35, align 4
  %40 = load i32, i32* @g36, align 4
  %41 = load i32, i32* @g37, align 4
  %42 = load i32, i32* @g38, align 4
  %43 = load i32, i32* @g39, align 4
  %44 = load i32, i32* @g40, align 4
  %45 = load i32, i32* @g41, align 4
  %46 = load i32, i32* @g42, align 4
  %47 = load i32, i32* @g43, align 4
  %48 = load i32, i32* @g44, align 4
  %49 = load i32, i32* @g45, align 4
  %50 = load i32, i32* @g46, align 4
  %51 = load i32, i32* @g47, align 4
  %52 = load i32, i32* @g48, align 4
  %53 = load i32, i32* @g49, align 4
  %54 = load i32, i32* @g50, align 4
  %55 = load i32, i32* @g51, align 4
  %56 = load i32, i32* @g52, align 4
  %57 = load i32, i32* @g53, align 4
  %58 = load i32, i32* @g54, align 4
  %59 = load i32, i32* @g55, align 4
  %60 = load i32, i32* @g56, align 4
  %61 = load i32, i32* @g57, align 4
  %62 = load i32, i32* @g58, align 4
  %63 = load i32, i32* @g59, align 4
  %64 = load i32, i32* @g60, align 4
  %65 = load i32, i32* @g61, align 4
  %66 = load i32, i32* @g62, align 4
  %67 = load i32, i32* @g63, align 4
  %68 = load i32, i32* @g64, align 4
  %69 = load i32, i32* @g65, align 4
  %call = call i32 @other4(i32* nonnull %0, i32 %n, i32* nonnull %i, i32 %5, i32 %6, i32 %7, i32 %8, i32 %9, i32 %10, i32 %11, i32 %12, i32 %13, i32 %14, i32 %15, i32 %16, i32 %17, i32 %18, i32 %19, i32 %20, i32 %21, i32 %22, i32 %23, i32 %24, i32 %25, i32 %26, i32 %27, i32 %28, i32 %29, i32 %30, i32 %31, i32 %32, i32 %33, i32 %34, i32 %35, i32 %36, i32 %37, i32 %38, i32 %39, i32 %40, i32 %41, i32 %42, i32 %43, i32 %44, i32 %45, i32 %46, i32 %47, i32 %48, i32 %49, i32 %50, i32 %51, i32 %52, i32 %53, i32 %54, i32 %55, i32 %56, i32 %57, i32 %58, i32 %59, i32 %60, i32 %61, i32 %62, i32 %63, i32 %64, i32 %65, i32 %66, i32 %67, i32 %68, i32 %69) #3
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %4) #3
  ret i32 %call

for.body:                                         ; preds = %for.body, %for.body.preheader.new
  %indvars.iv = phi i64 [ 0, %for.body.preheader.new ], [ %indvars.iv.next.7, %for.body ]
  %niter = phi i64 [ %unroll_iter, %for.body.preheader.new ], [ %niter.nsub.7, %for.body ]
  %70 = trunc i64 %indvars.iv to i32
  %sub = sub nsw i32 %n, %70
  %arrayidx = getelementptr inbounds i32, i32* %0, i64 %indvars.iv
  store i32 %sub, i32* %arrayidx, align 8
  %indvars.iv.next = or i64 %indvars.iv, 1
  %71 = trunc i64 %indvars.iv.next to i32
  %sub.1 = sub nsw i32 %n, %71
  %arrayidx.1 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next
  store i32 %sub.1, i32* %arrayidx.1, align 4
  %indvars.iv.next.1 = or i64 %indvars.iv, 2
  %72 = trunc i64 %indvars.iv.next.1 to i32
  %sub.2 = sub nsw i32 %n, %72
  %arrayidx.2 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.1
  store i32 %sub.2, i32* %arrayidx.2, align 8
  %indvars.iv.next.2 = or i64 %indvars.iv, 3
  %73 = trunc i64 %indvars.iv.next.2 to i32
  %sub.3 = sub nsw i32 %n, %73
  %arrayidx.3 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.2
  store i32 %sub.3, i32* %arrayidx.3, align 4
  %indvars.iv.next.3 = or i64 %indvars.iv, 4
  %74 = trunc i64 %indvars.iv.next.3 to i32
  %sub.4 = sub nsw i32 %n, %74
  %arrayidx.4 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.3
  store i32 %sub.4, i32* %arrayidx.4, align 8
  %indvars.iv.next.4 = or i64 %indvars.iv, 5
  %75 = trunc i64 %indvars.iv.next.4 to i32
  %sub.5 = sub nsw i32 %n, %75
  %arrayidx.5 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.4
  store i32 %sub.5, i32* %arrayidx.5, align 4
  %indvars.iv.next.5 = or i64 %indvars.iv, 6
  %76 = trunc i64 %indvars.iv.next.5 to i32
  %sub.6 = sub nsw i32 %n, %76
  %arrayidx.6 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.5
  store i32 %sub.6, i32* %arrayidx.6, align 8
  %indvars.iv.next.6 = or i64 %indvars.iv, 7
  %77 = trunc i64 %indvars.iv.next.6 to i32
  %sub.7 = sub nsw i32 %n, %77
  %arrayidx.7 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.6
  store i32 %sub.7, i32* %arrayidx.7, align 4
  %indvars.iv.next.7 = add nuw nsw i64 %indvars.iv, 8
  %niter.nsub.7 = add i64 %niter, -8
  %niter.ncmp.7 = icmp eq i64 %niter.nsub.7, 0
  br i1 %niter.ncmp.7, label %for.cond.cleanup.loopexit.unr-lcssa, label %for.body

for.body.epil.1:                                  ; preds = %for.body.epil
  %78 = trunc i64 %indvars.iv.next.epil to i32
  %sub.epil.1 = sub nsw i32 %n, %78
  %arrayidx.epil.1 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.epil
  store i32 %sub.epil.1, i32* %arrayidx.epil.1, align 4
  %indvars.iv.next.epil.1 = add nuw nsw i64 %indvars.iv.unr, 2
  %epil.iter.cmp.1 = icmp eq i64 %xtraiter, 2
  br i1 %epil.iter.cmp.1, label %for.cond.cleanup, label %for.body.epil.2

for.body.epil.2:                                  ; preds = %for.body.epil.1
  %79 = trunc i64 %indvars.iv.next.epil.1 to i32
  %sub.epil.2 = sub nsw i32 %n, %79
  %arrayidx.epil.2 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.epil.1
  store i32 %sub.epil.2, i32* %arrayidx.epil.2, align 4
  %indvars.iv.next.epil.2 = add nuw nsw i64 %indvars.iv.unr, 3
  %epil.iter.cmp.2 = icmp eq i64 %xtraiter, 3
  br i1 %epil.iter.cmp.2, label %for.cond.cleanup, label %for.body.epil.3

for.body.epil.3:                                  ; preds = %for.body.epil.2
  %80 = trunc i64 %indvars.iv.next.epil.2 to i32
  %sub.epil.3 = sub nsw i32 %n, %80
  %arrayidx.epil.3 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.epil.2
  store i32 %sub.epil.3, i32* %arrayidx.epil.3, align 4
  %indvars.iv.next.epil.3 = add nuw nsw i64 %indvars.iv.unr, 4
  %epil.iter.cmp.3 = icmp eq i64 %xtraiter, 4
  br i1 %epil.iter.cmp.3, label %for.cond.cleanup, label %for.body.epil.4

for.body.epil.4:                                  ; preds = %for.body.epil.3
  %81 = trunc i64 %indvars.iv.next.epil.3 to i32
  %sub.epil.4 = sub nsw i32 %n, %81
  %arrayidx.epil.4 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.epil.3
  store i32 %sub.epil.4, i32* %arrayidx.epil.4, align 4
  %indvars.iv.next.epil.4 = add nuw nsw i64 %indvars.iv.unr, 5
  %epil.iter.cmp.4 = icmp eq i64 %xtraiter, 5
  br i1 %epil.iter.cmp.4, label %for.cond.cleanup, label %for.body.epil.5

for.body.epil.5:                                  ; preds = %for.body.epil.4
  %82 = trunc i64 %indvars.iv.next.epil.4 to i32
  %sub.epil.5 = sub nsw i32 %n, %82
  %arrayidx.epil.5 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.epil.4
  store i32 %sub.epil.5, i32* %arrayidx.epil.5, align 4
  %indvars.iv.next.epil.5 = add nuw nsw i64 %indvars.iv.unr, 6
  %epil.iter.cmp.5 = icmp eq i64 %xtraiter, 6
  br i1 %epil.iter.cmp.5, label %for.cond.cleanup, label %for.body.epil.6

for.body.epil.6:                                  ; preds = %for.body.epil.5
  %83 = trunc i64 %indvars.iv.next.epil.5 to i32
  %sub.epil.6 = sub nsw i32 %n, %83
  %arrayidx.epil.6 = getelementptr inbounds i32, i32* %0, i64 %indvars.iv.next.epil.5
  store i32 %sub.epil.6, i32* %arrayidx.epil.6, align 4
  br label %for.cond.cleanup
}

declare i32 @other4(i32*, i32, i32*, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) #2

