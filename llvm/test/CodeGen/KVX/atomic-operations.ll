; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc --asm-verbose=false < %s -mtriple=kvx-kalray-cos | FileCheck %s

; Tests for atomic_fence operations
define i64 @thread_fence(i64 %0) {
; CHECK-LABEL: thread_fence:
; CHECK:         fence
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  fence release
  ret i64 %0
}

define i64 @signal_fence(i64 %0) {
; CHECK-LABEL: signal_fence:
; CHECK:         ret
; CHECK-NEXT:    ;;
  fence syncscope("singlethread") acquire
  ret i64 %0
}


; Tests for atomic_swap operations
define i64 @atomicrmw_i64_xchg(i64* %ptr, i64 %c, i64 %s) {
; CHECK-LABEL: atomicrmw_i64_xchg:
; CHECK:       .LBB2_1:
; CHECK-NEXT:    ld.u $r3 = 0[$r0]
; CHECK-NEXT:    copyd $r2 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    acswapd 0[$r0] = $r2r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    cb.even $r2 ? .LBB2_1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r0 = $r3
; CHECK-NEXT:    fence
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %res = atomicrmw xchg i64* %ptr, i64 %c release
  ret i64 %res
}

define i32 @atomicrmw_i32_xchg(i32* %ptr, i32 %c, i32 %s) {
; CHECK-LABEL: atomicrmw_i32_xchg:
; CHECK:       .LBB3_1:
; CHECK-NEXT:    lwz.u $r3 = 0[$r0]
; CHECK-NEXT:    copyw $r2 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    acswapw 0[$r0] = $r2r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    cb.even $r2 ? .LBB3_1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyw $r0 = $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %res = atomicrmw xchg i32* %ptr, i32 %c monotonic
  ret i32 %res
}

define i64 @atomicrmw_i64_xchg_as(i64 addrspace(1)* %ptr, i64 %c, i64 %s) {
; CHECK-LABEL: atomicrmw_i64_xchg_as:
; CHECK:         addd $r12 = $r12, -32
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    sd 24[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -8
; CHECK-NEXT:    sd 16[$r12] = $r20
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 20, -16
; CHECK-NEXT:    sq 0[$r12] = $r18r19
; CHECK-NEXT:    copyd $r18 = $r1
; CHECK-NEXT:    copyd $r19 = $r0
; CHECK-NEXT:    make $r20 = 5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 19, -24
; CHECK-NEXT:    .cfi_offset 18, -32
; CHECK-NEXT:    copyd $r0 = $r20
; CHECK-NEXT:    call __kvx_atomic_global_in
; CHECK-NEXT:    ;;
; CHECK-NEXT:  .LBB4_1:
; CHECK-NEXT:    ld.u $r1 = 0[$r19]
; CHECK-NEXT:    copyd $r0 = $r18
; CHECK-NEXT:    ;;
; CHECK-NEXT:    acswapd 0[$r19] = $r0r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    cb.even $r0 ? .LBB4_1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r18 = $r1
; CHECK-NEXT:    copyd $r0 = $r20
; CHECK-NEXT:    call __kvx_atomic_global_out
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r0 = $r18
; CHECK-NEXT:    lq $r18r19 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r20 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 24[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %res = atomicrmw xchg i64 addrspace(1)* %ptr, i64 %c seq_cst
  ret i64 %res
}

define i8 @atomic_test_and_set(i8* %ptr) {
; CHECK-LABEL: atomic_test_and_set:
; CHECK:         fence
; CHECK-NEXT:    make $r1 = 1
; CHECK-NEXT:    andd $r3 = $r0, 3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfd $r7 = $r3, 0
; CHECK-NEXT:    slld $r3 = $r3, 3
; CHECK-NEXT:    ;;
; CHECK-NEXT:  .LBB5_1:
; CHECK-NEXT:    lwz.u $r5 = $r7[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    srlw $r2 = $r5, $r3
; CHECK-NEXT:    sllw $r6 = $r1, $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    andw $r2 = $r2, 255
; CHECK-NEXT:    orw $r4 = $r5, $r6
; CHECK-NEXT:    ;;
; CHECK-NEXT:    cb.wnez $r2 ? .LBB5_2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    acswapw $r7[$r0] = $r4r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    cb.even $r4 ? .LBB5_1
; CHECK-NEXT:    ;;
; CHECK-NEXT:  .LBB5_2:
; CHECK-NEXT:    fence
; CHECK-NEXT:    zxbd $r0 = $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    compw.ne $r0 = $r0, 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %res = atomicrmw xchg i8* %ptr, i8 1 seq_cst
  %tobool = icmp ne i8 %res, 0
  %conv = zext i1 %tobool to i8
  ret i8 %conv
}


; Tests for atomic_cmp_swap operations
define i64 @cmpxchg_i64(i64* %ptr, i64 %c, i64 %s) {
; CHECK-LABEL: cmpxchg_i64:
; CHECK:         fence
; CHECK-NEXT:    copyd $r5 = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:  .LBB6_1:
; CHECK-NEXT:    copyd $r4 = $r2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    acswapd 0[$r0] = $r4r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    cb.odd $r4 ? .LBB6_2
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld.u $r3 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    compd.eq $r4 = $r3, $r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    cb.odd $r4 ? .LBB6_1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    goto .LBB6_3
; CHECK-NEXT:    ;;
; CHECK-NEXT:  .LBB6_2:
; CHECK-NEXT:    copyd $r3 = $r5
; CHECK-NEXT:    ;;
; CHECK-NEXT:  .LBB6_3:
; CHECK-NEXT:    copyd $r0 = $r3
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %cx = cmpxchg i64* %ptr, i64 %c, i64 %s seq_cst seq_cst
  %res = extractvalue { i64, i1 } %cx, 0
  ret i64 %res
}


; Tests for atomic_load operations
define i8 @atomic_load_i8(i8 *%ptr) {
; CHECK-LABEL: atomic_load_i8:
; CHECK:         lbz.u $r0 = 0[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %res = load atomic i8, i8 *%ptr monotonic, align 1 ; i.e. relaxed
  ret i8 %res
}

define i16 @atomic_load_i16(i16 *%ptr) {
; CHECK-LABEL: atomic_load_i16:
; CHECK:         fence
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lhz.u $r0 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fence
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %res = load atomic i16, i16 *%ptr seq_cst, align 2
  ret i16 %res
}

define i32 @atomic_load_i32(i32 addrspace(1)*%ptr) {
; CHECK-LABEL: atomic_load_i32:
; CHECK:         addd $r12 = $r12, -32
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    sd 16[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -16
; CHECK-NEXT:    sq 0[$r12] = $r18r19
; CHECK-NEXT:    copyd $r18 = $r0
; CHECK-NEXT:    make $r19 = 5
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 19, -24
; CHECK-NEXT:    .cfi_offset 18, -32
; CHECK-NEXT:    copyd $r0 = $r19
; CHECK-NEXT:    call __kvx_atomic_global_in
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lwz.u $r18 = 0[$r18]
; CHECK-NEXT:    copyd $r0 = $r19
; CHECK-NEXT:    call __kvx_atomic_global_out
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r0 = $r18
; CHECK-NEXT:    lq $r18r19 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %res = load atomic i32, i32 addrspace(1)*%ptr seq_cst, align 4
  ret i32 %res
}


; Tests for atomic_store operations
define void @atomic_store_i32(i32* %ptr, i32 %l) {
; CHECK-LABEL: atomic_store_i32:
; CHECK:         sw 0[$r0] = $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    fence
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  store atomic i32 %l, i32* %ptr release, align 4
  ret void
}

define void @atomic_store_i64(i64 addrspace(1)* %ptr, i64 %l) {
; CHECK-LABEL: atomic_store_i64:
; CHECK:         addd $r12 = $r12, -32
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    sd 24[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -8
; CHECK-NEXT:    sd 16[$r12] = $r20
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 20, -16
; CHECK-NEXT:    sq 0[$r12] = $r18r19
; CHECK-NEXT:    copyd $r18 = $r1
; CHECK-NEXT:    copyd $r19 = $r0
; CHECK-NEXT:    make $r20 = 3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 19, -24
; CHECK-NEXT:    .cfi_offset 18, -32
; CHECK-NEXT:    copyd $r0 = $r20
; CHECK-NEXT:    call __kvx_atomic_global_in
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sd 0[$r19] = $r18
; CHECK-NEXT:    copyd $r0 = $r20
; CHECK-NEXT:    call __kvx_atomic_global_out
; CHECK-NEXT:    ;;
; CHECK-NEXT:    lq $r18r19 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r20 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 24[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  store atomic i64 %l, i64 addrspace(1)* %ptr release, align 8
  ret void
}


; Tests for atomic rmw operations
define i8 @atomicrmw_i8_min(i8 *%src, i8 %b) {
; CHECK-LABEL: atomicrmw_i8_min:
; CHECK:         addd $r12 = $r12, -32
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    sd 0[$r12] = $r16
; CHECK-NEXT:    call __sync_fetch_and_min_1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -32
; CHECK-NEXT:    fence
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %res = atomicrmw min i8 *%src, i8 %b seq_cst
  ret i8 %res
}

define i64 @atomicrmw_i64_max(i64 *%src, i64 %b) {
; CHECK-LABEL: atomicrmw_i64_max:
; CHECK:         fence
; CHECK-NEXT:    ;;
; CHECK-NEXT:  .LBB13_1:
; CHECK-NEXT:    ld.u $r3 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    maxd $r2 = $r1, $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    acswapd 0[$r0] = $r2r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    cb.even $r2 ? .LBB13_1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r0 = $r3
; CHECK-NEXT:    fence
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %res = atomicrmw max i64 *%src, i64 %b seq_cst
  ret i64 %res
}

define i64 @atomicrmw_i64_add(i64 *%src, i64 %b) {
; CHECK-LABEL: atomicrmw_i64_add:
; CHECK:       .LBB14_1:
; CHECK-NEXT:    ld.u $r3 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r2 = $r1, $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    acswapd 0[$r0] = $r2r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    cb.even $r2 ? .LBB14_1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r0 = $r3
; CHECK-NEXT:    fence
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %res = atomicrmw add i64 *%src, i64 %b release
  ret i64 %res
}

define i64 @atomicrmw_i64_sub(i64 *%src, i64 %b) {
; CHECK-LABEL: atomicrmw_i64_sub:
; CHECK:       .LBB15_1:
; CHECK-NEXT:    ld.u $r3 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfd $r2 = $r1, $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    acswapd 0[$r0] = $r2r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    cb.even $r2 ? .LBB15_1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r0 = $r3
; CHECK-NEXT:    fence
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %res = atomicrmw sub i64 *%src, i64 %b release
  ret i64 %res
}

define i64 @atomicrmw_i64_nand(i64 *%src, i64 %b) {
; CHECK-LABEL: atomicrmw_i64_nand:
; CHECK:       .LBB16_1:
; CHECK-NEXT:    ld.u $r3 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    nandd $r2 = $r1, $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    acswapd 0[$r0] = $r2r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    cb.even $r2 ? .LBB16_1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r0 = $r3
; CHECK-NEXT:    fence
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %res = atomicrmw nand i64 *%src, i64 %b release
  ret i64 %res
}

define i32 @atomicrmw_i32_xor(i32 *%src, i32 %b) {
; CHECK-LABEL: atomicrmw_i32_xor:
; CHECK:       .LBB17_1:
; CHECK-NEXT:    lwz.u $r3 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    xorw $r2 = $r1, $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    acswapw 0[$r0] = $r2r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    cb.even $r2 ? .LBB17_1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyw $r0 = $r3
; CHECK-NEXT:    fence
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %res = atomicrmw xor i32 *%src, i32 %b release
  ret i32 %res
}

; Test for __global OpenCL AS
define i64 @atomicrmw_i64_sub_global_as(i64 addrspace(1)*%src, i64 %b) {
; CHECK-LABEL: atomicrmw_i64_sub_global_as:
; CHECK:         addd $r12 = $r12, -32
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    sd 24[$r12] = $r16
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 67, -8
; CHECK-NEXT:    sd 16[$r12] = $r20
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 20, -16
; CHECK-NEXT:    sq 0[$r12] = $r18r19
; CHECK-NEXT:    copyd $r18 = $r1
; CHECK-NEXT:    copyd $r19 = $r0
; CHECK-NEXT:    make $r20 = 3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_offset 19, -24
; CHECK-NEXT:    .cfi_offset 18, -32
; CHECK-NEXT:    copyd $r0 = $r20
; CHECK-NEXT:    call __kvx_atomic_global_in
; CHECK-NEXT:    ;;
; CHECK-NEXT:  .LBB18_1:
; CHECK-NEXT:    ld.u $r1 = 0[$r19]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    sbfd $r0 = $r18, $r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    acswapd 0[$r19] = $r0r1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    cb.even $r0 ? .LBB18_1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r18 = $r1
; CHECK-NEXT:    copyd $r0 = $r20
; CHECK-NEXT:    call __kvx_atomic_global_out
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r0 = $r18
; CHECK-NEXT:    lq $r18r19 = 0[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r20 = 16[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    ld $r16 = 24[$r12]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 32
; CHECK-NEXT:    ;;
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %res = atomicrmw sub i64 addrspace(1)*%src, i64 %b release
  ret i64 %res
}


; Test for immediates that doesn't hold on 37 bits
define i64 @bigimm(i64* %0, i64 %1) {
; CHECK-LABEL: bigimm:
; CHECK:         fence
; CHECK-NEXT:    addd $r0 = $r0, 4398046511104
; CHECK-NEXT:    ;;
; CHECK-NEXT:  .LBB19_1:
; CHECK-NEXT:    ld.u $r3 = 0[$r0]
; CHECK-NEXT:    ;;
; CHECK-NEXT:    addd $r2 = $r1, $r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    acswapd 0[$r0] = $r2r3
; CHECK-NEXT:    ;;
; CHECK-NEXT:    cb.even $r2 ? .LBB19_1
; CHECK-NEXT:    ;;
; CHECK-NEXT:    copyd $r0 = $r3
; CHECK-NEXT:    fence
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %3 = getelementptr inbounds i64, i64* %0, i64 549755813888
  %4 = atomicrmw add i64* %3, i64 %1 seq_cst
  ret i64 %4
}
