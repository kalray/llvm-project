; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mcpu=kv3-1 --asm-verbose=false -verify-machineinstrs -o - %s | FileCheck %s --check-prefixes=CHECK,CV1
; RUN: llc -mcpu=kv3-2 --asm-verbose=false -verify-machineinstrs -o - %s | FileCheck %s --check-prefixes=CHECK,CV2

target triple = "kvx-kalray-cos"

; Tests for atomic_fence operations
define i64 @thread_fence(i64 %0) {
; CHECK-LABEL: thread_fence:
; CHECK:         fence
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  fence release
  ret i64 %0
}

define i64 @signal_fence(i64 %0) {
; CHECK-LABEL: signal_fence:
; CHECK:         ret
; CHECK-NEXT:    ;;
  fence syncscope("singlethread") acquire
  ret i64 %0
}


; Tests for atomic_swap operations
define i64 @atomicrmw_i64_xchg(ptr %ptr, i64 %c, i64 %s) {
; CV1-LABEL: atomicrmw_i64_xchg:
; CV1:         fence
; CV1-NEXT:    copyd $r2 = $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    ld $r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:  .LBB2_1:
; CV1-NEXT:    copyq $r4r5 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    acswapd 0[$r0] = $r4r5
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    ld.u $r1 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    cmoved.even $r4 ? $r3 = $r1
; CV1-NEXT:    cb.even $r4 ? .LBB2_1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    copyd $r0 = $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 0)
;
; CV2-LABEL: atomicrmw_i64_xchg:
; CV2:         fence
; CV2-NEXT:    copyd $r2 = $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    ld $r1 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:  .LBB2_1:
; CV2-NEXT:    copyd $r3 = $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    acswapd.v $r1, [$r0] = $r2r3
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    compd.eq $r3 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    cb.even $r3 ? .LBB2_1
; CV2-NEXT:    ;;
; CV2-NEXT:    copyd $r0 = $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %res = atomicrmw xchg ptr %ptr, i64 %c release
  ret i64 %res
}

define i32 @atomicrmw_i32_xchg(ptr %ptr, i32 %c, i32 %s) {
; CV1-LABEL: atomicrmw_i32_xchg:
; CV1:         copyd $r2 = $r1
; CV1-NEXT:    lwz $r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:  .LBB3_1:
; CV1-NEXT:    copyq $r4r5 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    acswapw 0[$r0] = $r4r5
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    lwz.u $r1 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    cmoved.even $r4 ? $r3 = $r1
; CV1-NEXT:    cb.even $r4 ? .LBB3_1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    copyd $r0 = $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 0)
;
; CV2-LABEL: atomicrmw_i32_xchg:
; CV2:         lwz $r1 = 0[$r0]
; CV2-NEXT:    copyd $r2 = $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:  .LBB3_1:
; CV2-NEXT:    copyd $r3 = $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    acswapw.v $r1, [$r0] = $r2r3
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    compw.eq $r3 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    cb.even $r3 ? .LBB3_1
; CV2-NEXT:    ;;
; CV2-NEXT:    copyd $r0 = $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %res = atomicrmw xchg ptr %ptr, i32 %c monotonic
  ret i32 %res
}

define i64 @atomicrmw_i64_xchg_as(ptr addrspace(1) %ptr, i64 %c, i64 %s) {
; CV1-LABEL: atomicrmw_i64_xchg_as:
; CV1:         addd $r12 = $r12, -32
; CV1-NEXT:    get $r16 = $ra
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sd 24[$r12] = $r16
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sq 8[$r12] = $r20r21
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sd 0[$r12] = $r18
; CV1-NEXT:    make $r0 = 5
; CV1-NEXT:    copyd $r18 = $r0
; CV1-NEXT:    copyd $r20 = $r1
; CV1-NEXT:    call __kvx_atomic_global_in
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    ld $r21 = 0[$r18]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:  .LBB4_1:
; CV1-NEXT:    copyq $r0r1 = $r20, $r21
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    acswapd 0[$r18] = $r0r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    ld.u $r1 = 0[$r18]
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    cb.even $r0 ? .LBB4_1
; CV1-NEXT:    cmoved.even $r0 ? $r21 = $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    make $r0 = 5
; CV1-NEXT:    call __kvx_atomic_global_out
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    copyd $r0 = $r21
; CV1-NEXT:    ld $r18 = 0[$r12]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lq $r20r21 = 8[$r12]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    ld $r16 = 24[$r12]
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    set $ra = $r16
; CV1-NEXT:    addd $r12 = $r12, 32
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: atomicrmw_i64_xchg_as:
; CV2:         addd $r12 = $r12, -64
; CV2-NEXT:    get $r16 = $ra
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sd 56[$r12] = $r16
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sq 40[$r12] = $r20r21
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sq 24[$r12] = $r18r19
; CV2-NEXT:    make $r0 = 5
; CV2-NEXT:    copyd $r18 = $r0
; CV2-NEXT:    copyd $r20 = $r1
; CV2-NEXT:    call __kvx_atomic_global_in
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    ld $r19 = 0[$r18]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:  .LBB4_1:
; CV2-NEXT:    copyd $r21 = $r19
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    acswapd.v.g $r19, [$r18] = $r20r21
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    compd.eq $r0 = $r19, $r21
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    cb.even $r0 ? .LBB4_1
; CV2-NEXT:    ;;
; CV2-NEXT:    make $r0 = 5
; CV2-NEXT:    call __kvx_atomic_global_out
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lq $r18r19 = 24[$r12]
; CV2-NEXT:    copyd $r0 = $r19
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lq $r20r21 = 40[$r12]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    ld $r16 = 56[$r12]
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    set $ra = $r16
; CV2-NEXT:    addd $r12 = $r12, 64
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %res = atomicrmw xchg ptr addrspace(1) %ptr, i64 %c seq_cst
  ret i64 %res
}

define i8 @atomic_test_and_set(ptr %ptr) {
; CV1-LABEL: atomic_test_and_set:
; CV1:         fence
; CV1-NEXT:    andd $r0 = $r0, -4
; CV1-NEXT:    andw $r1 = $r0, 3
; CV1-NEXT:    make $r2 = 255
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sllw $r1 = $r1, 3
; CV1-NEXT:    make $r3 = 1
; CV1-NEXT:    lwz $r5 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sllw $r2 = $r2, $r1
; CV1-NEXT:    sllw $r3 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    notw $r2 = $r2
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:  .LBB5_1:
; CV1-NEXT:    andw $r4 = $r5, $r2
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    iorw $r4 = $r4, $r3
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    copyq $r6r7 = $r4, $r5
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    acswapw 0[$r0] = $r6r7
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    lwz.u $r4 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    cmoved.even $r6 ? $r5 = $r4
; CV1-NEXT:    cb.even $r6 ? .LBB5_1
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    fence
; CV1-NEXT:    srlw $r0 = $r5, $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    compw.ne $r0 = $r0, 0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 2)
;
; CV2-LABEL: atomic_test_and_set:
; CV2:         fence
; CV2-NEXT:    andd $r0 = $r0, -4
; CV2-NEXT:    andw $r1 = $r0, 3
; CV2-NEXT:    make $r2 = 255
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sllw $r1 = $r1, 3
; CV2-NEXT:    make $r3 = 1
; CV2-NEXT:    lwz $r4 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sllw $r2 = $r2, $r1
; CV2-NEXT:    sllw $r3 = $r3, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    notw $r2 = $r2
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:  .LBB5_1:
; CV2-NEXT:    copyd $r5 = $r4
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    andw $r4 = $r5, $r2
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    iorw $r4 = $r4, $r3
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    acswapw.v $r4, [$r0] = $r4r5
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    compw.eq $r5 = $r4, $r5
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    cb.even $r5 ? .LBB5_1
; CV2-NEXT:    ;;
; CV2-NEXT:    fence
; CV2-NEXT:    srlw $r0 = $r4, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    compw.ne $r0 = $r0, 0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 2)
  %res = atomicrmw xchg ptr %ptr, i8 1 seq_cst
  %tobool = icmp ne i8 %res, 0
  %conv = zext i1 %tobool to i8
  ret i8 %conv
}


; Tests for atomic_cmp_swap operations
define i64 @cmpxchg_i64(ptr %ptr, i64 %c, i64 %s) {
; CV1-LABEL: cmpxchg_i64:
; CV1:         fence
; CV1-NEXT:    copyd $r3 = $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    copyq $r4r5 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    acswapd 0[$r0] = $r4r5
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    ld.u $r0 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    cmoved.even $r4 ? $r3 = $r0
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    copyd $r0 = $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: cmpxchg_i64:
; CV2:         fence
; CV2-NEXT:    copyd $r3 = $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    acswapd.v $r0, [$r0] = $r2r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 1)
  %cx = cmpxchg ptr %ptr, i64 %c, i64 %s seq_cst seq_cst
  %res = extractvalue { i64, i1 } %cx, 0
  ret i64 %res
}


; Tests for atomic_load operations
define i8 @atomic_load_i8(ptr %ptr) {
; CHECK-LABEL: atomic_load_i8:
; CHECK:         lbz.u $r0 = 0[$r0]
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 0)
  %res = load atomic i8, ptr %ptr monotonic, align 1 ; i.e. relaxed
  ret i8 %res
}

define i16 @atomic_load_i16(ptr %ptr) {
; CHECK-LABEL: atomic_load_i16:
; CHECK:         fence
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    lhz.u $r0 = 0[$r0]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    fence
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 2)
  %res = load atomic i16, ptr %ptr seq_cst, align 2
  ret i16 %res
}

define i32 @atomic_load_i32(ptr addrspace(1)%ptr) {
; CHECK-LABEL: atomic_load_i32:
; CHECK:         addd $r12 = $r12, -32
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    sd 24[$r12] = $r16
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    sd 16[$r12] = $r18
; CHECK-NEXT:    make $r0 = 5
; CHECK-NEXT:    copyd $r18 = $r0
; CHECK-NEXT:    call __kvx_atomic_global_in
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    make $r0 = 5
; CHECK-NEXT:    lwz.u $r18 = 0[$r18]
; CHECK-NEXT:    call __kvx_atomic_global_out
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    copyd $r0 = $r18
; CHECK-NEXT:    ld $r18 = 16[$r12]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    ld $r16 = 24[$r12]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 32
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
  %res = load atomic i32, ptr addrspace(1)%ptr seq_cst, align 4
  ret i32 %res
}


; Tests for atomic_store operations
define void @atomic_store_i32(ptr %ptr, i32 %l) {
; CHECK-LABEL: atomic_store_i32:
; CHECK:         fence
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    sw 0[$r0] = $r1
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;; # (end cycle 1)
entry:
  store atomic i32 %l, ptr %ptr release, align 4
  ret void
}

define void @atomic_store_i64(ptr addrspace(1) %ptr, i64 %l) {
; CHECK-LABEL: atomic_store_i64:
; CHECK:         addd $r12 = $r12, -32
; CHECK-NEXT:    get $r16 = $ra
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    sd 24[$r12] = $r16
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    sq 8[$r12] = $r18r19
; CHECK-NEXT:    make $r0 = 3
; CHECK-NEXT:    copyd $r18 = $r1
; CHECK-NEXT:    copyd $r19 = $r0
; CHECK-NEXT:    call __kvx_atomic_global_in
; CHECK-NEXT:    ;; # (end cycle 2)
; CHECK-NEXT:    sd 0[$r19] = $r18
; CHECK-NEXT:    make $r0 = 3
; CHECK-NEXT:    call __kvx_atomic_global_out
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    lq $r18r19 = 8[$r12]
; CHECK-NEXT:    ;; # (end cycle 0)
; CHECK-NEXT:    ld $r16 = 24[$r12]
; CHECK-NEXT:    ;; # (end cycle 1)
; CHECK-NEXT:    set $ra = $r16
; CHECK-NEXT:    addd $r12 = $r12, 32
; CHECK-NEXT:    ;; # (end cycle 6)
; CHECK-NEXT:    ret
; CHECK-NEXT:    ;;
entry:
  store atomic i64 %l, ptr addrspace(1) %ptr release, align 8
  ret void
}


; Tests for atomic rmw operations
define i8 @atomicrmw_i8_min(ptr %src, i8 %b) {
; CV1-LABEL: atomicrmw_i8_min:
; CV1:         fence
; CV1-NEXT:    andd $r0 = $r0, -4
; CV1-NEXT:    andw $r2 = $r0, 3
; CV1-NEXT:    make $r3 = 255
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    sllw $r2 = $r2, 3
; CV1-NEXT:    lwz $r5 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sllw $r3 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    notw $r3 = $r3
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:  .LBB12_1:
; CV1-NEXT:    srlw $r4 = $r5, $r2
; CV1-NEXT:    andw $r6 = $r5, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    minw $r4 = $r4, $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    zxbd $r4 = $r4
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sllw $r4 = $r4, $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    iorw $r4 = $r6, $r4
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    copyq $r6r7 = $r4, $r5
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    acswapw 0[$r0] = $r6r7
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    lwz.u $r4 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    cmoved.even $r6 ? $r5 = $r4
; CV1-NEXT:    cb.even $r6 ? .LBB12_1
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    fence
; CV1-NEXT:    srlw $r0 = $r5, $r2
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 0)
;
; CV2-LABEL: atomicrmw_i8_min:
; CV2:         fence
; CV2-NEXT:    andd $r0 = $r0, -4
; CV2-NEXT:    andw $r2 = $r0, 3
; CV2-NEXT:    make $r3 = 255
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sxbd $r1 = $r1
; CV2-NEXT:    sllw $r2 = $r2, 3
; CV2-NEXT:    lwz $r4 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sllw $r3 = $r3, $r2
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    notw $r3 = $r3
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:  .LBB12_1:
; CV2-NEXT:    copyd $r5 = $r4
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srlw $r4 = $r5, $r2
; CV2-NEXT:    andw $r6 = $r5, $r3
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sxbd $r4 = $r4
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    minw $r4 = $r4, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    zxbd $r4 = $r4
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sllw $r4 = $r4, $r2
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    iorw $r4 = $r6, $r4
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    acswapw.v $r4, [$r0] = $r4r5
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    compw.eq $r5 = $r4, $r5
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    cb.even $r5 ? .LBB12_1
; CV2-NEXT:    ;;
; CV2-NEXT:    fence
; CV2-NEXT:    srlw $r0 = $r4, $r2
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %res = atomicrmw min ptr %src, i8 %b seq_cst
  ret i8 %res
}

define i64 @atomicrmw_i64_max(ptr %src, i64 %b) {
; CV1-LABEL: atomicrmw_i64_max:
; CV1:         fence
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    ld $r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:  .LBB13_1:
; CV1-NEXT:    maxd $r2 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    copyq $r4r5 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    acswapd 0[$r0] = $r4r5
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    ld.u $r2 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    cmoved.even $r4 ? $r3 = $r2
; CV1-NEXT:    cb.even $r4 ? .LBB13_1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    fence
; CV1-NEXT:    copyd $r0 = $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 0)
;
; CV2-LABEL: atomicrmw_i64_max:
; CV2:         fence
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    ld $r2 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:  .LBB13_1:
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    maxd $r2 = $r3, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    acswapd.v $r2, [$r0] = $r2r3
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    compd.eq $r3 = $r2, $r3
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    cb.even $r3 ? .LBB13_1
; CV2-NEXT:    ;;
; CV2-NEXT:    fence
; CV2-NEXT:    copyd $r0 = $r2
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %res = atomicrmw max ptr %src, i64 %b seq_cst
  ret i64 %res
}

define i64 @atomicrmw_i64_add(ptr %src, i64 %b) {
; CV1-LABEL: atomicrmw_i64_add:
; CV1:         fence
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    ld $r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:  .LBB14_1:
; CV1-NEXT:    addd $r2 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    copyq $r4r5 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    acswapd 0[$r0] = $r4r5
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    ld.u $r2 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    cmoved.even $r4 ? $r3 = $r2
; CV1-NEXT:    cb.even $r4 ? .LBB14_1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    copyd $r0 = $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 0)
;
; CV2-LABEL: atomicrmw_i64_add:
; CV2:         fence
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    ld $r2 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:  .LBB14_1:
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    addd $r2 = $r3, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    acswapd.v $r2, [$r0] = $r2r3
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    compd.eq $r3 = $r2, $r3
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    cb.even $r3 ? .LBB14_1
; CV2-NEXT:    ;;
; CV2-NEXT:    copyd $r0 = $r2
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %res = atomicrmw add ptr %src, i64 %b release
  ret i64 %res
}

define i64 @atomicrmw_i64_sub(ptr %src, i64 %b) {
; CV1-LABEL: atomicrmw_i64_sub:
; CV1:         fence
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    ld $r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:  .LBB15_1:
; CV1-NEXT:    sbfd $r2 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    copyq $r4r5 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    acswapd 0[$r0] = $r4r5
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    ld.u $r2 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    cmoved.even $r4 ? $r3 = $r2
; CV1-NEXT:    cb.even $r4 ? .LBB15_1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    copyd $r0 = $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 0)
;
; CV2-LABEL: atomicrmw_i64_sub:
; CV2:         fence
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    ld $r2 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:  .LBB15_1:
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sbfd $r2 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    acswapd.v $r2, [$r0] = $r2r3
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    compd.eq $r3 = $r2, $r3
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    cb.even $r3 ? .LBB15_1
; CV2-NEXT:    ;;
; CV2-NEXT:    copyd $r0 = $r2
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %res = atomicrmw sub ptr %src, i64 %b release
  ret i64 %res
}

define i64 @atomicrmw_i64_nand(ptr %src, i64 %b) {
; CV1-LABEL: atomicrmw_i64_nand:
; CV1:         fence
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    ld $r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:  .LBB16_1:
; CV1-NEXT:    nandd $r2 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    copyq $r4r5 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    acswapd 0[$r0] = $r4r5
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    ld.u $r2 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    cmoved.even $r4 ? $r3 = $r2
; CV1-NEXT:    cb.even $r4 ? .LBB16_1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    copyd $r0 = $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 0)
;
; CV2-LABEL: atomicrmw_i64_nand:
; CV2:         fence
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    ld $r2 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:  .LBB16_1:
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    nandd $r2 = $r3, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    acswapd.v $r2, [$r0] = $r2r3
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    compd.eq $r3 = $r2, $r3
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    cb.even $r3 ? .LBB16_1
; CV2-NEXT:    ;;
; CV2-NEXT:    copyd $r0 = $r2
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %res = atomicrmw nand ptr %src, i64 %b release
  ret i64 %res
}

define i32 @atomicrmw_i32_xor(ptr %src, i32 %b) {
; CV1-LABEL: atomicrmw_i32_xor:
; CV1:         fence
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lwz $r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:  .LBB17_1:
; CV1-NEXT:    eorw $r2 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    copyq $r4r5 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    acswapw 0[$r0] = $r4r5
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    lwz.u $r2 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    cmoved.even $r4 ? $r3 = $r2
; CV1-NEXT:    cb.even $r4 ? .LBB17_1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    copyd $r0 = $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 0)
;
; CV2-LABEL: atomicrmw_i32_xor:
; CV2:         fence
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lwz $r2 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:  .LBB17_1:
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    eorw $r2 = $r3, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    acswapw.v $r2, [$r0] = $r2r3
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    compw.eq $r3 = $r2, $r3
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    cb.even $r3 ? .LBB17_1
; CV2-NEXT:    ;;
; CV2-NEXT:    copyd $r0 = $r2
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %res = atomicrmw xor ptr %src, i32 %b release
  ret i32 %res
}

; Test for __global OpenCL AS
define i64 @atomicrmw_i64_sub_global_as(ptr addrspace(1)%src, i64 %b) {
; CV1-LABEL: atomicrmw_i64_sub_global_as:
; CV1:         addd $r12 = $r12, -64
; CV1-NEXT:    get $r16 = $ra
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sd 56[$r12] = $r16
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sq 40[$r12] = $r20r21
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sq 24[$r12] = $r18r19
; CV1-NEXT:    make $r0 = 3
; CV1-NEXT:    copyd $r18 = $r1
; CV1-NEXT:    copyd $r19 = $r0
; CV1-NEXT:    call __kvx_atomic_global_in
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    ld $r21 = 0[$r19]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:  .LBB18_1:
; CV1-NEXT:    sbfd $r20 = $r18, $r21
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    copyq $r0r1 = $r20, $r21
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    acswapd 0[$r19] = $r0r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    ld.u $r1 = 0[$r19]
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    cb.even $r0 ? .LBB18_1
; CV1-NEXT:    cmoved.even $r0 ? $r21 = $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    make $r0 = 3
; CV1-NEXT:    call __kvx_atomic_global_out
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lq $r18r19 = 24[$r12]
; CV1-NEXT:    copyd $r0 = $r21
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lq $r20r21 = 40[$r12]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    ld $r16 = 56[$r12]
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    set $ra = $r16
; CV1-NEXT:    addd $r12 = $r12, 64
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    ret
; CV1-NEXT:    ;;
;
; CV2-LABEL: atomicrmw_i64_sub_global_as:
; CV2:         addd $r12 = $r12, -32
; CV2-NEXT:    get $r16 = $ra
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sd 24[$r12] = $r16
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sd 16[$r12] = $r20
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sq 0[$r12] = $r18r19
; CV2-NEXT:    make $r0 = 3
; CV2-NEXT:    copyd $r18 = $r1
; CV2-NEXT:    copyd $r19 = $r0
; CV2-NEXT:    call __kvx_atomic_global_in
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    ld $r20 = 0[$r19]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:  .LBB18_1:
; CV2-NEXT:    copyd $r1 = $r20
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sbfd $r0 = $r18, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    acswapd.v.g $r20, [$r19] = $r0r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    compd.eq $r0 = $r20, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    cb.even $r0 ? .LBB18_1
; CV2-NEXT:    ;;
; CV2-NEXT:    make $r0 = 3
; CV2-NEXT:    call __kvx_atomic_global_out
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lq $r18r19 = 0[$r12]
; CV2-NEXT:    copyd $r0 = $r20
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    ld $r20 = 16[$r12]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    ld $r16 = 24[$r12]
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    set $ra = $r16
; CV2-NEXT:    addd $r12 = $r12, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    ret
; CV2-NEXT:    ;;
  %res = atomicrmw sub ptr addrspace(1)%src, i64 %b release
  ret i64 %res
}


; Test for immediates that doesn't hold on 37 bits
define i64 @bigimm(ptr %0, i64 %1) {
; CV1-LABEL: bigimm:
; CV1:         fence
; CV1-NEXT:    make $r4 = 0x40000000000
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    ld $r3 = 0x40000000000[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:  .LBB19_1:
; CV1-NEXT:    addd $r2 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    copyq $r6r7 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    acswapd $r4[$r0] = $r6r7
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    ld.u $r2 = $r4[$r0]
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    cmoved.even $r6 ? $r3 = $r2
; CV1-NEXT:    cb.even $r6 ? .LBB19_1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    fence
; CV1-NEXT:    copyd $r0 = $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 0)
;
; CV2-LABEL: bigimm:
; CV2:         fence
; CV2-NEXT:    make $r3 = 0x40000000000
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    ld $r2 = 0x40000000000[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:  .LBB19_1:
; CV2-NEXT:    copyd $r5 = $r2
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    addd $r4 = $r5, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    acswapd.v $r2, $r3[$r0] = $r4r5
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    compd.eq $r4 = $r2, $r5
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    cb.even $r4 ? .LBB19_1
; CV2-NEXT:    ;;
; CV2-NEXT:    fence
; CV2-NEXT:    copyd $r0 = $r2
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 0)
  %3 = getelementptr inbounds i64, ptr %0, i64 549755813888
  %4 = atomicrmw add ptr %3, i64 %1 seq_cst
  ret i64 %4
}
