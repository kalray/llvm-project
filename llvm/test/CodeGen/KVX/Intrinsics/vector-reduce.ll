; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mcpu=kv3-1 -O2 -o - %s | FileCheck %s --check-prefix=CV1
; RUN: llc -mcpu=kv3-2 -O2 -o - %s | FileCheck %s --check-prefix=CV2
; RUN: clang -O2 -march=kv3-1 -c -o /dev/null %s
; RUN: clang -O2 -march=kv3-2 -c -o /dev/null %s

target triple = "kvx-kalray-cos"

define i64 @longSextAdd(<8 x i8> %0) {
; CV1-LABEL: longSextAdd:
; CV1:       # %bb.0:
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    srlw $r3 = $r0, 24
; CV1-NEXT:    zxbd $r6 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    extfz $r0 = $r0, 23, 16
; CV1-NEXT:    extfz $r4 = $r2, 15, 8
; CV1-NEXT:    srlw $r5 = $r2, 24
; CV1-NEXT:    zxbd $r7 = $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    extfz $r2 = $r2, 23, 16
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sxbd $r7 = $r7
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    addwd $r3 = $r5, $r3
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    addwd $r0 = $r2, $r0
; CV1-NEXT:    addwd $r2 = $r7, $r6
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    addd $r0 = $r2, $r0
; CV1-NEXT:    addwd $r1 = $r4, $r1
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    addd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    addd $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 10)
;
; CV2-LABEL: longSextAdd:
; CV2:       # %bb.0:
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    srlw $r3 = $r0, 24
; CV2-NEXT:    zxbd $r6 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    extfz $r0 = $r0, 23, 16
; CV2-NEXT:    extfz $r4 = $r2, 15, 8
; CV2-NEXT:    srlw $r5 = $r2, 24
; CV2-NEXT:    zxbd $r7 = $r2
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sxbd $r1 = $r1
; CV2-NEXT:    extfz $r2 = $r2, 23, 16
; CV2-NEXT:    sxbd $r3 = $r3
; CV2-NEXT:    sxbd $r4 = $r4
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    sxbd $r5 = $r5
; CV2-NEXT:    sxbd $r6 = $r6
; CV2-NEXT:    sxbd $r7 = $r7
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    addwd $r1 = $r4, $r1
; CV2-NEXT:    sxbd $r2 = $r2
; CV2-NEXT:    addwd $r3 = $r5, $r3
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    addwd $r0 = $r2, $r0
; CV2-NEXT:    addd $r1 = $r1, $r3
; CV2-NEXT:    addwd $r2 = $r7, $r6
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    addd $r0 = $r2, $r0
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    addd $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = sext <8 x i8> %0 to <8 x i64>
  %3 = tail call i64 @llvm.vector.reduce.add.v8i64(<8 x i64> %2)
  ret i64 %3
}

define i64 @longAddSext(<8 x i8> %0) {
; CV1-LABEL: longAddSext:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    andd $r2 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    andd $r1 = $r2, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    andd $r2 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    andd $r3 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 19)
;
; CV2-LABEL: longAddSext:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 13)
  %2 = tail call i8 @llvm.vector.reduce.add.v8i8(<8 x i8> %0)
  %3 = sext i8 %2 to i64
  ret i64 %3
}

define i64 @longReduceSext(<8 x i8> %0) {
; CV1-LABEL: longReduceSext:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    andd $r2 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    andd $r1 = $r2, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    andd $r2 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    andd $r3 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 19)
;
; CV2-LABEL: longReduceSext:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 13)
  %2 = tail call i8 @llvm.vector.reduce.add.v8i8(<8 x i8> %0)
  %3 = sext i8 %2 to i64
  ret i64 %3
}

declare i8 @llvm.vector.reduce.add.v8i8(<8 x i8>)

define i64 @longSextReduce(<8 x i8> %0) {
; CV1-LABEL: longSextReduce:
; CV1:       # %bb.0:
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    srlw $r3 = $r0, 24
; CV1-NEXT:    zxbd $r6 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    extfz $r0 = $r0, 23, 16
; CV1-NEXT:    extfz $r4 = $r2, 15, 8
; CV1-NEXT:    srlw $r5 = $r2, 24
; CV1-NEXT:    zxbd $r7 = $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    extfz $r2 = $r2, 23, 16
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sxbd $r7 = $r7
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    addwd $r3 = $r5, $r3
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    addwd $r0 = $r2, $r0
; CV1-NEXT:    addwd $r2 = $r7, $r6
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    addd $r0 = $r2, $r0
; CV1-NEXT:    addwd $r1 = $r4, $r1
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    addd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    addd $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 10)
;
; CV2-LABEL: longSextReduce:
; CV2:       # %bb.0:
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    srlw $r3 = $r0, 24
; CV2-NEXT:    zxbd $r6 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    extfz $r0 = $r0, 23, 16
; CV2-NEXT:    extfz $r4 = $r2, 15, 8
; CV2-NEXT:    srlw $r5 = $r2, 24
; CV2-NEXT:    zxbd $r7 = $r2
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sxbd $r1 = $r1
; CV2-NEXT:    extfz $r2 = $r2, 23, 16
; CV2-NEXT:    sxbd $r3 = $r3
; CV2-NEXT:    sxbd $r4 = $r4
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    sxbd $r5 = $r5
; CV2-NEXT:    sxbd $r6 = $r6
; CV2-NEXT:    sxbd $r7 = $r7
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    addwd $r1 = $r4, $r1
; CV2-NEXT:    sxbd $r2 = $r2
; CV2-NEXT:    addwd $r3 = $r5, $r3
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    addwd $r0 = $r2, $r0
; CV2-NEXT:    addd $r1 = $r1, $r3
; CV2-NEXT:    addwd $r2 = $r7, $r6
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    addd $r0 = $r2, $r0
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    addd $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = sext <8 x i8> %0 to <8 x i64>
  %3 = tail call i64 @llvm.vector.reduce.add.v8i64(<8 x i64> %2)
  ret i64 %3
}

declare i64 @llvm.vector.reduce.add.v8i64(<8 x i64>)

define i64 @longLoopSextReduceVector(<8 x i8> %0) {
; CV1-LABEL: longLoopSextReduceVector:
; CV1:       # %bb.0:
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    srlw $r3 = $r0, 24
; CV1-NEXT:    zxbd $r6 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    extfz $r0 = $r0, 23, 16
; CV1-NEXT:    extfz $r4 = $r2, 15, 8
; CV1-NEXT:    srlw $r5 = $r2, 24
; CV1-NEXT:    zxbd $r7 = $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    extfz $r2 = $r2, 23, 16
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sxbd $r7 = $r7
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    addwd $r3 = $r5, $r3
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    addwd $r0 = $r2, $r0
; CV1-NEXT:    addwd $r2 = $r7, $r6
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    addd $r0 = $r2, $r0
; CV1-NEXT:    addwd $r1 = $r4, $r1
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    addd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    addd $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 10)
;
; CV2-LABEL: longLoopSextReduceVector:
; CV2:       # %bb.0:
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    srlw $r3 = $r0, 24
; CV2-NEXT:    zxbd $r6 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    extfz $r0 = $r0, 23, 16
; CV2-NEXT:    extfz $r4 = $r2, 15, 8
; CV2-NEXT:    srlw $r5 = $r2, 24
; CV2-NEXT:    zxbd $r7 = $r2
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sxbd $r1 = $r1
; CV2-NEXT:    extfz $r2 = $r2, 23, 16
; CV2-NEXT:    sxbd $r3 = $r3
; CV2-NEXT:    sxbd $r4 = $r4
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    sxbd $r5 = $r5
; CV2-NEXT:    sxbd $r6 = $r6
; CV2-NEXT:    sxbd $r7 = $r7
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    addwd $r1 = $r4, $r1
; CV2-NEXT:    sxbd $r2 = $r2
; CV2-NEXT:    addwd $r3 = $r5, $r3
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    addwd $r0 = $r2, $r0
; CV2-NEXT:    addd $r1 = $r1, $r3
; CV2-NEXT:    addwd $r2 = $r7, $r6
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    addd $r0 = $r2, $r0
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    addd $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = sext <8 x i8> %0 to <8 x i64>
  %3 = tail call i64 @llvm.vector.reduce.add.v8i64(<8 x i64> %2)
  ret i64 %3
}

define i64 @longLoopReduceSextVector(<8 x i8> %0) {
; CV1-LABEL: longLoopReduceSextVector:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    andd $r2 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    andd $r1 = $r2, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    andd $r2 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    andd $r3 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 19)
;
; CV2-LABEL: longLoopReduceSextVector:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 13)
  %2 = tail call i8 @llvm.vector.reduce.add.v8i8(<8 x i8> %0)
  %3 = sext i8 %2 to i64
  ret i64 %3
}

define i64 @longLoopSextReduceArray(ptr %0) {
; CV1-LABEL: longLoopSextReduceArray:
; CV1:       # %bb.0:
; CV1-NEXT:    ld $r0 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    extfz $r3 = $r0, 23, 16
; CV1-NEXT:    srlw $r4 = $r0, 24
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    extfz $r5 = $r2, 15, 8
; CV1-NEXT:    srlw $r6 = $r2, 24
; CV1-NEXT:    extfz $r7 = $r2, 23, 16
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    zxbd $r2 = $r2
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    sxbd $r7 = $r7
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    addwd $r0 = $r2, $r0
; CV1-NEXT:    addwd $r3 = $r7, $r3
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    addd $r0 = $r0, $r3
; CV1-NEXT:    addwd $r1 = $r5, $r1
; CV1-NEXT:    addwd $r2 = $r6, $r4
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    addd $r1 = $r1, $r2
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    addd $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 12)
;
; CV2-LABEL: longLoopSextReduceArray:
; CV2:       # %bb.0:
; CV2-NEXT:    ld $r0 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    srlw $r3 = $r0, 24
; CV2-NEXT:    zxbd $r4 = $r0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    extfz $r0 = $r0, 23, 16
; CV2-NEXT:    extfz $r5 = $r2, 15, 8
; CV2-NEXT:    srlw $r6 = $r2, 24
; CV2-NEXT:    zxbd $r7 = $r2
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sxbd $r1 = $r1
; CV2-NEXT:    extfz $r2 = $r2, 23, 16
; CV2-NEXT:    sxbd $r3 = $r3
; CV2-NEXT:    sxbd $r5 = $r5
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    sxbd $r4 = $r4
; CV2-NEXT:    sxbd $r6 = $r6
; CV2-NEXT:    sxbd $r7 = $r7
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    addwd $r1 = $r5, $r1
; CV2-NEXT:    sxbd $r2 = $r2
; CV2-NEXT:    addwd $r3 = $r6, $r3
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addwd $r0 = $r2, $r0
; CV2-NEXT:    addd $r1 = $r1, $r3
; CV2-NEXT:    addwd $r2 = $r7, $r4
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    addd $r0 = $r2, $r0
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    addd $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 10)
  %2 = load <8 x i8>, ptr %0
  %3 = sext <8 x i8> %2 to <8 x i64>
  %4 = tail call i64 @llvm.vector.reduce.add.v8i64(<8 x i64> %3)
  ret i64 %4
}

define i64 @longLoopReduceSextArray(ptr %0) {
; CV1-LABEL: longLoopReduceSextArray:
; CV1:       # %bb.0:
; CV1-NEXT:    ld $r0 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd $r1 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    copyd $r2 = $r0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    insf $r2 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r2 = $r2, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    andd $r3 = $r2, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    andd $r1 = $r2, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    andd $r2 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    andd $r3 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 22)
;
; CV2-LABEL: longLoopReduceSextArray:
; CV2:       # %bb.0:
; CV2-NEXT:    ld $r0 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 15)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 16)
  %2 = load <8 x i8>, ptr %0
  %3 = tail call i8 @llvm.vector.reduce.add.v8i8(<8 x i8> %2)
  %4 = sext i8 %3 to i64
  ret i64 %4
}

define i32 @intSextAdd(<8 x i8> %0) {
; CV1-LABEL: intSextAdd:
; CV1:       # %bb.0:
; CV1-NEXT:    sxmbhq $r0 = $r0
; CV1-NEXT:    sxlbhq $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxmhwp $r3 = $r1
; CV1-NEXT:    sxmhwp $r5 = $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sxlhwp $r0 = $r0
; CV1-NEXT:    sxlhwp $r2 = $r1
; CV1-NEXT:    addwp $r3 = $r3, $r5
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    addwp $r0 = $r2, $r0
; CV1-NEXT:    srld $r1 = $r3, 32
; CV1-NEXT:    copyd $r4 = $r3
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    insf $r4 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    addwp $r0 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    addwp $r1 = $r3, $r0
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    addwp $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    addwp $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 9)
;
; CV2-LABEL: intSextAdd:
; CV2:       # %bb.0:
; CV2-NEXT:    sxmbhq $r0 = $r0
; CV2-NEXT:    sxlbhq $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sxlhwp $r0 = $r0
; CV2-NEXT:    sxlhwp $r2 = $r1
; CV2-NEXT:    sxmhwp $r3 = $r1
; CV2-NEXT:    sxmhwp $r5 = $r0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    addwp $r0 = $r2, $r0
; CV2-NEXT:    addwp $r3 = $r3, $r5
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    srld $r1 = $r3, 32
; CV2-NEXT:    copyd $r4 = $r3
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    insf $r4 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    addwp $r0 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    addwp $r1 = $r3, $r0
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addwp $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    addwp $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 9)
  %2 = sext <8 x i8> %0 to <8 x i32>
  %3 = tail call i32 @llvm.vector.reduce.add.v8i32(<8 x i32> %2)
  ret i32 %3
}

define i32 @intAddSext(<8 x i8> %0) {
; CV1-LABEL: intAddSext:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    andd $r2 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    andd $r1 = $r2, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    andd $r2 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    andd $r3 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 19)
;
; CV2-LABEL: intAddSext:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 13)
  %2 = tail call i8 @llvm.vector.reduce.add.v8i8(<8 x i8> %0)
  %3 = sext i8 %2 to i32
  ret i32 %3
}

define i32 @intReduceSext(<8 x i8> %0) {
; CV1-LABEL: intReduceSext:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    andd $r2 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    andd $r1 = $r2, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    andd $r2 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    andd $r3 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 19)
;
; CV2-LABEL: intReduceSext:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 13)
  %2 = tail call i8 @llvm.vector.reduce.add.v8i8(<8 x i8> %0)
  %3 = sext i8 %2 to i32
  ret i32 %3
}

define i32 @intSextReduce(<8 x i8> %0) {
; CV1-LABEL: intSextReduce:
; CV1:       # %bb.0:
; CV1-NEXT:    sxmbhq $r0 = $r0
; CV1-NEXT:    sxlbhq $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxmhwp $r3 = $r1
; CV1-NEXT:    sxmhwp $r5 = $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sxlhwp $r0 = $r0
; CV1-NEXT:    sxlhwp $r2 = $r1
; CV1-NEXT:    addwp $r3 = $r3, $r5
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    addwp $r0 = $r2, $r0
; CV1-NEXT:    srld $r1 = $r3, 32
; CV1-NEXT:    copyd $r4 = $r3
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    insf $r4 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    addwp $r0 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    addwp $r1 = $r3, $r0
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    addwp $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    addwp $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 9)
;
; CV2-LABEL: intSextReduce:
; CV2:       # %bb.0:
; CV2-NEXT:    sxmbhq $r0 = $r0
; CV2-NEXT:    sxlbhq $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sxlhwp $r0 = $r0
; CV2-NEXT:    sxlhwp $r2 = $r1
; CV2-NEXT:    sxmhwp $r3 = $r1
; CV2-NEXT:    sxmhwp $r5 = $r0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    addwp $r0 = $r2, $r0
; CV2-NEXT:    addwp $r3 = $r3, $r5
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    srld $r1 = $r3, 32
; CV2-NEXT:    copyd $r4 = $r3
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    insf $r4 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    addwp $r0 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    addwp $r1 = $r3, $r0
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addwp $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    addwp $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 9)
  %2 = sext <8 x i8> %0 to <8 x i32>
  %3 = tail call i32 @llvm.vector.reduce.add.v8i32(<8 x i32> %2)
  ret i32 %3
}

declare i32 @llvm.vector.reduce.add.v8i32(<8 x i32>)

define i32 @intLoopSextReduceVector(<8 x i8> %0) {
; CV1-LABEL: intLoopSextReduceVector:
; CV1:       # %bb.0:
; CV1-NEXT:    sxmbhq $r0 = $r0
; CV1-NEXT:    sxlbhq $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxmhwp $r3 = $r1
; CV1-NEXT:    sxmhwp $r5 = $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sxlhwp $r0 = $r0
; CV1-NEXT:    sxlhwp $r2 = $r1
; CV1-NEXT:    addwp $r3 = $r3, $r5
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    addwp $r0 = $r2, $r0
; CV1-NEXT:    srld $r1 = $r3, 32
; CV1-NEXT:    copyd $r4 = $r3
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    insf $r4 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    addwp $r0 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    addwp $r1 = $r3, $r0
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    addwp $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    addwp $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 9)
;
; CV2-LABEL: intLoopSextReduceVector:
; CV2:       # %bb.0:
; CV2-NEXT:    sxmbhq $r0 = $r0
; CV2-NEXT:    sxlbhq $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sxlhwp $r0 = $r0
; CV2-NEXT:    sxlhwp $r2 = $r1
; CV2-NEXT:    sxmhwp $r3 = $r1
; CV2-NEXT:    sxmhwp $r5 = $r0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    addwp $r0 = $r2, $r0
; CV2-NEXT:    addwp $r3 = $r3, $r5
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    srld $r1 = $r3, 32
; CV2-NEXT:    copyd $r4 = $r3
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    insf $r4 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    addwp $r0 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    addwp $r1 = $r3, $r0
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addwp $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    addwp $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 9)
  %2 = sext <8 x i8> %0 to <8 x i32>
  %3 = tail call i32 @llvm.vector.reduce.add.v8i32(<8 x i32> %2)
  ret i32 %3
}

define i32 @intLoopReduceSextVector(<8 x i8> %0) {
; CV1-LABEL: intLoopReduceSextVector:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    andd $r2 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    andd $r1 = $r2, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    andd $r2 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    andd $r3 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 19)
;
; CV2-LABEL: intLoopReduceSextVector:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 13)
  %2 = tail call i8 @llvm.vector.reduce.add.v8i8(<8 x i8> %0)
  %3 = sext i8 %2 to i32
  ret i32 %3
}

define i32 @intLoopSextReduceArray(ptr %0) {
; CV1-LABEL: intLoopSextReduceArray:
; CV1:       # %bb.0:
; CV1-NEXT:    ld $r0 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxmbhq $r0 = $r0
; CV1-NEXT:    sxlbhq $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sxmhwp $r3 = $r1
; CV1-NEXT:    sxmhwp $r5 = $r0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sxlhwp $r0 = $r0
; CV1-NEXT:    sxlhwp $r2 = $r1
; CV1-NEXT:    addwp $r3 = $r3, $r5
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    addwp $r0 = $r2, $r0
; CV1-NEXT:    srld $r1 = $r3, 32
; CV1-NEXT:    copyd $r4 = $r3
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r4 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    addwp $r0 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    addwp $r1 = $r3, $r0
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    addwp $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    addwp $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 12)
;
; CV2-LABEL: intLoopSextReduceArray:
; CV2:       # %bb.0:
; CV2-NEXT:    ld $r0 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sxmbhq $r0 = $r0
; CV2-NEXT:    sxlbhq $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sxlhwp $r0 = $r0
; CV2-NEXT:    sxlhwp $r2 = $r1
; CV2-NEXT:    sxmhwp $r3 = $r1
; CV2-NEXT:    sxmhwp $r5 = $r0
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    addwp $r0 = $r2, $r0
; CV2-NEXT:    addwp $r3 = $r3, $r5
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    srld $r1 = $r3, 32
; CV2-NEXT:    copyd $r4 = $r3
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r4 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addwp $r0 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    addwp $r1 = $r3, $r0
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    addwp $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    addwp $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 12)
  %2 = load <8 x i8>, ptr %0
  %3 = sext <8 x i8> %2 to <8 x i32>
  %4 = tail call i32 @llvm.vector.reduce.add.v8i32(<8 x i32> %3)
  ret i32 %4
}

define i32 @intLoopReduceSextArray(ptr %0) {
; CV1-LABEL: intLoopReduceSextArray:
; CV1:       # %bb.0:
; CV1-NEXT:    ld $r0 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd $r1 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    copyd $r2 = $r0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    insf $r2 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r2 = $r2, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    andd $r3 = $r2, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    andd $r1 = $r2, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    andd $r2 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    andd $r3 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 22)
;
; CV2-LABEL: intLoopReduceSextArray:
; CV2:       # %bb.0:
; CV2-NEXT:    ld $r0 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 15)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 16)
  %2 = load <8 x i8>, ptr %0
  %3 = tail call i8 @llvm.vector.reduce.add.v8i8(<8 x i8> %2)
  %4 = sext i8 %3 to i32
  ret i32 %4
}

define i16 @shortSextAdd(<8 x i8> %0) {
; CV1-LABEL: shortSextAdd:
; CV1:       # %bb.0:
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxmbhq $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 8)
;
; CV2-LABEL: shortSextAdd:
; CV2:       # %bb.0:
; CV2-NEXT:    sxlbhq $r0 = $r0
; CV2-NEXT:    sxmbhq $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    addhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    addhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    addhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
  %2 = sext <8 x i8> %0 to <8 x i16>
  %3 = tail call i16 @llvm.vector.reduce.add.v8i16(<8 x i16> %2)
  ret i16 %3
}

define i16 @shortAddSext(<8 x i8> %0) {
; CV1-LABEL: shortAddSext:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    andd $r2 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    andd $r1 = $r2, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    andd $r2 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    andd $r3 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 19)
;
; CV2-LABEL: shortAddSext:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 13)
  %2 = tail call i8 @llvm.vector.reduce.add.v8i8(<8 x i8> %0)
  %3 = sext i8 %2 to i16
  ret i16 %3
}

define i16 @shortReduceSext(<8 x i8> %0) {
; CV1-LABEL: shortReduceSext:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    andd $r2 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    andd $r1 = $r2, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    andd $r2 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    andd $r3 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 19)
;
; CV2-LABEL: shortReduceSext:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 13)
  %2 = tail call i8 @llvm.vector.reduce.add.v8i8(<8 x i8> %0)
  %3 = sext i8 %2 to i16
  ret i16 %3
}

define i16 @shortSextReduce(<8 x i8> %0) {
; CV1-LABEL: shortSextReduce:
; CV1:       # %bb.0:
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxmbhq $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 8)
;
; CV2-LABEL: shortSextReduce:
; CV2:       # %bb.0:
; CV2-NEXT:    sxlbhq $r0 = $r0
; CV2-NEXT:    sxmbhq $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    addhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    addhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    addhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
  %2 = sext <8 x i8> %0 to <8 x i16>
  %3 = tail call i16 @llvm.vector.reduce.add.v8i16(<8 x i16> %2)
  ret i16 %3
}

declare i16 @llvm.vector.reduce.add.v8i16(<8 x i16>)

define i16 @shortLoopSextReduceVector(<8 x i8> %0) {
; CV1-LABEL: shortLoopSextReduceVector:
; CV1:       # %bb.0:
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxmbhq $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 8)
;
; CV2-LABEL: shortLoopSextReduceVector:
; CV2:       # %bb.0:
; CV2-NEXT:    sxlbhq $r0 = $r0
; CV2-NEXT:    sxmbhq $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    addhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    addhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    addhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
  %2 = sext <8 x i8> %0 to <8 x i16>
  %3 = tail call i16 @llvm.vector.reduce.add.v8i16(<8 x i16> %2)
  ret i16 %3
}

define i16 @shortLoopReduceSextVector(<8 x i8> %0) {
; CV1-LABEL: shortLoopReduceSextVector:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    andd $r2 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    andd $r1 = $r2, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    andd $r2 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    andd $r3 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 19)
;
; CV2-LABEL: shortLoopReduceSextVector:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 13)
  %2 = tail call i8 @llvm.vector.reduce.add.v8i8(<8 x i8> %0)
  %3 = sext i8 %2 to i16
  ret i16 %3
}

define i16 @shortLoopSextReduceArray(ptr %0) {
; CV1-LABEL: shortLoopSextReduceArray:
; CV1:       # %bb.0:
; CV1-NEXT:    ld $r0 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxmbhq $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 11)
;
; CV2-LABEL: shortLoopSextReduceArray:
; CV2:       # %bb.0:
; CV2-NEXT:    ld $r0 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sxlbhq $r0 = $r0
; CV2-NEXT:    sxmbhq $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    addhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    addhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 11)
  %2 = load <8 x i8>, ptr %0
  %3 = sext <8 x i8> %2 to <8 x i16>
  %4 = tail call i16 @llvm.vector.reduce.add.v8i16(<8 x i16> %3)
  ret i16 %4
}

define i16 @shortLoopReduceSextArray(ptr %0) {
; CV1-LABEL: shortLoopReduceSextArray:
; CV1:       # %bb.0:
; CV1-NEXT:    ld $r0 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd $r1 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    copyd $r2 = $r0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    insf $r2 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r2 = $r2, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    andd $r3 = $r2, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    andd $r1 = $r2, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    andd $r2 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    andd $r3 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 22)
;
; CV2-LABEL: shortLoopReduceSextArray:
; CV2:       # %bb.0:
; CV2-NEXT:    ld $r0 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 15)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 16)
  %2 = load <8 x i8>, ptr %0
  %3 = tail call i8 @llvm.vector.reduce.add.v8i8(<8 x i8> %2)
  %4 = sext i8 %3 to i16
  ret i16 %4
}

define i8 @charAdd(<8 x i8> %0) {
; CV1-LABEL: charAdd:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    andd $r2 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    andd $r1 = $r2, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    andd $r2 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    andd $r3 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 18)
;
; CV2-LABEL: charAdd:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 12)
  %2 = tail call i8 @llvm.vector.reduce.add.v8i8(<8 x i8> %0)
  ret i8 %2
}

define i8 @charReduce(<8 x i8> %0) {
; CV1-LABEL: charReduce:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    andd $r2 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    andd $r1 = $r2, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    andd $r2 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    andd $r3 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 18)
;
; CV2-LABEL: charReduce:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 12)
  %2 = tail call i8 @llvm.vector.reduce.add.v8i8(<8 x i8> %0)
  ret i8 %2
}

define i8 @charLoopReduceVector(<8 x i8> %0) {
; CV1-LABEL: charLoopReduceVector:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    andd $r2 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    andd $r1 = $r2, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    andd $r2 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    andd $r3 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 18)
;
; CV2-LABEL: charLoopReduceVector:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 12)
  %2 = tail call i8 @llvm.vector.reduce.add.v8i8(<8 x i8> %0)
  ret i8 %2
}

define i8 @charLoopReduceArray(ptr %0) {
; CV1-LABEL: charLoopReduceArray:
; CV1:       # %bb.0:
; CV1-NEXT:    ld $r0 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd $r1 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    copyd $r2 = $r0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    insf $r2 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r2 = $r2, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    andd $r3 = $r2, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    andd $r3 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    andd $r1 = $r2, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    andd $r2 = $r0, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    andd $r3 = $r1, 0x7f7f7f7f.@
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    andd $r0 = $r0, 0x80808080.@
; CV1-NEXT:    addd $r1 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    eord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 21)
;
; CV2-LABEL: charLoopReduceArray:
; CV2:       # %bb.0:
; CV2-NEXT:    ld $r0 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 15)
  %2 = load <8 x i8>, ptr %0
  %3 = tail call i8 @llvm.vector.reduce.add.v8i8(<8 x i8> %2)
  ret i8 %3
}


define i64 @addrbod(<8 x i8> %V) {
; CV1-LABEL: addrbod:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    srlw $r3 = $r0, 24
; CV1-NEXT:    zxbd $r6 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    extfz $r0 = $r0, 23, 16
; CV1-NEXT:    extfz $r4 = $r2, 15, 8
; CV1-NEXT:    srlw $r5 = $r2, 24
; CV1-NEXT:    zxbd $r7 = $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    extfz $r2 = $r2, 23, 16
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sxbd $r7 = $r7
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    addwd $r3 = $r5, $r3
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    addwd $r0 = $r2, $r0
; CV1-NEXT:    addwd $r2 = $r7, $r6
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    addd $r0 = $r2, $r0
; CV1-NEXT:    addwd $r1 = $r4, $r1
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    addd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    addd $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 10)
;
; CV2-LABEL: addrbod:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    srlw $r3 = $r0, 24
; CV2-NEXT:    zxbd $r6 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    extfz $r0 = $r0, 23, 16
; CV2-NEXT:    extfz $r4 = $r2, 15, 8
; CV2-NEXT:    srlw $r5 = $r2, 24
; CV2-NEXT:    zxbd $r7 = $r2
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sxbd $r1 = $r1
; CV2-NEXT:    extfz $r2 = $r2, 23, 16
; CV2-NEXT:    sxbd $r3 = $r3
; CV2-NEXT:    sxbd $r4 = $r4
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    sxbd $r5 = $r5
; CV2-NEXT:    sxbd $r6 = $r6
; CV2-NEXT:    sxbd $r7 = $r7
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    addwd $r1 = $r4, $r1
; CV2-NEXT:    sxbd $r2 = $r2
; CV2-NEXT:    addwd $r3 = $r5, $r3
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    addwd $r0 = $r2, $r0
; CV2-NEXT:    addd $r1 = $r1, $r3
; CV2-NEXT:    addwd $r2 = $r7, $r6
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    addd $r0 = $r2, $r0
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    addd $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
entry:
  %conv.i = sext <8 x i8> %V to <8 x i64>
  %rdx.add.i = tail call i64 @llvm.vector.reduce.add.v8i64(<8 x i64> %conv.i)
  ret i64 %rdx.add.i
}

define i64 @addrbpd(<2 x i8> %V) {
; CV1-LABEL: addrbpd:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    extfs $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    addd $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 1)
;
; CV2-LABEL: addrbpd:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    extfs $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    addd $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 1)
entry:
  %conv.i = sext <2 x i8> %V to <2 x i64>
  %rdx.add.i = tail call i64 @llvm.vector.reduce.add.v2i64(<2 x i64> %conv.i)
  ret i64 %rdx.add.i
}

define i64 @addrbqd(<4 x i8> %V) {
; CV1-LABEL: addrbqd:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    srlw $r0 = $r0, 24
; CV1-NEXT:    zxbd $r1 = $r0
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    extfz $r3 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    addwd $r0 = $r0, $r3
; CV1-NEXT:    addwd $r1 = $r2, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    addd $r0 = $r1, $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 4)
;
; CV2-LABEL: addrbqd:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    srlw $r0 = $r0, 24
; CV2-NEXT:    zxbd $r1 = $r0
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    extfz $r3 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    sxbd $r1 = $r1
; CV2-NEXT:    sxbd $r2 = $r2
; CV2-NEXT:    sxbd $r3 = $r3
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    addwd $r0 = $r0, $r3
; CV2-NEXT:    addwd $r1 = $r2, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    addd $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 3)
entry:
  %conv.i = sext <4 x i8> %V to <4 x i64>
  %rdx.add.i = tail call i64 @llvm.vector.reduce.add.v4i64(<4 x i64> %conv.i)
  ret i64 %rdx.add.i
}

define i64 @addrbvd(<32 x i8> %V) {
; CV1-LABEL: addrbvd:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    srld $r4 = $r0, 32
; CV1-NEXT:    srld $r5 = $r2, 32
; CV1-NEXT:    srld $r8 = $r1, 32
; CV1-NEXT:    srld $r9 = $r3, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    extfz $r6 = $r4, 15, 8
; CV1-NEXT:    extfz $r7 = $r5, 15, 8
; CV1-NEXT:    srlw $r33 = $r4, 24
; CV1-NEXT:    srlw $r34 = $r5, 24
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    extfz $r10 = $r8, 15, 8
; CV1-NEXT:    extfz $r11 = $r9, 15, 8
; CV1-NEXT:    srlw $r35 = $r8, 24
; CV1-NEXT:    srlw $r36 = $r9, 24
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    sxbd $r7 = $r7
; CV1-NEXT:    srlw $r37 = $r0, 24
; CV1-NEXT:    srlw $r38 = $r2, 24
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sxbd $r10 = $r10
; CV1-NEXT:    sxbd $r11 = $r11
; CV1-NEXT:    srlw $r39 = $r1, 24
; CV1-NEXT:    srlw $r40 = $r3, 24
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    extfz $r15 = $r0, 15, 8
; CV1-NEXT:    extfz $r16 = $r2, 15, 8
; CV1-NEXT:    zxbd $r43 = $r8
; CV1-NEXT:    zxbd $r44 = $r9
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    extfz $r8 = $r8, 23, 16
; CV1-NEXT:    extfz $r9 = $r9, 23, 16
; CV1-NEXT:    zxbd $r45 = $r0
; CV1-NEXT:    zxbd $r46 = $r2
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    extfz $r0 = $r0, 23, 16
; CV1-NEXT:    extfz $r2 = $r2, 23, 16
; CV1-NEXT:    zxbd $r41 = $r4
; CV1-NEXT:    zxbd $r42 = $r5
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    extfz $r17 = $r1, 15, 8
; CV1-NEXT:    extfz $r32 = $r3, 15, 8
; CV1-NEXT:    zxbd $r47 = $r1
; CV1-NEXT:    zxbd $r48 = $r3
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    sxbd $r33 = $r33
; CV1-NEXT:    sxbd $r34 = $r34
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    sxbd $r35 = $r35
; CV1-NEXT:    sxbd $r36 = $r36
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sxbd $r37 = $r37
; CV1-NEXT:    sxbd $r38 = $r38
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    sxbd $r39 = $r39
; CV1-NEXT:    sxbd $r40 = $r40
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    extfz $r4 = $r4, 23, 16
; CV1-NEXT:    extfz $r5 = $r5, 23, 16
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    sxbd $r8 = $r8
; CV1-NEXT:    sxbd $r9 = $r9
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    extfz $r1 = $r1, 23, 16
; CV1-NEXT:    extfz $r3 = $r3, 23, 16
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    addwd $r6 = $r7, $r6
; CV1-NEXT:    addwd $r10 = $r11, $r10
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    addd $r6 = $r6, $r10
; CV1-NEXT:    sxbd $r15 = $r15
; CV1-NEXT:    sxbd $r16 = $r16
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    sxbd $r17 = $r17
; CV1-NEXT:    sxbd $r32 = $r32
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    sxbd $r41 = $r41
; CV1-NEXT:    sxbd $r42 = $r42
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    sxbd $r43 = $r43
; CV1-NEXT:    sxbd $r44 = $r44
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    sxbd $r45 = $r45
; CV1-NEXT:    sxbd $r46 = $r46
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    sxbd $r47 = $r47
; CV1-NEXT:    sxbd $r48 = $r48
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    addwd $r0 = $r2, $r0
; CV1-NEXT:    addwd $r2 = $r9, $r8
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    addwd $r8 = $r40, $r39
; CV1-NEXT:    addwd $r9 = $r38, $r37
; CV1-NEXT:    ;; # (end cycle 27)
; CV1-NEXT:    addd $r8 = $r9, $r8
; CV1-NEXT:    addwd $r10 = $r36, $r35
; CV1-NEXT:    addwd $r11 = $r34, $r33
; CV1-NEXT:    ;; # (end cycle 28)
; CV1-NEXT:    addwd $r1 = $r3, $r1
; CV1-NEXT:    addwd $r3 = $r5, $r4
; CV1-NEXT:    addd $r10 = $r11, $r10
; CV1-NEXT:    ;; # (end cycle 29)
; CV1-NEXT:    addd $r0 = $r0, $r1
; CV1-NEXT:    addd $r2 = $r3, $r2
; CV1-NEXT:    addwd $r4 = $r48, $r47
; CV1-NEXT:    addwd $r5 = $r46, $r45
; CV1-NEXT:    ;; # (end cycle 30)
; CV1-NEXT:    addd $r0 = $r0, $r2
; CV1-NEXT:    addd $r4 = $r5, $r4
; CV1-NEXT:    addwd $r15 = $r16, $r15
; CV1-NEXT:    addwd $r17 = $r32, $r17
; CV1-NEXT:    ;; # (end cycle 31)
; CV1-NEXT:    addd $r2 = $r8, $r10
; CV1-NEXT:    addd $r7 = $r15, $r17
; CV1-NEXT:    addwd $r9 = $r44, $r43
; CV1-NEXT:    addwd $r11 = $r42, $r41
; CV1-NEXT:    ;; # (end cycle 32)
; CV1-NEXT:    addd $r3 = $r7, $r6
; CV1-NEXT:    addd $r9 = $r11, $r9
; CV1-NEXT:    ;; # (end cycle 33)
; CV1-NEXT:    addd $r1 = $r4, $r9
; CV1-NEXT:    addd $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 34)
; CV1-NEXT:    addd $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 35)
; CV1-NEXT:    addd $r0 = $r0, $r2
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 36)
;
; CV2-LABEL: addrbvd:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    srld $r4 = $r0, 32
; CV2-NEXT:    srld $r5 = $r2, 32
; CV2-NEXT:    srld $r6 = $r1, 32
; CV2-NEXT:    srld $r7 = $r3, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    extfz $r8 = $r4, 15, 8
; CV2-NEXT:    extfz $r9 = $r5, 15, 8
; CV2-NEXT:    extfz $r10 = $r6, 15, 8
; CV2-NEXT:    extfz $r11 = $r7, 15, 8
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    extfz $r15 = $r0, 15, 8
; CV2-NEXT:    extfz $r16 = $r2, 15, 8
; CV2-NEXT:    extfz $r17 = $r1, 15, 8
; CV2-NEXT:    extfz $r32 = $r3, 15, 8
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    srlw $r33 = $r4, 24
; CV2-NEXT:    srlw $r34 = $r5, 24
; CV2-NEXT:    srlw $r35 = $r6, 24
; CV2-NEXT:    srlw $r36 = $r7, 24
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r37 = $r0, 24
; CV2-NEXT:    srlw $r38 = $r2, 24
; CV2-NEXT:    srlw $r39 = $r1, 24
; CV2-NEXT:    srlw $r40 = $r3, 24
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    zxbd $r41 = $r4
; CV2-NEXT:    zxbd $r42 = $r5
; CV2-NEXT:    zxbd $r43 = $r6
; CV2-NEXT:    zxbd $r44 = $r7
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    zxbd $r45 = $r0
; CV2-NEXT:    zxbd $r46 = $r2
; CV2-NEXT:    zxbd $r47 = $r1
; CV2-NEXT:    zxbd $r48 = $r3
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    extfz $r4 = $r4, 23, 16
; CV2-NEXT:    extfz $r5 = $r5, 23, 16
; CV2-NEXT:    extfz $r6 = $r6, 23, 16
; CV2-NEXT:    extfz $r7 = $r7, 23, 16
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    extfz $r0 = $r0, 23, 16
; CV2-NEXT:    extfz $r1 = $r1, 23, 16
; CV2-NEXT:    extfz $r2 = $r2, 23, 16
; CV2-NEXT:    extfz $r3 = $r3, 23, 16
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    sxbd $r8 = $r8
; CV2-NEXT:    sxbd $r9 = $r9
; CV2-NEXT:    sxbd $r10 = $r10
; CV2-NEXT:    sxbd $r11 = $r11
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    sxbd $r15 = $r15
; CV2-NEXT:    sxbd $r16 = $r16
; CV2-NEXT:    sxbd $r17 = $r17
; CV2-NEXT:    sxbd $r32 = $r32
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    sxbd $r33 = $r33
; CV2-NEXT:    sxbd $r34 = $r34
; CV2-NEXT:    sxbd $r35 = $r35
; CV2-NEXT:    sxbd $r36 = $r36
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    sxbd $r37 = $r37
; CV2-NEXT:    sxbd $r38 = $r38
; CV2-NEXT:    sxbd $r39 = $r39
; CV2-NEXT:    sxbd $r40 = $r40
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    sxbd $r41 = $r41
; CV2-NEXT:    sxbd $r42 = $r42
; CV2-NEXT:    sxbd $r43 = $r43
; CV2-NEXT:    sxbd $r44 = $r44
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    sxbd $r45 = $r45
; CV2-NEXT:    sxbd $r46 = $r46
; CV2-NEXT:    sxbd $r47 = $r47
; CV2-NEXT:    sxbd $r48 = $r48
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    sxbd $r4 = $r4
; CV2-NEXT:    sxbd $r5 = $r5
; CV2-NEXT:    sxbd $r6 = $r6
; CV2-NEXT:    sxbd $r7 = $r7
; CV2-NEXT:    ;; # (end cycle 15)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    sxbd $r1 = $r1
; CV2-NEXT:    sxbd $r2 = $r2
; CV2-NEXT:    sxbd $r3 = $r3
; CV2-NEXT:    ;; # (end cycle 16)
; CV2-NEXT:    addwd $r0 = $r2, $r0
; CV2-NEXT:    addwd $r1 = $r3, $r1
; CV2-NEXT:    addwd $r2 = $r7, $r6
; CV2-NEXT:    addwd $r3 = $r5, $r4
; CV2-NEXT:    ;; # (end cycle 17)
; CV2-NEXT:    addwd $r4 = $r48, $r47
; CV2-NEXT:    addwd $r5 = $r46, $r45
; CV2-NEXT:    addwd $r6 = $r44, $r43
; CV2-NEXT:    addwd $r7 = $r42, $r41
; CV2-NEXT:    ;; # (end cycle 18)
; CV2-NEXT:    addwd $r33 = $r34, $r33
; CV2-NEXT:    addwd $r35 = $r36, $r35
; CV2-NEXT:    addwd $r37 = $r38, $r37
; CV2-NEXT:    addwd $r39 = $r40, $r39
; CV2-NEXT:    ;; # (end cycle 19)
; CV2-NEXT:    addwd $r8 = $r9, $r8
; CV2-NEXT:    addwd $r10 = $r11, $r10
; CV2-NEXT:    addwd $r15 = $r16, $r15
; CV2-NEXT:    addwd $r17 = $r32, $r17
; CV2-NEXT:    ;; # (end cycle 20)
; CV2-NEXT:    addd $r8 = $r8, $r10
; CV2-NEXT:    addd $r9 = $r15, $r17
; CV2-NEXT:    addd $r10 = $r33, $r35
; CV2-NEXT:    addd $r11 = $r37, $r39
; CV2-NEXT:    ;; # (end cycle 21)
; CV2-NEXT:    addd $r0 = $r0, $r1
; CV2-NEXT:    addd $r2 = $r3, $r2
; CV2-NEXT:    addd $r4 = $r5, $r4
; CV2-NEXT:    addd $r6 = $r7, $r6
; CV2-NEXT:    ;; # (end cycle 22)
; CV2-NEXT:    addd $r0 = $r0, $r2
; CV2-NEXT:    addd $r1 = $r4, $r6
; CV2-NEXT:    addd $r2 = $r11, $r10
; CV2-NEXT:    addd $r3 = $r9, $r8
; CV2-NEXT:    ;; # (end cycle 23)
; CV2-NEXT:    addd $r0 = $r1, $r0
; CV2-NEXT:    addd $r2 = $r3, $r2
; CV2-NEXT:    ;; # (end cycle 24)
; CV2-NEXT:    addd $r0 = $r0, $r2
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 25)
entry:
  %conv.i = sext <32 x i8> %V to <32 x i64>
  %rdx.add.i = tail call i64 @llvm.vector.reduce.add.v32i64(<32 x i64> %conv.i)
  ret i64 %rdx.add.i
}

define i64 @addrbxd(<16 x i8> %V) {
; CV1-LABEL: addrbxd:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    extfz $r3 = $r1, 23, 16
; CV1-NEXT:    srld $r4 = $r0, 32
; CV1-NEXT:    srld $r5 = $r1, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    extfz $r6 = $r4, 23, 16
; CV1-NEXT:    extfz $r7 = $r5, 23, 16
; CV1-NEXT:    zxbd $r8 = $r0
; CV1-NEXT:    zxbd $r9 = $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    zxbd $r10 = $r4
; CV1-NEXT:    zxbd $r11 = $r5
; CV1-NEXT:    srlw $r15 = $r0, 24
; CV1-NEXT:    srlw $r16 = $r1, 24
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    extfz $r0 = $r0, 15, 8
; CV1-NEXT:    extfz $r1 = $r1, 15, 8
; CV1-NEXT:    srlw $r17 = $r4, 24
; CV1-NEXT:    srlw $r32 = $r5, 24
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    extfz $r4 = $r4, 15, 8
; CV1-NEXT:    extfz $r5 = $r5, 15, 8
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    sxbd $r7 = $r7
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    sxbd $r8 = $r8
; CV1-NEXT:    sxbd $r9 = $r9
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    sxbd $r10 = $r10
; CV1-NEXT:    sxbd $r11 = $r11
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    sxbd $r15 = $r15
; CV1-NEXT:    sxbd $r16 = $r16
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    sxbd $r17 = $r17
; CV1-NEXT:    sxbd $r32 = $r32
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    addwd $r0 = $r1, $r0
; CV1-NEXT:    addwd $r4 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    addd $r0 = $r0, $r4
; CV1-NEXT:    addwd $r1 = $r32, $r17
; CV1-NEXT:    addwd $r5 = $r16, $r15
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    addd $r1 = $r5, $r1
; CV1-NEXT:    addwd $r8 = $r9, $r8
; CV1-NEXT:    addwd $r10 = $r11, $r10
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    addd $r0 = $r0, $r1
; CV1-NEXT:    addwd $r2 = $r3, $r2
; CV1-NEXT:    addd $r3 = $r8, $r10
; CV1-NEXT:    addwd $r6 = $r7, $r6
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    addd $r2 = $r2, $r6
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    addd $r0 = $r1, $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 19)
;
; CV2-LABEL: addrbxd:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    srld $r3 = $r1, 32
; CV2-NEXT:    extfz $r4 = $r0, 23, 16
; CV2-NEXT:    extfz $r5 = $r1, 23, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    extfz $r6 = $r2, 23, 16
; CV2-NEXT:    extfz $r7 = $r3, 23, 16
; CV2-NEXT:    zxbd $r8 = $r0
; CV2-NEXT:    zxbd $r9 = $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    zxbd $r10 = $r2
; CV2-NEXT:    zxbd $r11 = $r3
; CV2-NEXT:    srlw $r15 = $r0, 24
; CV2-NEXT:    srlw $r16 = $r1, 24
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    extfz $r0 = $r0, 15, 8
; CV2-NEXT:    extfz $r1 = $r1, 15, 8
; CV2-NEXT:    srlw $r17 = $r2, 24
; CV2-NEXT:    srlw $r32 = $r3, 24
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    extfz $r2 = $r2, 15, 8
; CV2-NEXT:    extfz $r3 = $r3, 15, 8
; CV2-NEXT:    sxbd $r4 = $r4
; CV2-NEXT:    sxbd $r5 = $r5
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sxbd $r6 = $r6
; CV2-NEXT:    sxbd $r7 = $r7
; CV2-NEXT:    sxbd $r8 = $r8
; CV2-NEXT:    sxbd $r9 = $r9
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    sxbd $r10 = $r10
; CV2-NEXT:    sxbd $r11 = $r11
; CV2-NEXT:    sxbd $r15 = $r15
; CV2-NEXT:    sxbd $r16 = $r16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    sxbd $r1 = $r1
; CV2-NEXT:    sxbd $r17 = $r17
; CV2-NEXT:    sxbd $r32 = $r32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addwd $r0 = $r1, $r0
; CV2-NEXT:    addwd $r1 = $r32, $r17
; CV2-NEXT:    sxbd $r2 = $r2
; CV2-NEXT:    sxbd $r3 = $r3
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    addwd $r2 = $r3, $r2
; CV2-NEXT:    addwd $r3 = $r16, $r15
; CV2-NEXT:    addwd $r8 = $r9, $r8
; CV2-NEXT:    addwd $r10 = $r11, $r10
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    addd $r1 = $r3, $r1
; CV2-NEXT:    addwd $r4 = $r5, $r4
; CV2-NEXT:    addd $r5 = $r8, $r10
; CV2-NEXT:    addwd $r6 = $r7, $r6
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    addd $r0 = $r0, $r2
; CV2-NEXT:    addd $r4 = $r4, $r6
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    addd $r0 = $r0, $r1
; CV2-NEXT:    addd $r1 = $r5, $r4
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    addd $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 13)
entry:
  %conv.i = sext <16 x i8> %V to <16 x i64>
  %rdx.add.i = tail call i64 @llvm.vector.reduce.add.v16i64(<16 x i64> %conv.i)
  ret i64 %rdx.add.i
}

define i64 @addrhod(<8 x i16> %V) {
; CV1-LABEL: addrhod:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    srlw $r2 = $r0, 16
; CV1-NEXT:    srlw $r3 = $r1, 16
; CV1-NEXT:    srld $r4 = $r0, 48
; CV1-NEXT:    srld $r5 = $r1, 48
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    zxhd $r6 = $r0
; CV1-NEXT:    zxhd $r7 = $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    extfz $r0 = $r0, 47, 32
; CV1-NEXT:    extfz $r1 = $r1, 47, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sxhd $r2 = $r2
; CV1-NEXT:    sxhd $r3 = $r3
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sxhd $r4 = $r4
; CV1-NEXT:    sxhd $r5 = $r5
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sxhd $r6 = $r6
; CV1-NEXT:    sxhd $r7 = $r7
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sxhd $r0 = $r0
; CV1-NEXT:    sxhd $r1 = $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    addwd $r0 = $r1, $r0
; CV1-NEXT:    addwd $r1 = $r7, $r6
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    addd $r0 = $r1, $r0
; CV1-NEXT:    addwd $r2 = $r3, $r2
; CV1-NEXT:    addwd $r4 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    addd $r2 = $r2, $r4
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    addd $r0 = $r0, $r2
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 10)
;
; CV2-LABEL: addrhod:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    srlw $r2 = $r0, 16
; CV2-NEXT:    srlw $r3 = $r1, 16
; CV2-NEXT:    srld $r4 = $r0, 48
; CV2-NEXT:    srld $r5 = $r1, 48
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    extfz $r0 = $r0, 47, 32
; CV2-NEXT:    extfz $r1 = $r1, 47, 32
; CV2-NEXT:    zxhd $r6 = $r0
; CV2-NEXT:    zxhd $r7 = $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sxhd $r2 = $r2
; CV2-NEXT:    sxhd $r3 = $r3
; CV2-NEXT:    sxhd $r4 = $r4
; CV2-NEXT:    sxhd $r5 = $r5
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sxhd $r0 = $r0
; CV2-NEXT:    sxhd $r1 = $r1
; CV2-NEXT:    sxhd $r6 = $r6
; CV2-NEXT:    sxhd $r7 = $r7
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    addwd $r0 = $r1, $r0
; CV2-NEXT:    addwd $r1 = $r7, $r6
; CV2-NEXT:    addwd $r2 = $r3, $r2
; CV2-NEXT:    addwd $r4 = $r5, $r4
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    addd $r0 = $r1, $r0
; CV2-NEXT:    addd $r2 = $r2, $r4
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    addd $r0 = $r0, $r2
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 6)
entry:
  %conv.i = sext <8 x i16> %V to <8 x i64>
  %rdx.add.i = tail call i64 @llvm.vector.reduce.add.v8i64(<8 x i64> %conv.i)
  ret i64 %rdx.add.i
}

define i64 @addrhpd(<2 x i16> %V) {
; CV1-LABEL: addrhpd:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    sxhd $r0 = $r0
; CV1-NEXT:    extfs $r1 = $r0, 31, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    addd $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 1)
;
; CV2-LABEL: addrhpd:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    sxhd $r0 = $r0
; CV2-NEXT:    extfs $r1 = $r0, 31, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    addd $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 1)
entry:
  %conv.i = sext <2 x i16> %V to <2 x i64>
  %rdx.add.i = tail call i64 @llvm.vector.reduce.add.v2i64(<2 x i64> %conv.i)
  ret i64 %rdx.add.i
}

define i64 @addrhqd(<4 x i16> %V) {
; CV1-LABEL: addrhqd:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    srld $r0 = $r0, 48
; CV1-NEXT:    zxhd $r1 = $r0
; CV1-NEXT:    extfz $r2 = $r0, 47, 32
; CV1-NEXT:    srlw $r3 = $r0, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxhd $r1 = $r1
; CV1-NEXT:    sxhd $r2 = $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sxhd $r0 = $r0
; CV1-NEXT:    sxhd $r3 = $r3
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    addwd $r0 = $r0, $r3
; CV1-NEXT:    addwd $r1 = $r2, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    addd $r0 = $r1, $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 4)
;
; CV2-LABEL: addrhqd:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    srld $r0 = $r0, 48
; CV2-NEXT:    zxhd $r1 = $r0
; CV2-NEXT:    extfz $r2 = $r0, 47, 32
; CV2-NEXT:    srlw $r3 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sxhd $r0 = $r0
; CV2-NEXT:    sxhd $r1 = $r1
; CV2-NEXT:    sxhd $r2 = $r2
; CV2-NEXT:    sxhd $r3 = $r3
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    addwd $r0 = $r0, $r3
; CV2-NEXT:    addwd $r1 = $r2, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    addd $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 3)
entry:
  %conv.i = sext <4 x i16> %V to <4 x i64>
  %rdx.add.i = tail call i64 @llvm.vector.reduce.add.v4i64(<4 x i64> %conv.i)
  ret i64 %rdx.add.i
}

define i64 @addrhvd(ptr %0) {
; CV1-LABEL: addrhvd:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r0r1r2r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srlw $r9 = $r5, 16
; CV1-NEXT:    srlw $r11 = $r7, 16
; CV1-NEXT:    srlw $r32 = $r6, 16
; CV1-NEXT:    srld $r34 = $r5, 48
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srlw $r8 = $r1, 16
; CV1-NEXT:    sxhd $r9 = $r9
; CV1-NEXT:    srlw $r10 = $r3, 16
; CV1-NEXT:    sxhd $r11 = $r11
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sxhd $r8 = $r8
; CV1-NEXT:    sxhd $r10 = $r10
; CV1-NEXT:    srlw $r17 = $r2, 16
; CV1-NEXT:    srld $r33 = $r1, 48
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srld $r35 = $r3, 48
; CV1-NEXT:    srld $r36 = $r7, 48
; CV1-NEXT:    srld $r37 = $r0, 48
; CV1-NEXT:    srld $r38 = $r4, 48
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    srld $r39 = $r2, 48
; CV1-NEXT:    srld $r40 = $r6, 48
; CV1-NEXT:    zxhd $r43 = $r3
; CV1-NEXT:    zxhd $r44 = $r7
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    srlw $r15 = $r0, 16
; CV1-NEXT:    srlw $r16 = $r4, 16
; CV1-NEXT:    zxhd $r47 = $r2
; CV1-NEXT:    zxhd $r48 = $r6
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    extfz $r3 = $r3, 47, 32
; CV1-NEXT:    extfz $r7 = $r7, 47, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    extfz $r2 = $r2, 47, 32
; CV1-NEXT:    extfz $r6 = $r6, 47, 32
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sxhd $r33 = $r33
; CV1-NEXT:    sxhd $r34 = $r34
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    sxhd $r35 = $r35
; CV1-NEXT:    sxhd $r36 = $r36
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    sxhd $r37 = $r37
; CV1-NEXT:    sxhd $r38 = $r38
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    sxhd $r39 = $r39
; CV1-NEXT:    sxhd $r40 = $r40
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    zxhd $r41 = $r1
; CV1-NEXT:    zxhd $r42 = $r5
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    zxhd $r45 = $r0
; CV1-NEXT:    zxhd $r46 = $r4
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    extfz $r1 = $r1, 47, 32
; CV1-NEXT:    extfz $r5 = $r5, 47, 32
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    sxhd $r3 = $r3
; CV1-NEXT:    sxhd $r7 = $r7
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    extfz $r0 = $r0, 47, 32
; CV1-NEXT:    extfz $r4 = $r4, 47, 32
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    sxhd $r2 = $r2
; CV1-NEXT:    sxhd $r6 = $r6
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    addwd $r8 = $r9, $r8
; CV1-NEXT:    addwd $r10 = $r11, $r10
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    addd $r8 = $r8, $r10
; CV1-NEXT:    sxhd $r15 = $r15
; CV1-NEXT:    sxhd $r16 = $r16
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    sxhd $r17 = $r17
; CV1-NEXT:    sxhd $r32 = $r32
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    sxhd $r41 = $r41
; CV1-NEXT:    sxhd $r42 = $r42
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    sxhd $r43 = $r43
; CV1-NEXT:    sxhd $r44 = $r44
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    sxhd $r45 = $r45
; CV1-NEXT:    sxhd $r46 = $r46
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    sxhd $r47 = $r47
; CV1-NEXT:    sxhd $r48 = $r48
; CV1-NEXT:    ;; # (end cycle 27)
; CV1-NEXT:    sxhd $r1 = $r1
; CV1-NEXT:    sxhd $r5 = $r5
; CV1-NEXT:    ;; # (end cycle 28)
; CV1-NEXT:    sxhd $r0 = $r0
; CV1-NEXT:    sxhd $r4 = $r4
; CV1-NEXT:    ;; # (end cycle 29)
; CV1-NEXT:    addwd $r2 = $r6, $r2
; CV1-NEXT:    addwd $r3 = $r7, $r3
; CV1-NEXT:    ;; # (end cycle 30)
; CV1-NEXT:    addwd $r6 = $r40, $r39
; CV1-NEXT:    addwd $r7 = $r38, $r37
; CV1-NEXT:    ;; # (end cycle 31)
; CV1-NEXT:    addd $r6 = $r7, $r6
; CV1-NEXT:    addwd $r10 = $r36, $r35
; CV1-NEXT:    addwd $r11 = $r34, $r33
; CV1-NEXT:    ;; # (end cycle 32)
; CV1-NEXT:    addwd $r0 = $r4, $r0
; CV1-NEXT:    addwd $r1 = $r5, $r1
; CV1-NEXT:    addd $r10 = $r11, $r10
; CV1-NEXT:    ;; # (end cycle 33)
; CV1-NEXT:    addd $r0 = $r0, $r2
; CV1-NEXT:    addd $r1 = $r1, $r3
; CV1-NEXT:    addwd $r4 = $r48, $r47
; CV1-NEXT:    addwd $r5 = $r46, $r45
; CV1-NEXT:    ;; # (end cycle 34)
; CV1-NEXT:    addd $r0 = $r0, $r1
; CV1-NEXT:    addd $r4 = $r5, $r4
; CV1-NEXT:    addwd $r15 = $r16, $r15
; CV1-NEXT:    addwd $r17 = $r32, $r17
; CV1-NEXT:    ;; # (end cycle 35)
; CV1-NEXT:    addd $r2 = $r6, $r10
; CV1-NEXT:    addwd $r7 = $r44, $r43
; CV1-NEXT:    addd $r9 = $r15, $r17
; CV1-NEXT:    addwd $r11 = $r42, $r41
; CV1-NEXT:    ;; # (end cycle 36)
; CV1-NEXT:    addd $r3 = $r9, $r8
; CV1-NEXT:    addd $r7 = $r11, $r7
; CV1-NEXT:    ;; # (end cycle 37)
; CV1-NEXT:    addd $r1 = $r4, $r7
; CV1-NEXT:    addd $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 38)
; CV1-NEXT:    addd $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 39)
; CV1-NEXT:    addd $r0 = $r0, $r2
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 40)
;
; CV2-LABEL: addrhvd:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r0r1r2r3 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    srlw $r9 = $r5, 16
; CV2-NEXT:    srlw $r11 = $r7, 16
; CV2-NEXT:    srlw $r16 = $r4, 16
; CV2-NEXT:    srlw $r32 = $r6, 16
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r8 = $r1, 16
; CV2-NEXT:    srlw $r10 = $r3, 16
; CV2-NEXT:    srlw $r15 = $r0, 16
; CV2-NEXT:    srlw $r17 = $r2, 16
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    srld $r33 = $r1, 48
; CV2-NEXT:    srld $r34 = $r5, 48
; CV2-NEXT:    srld $r35 = $r3, 48
; CV2-NEXT:    srld $r36 = $r7, 48
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    srld $r37 = $r0, 48
; CV2-NEXT:    srld $r38 = $r4, 48
; CV2-NEXT:    srld $r39 = $r2, 48
; CV2-NEXT:    srld $r40 = $r6, 48
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    zxhd $r41 = $r1
; CV2-NEXT:    zxhd $r42 = $r5
; CV2-NEXT:    zxhd $r43 = $r3
; CV2-NEXT:    zxhd $r44 = $r7
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    zxhd $r45 = $r0
; CV2-NEXT:    zxhd $r46 = $r4
; CV2-NEXT:    zxhd $r47 = $r2
; CV2-NEXT:    zxhd $r48 = $r6
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r1 = $r1, 47, 32
; CV2-NEXT:    extfz $r3 = $r3, 47, 32
; CV2-NEXT:    extfz $r5 = $r5, 47, 32
; CV2-NEXT:    extfz $r7 = $r7, 47, 32
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    extfz $r0 = $r0, 47, 32
; CV2-NEXT:    extfz $r2 = $r2, 47, 32
; CV2-NEXT:    extfz $r4 = $r4, 47, 32
; CV2-NEXT:    extfz $r6 = $r6, 47, 32
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    sxhd $r8 = $r8
; CV2-NEXT:    sxhd $r9 = $r9
; CV2-NEXT:    sxhd $r10 = $r10
; CV2-NEXT:    sxhd $r11 = $r11
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    sxhd $r15 = $r15
; CV2-NEXT:    sxhd $r16 = $r16
; CV2-NEXT:    sxhd $r17 = $r17
; CV2-NEXT:    sxhd $r32 = $r32
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    sxhd $r33 = $r33
; CV2-NEXT:    sxhd $r34 = $r34
; CV2-NEXT:    sxhd $r35 = $r35
; CV2-NEXT:    sxhd $r36 = $r36
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    sxhd $r37 = $r37
; CV2-NEXT:    sxhd $r38 = $r38
; CV2-NEXT:    sxhd $r39 = $r39
; CV2-NEXT:    sxhd $r40 = $r40
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    sxhd $r41 = $r41
; CV2-NEXT:    sxhd $r42 = $r42
; CV2-NEXT:    sxhd $r43 = $r43
; CV2-NEXT:    sxhd $r44 = $r44
; CV2-NEXT:    ;; # (end cycle 15)
; CV2-NEXT:    sxhd $r45 = $r45
; CV2-NEXT:    sxhd $r46 = $r46
; CV2-NEXT:    sxhd $r47 = $r47
; CV2-NEXT:    sxhd $r48 = $r48
; CV2-NEXT:    ;; # (end cycle 16)
; CV2-NEXT:    sxhd $r1 = $r1
; CV2-NEXT:    sxhd $r3 = $r3
; CV2-NEXT:    sxhd $r5 = $r5
; CV2-NEXT:    sxhd $r7 = $r7
; CV2-NEXT:    ;; # (end cycle 17)
; CV2-NEXT:    sxhd $r0 = $r0
; CV2-NEXT:    sxhd $r2 = $r2
; CV2-NEXT:    sxhd $r4 = $r4
; CV2-NEXT:    sxhd $r6 = $r6
; CV2-NEXT:    ;; # (end cycle 18)
; CV2-NEXT:    addwd $r0 = $r4, $r0
; CV2-NEXT:    addwd $r1 = $r5, $r1
; CV2-NEXT:    addwd $r2 = $r6, $r2
; CV2-NEXT:    addwd $r3 = $r7, $r3
; CV2-NEXT:    ;; # (end cycle 19)
; CV2-NEXT:    addwd $r4 = $r48, $r47
; CV2-NEXT:    addwd $r5 = $r46, $r45
; CV2-NEXT:    addwd $r6 = $r44, $r43
; CV2-NEXT:    addwd $r7 = $r42, $r41
; CV2-NEXT:    ;; # (end cycle 20)
; CV2-NEXT:    addwd $r33 = $r34, $r33
; CV2-NEXT:    addwd $r35 = $r36, $r35
; CV2-NEXT:    addwd $r37 = $r38, $r37
; CV2-NEXT:    addwd $r39 = $r40, $r39
; CV2-NEXT:    ;; # (end cycle 21)
; CV2-NEXT:    addwd $r8 = $r9, $r8
; CV2-NEXT:    addwd $r10 = $r11, $r10
; CV2-NEXT:    addwd $r15 = $r16, $r15
; CV2-NEXT:    addwd $r17 = $r32, $r17
; CV2-NEXT:    ;; # (end cycle 22)
; CV2-NEXT:    addd $r8 = $r8, $r10
; CV2-NEXT:    addd $r9 = $r15, $r17
; CV2-NEXT:    addd $r10 = $r33, $r35
; CV2-NEXT:    addd $r11 = $r37, $r39
; CV2-NEXT:    ;; # (end cycle 23)
; CV2-NEXT:    addd $r0 = $r0, $r2
; CV2-NEXT:    addd $r1 = $r1, $r3
; CV2-NEXT:    addd $r4 = $r5, $r4
; CV2-NEXT:    addd $r6 = $r7, $r6
; CV2-NEXT:    ;; # (end cycle 24)
; CV2-NEXT:    addd $r0 = $r0, $r1
; CV2-NEXT:    addd $r1 = $r4, $r6
; CV2-NEXT:    addd $r2 = $r11, $r10
; CV2-NEXT:    addd $r3 = $r9, $r8
; CV2-NEXT:    ;; # (end cycle 25)
; CV2-NEXT:    addd $r0 = $r1, $r0
; CV2-NEXT:    addd $r2 = $r3, $r2
; CV2-NEXT:    ;; # (end cycle 26)
; CV2-NEXT:    addd $r0 = $r0, $r2
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 27)
entry:
  %V = load <32 x i16>, ptr %0
  %conv.i = sext <32 x i16> %V to <32 x i64>
  %rdx.add.i = tail call i64 @llvm.vector.reduce.add.v32i64(<32 x i64> %conv.i)
  ret i64 %rdx.add.i
}

define i64 @addrhxd(<16 x i16> %V) {
; CV1-LABEL: addrhxd:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    extfz $r4 = $r0, 47, 32
; CV1-NEXT:    extfz $r5 = $r2, 47, 32
; CV1-NEXT:    srld $r15 = $r0, 48
; CV1-NEXT:    srld $r16 = $r2, 48
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    extfz $r6 = $r1, 47, 32
; CV1-NEXT:    extfz $r7 = $r3, 47, 32
; CV1-NEXT:    srld $r17 = $r1, 48
; CV1-NEXT:    srld $r32 = $r3, 48
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srlw $r0 = $r0, 16
; CV1-NEXT:    srlw $r2 = $r2, 16
; CV1-NEXT:    zxhd $r8 = $r0
; CV1-NEXT:    zxhd $r9 = $r2
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    srlw $r1 = $r1, 16
; CV1-NEXT:    srlw $r3 = $r3, 16
; CV1-NEXT:    zxhd $r10 = $r1
; CV1-NEXT:    zxhd $r11 = $r3
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sxhd $r4 = $r4
; CV1-NEXT:    sxhd $r5 = $r5
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sxhd $r6 = $r6
; CV1-NEXT:    sxhd $r7 = $r7
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sxhd $r8 = $r8
; CV1-NEXT:    sxhd $r9 = $r9
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    sxhd $r10 = $r10
; CV1-NEXT:    sxhd $r11 = $r11
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    sxhd $r15 = $r15
; CV1-NEXT:    sxhd $r16 = $r16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    sxhd $r17 = $r17
; CV1-NEXT:    sxhd $r32 = $r32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    sxhd $r0 = $r0
; CV1-NEXT:    sxhd $r2 = $r2
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sxhd $r1 = $r1
; CV1-NEXT:    sxhd $r3 = $r3
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    addwd $r0 = $r2, $r0
; CV1-NEXT:    addwd $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    addd $r0 = $r0, $r1
; CV1-NEXT:    addwd $r2 = $r32, $r17
; CV1-NEXT:    addwd $r3 = $r16, $r15
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    addd $r2 = $r3, $r2
; CV1-NEXT:    addwd $r8 = $r9, $r8
; CV1-NEXT:    addwd $r10 = $r11, $r10
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    addd $r0 = $r0, $r2
; CV1-NEXT:    addwd $r4 = $r5, $r4
; CV1-NEXT:    addd $r5 = $r8, $r10
; CV1-NEXT:    addwd $r6 = $r7, $r6
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    addd $r4 = $r4, $r6
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    addd $r1 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    addd $r0 = $r1, $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 18)
;
; CV2-LABEL: addrhxd:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    extfz $r4 = $r0, 47, 32
; CV2-NEXT:    extfz $r5 = $r2, 47, 32
; CV2-NEXT:    extfz $r6 = $r1, 47, 32
; CV2-NEXT:    extfz $r7 = $r3, 47, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    zxhd $r8 = $r0
; CV2-NEXT:    zxhd $r9 = $r2
; CV2-NEXT:    zxhd $r10 = $r1
; CV2-NEXT:    zxhd $r11 = $r3
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    srld $r15 = $r0, 48
; CV2-NEXT:    srld $r16 = $r2, 48
; CV2-NEXT:    srld $r17 = $r1, 48
; CV2-NEXT:    srld $r32 = $r3, 48
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    srlw $r0 = $r0, 16
; CV2-NEXT:    srlw $r1 = $r1, 16
; CV2-NEXT:    srlw $r2 = $r2, 16
; CV2-NEXT:    srlw $r3 = $r3, 16
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sxhd $r4 = $r4
; CV2-NEXT:    sxhd $r5 = $r5
; CV2-NEXT:    sxhd $r6 = $r6
; CV2-NEXT:    sxhd $r7 = $r7
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sxhd $r8 = $r8
; CV2-NEXT:    sxhd $r9 = $r9
; CV2-NEXT:    sxhd $r10 = $r10
; CV2-NEXT:    sxhd $r11 = $r11
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    sxhd $r15 = $r15
; CV2-NEXT:    sxhd $r16 = $r16
; CV2-NEXT:    sxhd $r17 = $r17
; CV2-NEXT:    sxhd $r32 = $r32
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    sxhd $r0 = $r0
; CV2-NEXT:    sxhd $r1 = $r1
; CV2-NEXT:    sxhd $r2 = $r2
; CV2-NEXT:    sxhd $r3 = $r3
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addwd $r0 = $r2, $r0
; CV2-NEXT:    addwd $r1 = $r3, $r1
; CV2-NEXT:    addwd $r2 = $r32, $r17
; CV2-NEXT:    addwd $r3 = $r16, $r15
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    addwd $r4 = $r5, $r4
; CV2-NEXT:    addwd $r6 = $r7, $r6
; CV2-NEXT:    addwd $r8 = $r9, $r8
; CV2-NEXT:    addwd $r10 = $r11, $r10
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    addd $r0 = $r0, $r1
; CV2-NEXT:    addd $r2 = $r3, $r2
; CV2-NEXT:    addd $r4 = $r4, $r6
; CV2-NEXT:    addd $r5 = $r8, $r10
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    addd $r0 = $r0, $r2
; CV2-NEXT:    addd $r1 = $r5, $r4
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    addd $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 12)
entry:
  %conv.i = sext <16 x i16> %V to <16 x i64>
  %rdx.add.i = tail call i64 @llvm.vector.reduce.add.v16i64(<16 x i64> %conv.i)
  ret i64 %rdx.add.i
}

define i64 @addrwod(<8 x i32> %V) {
; CV1-LABEL: addrwod:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    sxwd $r0 = $r0
; CV1-NEXT:    srad $r4 = $r0, 32
; CV1-NEXT:    srad $r5 = $r2, 32
; CV1-NEXT:    sxwd $r6 = $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    addwd $r0 = $r2, $r0
; CV1-NEXT:    srad $r1 = $r1, 32
; CV1-NEXT:    addwd $r3 = $r3, $r6
; CV1-NEXT:    srad $r7 = $r3, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    addd $r0 = $r0, $r3
; CV1-NEXT:    addwd $r1 = $r7, $r1
; CV1-NEXT:    addwd $r2 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    addd $r1 = $r2, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    addd $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 4)
;
; CV2-LABEL: addrwod:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    srad $r4 = $r0, 32
; CV2-NEXT:    srad $r5 = $r2, 32
; CV2-NEXT:    srad $r6 = $r1, 32
; CV2-NEXT:    srad $r7 = $r3, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sxwd $r0 = $r0
; CV2-NEXT:    sxwd $r1 = $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    addwd $r0 = $r2, $r0
; CV2-NEXT:    addwd $r1 = $r3, $r1
; CV2-NEXT:    addwd $r2 = $r7, $r6
; CV2-NEXT:    addwd $r3 = $r5, $r4
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    addd $r0 = $r0, $r1
; CV2-NEXT:    addd $r2 = $r3, $r2
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    addd $r0 = $r0, $r2
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 4)
entry:
  %conv.i = sext <8 x i32> %V to <8 x i64>
  %rdx.add.i = tail call i64 @llvm.vector.reduce.add.v8i64(<8 x i64> %conv.i)
  ret i64 %rdx.add.i
}

define i64 @addrwpd(<2 x i32> %V) {
; CV1-LABEL: addrwpd:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    sxwd $r0 = $r0
; CV1-NEXT:    extfs $r1 = $r0, 63, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    addd $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 1)
;
; CV2-LABEL: addrwpd:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    sxwd $r0 = $r0
; CV2-NEXT:    extfs $r1 = $r0, 63, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    addd $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 1)
entry:
  %conv.i = sext <2 x i32> %V to <2 x i64>
  %rdx.add.i = tail call i64 @llvm.vector.reduce.add.v2i64(<2 x i64> %conv.i)
  ret i64 %rdx.add.i
}

define i64 @addrwqd(<4 x i32> %V) {
; CV1-LABEL: addrwqd:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    srad $r0 = $r0, 32
; CV1-NEXT:    sxwd $r2 = $r0
; CV1-NEXT:    srad $r3 = $r1, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    addwd $r0 = $r3, $r0
; CV1-NEXT:    addwd $r1 = $r1, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    addd $r0 = $r1, $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 2)
;
; CV2-LABEL: addrwqd:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    srad $r0 = $r0, 32
; CV2-NEXT:    sxwd $r2 = $r0
; CV2-NEXT:    srad $r3 = $r1, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    addwd $r0 = $r3, $r0
; CV2-NEXT:    addwd $r1 = $r1, $r2
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    addd $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 2)
entry:
  %conv.i = sext <4 x i32> %V to <4 x i64>
  %rdx.add.i = tail call i64 @llvm.vector.reduce.add.v4i64(<4 x i64> %conv.i)
  ret i64 %rdx.add.i
}

define i64 @addrwvd(ptr %0) {
; CV1-LABEL: addrwvd:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    lo $r8r9r10r11 = 32[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r32r33r34r35 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    lo $r4r5r6r7 = 96[$r0]
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    lo $r0r1r2r3 = 64[$r0]
; CV1-NEXT:    srld $r9 = $r9, 32
; CV1-NEXT:    sxwd $r36 = $r8
; CV1-NEXT:    sxwd $r40 = $r9
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sxwd $r17 = $r32
; CV1-NEXT:    srld $r32 = $r32, 32
; CV1-NEXT:    srld $r33 = $r33, 32
; CV1-NEXT:    sxwd $r39 = $r33
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    srld $r8 = $r8, 32
; CV1-NEXT:    sxwd $r9 = $r9
; CV1-NEXT:    sxwd $r33 = $r33
; CV1-NEXT:    srld $r42 = $r5, 32
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sxwd $r15 = $r34
; CV1-NEXT:    sxwd $r16 = $r10
; CV1-NEXT:    srld $r34 = $r34, 32
; CV1-NEXT:    srld $r41 = $r1, 32
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    srld $r10 = $r10, 32
; CV1-NEXT:    srld $r35 = $r35, 32
; CV1-NEXT:    sxwd $r37 = $r35
; CV1-NEXT:    sxwd $r38 = $r11
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    addwd $r9 = $r42, $r9
; CV1-NEXT:    srld $r11 = $r11, 32
; CV1-NEXT:    addwd $r33 = $r41, $r33
; CV1-NEXT:    srld $r41 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    addwd $r0 = $r0, $r17
; CV1-NEXT:    addwd $r4 = $r4, $r36
; CV1-NEXT:    srld $r42 = $r4, 32
; CV1-NEXT:    srld $r43 = $r3, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    sxwd $r10 = $r10
; CV1-NEXT:    srld $r17 = $r2, 32
; CV1-NEXT:    sxwd $r34 = $r34
; CV1-NEXT:    srld $r44 = $r7, 32
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    addd $r0 = $r0, $r4
; CV1-NEXT:    sxwd $r8 = $r8
; CV1-NEXT:    sxwd $r32 = $r32
; CV1-NEXT:    srld $r36 = $r6, 32
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    sxwd $r11 = $r11
; CV1-NEXT:    sxwd $r35 = $r35
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    addwd $r1 = $r1, $r39
; CV1-NEXT:    addwd $r5 = $r5, $r40
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    addd $r1 = $r1, $r5
; CV1-NEXT:    addwd $r3 = $r3, $r37
; CV1-NEXT:    addwd $r4 = $r7, $r38
; CV1-NEXT:    addd $r7 = $r33, $r9
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    addd $r3 = $r3, $r4
; CV1-NEXT:    addwd $r11 = $r44, $r11
; CV1-NEXT:    addwd $r35 = $r43, $r35
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    addd $r1 = $r1, $r3
; CV1-NEXT:    addwd $r8 = $r42, $r8
; CV1-NEXT:    addwd $r32 = $r41, $r32
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    addwd $r2 = $r2, $r15
; CV1-NEXT:    addwd $r6 = $r6, $r16
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    addd $r2 = $r2, $r6
; CV1-NEXT:    addwd $r4 = $r36, $r10
; CV1-NEXT:    addwd $r5 = $r17, $r34
; CV1-NEXT:    addd $r6 = $r35, $r11
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    addd $r0 = $r0, $r2
; CV1-NEXT:    addd $r4 = $r5, $r4
; CV1-NEXT:    addd $r5 = $r32, $r8
; CV1-NEXT:    addd $r6 = $r7, $r6
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    addd $r0 = $r0, $r1
; CV1-NEXT:    addd $r4 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    addd $r1 = $r4, $r6
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    addd $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 22)
;
; CV2-LABEL: addrwvd:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    lo $r8r9r10r11 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r32r33r34r35 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    lo $r4r5r6r7 = 96[$r0]
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    lo $r0r1r2r3 = 64[$r0]
; CV2-NEXT:    sxwd $r16 = $r10
; CV2-NEXT:    sxwd $r36 = $r8
; CV2-NEXT:    sxwd $r38 = $r11
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sxwd $r15 = $r34
; CV2-NEXT:    sxwd $r17 = $r32
; CV2-NEXT:    sxwd $r37 = $r35
; CV2-NEXT:    sxwd $r39 = $r33
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    srld $r10 = $r10, 32
; CV2-NEXT:    srld $r32 = $r32, 32
; CV2-NEXT:    srld $r34 = $r34, 32
; CV2-NEXT:    sxwd $r40 = $r9
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    srld $r8 = $r8, 32
; CV2-NEXT:    srld $r11 = $r11, 32
; CV2-NEXT:    srld $r33 = $r33, 32
; CV2-NEXT:    srld $r35 = $r35, 32
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    srld $r9 = $r9, 32
; CV2-NEXT:    srld $r41 = $r2, 32
; CV2-NEXT:    srld $r42 = $r6, 32
; CV2-NEXT:    srld $r43 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    sxwd $r10 = $r10
; CV2-NEXT:    sxwd $r32 = $r32
; CV2-NEXT:    sxwd $r34 = $r34
; CV2-NEXT:    srld $r44 = $r4, 32
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    sxwd $r8 = $r8
; CV2-NEXT:    sxwd $r35 = $r35
; CV2-NEXT:    srld $r45 = $r3, 32
; CV2-NEXT:    srld $r46 = $r7, 32
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    sxwd $r11 = $r11
; CV2-NEXT:    sxwd $r33 = $r33
; CV2-NEXT:    srld $r47 = $r1, 32
; CV2-NEXT:    srld $r48 = $r5, 32
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    sxwd $r9 = $r9
; CV2-NEXT:    addwd $r11 = $r46, $r11
; CV2-NEXT:    addwd $r33 = $r47, $r33
; CV2-NEXT:    addwd $r35 = $r45, $r35
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    addwd $r8 = $r44, $r8
; CV2-NEXT:    addwd $r9 = $r48, $r9
; CV2-NEXT:    addwd $r10 = $r42, $r10
; CV2-NEXT:    addwd $r32 = $r43, $r32
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    addwd $r1 = $r1, $r39
; CV2-NEXT:    addwd $r5 = $r5, $r40
; CV2-NEXT:    addwd $r7 = $r7, $r38
; CV2-NEXT:    addwd $r34 = $r41, $r34
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    addwd $r0 = $r0, $r17
; CV2-NEXT:    addwd $r3 = $r3, $r37
; CV2-NEXT:    addwd $r4 = $r4, $r36
; CV2-NEXT:    addwd $r6 = $r6, $r16
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    addd $r0 = $r0, $r4
; CV2-NEXT:    addd $r1 = $r1, $r5
; CV2-NEXT:    addwd $r2 = $r2, $r15
; CV2-NEXT:    addd $r3 = $r3, $r7
; CV2-NEXT:    ;; # (end cycle 15)
; CV2-NEXT:    addd $r2 = $r2, $r6
; CV2-NEXT:    addd $r4 = $r34, $r10
; CV2-NEXT:    addd $r5 = $r32, $r8
; CV2-NEXT:    addd $r6 = $r35, $r11
; CV2-NEXT:    ;; # (end cycle 16)
; CV2-NEXT:    addd $r0 = $r0, $r2
; CV2-NEXT:    addd $r1 = $r1, $r3
; CV2-NEXT:    addd $r4 = $r5, $r4
; CV2-NEXT:    addd $r7 = $r33, $r9
; CV2-NEXT:    ;; # (end cycle 17)
; CV2-NEXT:    addd $r0 = $r0, $r1
; CV2-NEXT:    addd $r6 = $r7, $r6
; CV2-NEXT:    ;; # (end cycle 18)
; CV2-NEXT:    addd $r1 = $r4, $r6
; CV2-NEXT:    ;; # (end cycle 19)
; CV2-NEXT:    addd $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 20)
entry:
  %V = load <32 x i32>, ptr %0
  %conv.i = sext <32 x i32> %V to <32 x i64>
  %rdx.add.i = tail call i64 @llvm.vector.reduce.add.v32i64(<32 x i64> %conv.i)
  ret i64 %rdx.add.i
}

define i64 @addrwxd(ptr %0) {
; CV1-LABEL: addrwxd:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r0r1r2r3 = 32[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srld $r5 = $r5, 32
; CV1-NEXT:    srld $r7 = $r7, 32
; CV1-NEXT:    sxwd $r8 = $r5
; CV1-NEXT:    sxwd $r9 = $r7
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srld $r4 = $r4, 32
; CV1-NEXT:    srld $r6 = $r6, 32
; CV1-NEXT:    sxwd $r10 = $r4
; CV1-NEXT:    sxwd $r11 = $r6
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sxwd $r5 = $r5
; CV1-NEXT:    sxwd $r7 = $r7
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sxwd $r4 = $r4
; CV1-NEXT:    sxwd $r6 = $r6
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    srld $r0 = $r0, 32
; CV1-NEXT:    srld $r2 = $r2, 32
; CV1-NEXT:    addwd $r10 = $r0, $r10
; CV1-NEXT:    addwd $r11 = $r2, $r11
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    srld $r1 = $r1, 32
; CV1-NEXT:    srld $r3 = $r3, 32
; CV1-NEXT:    addwd $r8 = $r1, $r8
; CV1-NEXT:    addwd $r9 = $r3, $r9
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    addwd $r0 = $r0, $r4
; CV1-NEXT:    addwd $r2 = $r2, $r6
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    addd $r0 = $r0, $r2
; CV1-NEXT:    addwd $r1 = $r1, $r5
; CV1-NEXT:    addd $r2 = $r8, $r9
; CV1-NEXT:    addwd $r3 = $r3, $r7
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    addd $r1 = $r1, $r3
; CV1-NEXT:    addd $r3 = $r10, $r11
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    addd $r0 = $r0, $r1
; CV1-NEXT:    addd $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    addd $r0 = $r2, $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 13)
;
; CV2-LABEL: addrwxd:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r0r1r2r3 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    srld $r9 = $r5, 32
; CV2-NEXT:    srld $r11 = $r7, 32
; CV2-NEXT:    srld $r16 = $r4, 32
; CV2-NEXT:    srld $r32 = $r6, 32
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srld $r8 = $r1, 32
; CV2-NEXT:    srld $r10 = $r3, 32
; CV2-NEXT:    srld $r15 = $r0, 32
; CV2-NEXT:    srld $r17 = $r2, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sxwd $r4 = $r4
; CV2-NEXT:    sxwd $r5 = $r5
; CV2-NEXT:    sxwd $r6 = $r6
; CV2-NEXT:    sxwd $r7 = $r7
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    sxwd $r9 = $r9
; CV2-NEXT:    sxwd $r11 = $r11
; CV2-NEXT:    sxwd $r16 = $r16
; CV2-NEXT:    sxwd $r32 = $r32
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    addwd $r0 = $r0, $r4
; CV2-NEXT:    addwd $r1 = $r1, $r5
; CV2-NEXT:    addwd $r2 = $r2, $r6
; CV2-NEXT:    addwd $r3 = $r3, $r7
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addwd $r4 = $r17, $r32
; CV2-NEXT:    addwd $r5 = $r15, $r16
; CV2-NEXT:    addwd $r6 = $r10, $r11
; CV2-NEXT:    addwd $r7 = $r8, $r9
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    addd $r0 = $r0, $r2
; CV2-NEXT:    addd $r1 = $r1, $r3
; CV2-NEXT:    addd $r4 = $r5, $r4
; CV2-NEXT:    addd $r6 = $r7, $r6
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    addd $r0 = $r0, $r1
; CV2-NEXT:    addd $r1 = $r4, $r6
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    addd $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 11)
entry:
  %V = load <16 x i32>, ptr %0
  %conv.i = sext <16 x i32> %V to <16 x i64>
  %rdx.add.i = tail call i64 @llvm.vector.reduce.add.v16i64(<16 x i64> %conv.i)
  ret i64 %rdx.add.i
}

define i64 @addurbod(<8 x i8> %V) {
; CV1-LABEL: addurbod:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    extfz $r1 = $r0, 23, 16
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    zxbd $r3 = $r0
; CV1-NEXT:    srlw $r4 = $r0, 24
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    extfz $r0 = $r0, 15, 8
; CV1-NEXT:    extfz $r5 = $r2, 23, 16
; CV1-NEXT:    zxbd $r6 = $r2
; CV1-NEXT:    srlw $r7 = $r2, 24
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andd $r1 = $r1, 255
; CV1-NEXT:    extfz $r2 = $r2, 15, 8
; CV1-NEXT:    andd $r4 = $r4, 255
; CV1-NEXT:    andd $r5 = $r5, 255
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    andd $r0 = $r0, 255
; CV1-NEXT:    andd $r2 = $r2, 255
; CV1-NEXT:    andd $r3 = $r3, 255
; CV1-NEXT:    andd $r7 = $r7, 255
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    adduwd $r0 = $r2, $r0
; CV1-NEXT:    adduwd $r2 = $r7, $r4
; CV1-NEXT:    andd $r6 = $r6, 255
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    adduwd $r1 = $r5, $r1
; CV1-NEXT:    adduwd $r3 = $r6, $r3
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    adduwd $r0 = $r2, $r0
; CV1-NEXT:    adduwd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    adduwd $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: addurbod:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    extfz $r1 = $r0, 23, 16
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    zxbd $r3 = $r0
; CV2-NEXT:    srlw $r6 = $r0, 24
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    extfz $r0 = $r0, 15, 8
; CV2-NEXT:    extfz $r4 = $r2, 23, 16
; CV2-NEXT:    zxbd $r5 = $r2
; CV2-NEXT:    srlw $r7 = $r2, 24
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    andd $r1 = $r1, 255
; CV2-NEXT:    extfz $r2 = $r2, 15, 8
; CV2-NEXT:    andd $r3 = $r3, 255
; CV2-NEXT:    andd $r4 = $r4, 255
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    andd $r0 = $r0, 255
; CV2-NEXT:    andd $r5 = $r5, 255
; CV2-NEXT:    andd $r6 = $r6, 255
; CV2-NEXT:    andd $r7 = $r7, 255
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    adduwd $r1 = $r4, $r1
; CV2-NEXT:    andd $r2 = $r2, 255
; CV2-NEXT:    adduwd $r3 = $r5, $r3
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    adduwd $r0 = $r2, $r0
; CV2-NEXT:    adduwd $r1 = $r1, $r3
; CV2-NEXT:    adduwd $r2 = $r7, $r6
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    adduwd $r0 = $r2, $r0
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    adduwd $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
entry:
  %conv.i = zext <8 x i8> %V to <8 x i64>
  %rdx.add.i = tail call i64 @llvm.vector.reduce.add.v8i64(<8 x i64> %conv.i)
  ret i64 %rdx.add.i
}

define i64 @addurbpd(<2 x i8> %V) {
; CV1-LABEL: addurbpd:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    adduwd $r0 = $r1, $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 1)
;
; CV2-LABEL: addurbpd:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    adduwd $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 1)
entry:
  %conv.i = zext <2 x i8> %V to <2 x i64>
  %rdx.add.i = tail call i64 @llvm.vector.reduce.add.v2i64(<2 x i64> %conv.i)
  ret i64 %rdx.add.i
}

define i64 @addurbqd(<4 x i8> %V) {
; CV1-LABEL: addurbqd:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    extfz $r0 = $r0, 23, 16
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    srlw $r2 = $r0, 24
; CV1-NEXT:    zxbd $r3 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd $r0 = $r0, 255
; CV1-NEXT:    andd $r1 = $r1, 255
; CV1-NEXT:    andd $r2 = $r2, 255
; CV1-NEXT:    andd $r3 = $r3, 255
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    adduwd $r0 = $r0, $r3
; CV1-NEXT:    adduwd $r1 = $r2, $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    adduwd $r0 = $r1, $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: addurbqd:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    extfz $r0 = $r0, 23, 16
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    srlw $r2 = $r0, 24
; CV2-NEXT:    zxbd $r3 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    andd $r0 = $r0, 255
; CV2-NEXT:    andd $r1 = $r1, 255
; CV2-NEXT:    andd $r2 = $r2, 255
; CV2-NEXT:    andd $r3 = $r3, 255
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    adduwd $r0 = $r0, $r3
; CV2-NEXT:    adduwd $r1 = $r2, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    adduwd $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 3)
entry:
  %conv.i = zext <4 x i8> %V to <4 x i64>
  %rdx.add.i = tail call i64 @llvm.vector.reduce.add.v4i64(<4 x i64> %conv.i)
  ret i64 %rdx.add.i
}

define i64 @addurbvd(<32 x i8> %V) {
; CV1-LABEL: addurbvd:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    srld $r8 = $r1, 32
; CV1-NEXT:    srld $r9 = $r3, 32
; CV1-NEXT:    srld $r15 = $r0, 32
; CV1-NEXT:    srld $r16 = $r2, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    extfz $r4 = $r1, 23, 16
; CV1-NEXT:    extfz $r5 = $r3, 23, 16
; CV1-NEXT:    zxbd $r33 = $r1
; CV1-NEXT:    zxbd $r34 = $r3
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    extfz $r6 = $r0, 23, 16
; CV1-NEXT:    extfz $r7 = $r2, 23, 16
; CV1-NEXT:    zxbd $r35 = $r8
; CV1-NEXT:    zxbd $r36 = $r9
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    extfz $r10 = $r8, 23, 16
; CV1-NEXT:    extfz $r11 = $r9, 23, 16
; CV1-NEXT:    zxbd $r37 = $r0
; CV1-NEXT:    zxbd $r38 = $r2
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    extfz $r17 = $r15, 23, 16
; CV1-NEXT:    extfz $r32 = $r16, 23, 16
; CV1-NEXT:    zxbd $r39 = $r15
; CV1-NEXT:    zxbd $r40 = $r16
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    srlw $r41 = $r1, 24
; CV1-NEXT:    srlw $r42 = $r3, 24
; CV1-NEXT:    srlw $r43 = $r0, 24
; CV1-NEXT:    srlw $r44 = $r2, 24
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    extfz $r1 = $r1, 15, 8
; CV1-NEXT:    extfz $r3 = $r3, 15, 8
; CV1-NEXT:    srlw $r45 = $r8, 24
; CV1-NEXT:    srlw $r46 = $r9, 24
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    extfz $r0 = $r0, 15, 8
; CV1-NEXT:    extfz $r2 = $r2, 15, 8
; CV1-NEXT:    srlw $r47 = $r15, 24
; CV1-NEXT:    srlw $r48 = $r16, 24
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    andd $r1 = $r1, 255
; CV1-NEXT:    andd $r3 = $r3, 255
; CV1-NEXT:    extfz $r8 = $r8, 15, 8
; CV1-NEXT:    extfz $r9 = $r9, 15, 8
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    andd $r8 = $r8, 255
; CV1-NEXT:    andd $r9 = $r9, 255
; CV1-NEXT:    extfz $r15 = $r15, 15, 8
; CV1-NEXT:    extfz $r16 = $r16, 15, 8
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    andd $r0 = $r0, 255
; CV1-NEXT:    andd $r2 = $r2, 255
; CV1-NEXT:    andd $r15 = $r15, 255
; CV1-NEXT:    andd $r16 = $r16, 255
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    andd $r4 = $r4, 255
; CV1-NEXT:    andd $r5 = $r5, 255
; CV1-NEXT:    andd $r10 = $r10, 255
; CV1-NEXT:    andd $r11 = $r11, 255
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    andd $r33 = $r33, 255
; CV1-NEXT:    andd $r34 = $r34, 255
; CV1-NEXT:    andd $r35 = $r35, 255
; CV1-NEXT:    andd $r36 = $r36, 255
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    andd $r41 = $r41, 255
; CV1-NEXT:    andd $r42 = $r42, 255
; CV1-NEXT:    andd $r45 = $r45, 255
; CV1-NEXT:    andd $r46 = $r46, 255
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    adduwd $r8 = $r9, $r8
; CV1-NEXT:    andd $r9 = $r47, 255
; CV1-NEXT:    adduwd $r15 = $r16, $r15
; CV1-NEXT:    andd $r16 = $r48, 255
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    adduwd $r0 = $r2, $r0
; CV1-NEXT:    adduwd $r1 = $r3, $r1
; CV1-NEXT:    andd $r2 = $r43, 255
; CV1-NEXT:    andd $r3 = $r44, 255
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    andd $r37 = $r37, 255
; CV1-NEXT:    andd $r38 = $r38, 255
; CV1-NEXT:    andd $r39 = $r39, 255
; CV1-NEXT:    andd $r40 = $r40, 255
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    andd $r6 = $r6, 255
; CV1-NEXT:    andd $r7 = $r7, 255
; CV1-NEXT:    andd $r17 = $r17, 255
; CV1-NEXT:    andd $r32 = $r32, 255
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    adduwd $r9 = $r16, $r9
; CV1-NEXT:    adduwd $r16 = $r46, $r45
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    adduwd $r2 = $r3, $r2
; CV1-NEXT:    adduwd $r3 = $r42, $r41
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    adduwd $r35 = $r36, $r35
; CV1-NEXT:    adduwd $r39 = $r40, $r39
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    adduwd $r33 = $r34, $r33
; CV1-NEXT:    adduwd $r36 = $r38, $r37
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    adduwd $r10 = $r11, $r10
; CV1-NEXT:    adduwd $r17 = $r32, $r17
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    adduwd $r4 = $r5, $r4
; CV1-NEXT:    adduwd $r6 = $r7, $r6
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    adduwd $r4 = $r4, $r6
; CV1-NEXT:    adduwd $r5 = $r10, $r17
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    adduwd $r6 = $r33, $r36
; CV1-NEXT:    adduwd $r7 = $r35, $r39
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    adduwd $r2 = $r3, $r2
; CV1-NEXT:    adduwd $r3 = $r16, $r9
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    adduwd $r0 = $r1, $r0
; CV1-NEXT:    adduwd $r1 = $r8, $r15
; CV1-NEXT:    ;; # (end cycle 27)
; CV1-NEXT:    adduwd $r0 = $r1, $r0
; CV1-NEXT:    adduwd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 28)
; CV1-NEXT:    adduwd $r2 = $r7, $r6
; CV1-NEXT:    adduwd $r3 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 29)
; CV1-NEXT:    adduwd $r0 = $r1, $r0
; CV1-NEXT:    adduwd $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 30)
; CV1-NEXT:    adduwd $r0 = $r0, $r2
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 31)
;
; CV2-LABEL: addurbvd:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    srld $r8 = $r1, 32
; CV2-NEXT:    srld $r9 = $r3, 32
; CV2-NEXT:    srld $r10 = $r0, 32
; CV2-NEXT:    srld $r11 = $r2, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    extfz $r4 = $r1, 23, 16
; CV2-NEXT:    extfz $r5 = $r3, 23, 16
; CV2-NEXT:    extfz $r6 = $r0, 23, 16
; CV2-NEXT:    extfz $r7 = $r2, 23, 16
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    extfz $r15 = $r8, 23, 16
; CV2-NEXT:    extfz $r16 = $r9, 23, 16
; CV2-NEXT:    extfz $r17 = $r10, 23, 16
; CV2-NEXT:    extfz $r32 = $r11, 23, 16
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    zxbd $r33 = $r1
; CV2-NEXT:    zxbd $r34 = $r3
; CV2-NEXT:    zxbd $r35 = $r0
; CV2-NEXT:    zxbd $r36 = $r2
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    zxbd $r37 = $r8
; CV2-NEXT:    zxbd $r38 = $r9
; CV2-NEXT:    zxbd $r39 = $r10
; CV2-NEXT:    zxbd $r40 = $r11
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    srlw $r41 = $r1, 24
; CV2-NEXT:    srlw $r42 = $r3, 24
; CV2-NEXT:    srlw $r43 = $r0, 24
; CV2-NEXT:    srlw $r44 = $r2, 24
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    srlw $r45 = $r8, 24
; CV2-NEXT:    srlw $r46 = $r9, 24
; CV2-NEXT:    srlw $r47 = $r10, 24
; CV2-NEXT:    srlw $r48 = $r11, 24
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    extfz $r0 = $r0, 15, 8
; CV2-NEXT:    extfz $r1 = $r1, 15, 8
; CV2-NEXT:    extfz $r2 = $r2, 15, 8
; CV2-NEXT:    extfz $r3 = $r3, 15, 8
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    extfz $r8 = $r8, 15, 8
; CV2-NEXT:    extfz $r9 = $r9, 15, 8
; CV2-NEXT:    extfz $r10 = $r10, 15, 8
; CV2-NEXT:    extfz $r11 = $r11, 15, 8
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    andd $r4 = $r4, 255
; CV2-NEXT:    andd $r5 = $r5, 255
; CV2-NEXT:    andd $r6 = $r6, 255
; CV2-NEXT:    andd $r7 = $r7, 255
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    andd $r15 = $r15, 255
; CV2-NEXT:    andd $r16 = $r16, 255
; CV2-NEXT:    andd $r17 = $r17, 255
; CV2-NEXT:    andd $r32 = $r32, 255
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    andd $r33 = $r33, 255
; CV2-NEXT:    andd $r34 = $r34, 255
; CV2-NEXT:    andd $r35 = $r35, 255
; CV2-NEXT:    andd $r36 = $r36, 255
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    andd $r37 = $r37, 255
; CV2-NEXT:    andd $r38 = $r38, 255
; CV2-NEXT:    andd $r39 = $r39, 255
; CV2-NEXT:    andd $r40 = $r40, 255
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    andd $r41 = $r41, 255
; CV2-NEXT:    andd $r42 = $r42, 255
; CV2-NEXT:    andd $r43 = $r43, 255
; CV2-NEXT:    andd $r44 = $r44, 255
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    andd $r45 = $r45, 255
; CV2-NEXT:    andd $r46 = $r46, 255
; CV2-NEXT:    andd $r47 = $r47, 255
; CV2-NEXT:    andd $r48 = $r48, 255
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    andd $r0 = $r0, 255
; CV2-NEXT:    andd $r1 = $r1, 255
; CV2-NEXT:    andd $r2 = $r2, 255
; CV2-NEXT:    andd $r3 = $r3, 255
; CV2-NEXT:    ;; # (end cycle 15)
; CV2-NEXT:    andd $r8 = $r8, 255
; CV2-NEXT:    andd $r9 = $r9, 255
; CV2-NEXT:    andd $r10 = $r10, 255
; CV2-NEXT:    andd $r11 = $r11, 255
; CV2-NEXT:    ;; # (end cycle 16)
; CV2-NEXT:    adduwd $r0 = $r2, $r0
; CV2-NEXT:    adduwd $r1 = $r3, $r1
; CV2-NEXT:    adduwd $r8 = $r9, $r8
; CV2-NEXT:    adduwd $r10 = $r11, $r10
; CV2-NEXT:    ;; # (end cycle 17)
; CV2-NEXT:    adduwd $r2 = $r48, $r47
; CV2-NEXT:    adduwd $r3 = $r46, $r45
; CV2-NEXT:    adduwd $r9 = $r44, $r43
; CV2-NEXT:    adduwd $r11 = $r42, $r41
; CV2-NEXT:    ;; # (end cycle 18)
; CV2-NEXT:    adduwd $r33 = $r34, $r33
; CV2-NEXT:    adduwd $r35 = $r36, $r35
; CV2-NEXT:    adduwd $r37 = $r38, $r37
; CV2-NEXT:    adduwd $r39 = $r40, $r39
; CV2-NEXT:    ;; # (end cycle 19)
; CV2-NEXT:    adduwd $r4 = $r5, $r4
; CV2-NEXT:    adduwd $r6 = $r7, $r6
; CV2-NEXT:    adduwd $r15 = $r16, $r15
; CV2-NEXT:    adduwd $r17 = $r32, $r17
; CV2-NEXT:    ;; # (end cycle 20)
; CV2-NEXT:    adduwd $r4 = $r4, $r6
; CV2-NEXT:    adduwd $r5 = $r15, $r17
; CV2-NEXT:    adduwd $r6 = $r33, $r35
; CV2-NEXT:    adduwd $r7 = $r37, $r39
; CV2-NEXT:    ;; # (end cycle 21)
; CV2-NEXT:    adduwd $r0 = $r1, $r0
; CV2-NEXT:    adduwd $r1 = $r8, $r10
; CV2-NEXT:    adduwd $r2 = $r3, $r2
; CV2-NEXT:    adduwd $r9 = $r11, $r9
; CV2-NEXT:    ;; # (end cycle 22)
; CV2-NEXT:    adduwd $r0 = $r1, $r0
; CV2-NEXT:    adduwd $r1 = $r2, $r9
; CV2-NEXT:    adduwd $r2 = $r7, $r6
; CV2-NEXT:    adduwd $r3 = $r5, $r4
; CV2-NEXT:    ;; # (end cycle 23)
; CV2-NEXT:    adduwd $r0 = $r1, $r0
; CV2-NEXT:    adduwd $r2 = $r3, $r2
; CV2-NEXT:    ;; # (end cycle 24)
; CV2-NEXT:    adduwd $r0 = $r0, $r2
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 25)
entry:
  %conv.i = zext <32 x i8> %V to <32 x i64>
  %rdx.add.i = tail call i64 @llvm.vector.reduce.add.v32i64(<32 x i64> %conv.i)
  ret i64 %rdx.add.i
}

define i64 @addurbxd(<16 x i8> %V) {
; CV1-LABEL: addurbxd:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    srld $r3 = $r1, 32
; CV1-NEXT:    extfz $r4 = $r0, 15, 8
; CV1-NEXT:    extfz $r5 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    extfz $r6 = $r2, 15, 8
; CV1-NEXT:    extfz $r7 = $r3, 15, 8
; CV1-NEXT:    srlw $r8 = $r2, 24
; CV1-NEXT:    srlw $r9 = $r3, 24
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srlw $r10 = $r0, 24
; CV1-NEXT:    srlw $r11 = $r1, 24
; CV1-NEXT:    zxbd $r15 = $r2
; CV1-NEXT:    zxbd $r16 = $r3
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    extfz $r2 = $r2, 23, 16
; CV1-NEXT:    extfz $r3 = $r3, 23, 16
; CV1-NEXT:    zxbd $r17 = $r0
; CV1-NEXT:    zxbd $r32 = $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    extfz $r0 = $r0, 23, 16
; CV1-NEXT:    extfz $r1 = $r1, 23, 16
; CV1-NEXT:    andd $r2 = $r2, 255
; CV1-NEXT:    andd $r3 = $r3, 255
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    andd $r0 = $r0, 255
; CV1-NEXT:    andd $r1 = $r1, 255
; CV1-NEXT:    andd $r6 = $r6, 255
; CV1-NEXT:    andd $r7 = $r7, 255
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    andd $r8 = $r8, 255
; CV1-NEXT:    andd $r9 = $r9, 255
; CV1-NEXT:    andd $r15 = $r15, 255
; CV1-NEXT:    andd $r16 = $r16, 255
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    adduwd $r0 = $r1, $r0
; CV1-NEXT:    adduwd $r1 = $r3, $r2
; CV1-NEXT:    andd $r17 = $r17, 255
; CV1-NEXT:    andd $r32 = $r32, 255
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    andd $r2 = $r10, 255
; CV1-NEXT:    andd $r3 = $r11, 255
; CV1-NEXT:    andd $r4 = $r4, 255
; CV1-NEXT:    andd $r5 = $r5, 255
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    adduwd $r10 = $r32, $r17
; CV1-NEXT:    adduwd $r11 = $r16, $r15
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    adduwd $r2 = $r3, $r2
; CV1-NEXT:    adduwd $r3 = $r9, $r8
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    adduwd $r4 = $r5, $r4
; CV1-NEXT:    adduwd $r5 = $r7, $r6
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    adduwd $r2 = $r3, $r2
; CV1-NEXT:    adduwd $r4 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    adduwd $r0 = $r1, $r0
; CV1-NEXT:    adduwd $r3 = $r11, $r10
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    adduwd $r0 = $r0, $r3
; CV1-NEXT:    adduwd $r1 = $r2, $r4
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    adduwd $r0 = $r1, $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 15)
;
; CV2-LABEL: addurbxd:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    srld $r3 = $r1, 32
; CV2-NEXT:    extfz $r4 = $r0, 15, 8
; CV2-NEXT:    extfz $r5 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    extfz $r6 = $r2, 15, 8
; CV2-NEXT:    extfz $r7 = $r3, 15, 8
; CV2-NEXT:    srlw $r8 = $r2, 24
; CV2-NEXT:    srlw $r9 = $r3, 24
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    srlw $r10 = $r0, 24
; CV2-NEXT:    srlw $r11 = $r1, 24
; CV2-NEXT:    zxbd $r15 = $r2
; CV2-NEXT:    zxbd $r16 = $r3
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    extfz $r2 = $r2, 23, 16
; CV2-NEXT:    extfz $r3 = $r3, 23, 16
; CV2-NEXT:    zxbd $r17 = $r0
; CV2-NEXT:    zxbd $r32 = $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    extfz $r0 = $r0, 23, 16
; CV2-NEXT:    extfz $r1 = $r1, 23, 16
; CV2-NEXT:    andd $r4 = $r4, 255
; CV2-NEXT:    andd $r5 = $r5, 255
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    andd $r6 = $r6, 255
; CV2-NEXT:    andd $r7 = $r7, 255
; CV2-NEXT:    andd $r8 = $r8, 255
; CV2-NEXT:    andd $r9 = $r9, 255
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    andd $r10 = $r10, 255
; CV2-NEXT:    andd $r11 = $r11, 255
; CV2-NEXT:    andd $r15 = $r15, 255
; CV2-NEXT:    andd $r16 = $r16, 255
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    andd $r2 = $r2, 255
; CV2-NEXT:    andd $r3 = $r3, 255
; CV2-NEXT:    andd $r17 = $r17, 255
; CV2-NEXT:    andd $r32 = $r32, 255
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    andd $r0 = $r0, 255
; CV2-NEXT:    andd $r1 = $r1, 255
; CV2-NEXT:    adduwd $r8 = $r9, $r8
; CV2-NEXT:    adduwd $r10 = $r11, $r10
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    adduwd $r0 = $r1, $r0
; CV2-NEXT:    adduwd $r1 = $r3, $r2
; CV2-NEXT:    adduwd $r2 = $r32, $r17
; CV2-NEXT:    adduwd $r3 = $r16, $r15
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    adduwd $r0 = $r1, $r0
; CV2-NEXT:    adduwd $r2 = $r3, $r2
; CV2-NEXT:    adduwd $r4 = $r5, $r4
; CV2-NEXT:    adduwd $r5 = $r7, $r6
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    adduwd $r0 = $r0, $r2
; CV2-NEXT:    adduwd $r4 = $r5, $r4
; CV2-NEXT:    adduwd $r5 = $r8, $r10
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    adduwd $r1 = $r5, $r4
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    adduwd $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 13)
entry:
  %conv.i = zext <16 x i8> %V to <16 x i64>
  %rdx.add.i = tail call i64 @llvm.vector.reduce.add.v16i64(<16 x i64> %conv.i)
  ret i64 %rdx.add.i
}

define i64 @addurhod(<8 x i16> %V) {
; CV1-LABEL: addurhod:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    extfz $r2 = $r0, 47, 32
; CV1-NEXT:    extfz $r3 = $r1, 47, 32
; CV1-NEXT:    srld $r6 = $r0, 48
; CV1-NEXT:    srld $r7 = $r1, 48
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srlw $r0 = $r0, 16
; CV1-NEXT:    srlw $r1 = $r1, 16
; CV1-NEXT:    zxhd $r4 = $r0
; CV1-NEXT:    zxhd $r5 = $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    clrf $r2 = $r2, 63, 16
; CV1-NEXT:    clrf $r3 = $r3, 63, 16
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    clrf $r4 = $r4, 63, 16
; CV1-NEXT:    clrf $r5 = $r5, 63, 16
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    clrf $r6 = $r6, 63, 16
; CV1-NEXT:    clrf $r7 = $r7, 63, 16
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    clrf $r0 = $r0, 63, 16
; CV1-NEXT:    clrf $r1 = $r1, 63, 16
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    adduwd $r0 = $r1, $r0
; CV1-NEXT:    adduwd $r1 = $r7, $r6
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    adduwd $r2 = $r3, $r2
; CV1-NEXT:    adduwd $r4 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    adduwd $r0 = $r1, $r0
; CV1-NEXT:    adduwd $r2 = $r2, $r4
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    adduwd $r0 = $r0, $r2
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 9)
;
; CV2-LABEL: addurhod:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    extfz $r2 = $r0, 47, 32
; CV2-NEXT:    extfz $r3 = $r1, 47, 32
; CV2-NEXT:    zxhd $r4 = $r0
; CV2-NEXT:    zxhd $r5 = $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srlw $r0 = $r0, 16
; CV2-NEXT:    srlw $r1 = $r1, 16
; CV2-NEXT:    srld $r6 = $r0, 48
; CV2-NEXT:    srld $r7 = $r1, 48
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    clrf $r2 = $r2, 63, 16
; CV2-NEXT:    clrf $r3 = $r3, 63, 16
; CV2-NEXT:    clrf $r4 = $r4, 63, 16
; CV2-NEXT:    clrf $r5 = $r5, 63, 16
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    clrf $r0 = $r0, 63, 16
; CV2-NEXT:    clrf $r1 = $r1, 63, 16
; CV2-NEXT:    clrf $r6 = $r6, 63, 16
; CV2-NEXT:    clrf $r7 = $r7, 63, 16
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    adduwd $r0 = $r1, $r0
; CV2-NEXT:    adduwd $r1 = $r7, $r6
; CV2-NEXT:    adduwd $r2 = $r3, $r2
; CV2-NEXT:    adduwd $r4 = $r5, $r4
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    adduwd $r0 = $r1, $r0
; CV2-NEXT:    adduwd $r2 = $r2, $r4
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    adduwd $r0 = $r0, $r2
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 6)
entry:
  %conv.i = zext <8 x i16> %V to <8 x i64>
  %rdx.add.i = tail call i64 @llvm.vector.reduce.add.v8i64(<8 x i64> %conv.i)
  ret i64 %rdx.add.i
}

define i64 @addurhpd(<2 x i16> %V) {
; CV1-LABEL: addurhpd:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    extfz $r1 = $r0, 31, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    adduwd $r0 = $r1, $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 1)
;
; CV2-LABEL: addurhpd:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    extfz $r1 = $r0, 31, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    adduwd $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 1)
entry:
  %conv.i = zext <2 x i16> %V to <2 x i64>
  %rdx.add.i = tail call i64 @llvm.vector.reduce.add.v2i64(<2 x i64> %conv.i)
  ret i64 %rdx.add.i
}

define i64 @addurhqd(<4 x i16> %V) {
; CV1-LABEL: addurhqd:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    extfz $r0 = $r0, 47, 32
; CV1-NEXT:    srlw $r1 = $r0, 16
; CV1-NEXT:    srld $r2 = $r0, 48
; CV1-NEXT:    zxhd $r3 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    clrf $r1 = $r1, 63, 16
; CV1-NEXT:    clrf $r2 = $r2, 63, 16
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    clrf $r0 = $r0, 63, 16
; CV1-NEXT:    clrf $r3 = $r3, 63, 16
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    adduwd $r0 = $r0, $r3
; CV1-NEXT:    adduwd $r1 = $r2, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    adduwd $r0 = $r1, $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 4)
;
; CV2-LABEL: addurhqd:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    extfz $r0 = $r0, 47, 32
; CV2-NEXT:    srlw $r1 = $r0, 16
; CV2-NEXT:    srld $r2 = $r0, 48
; CV2-NEXT:    zxhd $r3 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    clrf $r0 = $r0, 63, 16
; CV2-NEXT:    clrf $r1 = $r1, 63, 16
; CV2-NEXT:    clrf $r2 = $r2, 63, 16
; CV2-NEXT:    clrf $r3 = $r3, 63, 16
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    adduwd $r0 = $r0, $r3
; CV2-NEXT:    adduwd $r1 = $r2, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    adduwd $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 3)
entry:
  %conv.i = zext <4 x i16> %V to <4 x i64>
  %rdx.add.i = tail call i64 @llvm.vector.reduce.add.v4i64(<4 x i64> %conv.i)
  ret i64 %rdx.add.i
}

define i64 @addurhvd(ptr %0) {
; CV1-LABEL: addurhvd:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r0r1r2r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    extfz $r9 = $r6, 47, 32
; CV1-NEXT:    extfz $r11 = $r4, 47, 32
; CV1-NEXT:    srld $r42 = $r6, 48
; CV1-NEXT:    srld $r44 = $r4, 48
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    extfz $r8 = $r2, 47, 32
; CV1-NEXT:    extfz $r10 = $r0, 47, 32
; CV1-NEXT:    srld $r41 = $r2, 48
; CV1-NEXT:    srld $r43 = $r0, 48
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    extfz $r15 = $r3, 47, 32
; CV1-NEXT:    extfz $r16 = $r7, 47, 32
; CV1-NEXT:    srld $r45 = $r3, 48
; CV1-NEXT:    srld $r46 = $r7, 48
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    extfz $r17 = $r1, 47, 32
; CV1-NEXT:    extfz $r32 = $r5, 47, 32
; CV1-NEXT:    srld $r47 = $r1, 48
; CV1-NEXT:    srld $r48 = $r5, 48
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    srlw $r2 = $r2, 16
; CV1-NEXT:    srlw $r6 = $r6, 16
; CV1-NEXT:    zxhd $r33 = $r2
; CV1-NEXT:    zxhd $r34 = $r6
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    srlw $r0 = $r0, 16
; CV1-NEXT:    srlw $r4 = $r4, 16
; CV1-NEXT:    zxhd $r35 = $r0
; CV1-NEXT:    zxhd $r36 = $r4
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    srlw $r3 = $r3, 16
; CV1-NEXT:    srlw $r7 = $r7, 16
; CV1-NEXT:    zxhd $r37 = $r3
; CV1-NEXT:    zxhd $r38 = $r7
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    srlw $r1 = $r1, 16
; CV1-NEXT:    srlw $r5 = $r5, 16
; CV1-NEXT:    zxhd $r39 = $r1
; CV1-NEXT:    zxhd $r40 = $r5
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    clrf $r8 = $r8, 63, 16
; CV1-NEXT:    clrf $r9 = $r9, 63, 16
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    clrf $r10 = $r10, 63, 16
; CV1-NEXT:    clrf $r11 = $r11, 63, 16
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    clrf $r15 = $r15, 63, 16
; CV1-NEXT:    clrf $r16 = $r16, 63, 16
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    clrf $r17 = $r17, 63, 16
; CV1-NEXT:    clrf $r32 = $r32, 63, 16
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    clrf $r33 = $r33, 63, 16
; CV1-NEXT:    clrf $r34 = $r34, 63, 16
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    clrf $r35 = $r35, 63, 16
; CV1-NEXT:    clrf $r36 = $r36, 63, 16
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    clrf $r37 = $r37, 63, 16
; CV1-NEXT:    clrf $r38 = $r38, 63, 16
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    clrf $r39 = $r39, 63, 16
; CV1-NEXT:    clrf $r40 = $r40, 63, 16
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    clrf $r41 = $r41, 63, 16
; CV1-NEXT:    clrf $r42 = $r42, 63, 16
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    clrf $r43 = $r43, 63, 16
; CV1-NEXT:    clrf $r44 = $r44, 63, 16
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    clrf $r45 = $r45, 63, 16
; CV1-NEXT:    clrf $r46 = $r46, 63, 16
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    clrf $r47 = $r47, 63, 16
; CV1-NEXT:    clrf $r48 = $r48, 63, 16
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    clrf $r2 = $r2, 63, 16
; CV1-NEXT:    clrf $r6 = $r6, 63, 16
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    clrf $r0 = $r0, 63, 16
; CV1-NEXT:    clrf $r4 = $r4, 63, 16
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    clrf $r3 = $r3, 63, 16
; CV1-NEXT:    clrf $r7 = $r7, 63, 16
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    clrf $r1 = $r1, 63, 16
; CV1-NEXT:    clrf $r5 = $r5, 63, 16
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    adduwd $r1 = $r5, $r1
; CV1-NEXT:    adduwd $r3 = $r7, $r3
; CV1-NEXT:    ;; # (end cycle 27)
; CV1-NEXT:    adduwd $r0 = $r4, $r0
; CV1-NEXT:    adduwd $r2 = $r6, $r2
; CV1-NEXT:    ;; # (end cycle 28)
; CV1-NEXT:    adduwd $r4 = $r48, $r47
; CV1-NEXT:    adduwd $r5 = $r46, $r45
; CV1-NEXT:    ;; # (end cycle 29)
; CV1-NEXT:    adduwd $r6 = $r44, $r43
; CV1-NEXT:    adduwd $r7 = $r42, $r41
; CV1-NEXT:    ;; # (end cycle 30)
; CV1-NEXT:    adduwd $r37 = $r38, $r37
; CV1-NEXT:    adduwd $r39 = $r40, $r39
; CV1-NEXT:    ;; # (end cycle 31)
; CV1-NEXT:    adduwd $r33 = $r34, $r33
; CV1-NEXT:    adduwd $r35 = $r36, $r35
; CV1-NEXT:    ;; # (end cycle 32)
; CV1-NEXT:    adduwd $r15 = $r16, $r15
; CV1-NEXT:    adduwd $r17 = $r32, $r17
; CV1-NEXT:    ;; # (end cycle 33)
; CV1-NEXT:    adduwd $r8 = $r9, $r8
; CV1-NEXT:    adduwd $r10 = $r11, $r10
; CV1-NEXT:    ;; # (end cycle 34)
; CV1-NEXT:    adduwd $r8 = $r8, $r10
; CV1-NEXT:    adduwd $r9 = $r15, $r17
; CV1-NEXT:    ;; # (end cycle 35)
; CV1-NEXT:    adduwd $r10 = $r33, $r35
; CV1-NEXT:    adduwd $r11 = $r37, $r39
; CV1-NEXT:    ;; # (end cycle 36)
; CV1-NEXT:    adduwd $r4 = $r5, $r4
; CV1-NEXT:    adduwd $r6 = $r7, $r6
; CV1-NEXT:    ;; # (end cycle 37)
; CV1-NEXT:    adduwd $r0 = $r2, $r0
; CV1-NEXT:    adduwd $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 38)
; CV1-NEXT:    adduwd $r0 = $r1, $r0
; CV1-NEXT:    adduwd $r1 = $r4, $r6
; CV1-NEXT:    ;; # (end cycle 39)
; CV1-NEXT:    adduwd $r2 = $r11, $r10
; CV1-NEXT:    adduwd $r3 = $r9, $r8
; CV1-NEXT:    ;; # (end cycle 40)
; CV1-NEXT:    adduwd $r0 = $r1, $r0
; CV1-NEXT:    adduwd $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 41)
; CV1-NEXT:    adduwd $r0 = $r0, $r2
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 42)
;
; CV2-LABEL: addurhvd:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r0r1r2r3 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    extfz $r9 = $r6, 47, 32
; CV2-NEXT:    extfz $r11 = $r4, 47, 32
; CV2-NEXT:    extfz $r16 = $r7, 47, 32
; CV2-NEXT:    extfz $r32 = $r5, 47, 32
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    extfz $r8 = $r2, 47, 32
; CV2-NEXT:    extfz $r10 = $r0, 47, 32
; CV2-NEXT:    extfz $r15 = $r3, 47, 32
; CV2-NEXT:    extfz $r17 = $r1, 47, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    zxhd $r33 = $r2
; CV2-NEXT:    zxhd $r34 = $r6
; CV2-NEXT:    zxhd $r35 = $r0
; CV2-NEXT:    zxhd $r36 = $r4
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    zxhd $r37 = $r3
; CV2-NEXT:    zxhd $r38 = $r7
; CV2-NEXT:    zxhd $r39 = $r1
; CV2-NEXT:    zxhd $r40 = $r5
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    srld $r41 = $r2, 48
; CV2-NEXT:    srld $r42 = $r6, 48
; CV2-NEXT:    srld $r43 = $r0, 48
; CV2-NEXT:    srld $r44 = $r4, 48
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    srld $r45 = $r3, 48
; CV2-NEXT:    srld $r46 = $r7, 48
; CV2-NEXT:    srld $r47 = $r1, 48
; CV2-NEXT:    srld $r48 = $r5, 48
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    srlw $r0 = $r0, 16
; CV2-NEXT:    srlw $r2 = $r2, 16
; CV2-NEXT:    srlw $r4 = $r4, 16
; CV2-NEXT:    srlw $r6 = $r6, 16
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    srlw $r1 = $r1, 16
; CV2-NEXT:    srlw $r3 = $r3, 16
; CV2-NEXT:    srlw $r5 = $r5, 16
; CV2-NEXT:    srlw $r7 = $r7, 16
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    clrf $r8 = $r8, 63, 16
; CV2-NEXT:    clrf $r9 = $r9, 63, 16
; CV2-NEXT:    clrf $r10 = $r10, 63, 16
; CV2-NEXT:    clrf $r11 = $r11, 63, 16
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    clrf $r15 = $r15, 63, 16
; CV2-NEXT:    clrf $r16 = $r16, 63, 16
; CV2-NEXT:    clrf $r17 = $r17, 63, 16
; CV2-NEXT:    clrf $r32 = $r32, 63, 16
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    clrf $r33 = $r33, 63, 16
; CV2-NEXT:    clrf $r34 = $r34, 63, 16
; CV2-NEXT:    clrf $r35 = $r35, 63, 16
; CV2-NEXT:    clrf $r36 = $r36, 63, 16
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    clrf $r37 = $r37, 63, 16
; CV2-NEXT:    clrf $r38 = $r38, 63, 16
; CV2-NEXT:    clrf $r39 = $r39, 63, 16
; CV2-NEXT:    clrf $r40 = $r40, 63, 16
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    clrf $r41 = $r41, 63, 16
; CV2-NEXT:    clrf $r42 = $r42, 63, 16
; CV2-NEXT:    clrf $r43 = $r43, 63, 16
; CV2-NEXT:    clrf $r44 = $r44, 63, 16
; CV2-NEXT:    ;; # (end cycle 15)
; CV2-NEXT:    clrf $r45 = $r45, 63, 16
; CV2-NEXT:    clrf $r46 = $r46, 63, 16
; CV2-NEXT:    clrf $r47 = $r47, 63, 16
; CV2-NEXT:    clrf $r48 = $r48, 63, 16
; CV2-NEXT:    ;; # (end cycle 16)
; CV2-NEXT:    clrf $r0 = $r0, 63, 16
; CV2-NEXT:    clrf $r2 = $r2, 63, 16
; CV2-NEXT:    clrf $r4 = $r4, 63, 16
; CV2-NEXT:    clrf $r6 = $r6, 63, 16
; CV2-NEXT:    ;; # (end cycle 17)
; CV2-NEXT:    clrf $r1 = $r1, 63, 16
; CV2-NEXT:    clrf $r3 = $r3, 63, 16
; CV2-NEXT:    clrf $r5 = $r5, 63, 16
; CV2-NEXT:    clrf $r7 = $r7, 63, 16
; CV2-NEXT:    ;; # (end cycle 18)
; CV2-NEXT:    adduwd $r0 = $r4, $r0
; CV2-NEXT:    adduwd $r1 = $r5, $r1
; CV2-NEXT:    adduwd $r2 = $r6, $r2
; CV2-NEXT:    adduwd $r3 = $r7, $r3
; CV2-NEXT:    ;; # (end cycle 19)
; CV2-NEXT:    adduwd $r4 = $r48, $r47
; CV2-NEXT:    adduwd $r5 = $r46, $r45
; CV2-NEXT:    adduwd $r6 = $r44, $r43
; CV2-NEXT:    adduwd $r7 = $r42, $r41
; CV2-NEXT:    ;; # (end cycle 20)
; CV2-NEXT:    adduwd $r33 = $r34, $r33
; CV2-NEXT:    adduwd $r35 = $r36, $r35
; CV2-NEXT:    adduwd $r37 = $r38, $r37
; CV2-NEXT:    adduwd $r39 = $r40, $r39
; CV2-NEXT:    ;; # (end cycle 21)
; CV2-NEXT:    adduwd $r8 = $r9, $r8
; CV2-NEXT:    adduwd $r10 = $r11, $r10
; CV2-NEXT:    adduwd $r15 = $r16, $r15
; CV2-NEXT:    adduwd $r17 = $r32, $r17
; CV2-NEXT:    ;; # (end cycle 22)
; CV2-NEXT:    adduwd $r8 = $r8, $r10
; CV2-NEXT:    adduwd $r9 = $r15, $r17
; CV2-NEXT:    adduwd $r10 = $r33, $r35
; CV2-NEXT:    adduwd $r11 = $r37, $r39
; CV2-NEXT:    ;; # (end cycle 23)
; CV2-NEXT:    adduwd $r0 = $r2, $r0
; CV2-NEXT:    adduwd $r1 = $r3, $r1
; CV2-NEXT:    adduwd $r4 = $r5, $r4
; CV2-NEXT:    adduwd $r6 = $r7, $r6
; CV2-NEXT:    ;; # (end cycle 24)
; CV2-NEXT:    adduwd $r0 = $r1, $r0
; CV2-NEXT:    adduwd $r1 = $r4, $r6
; CV2-NEXT:    adduwd $r2 = $r11, $r10
; CV2-NEXT:    adduwd $r3 = $r9, $r8
; CV2-NEXT:    ;; # (end cycle 25)
; CV2-NEXT:    adduwd $r0 = $r1, $r0
; CV2-NEXT:    adduwd $r2 = $r3, $r2
; CV2-NEXT:    ;; # (end cycle 26)
; CV2-NEXT:    adduwd $r0 = $r0, $r2
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 27)
entry:
  %V = load <32 x i16>, ptr %0
  %conv.i = zext <32 x i16> %V to <32 x i64>
  %rdx.add.i = tail call i64 @llvm.vector.reduce.add.v32i64(<32 x i64> %conv.i)
  ret i64 %rdx.add.i
}

define i64 @addurhxd(<16 x i16> %V) {
; CV1-LABEL: addurhxd:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    srlw $r4 = $r1, 16
; CV1-NEXT:    srlw $r5 = $r3, 16
; CV1-NEXT:    srlw $r6 = $r0, 16
; CV1-NEXT:    srlw $r7 = $r2, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srld $r8 = $r1, 48
; CV1-NEXT:    srld $r9 = $r3, 48
; CV1-NEXT:    srld $r10 = $r0, 48
; CV1-NEXT:    srld $r11 = $r2, 48
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    zxhd $r15 = $r1
; CV1-NEXT:    zxhd $r16 = $r3
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    zxhd $r17 = $r0
; CV1-NEXT:    zxhd $r32 = $r2
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    extfz $r1 = $r1, 47, 32
; CV1-NEXT:    extfz $r3 = $r3, 47, 32
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    extfz $r0 = $r0, 47, 32
; CV1-NEXT:    extfz $r2 = $r2, 47, 32
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    clrf $r4 = $r4, 63, 16
; CV1-NEXT:    clrf $r5 = $r5, 63, 16
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    clrf $r6 = $r6, 63, 16
; CV1-NEXT:    clrf $r7 = $r7, 63, 16
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    clrf $r8 = $r8, 63, 16
; CV1-NEXT:    clrf $r9 = $r9, 63, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    clrf $r10 = $r10, 63, 16
; CV1-NEXT:    clrf $r11 = $r11, 63, 16
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    clrf $r15 = $r15, 63, 16
; CV1-NEXT:    clrf $r16 = $r16, 63, 16
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    clrf $r17 = $r17, 63, 16
; CV1-NEXT:    clrf $r32 = $r32, 63, 16
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    clrf $r1 = $r1, 63, 16
; CV1-NEXT:    clrf $r3 = $r3, 63, 16
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    clrf $r0 = $r0, 63, 16
; CV1-NEXT:    clrf $r2 = $r2, 63, 16
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    adduwd $r0 = $r2, $r0
; CV1-NEXT:    adduwd $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    adduwd $r2 = $r32, $r17
; CV1-NEXT:    adduwd $r3 = $r16, $r15
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    adduwd $r8 = $r9, $r8
; CV1-NEXT:    adduwd $r10 = $r11, $r10
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    adduwd $r4 = $r5, $r4
; CV1-NEXT:    adduwd $r6 = $r7, $r6
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    adduwd $r4 = $r4, $r6
; CV1-NEXT:    adduwd $r5 = $r8, $r10
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    adduwd $r0 = $r1, $r0
; CV1-NEXT:    adduwd $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    adduwd $r0 = $r0, $r2
; CV1-NEXT:    adduwd $r1 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    adduwd $r0 = $r1, $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 21)
;
; CV2-LABEL: addurhxd:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    srlw $r4 = $r1, 16
; CV2-NEXT:    srlw $r5 = $r3, 16
; CV2-NEXT:    srlw $r6 = $r0, 16
; CV2-NEXT:    srlw $r7 = $r2, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srld $r8 = $r1, 48
; CV2-NEXT:    srld $r9 = $r3, 48
; CV2-NEXT:    srld $r10 = $r0, 48
; CV2-NEXT:    srld $r11 = $r2, 48
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    zxhd $r15 = $r1
; CV2-NEXT:    zxhd $r16 = $r3
; CV2-NEXT:    zxhd $r17 = $r0
; CV2-NEXT:    zxhd $r32 = $r2
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    extfz $r0 = $r0, 47, 32
; CV2-NEXT:    extfz $r1 = $r1, 47, 32
; CV2-NEXT:    extfz $r2 = $r2, 47, 32
; CV2-NEXT:    extfz $r3 = $r3, 47, 32
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    clrf $r4 = $r4, 63, 16
; CV2-NEXT:    clrf $r5 = $r5, 63, 16
; CV2-NEXT:    clrf $r6 = $r6, 63, 16
; CV2-NEXT:    clrf $r7 = $r7, 63, 16
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    clrf $r8 = $r8, 63, 16
; CV2-NEXT:    clrf $r9 = $r9, 63, 16
; CV2-NEXT:    clrf $r10 = $r10, 63, 16
; CV2-NEXT:    clrf $r11 = $r11, 63, 16
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    clrf $r15 = $r15, 63, 16
; CV2-NEXT:    clrf $r16 = $r16, 63, 16
; CV2-NEXT:    clrf $r17 = $r17, 63, 16
; CV2-NEXT:    clrf $r32 = $r32, 63, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    clrf $r0 = $r0, 63, 16
; CV2-NEXT:    clrf $r1 = $r1, 63, 16
; CV2-NEXT:    clrf $r2 = $r2, 63, 16
; CV2-NEXT:    clrf $r3 = $r3, 63, 16
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    adduwd $r0 = $r2, $r0
; CV2-NEXT:    adduwd $r1 = $r3, $r1
; CV2-NEXT:    adduwd $r2 = $r32, $r17
; CV2-NEXT:    adduwd $r3 = $r16, $r15
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    adduwd $r4 = $r5, $r4
; CV2-NEXT:    adduwd $r6 = $r7, $r6
; CV2-NEXT:    adduwd $r8 = $r9, $r8
; CV2-NEXT:    adduwd $r10 = $r11, $r10
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    adduwd $r0 = $r1, $r0
; CV2-NEXT:    adduwd $r2 = $r3, $r2
; CV2-NEXT:    adduwd $r4 = $r4, $r6
; CV2-NEXT:    adduwd $r5 = $r8, $r10
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    adduwd $r0 = $r0, $r2
; CV2-NEXT:    adduwd $r1 = $r5, $r4
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    adduwd $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 12)
entry:
  %conv.i = zext <16 x i16> %V to <16 x i64>
  %rdx.add.i = tail call i64 @llvm.vector.reduce.add.v16i64(<16 x i64> %conv.i)
  ret i64 %rdx.add.i
}

define i64 @addurwod(<8 x i32> %V) {
; CV1-LABEL: addurwod:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    srld $r4 = $r0, 32
; CV1-NEXT:    srad $r5 = $r2, 32
; CV1-NEXT:    zxwd $r6 = $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    adduwd $r0 = $r2, $r0
; CV1-NEXT:    srld $r1 = $r1, 32
; CV1-NEXT:    adduwd $r3 = $r3, $r6
; CV1-NEXT:    srad $r7 = $r3, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    addd $r0 = $r0, $r3
; CV1-NEXT:    adduwd $r1 = $r7, $r1
; CV1-NEXT:    adduwd $r2 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    addd $r1 = $r2, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    addd $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 4)
;
; CV2-LABEL: addurwod:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    srld $r4 = $r0, 32
; CV2-NEXT:    srad $r5 = $r2, 32
; CV2-NEXT:    srld $r6 = $r1, 32
; CV2-NEXT:    srad $r7 = $r3, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    zxwd $r0 = $r0
; CV2-NEXT:    zxwd $r1 = $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    adduwd $r0 = $r2, $r0
; CV2-NEXT:    adduwd $r1 = $r3, $r1
; CV2-NEXT:    adduwd $r2 = $r7, $r6
; CV2-NEXT:    adduwd $r3 = $r5, $r4
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    addd $r0 = $r0, $r1
; CV2-NEXT:    addd $r2 = $r3, $r2
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    addd $r0 = $r0, $r2
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 4)
entry:
  %conv.i = zext <8 x i32> %V to <8 x i64>
  %rdx.add.i = tail call i64 @llvm.vector.reduce.add.v8i64(<8 x i64> %conv.i)
  ret i64 %rdx.add.i
}

define i64 @addurwpd(<2 x i32> %V) {
; CV1-LABEL: addurwpd:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    extfz $r1 = $r0, 63, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    adduwd $r0 = $r1, $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 1)
;
; CV2-LABEL: addurwpd:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    zxwd $r0 = $r0
; CV2-NEXT:    extfz $r1 = $r0, 63, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    adduwd $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 1)
entry:
  %conv.i = zext <2 x i32> %V to <2 x i64>
  %rdx.add.i = tail call i64 @llvm.vector.reduce.add.v2i64(<2 x i64> %conv.i)
  ret i64 %rdx.add.i
}

define i64 @addurwqd(<4 x i32> %V) {
; CV1-LABEL: addurwqd:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    srld $r0 = $r0, 32
; CV1-NEXT:    zxwd $r2 = $r0
; CV1-NEXT:    srad $r3 = $r1, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    adduwd $r0 = $r3, $r0
; CV1-NEXT:    adduwd $r1 = $r1, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    addd $r0 = $r1, $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 2)
;
; CV2-LABEL: addurwqd:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    srld $r0 = $r0, 32
; CV2-NEXT:    zxwd $r2 = $r0
; CV2-NEXT:    srad $r3 = $r1, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    adduwd $r0 = $r3, $r0
; CV2-NEXT:    adduwd $r1 = $r1, $r2
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    addd $r0 = $r1, $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 2)
entry:
  %conv.i = zext <4 x i32> %V to <4 x i64>
  %rdx.add.i = tail call i64 @llvm.vector.reduce.add.v4i64(<4 x i64> %conv.i)
  ret i64 %rdx.add.i
}

define i64 @addurwvd(ptr %0) {
; CV1-LABEL: addurwvd:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    lo $r32r33r34r35 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r4r5r6r7 = 64[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    lo $r8r9r10r11 = 32[$r0]
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    lo $r0r1r2r3 = 96[$r0]
; CV1-NEXT:    srld $r16 = $r34, 32
; CV1-NEXT:    srld $r40 = $r35, 32
; CV1-NEXT:    srld $r44 = $r33, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srld $r15 = $r6, 32
; CV1-NEXT:    srld $r37 = $r32, 32
; CV1-NEXT:    srld $r39 = $r7, 32
; CV1-NEXT:    srld $r43 = $r5, 32
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    srld $r17 = $r10, 32
; CV1-NEXT:    srld $r38 = $r8, 32
; CV1-NEXT:    srld $r42 = $r11, 32
; CV1-NEXT:    srld $r45 = $r9, 32
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    zxwd $r9 = $r9
; CV1-NEXT:    zxwd $r16 = $r16
; CV1-NEXT:    zxwd $r40 = $r40
; CV1-NEXT:    zxwd $r44 = $r44
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    zxwd $r11 = $r11
; CV1-NEXT:    srld $r36 = $r4, 32
; CV1-NEXT:    zxwd $r37 = $r37
; CV1-NEXT:    srld $r41 = $r3, 32
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    zxwd $r42 = $r42
; CV1-NEXT:    adduwd $r43 = $r43, $r44
; CV1-NEXT:    zxwd $r45 = $r45
; CV1-NEXT:    srld $r46 = $r1, 32
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    zxwd $r38 = $r38
; CV1-NEXT:    adduwd $r39 = $r39, $r40
; CV1-NEXT:    srld $r40 = $r2, 32
; CV1-NEXT:    srld $r44 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    adduwd $r15 = $r15, $r16
; CV1-NEXT:    zxwd $r16 = $r35
; CV1-NEXT:    zxwd $r17 = $r17
; CV1-NEXT:    zxwd $r33 = $r33
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    adduwd $r1 = $r1, $r9
; CV1-NEXT:    adduwd $r3 = $r3, $r11
; CV1-NEXT:    zxwd $r8 = $r8
; CV1-NEXT:    zxwd $r9 = $r32
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    zxwd $r10 = $r10
; CV1-NEXT:    zxwd $r11 = $r34
; CV1-NEXT:    adduwd $r41 = $r41, $r42
; CV1-NEXT:    adduwd $r45 = $r46, $r45
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    adduwd $r36 = $r36, $r37
; CV1-NEXT:    adduwd $r38 = $r44, $r38
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    adduwd $r5 = $r5, $r33
; CV1-NEXT:    adduwd $r17 = $r40, $r17
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    adduwd $r0 = $r0, $r8
; CV1-NEXT:    addd $r1 = $r5, $r1
; CV1-NEXT:    addd $r5 = $r36, $r38
; CV1-NEXT:    adduwd $r7 = $r7, $r16
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    adduwd $r2 = $r2, $r10
; CV1-NEXT:    addd $r3 = $r7, $r3
; CV1-NEXT:    adduwd $r4 = $r4, $r9
; CV1-NEXT:    addd $r7 = $r43, $r45
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    addd $r0 = $r4, $r0
; CV1-NEXT:    addd $r1 = $r1, $r3
; CV1-NEXT:    addd $r4 = $r15, $r17
; CV1-NEXT:    adduwd $r6 = $r6, $r11
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    addd $r2 = $r6, $r2
; CV1-NEXT:    addd $r4 = $r5, $r4
; CV1-NEXT:    addd $r6 = $r39, $r41
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    addd $r0 = $r0, $r2
; CV1-NEXT:    addd $r6 = $r7, $r6
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    addd $r0 = $r0, $r1
; CV1-NEXT:    addd $r1 = $r4, $r6
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    addd $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 21)
;
; CV2-LABEL: addurwvd:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    lo $r8r9r10r11 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r32r33r34r35 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    lo $r4r5r6r7 = 96[$r0]
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    lo $r0r1r2r3 = 64[$r0]
; CV2-NEXT:    zxwd $r16 = $r10
; CV2-NEXT:    zxwd $r36 = $r8
; CV2-NEXT:    zxwd $r38 = $r11
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    zxwd $r15 = $r34
; CV2-NEXT:    zxwd $r17 = $r32
; CV2-NEXT:    zxwd $r37 = $r35
; CV2-NEXT:    zxwd $r39 = $r33
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    srld $r10 = $r10, 32
; CV2-NEXT:    srld $r32 = $r32, 32
; CV2-NEXT:    srld $r34 = $r34, 32
; CV2-NEXT:    zxwd $r40 = $r9
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    srld $r8 = $r8, 32
; CV2-NEXT:    srld $r11 = $r11, 32
; CV2-NEXT:    srld $r33 = $r33, 32
; CV2-NEXT:    srld $r35 = $r35, 32
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    srld $r9 = $r9, 32
; CV2-NEXT:    srld $r41 = $r2, 32
; CV2-NEXT:    srld $r42 = $r6, 32
; CV2-NEXT:    srld $r43 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    zxwd $r10 = $r10
; CV2-NEXT:    zxwd $r32 = $r32
; CV2-NEXT:    zxwd $r34 = $r34
; CV2-NEXT:    srld $r44 = $r4, 32
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    zxwd $r8 = $r8
; CV2-NEXT:    zxwd $r35 = $r35
; CV2-NEXT:    srld $r45 = $r3, 32
; CV2-NEXT:    srld $r46 = $r7, 32
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    zxwd $r11 = $r11
; CV2-NEXT:    zxwd $r33 = $r33
; CV2-NEXT:    srld $r47 = $r1, 32
; CV2-NEXT:    srld $r48 = $r5, 32
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    zxwd $r9 = $r9
; CV2-NEXT:    adduwd $r11 = $r46, $r11
; CV2-NEXT:    adduwd $r33 = $r47, $r33
; CV2-NEXT:    adduwd $r35 = $r45, $r35
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    adduwd $r8 = $r44, $r8
; CV2-NEXT:    adduwd $r9 = $r48, $r9
; CV2-NEXT:    adduwd $r10 = $r42, $r10
; CV2-NEXT:    adduwd $r32 = $r43, $r32
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    adduwd $r1 = $r1, $r39
; CV2-NEXT:    adduwd $r5 = $r5, $r40
; CV2-NEXT:    adduwd $r7 = $r7, $r38
; CV2-NEXT:    adduwd $r34 = $r41, $r34
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    adduwd $r0 = $r0, $r17
; CV2-NEXT:    adduwd $r3 = $r3, $r37
; CV2-NEXT:    adduwd $r4 = $r4, $r36
; CV2-NEXT:    adduwd $r6 = $r6, $r16
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    addd $r0 = $r0, $r4
; CV2-NEXT:    addd $r1 = $r1, $r5
; CV2-NEXT:    adduwd $r2 = $r2, $r15
; CV2-NEXT:    addd $r3 = $r3, $r7
; CV2-NEXT:    ;; # (end cycle 15)
; CV2-NEXT:    addd $r2 = $r2, $r6
; CV2-NEXT:    addd $r4 = $r34, $r10
; CV2-NEXT:    addd $r5 = $r32, $r8
; CV2-NEXT:    addd $r6 = $r35, $r11
; CV2-NEXT:    ;; # (end cycle 16)
; CV2-NEXT:    addd $r0 = $r0, $r2
; CV2-NEXT:    addd $r1 = $r1, $r3
; CV2-NEXT:    addd $r4 = $r5, $r4
; CV2-NEXT:    addd $r7 = $r33, $r9
; CV2-NEXT:    ;; # (end cycle 17)
; CV2-NEXT:    addd $r0 = $r0, $r1
; CV2-NEXT:    addd $r6 = $r7, $r6
; CV2-NEXT:    ;; # (end cycle 18)
; CV2-NEXT:    addd $r1 = $r4, $r6
; CV2-NEXT:    ;; # (end cycle 19)
; CV2-NEXT:    addd $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 20)
entry:
  %V = load <32 x i32>, ptr %0
  %conv.i = zext <32 x i32> %V to <32 x i64>
  %rdx.add.i = tail call i64 @llvm.vector.reduce.add.v32i64(<32 x i64> %conv.i)
  ret i64 %rdx.add.i
}

define i64 @addurwxd(ptr %0) {
; CV1-LABEL: addurwxd:
; CV1:       # %bb.0: # %entry
; CV1-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r0r1r2r3 = 32[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srld $r9 = $r5, 32
; CV1-NEXT:    srld $r11 = $r7, 32
; CV1-NEXT:    srld $r16 = $r4, 32
; CV1-NEXT:    srld $r32 = $r6, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    zxwd $r4 = $r4
; CV1-NEXT:    zxwd $r5 = $r5
; CV1-NEXT:    zxwd $r6 = $r6
; CV1-NEXT:    zxwd $r7 = $r7
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    srld $r8 = $r1, 32
; CV1-NEXT:    srld $r10 = $r3, 32
; CV1-NEXT:    srld $r15 = $r0, 32
; CV1-NEXT:    srld $r17 = $r2, 32
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    adduwd $r0 = $r0, $r4
; CV1-NEXT:    adduwd $r2 = $r2, $r6
; CV1-NEXT:    zxwd $r4 = $r16
; CV1-NEXT:    zxwd $r6 = $r32
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    adduwd $r1 = $r1, $r5
; CV1-NEXT:    adduwd $r3 = $r3, $r7
; CV1-NEXT:    zxwd $r5 = $r9
; CV1-NEXT:    zxwd $r7 = $r11
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    addd $r0 = $r0, $r2
; CV1-NEXT:    addd $r1 = $r1, $r3
; CV1-NEXT:    adduwd $r4 = $r15, $r4
; CV1-NEXT:    adduwd $r6 = $r17, $r6
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    addd $r0 = $r0, $r1
; CV1-NEXT:    addd $r4 = $r4, $r6
; CV1-NEXT:    adduwd $r5 = $r8, $r5
; CV1-NEXT:    adduwd $r7 = $r10, $r7
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    addd $r5 = $r5, $r7
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    addd $r1 = $r4, $r5
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    addd $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 12)
;
; CV2-LABEL: addurwxd:
; CV2:       # %bb.0: # %entry
; CV2-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r0r1r2r3 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    srld $r9 = $r5, 32
; CV2-NEXT:    srld $r11 = $r7, 32
; CV2-NEXT:    srld $r16 = $r4, 32
; CV2-NEXT:    srld $r32 = $r6, 32
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srld $r8 = $r1, 32
; CV2-NEXT:    srld $r10 = $r3, 32
; CV2-NEXT:    srld $r15 = $r0, 32
; CV2-NEXT:    srld $r17 = $r2, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    zxwd $r4 = $r4
; CV2-NEXT:    zxwd $r5 = $r5
; CV2-NEXT:    zxwd $r6 = $r6
; CV2-NEXT:    zxwd $r7 = $r7
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    zxwd $r9 = $r9
; CV2-NEXT:    zxwd $r11 = $r11
; CV2-NEXT:    zxwd $r16 = $r16
; CV2-NEXT:    zxwd $r32 = $r32
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    adduwd $r0 = $r0, $r4
; CV2-NEXT:    adduwd $r1 = $r1, $r5
; CV2-NEXT:    adduwd $r2 = $r2, $r6
; CV2-NEXT:    adduwd $r3 = $r3, $r7
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    adduwd $r4 = $r17, $r32
; CV2-NEXT:    adduwd $r5 = $r15, $r16
; CV2-NEXT:    adduwd $r6 = $r10, $r11
; CV2-NEXT:    adduwd $r7 = $r8, $r9
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    addd $r0 = $r0, $r2
; CV2-NEXT:    addd $r1 = $r1, $r3
; CV2-NEXT:    addd $r4 = $r5, $r4
; CV2-NEXT:    addd $r6 = $r7, $r6
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    addd $r0 = $r0, $r1
; CV2-NEXT:    addd $r1 = $r4, $r6
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    addd $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 11)
entry:
  %V = load <16 x i32>, ptr %0
  %conv.i = zext <16 x i32> %V to <16 x i64>
  %rdx.add.i = tail call i64 @llvm.vector.reduce.add.v16i64(<16 x i64> %conv.i)
  ret i64 %rdx.add.i
}

declare i64 @llvm.vector.reduce.add.v2i64(<2 x i64>)

declare i64 @llvm.vector.reduce.add.v4i64(<4 x i64>)

declare i64 @llvm.vector.reduce.add.v32i64(<32 x i64>)

declare i64 @llvm.vector.reduce.add.v16i64(<16 x i64>)

define i64 @maxrbod(<8 x i8> %0) {
; CV1-LABEL: maxrbod:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    maxhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sllhqs $r1 = $r2, 8
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    maxhq $r0 = $r0, $r2
; CV1-NEXT:    maxhq $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    srlhqs $r1 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    maxhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 22)
;
; CV2-LABEL: maxrbod:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    maxbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    maxbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    maxbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 13)
  %2 = tail call i8 @llvm.vector.reduce.smax.v8i8(<8 x i8> %0)
  %3 = sext i8 %2 to i64
  ret i64 %3
}

define i64 @maxrbpd(<2 x i8> %0) {
; CV1-LABEL: maxrbpd:
; CV1:       # %bb.0:
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x401
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: maxrbpd:
; CV2:       # %bb.0:
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    maxbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 4)
  %2 = tail call i8 @llvm.vector.reduce.smax.v2i8(<2 x i8> %0)
  %3 = sext i8 %2 to i64
  ret i64 %3
}

define i64 @maxrbqd(<4 x i8> %0) {
; CV1-LABEL: maxrbqd:
; CV1:       # %bb.0:
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sxlbhq $r1 = $r2
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x40100401
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    extfz $r1 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 12)
;
; CV2-LABEL: maxrbqd:
; CV2:       # %bb.0:
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    maxbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    maxbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
  %2 = tail call i8 @llvm.vector.reduce.smax.v4i8(<4 x i8> %0)
  %3 = sext i8 %2 to i64
  ret i64 %3
}

define i64 @maxrbvd(<32 x i8> %0) {
; CV1-LABEL: maxrbvd:
; CV1:       # %bb.0:
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r4 = $r2, 8
; CV1-NEXT:    sllhqs $r5 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd $r3 = $r3, 0xff00ff00.@
; CV1-NEXT:    maxhq $r4 = $r5, $r4
; CV1-NEXT:    sllhqs $r6 = $r3, 8
; CV1-NEXT:    sllhqs $r7 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    maxhq $r0 = $r0, $r2
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    srlhqs $r2 = $r4, 8
; CV1-NEXT:    maxhq $r5 = $r7, $r6
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r2
; CV1-NEXT:    maxhq $r1 = $r1, $r3
; CV1-NEXT:    srlhqs $r3 = $r5, 8
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    iord $r1 = $r1, $r3
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r2 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    maxhq $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    maxhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    sllhqs $r1 = $r2, 8
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    maxhq $r0 = $r0, $r2
; CV1-NEXT:    maxhq $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    srlhqs $r1 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    maxhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 27)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 28)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 29)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 30)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 31)
;
; CV2-LABEL: maxrbvd:
; CV2:       # %bb.0:
; CV2-NEXT:    maxbo $r0 = $r0, $r2
; CV2-NEXT:    maxbo $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    maxbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    maxbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    maxbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    maxbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 15)
  %2 = tail call i8 @llvm.vector.reduce.smax.v32i8(<32 x i8> %0)
  %3 = sext i8 %2 to i64
  ret i64 %3
}

define i64 @maxrbxd(<16 x i8> %0) {
; CV1-LABEL: maxrbxd:
; CV1:       # %bb.0:
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r2 = $r1, 8
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    maxhq $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    maxhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    sllhqs $r1 = $r2, 8
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    maxhq $r0 = $r0, $r2
; CV1-NEXT:    maxhq $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    srlhqs $r1 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    maxhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 26)
;
; CV2-LABEL: maxrbxd:
; CV2:       # %bb.0:
; CV2-NEXT:    maxbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    maxbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    maxbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    maxbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 14)
  %2 = tail call i8 @llvm.vector.reduce.smax.v16i8(<16 x i8> %0)
  %3 = sext i8 %2 to i64
  ret i64 %3
}

define i64 @maxrhod(<8 x i16> %0) {
; CV1-LABEL: maxrhod:
; CV1:       # %bb.0:
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    sxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 8)
;
; CV2-LABEL: maxrhod:
; CV2:       # %bb.0:
; CV2-NEXT:    maxhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    maxhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    maxhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    sxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
  %2 = tail call i16 @llvm.vector.reduce.smax.v8i16(<8 x i16> %0)
  %3 = sext i16 %2 to i64
  ret i64 %3
}

define i64 @maxrhpd(<2 x i16> %0) {
; CV1-LABEL: maxrhpd:
; CV1:       # %bb.0:
; CV1-NEXT:    srlw $r1 = $r0, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 31, 16
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: maxrhpd:
; CV2:       # %bb.0:
; CV2-NEXT:    srlw $r1 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 31, 16
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    maxhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 3)
  %2 = tail call i16 @llvm.vector.reduce.smax.v2i16(<2 x i16> %0)
  %3 = sext i16 %2 to i64
  ret i64 %3
}

define i64 @maxrhqd(<4 x i16> %0) {
; CV1-LABEL: maxrhqd:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    sxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: maxrhqd:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    maxhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    maxhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    sxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = tail call i16 @llvm.vector.reduce.smax.v4i16(<4 x i16> %0)
  %3 = sext i16 %2 to i64
  ret i64 %3
}

define i64 @maxrhvd(ptr %0) {
; CV1-LABEL: maxrhvd:
; CV1:       # %bb.0:
; CV1-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r0r1r2r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    maxhq $r0 = $r0, $r4
; CV1-NEXT:    maxhq $r1 = $r1, $r5
; CV1-NEXT:    maxhq $r2 = $r2, $r6
; CV1-NEXT:    maxhq $r3 = $r3, $r7
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    maxhq $r0 = $r0, $r2
; CV1-NEXT:    maxhq $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    sxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 14)
;
; CV2-LABEL: maxrhvd:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r0r1r2r3 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    maxhq $r0 = $r0, $r4
; CV2-NEXT:    maxhq $r1 = $r1, $r5
; CV2-NEXT:    maxhq $r2 = $r2, $r6
; CV2-NEXT:    maxhq $r3 = $r3, $r7
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    maxhq $r0 = $r0, $r2
; CV2-NEXT:    maxhq $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    maxhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    maxhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    maxhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    sxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 14)
  %2 = load <32 x i16>, ptr %0
  %3 = tail call i16 @llvm.vector.reduce.smax.v32i16(<32 x i16> %2)
  %4 = sext i16 %3 to i64
  ret i64 %4
}

define i64 @maxrhxd(<16 x i16> %0) {
; CV1-LABEL: maxrhxd:
; CV1:       # %bb.0:
; CV1-NEXT:    maxhq $r0 = $r0, $r2
; CV1-NEXT:    maxhq $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    sxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 9)
;
; CV2-LABEL: maxrhxd:
; CV2:       # %bb.0:
; CV2-NEXT:    maxhq $r0 = $r0, $r2
; CV2-NEXT:    maxhq $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    maxhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    maxhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    maxhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    sxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 9)
  %2 = tail call i16 @llvm.vector.reduce.smax.v16i16(<16 x i16> %0)
  %3 = sext i16 %2 to i64
  ret i64 %3
}

define i64 @maxrwod(<8 x i32> %0) {
; CV1-LABEL: maxrwod:
; CV1:       # %bb.0:
; CV1-NEXT:    maxwp $r0 = $r0, $r2
; CV1-NEXT:    maxwp $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srld $r3 = $r1, 32
; CV1-NEXT:    copyd $r4 = $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r4 = $r3, 63, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    maxwp $r0 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    maxwp $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    sxwd $r0 = $r0
; CV1-NEXT:    maxwp $r1 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    maxwp $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 8)
;
; CV2-LABEL: maxrwod:
; CV2:       # %bb.0:
; CV2-NEXT:    maxwp $r0 = $r0, $r2
; CV2-NEXT:    maxwp $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srld $r3 = $r1, 32
; CV2-NEXT:    copyd $r4 = $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r4 = $r3, 63, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    maxwp $r0 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    maxwp $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    sxwd $r0 = $r0
; CV2-NEXT:    maxwp $r1 = $r1, $r0
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    maxwp $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
  %2 = tail call i32 @llvm.vector.reduce.smax.v8i32(<8 x i32> %0)
  %3 = sext i32 %2 to i64
  ret i64 %3
}

define i64 @maxrwpd(<2 x i32> %0) {
; CV1-LABEL: maxrwpd:
; CV1:       # %bb.0:
; CV1-NEXT:    srad $r1 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 63, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    maxwp $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sxwd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: maxrwpd:
; CV2:       # %bb.0:
; CV2-NEXT:    srad $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 63, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    maxwp $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sxwd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 3)
  %2 = tail call i32 @llvm.vector.reduce.smax.v2i32(<2 x i32> %0)
  %3 = sext i32 %2 to i64
  ret i64 %3
}

define i64 @maxrwqd(<4 x i32> %0) {
; CV1-LABEL: maxrwqd:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r2 = $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    maxwp $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    maxwp $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sxwd $r0 = $r0
; CV1-NEXT:    maxwp $r1 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    maxwp $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: maxrwqd:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r2 = $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    maxwp $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    maxwp $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sxwd $r0 = $r0
; CV2-NEXT:    maxwp $r1 = $r1, $r0
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    maxwp $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 6)
  %2 = tail call i32 @llvm.vector.reduce.smax.v4i32(<4 x i32> %0)
  %3 = sext i32 %2 to i64
  ret i64 %3
}

define i64 @maxrwvd(ptr %0) {
; CV1-LABEL: maxrwvd:
; CV1:       # %bb.0:
; CV1-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r8r9r10r11 = 96[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    lo $r32r33r34r35 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    lo $r0r1r2r3 = 64[$r0]
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    maxwp $r4 = $r4, $r8
; CV1-NEXT:    maxwp $r5 = $r5, $r9
; CV1-NEXT:    maxwp $r6 = $r6, $r10
; CV1-NEXT:    maxwp $r7 = $r7, $r11
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    maxwp $r0 = $r32, $r0
; CV1-NEXT:    maxwp $r1 = $r33, $r1
; CV1-NEXT:    maxwp $r2 = $r34, $r2
; CV1-NEXT:    maxwp $r3 = $r35, $r3
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    maxwp $r0 = $r0, $r4
; CV1-NEXT:    maxwp $r1 = $r1, $r5
; CV1-NEXT:    maxwp $r2 = $r2, $r6
; CV1-NEXT:    maxwp $r3 = $r3, $r7
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    maxwp $r0 = $r0, $r2
; CV1-NEXT:    maxwp $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    srld $r3 = $r1, 32
; CV1-NEXT:    copyd $r4 = $r1
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r4 = $r3, 63, 32
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    maxwp $r0 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    maxwp $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    sxwd $r0 = $r0
; CV1-NEXT:    maxwp $r1 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    maxwp $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 16)
;
; CV2-LABEL: maxrwvd:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r8r9r10r11 = 96[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    lo $r32r33r34r35 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    lo $r0r1r2r3 = 64[$r0]
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    maxwp $r4 = $r4, $r8
; CV2-NEXT:    maxwp $r5 = $r5, $r9
; CV2-NEXT:    maxwp $r6 = $r6, $r10
; CV2-NEXT:    maxwp $r7 = $r7, $r11
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    maxwp $r0 = $r32, $r0
; CV2-NEXT:    maxwp $r1 = $r33, $r1
; CV2-NEXT:    maxwp $r2 = $r34, $r2
; CV2-NEXT:    maxwp $r3 = $r35, $r3
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    maxwp $r0 = $r0, $r4
; CV2-NEXT:    maxwp $r1 = $r1, $r5
; CV2-NEXT:    maxwp $r2 = $r2, $r6
; CV2-NEXT:    maxwp $r3 = $r3, $r7
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    maxwp $r0 = $r0, $r2
; CV2-NEXT:    maxwp $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    srld $r3 = $r1, 32
; CV2-NEXT:    copyd $r4 = $r1
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r4 = $r3, 63, 32
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    maxwp $r0 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    maxwp $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    sxwd $r0 = $r0
; CV2-NEXT:    maxwp $r1 = $r1, $r0
; CV2-NEXT:    ;; # (end cycle 15)
; CV2-NEXT:    maxwp $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 16)
  %2 = load <32 x i32>, ptr %0
  %3 = tail call i32 @llvm.vector.reduce.smax.v32i32(<32 x i32> %2)
  %4 = sext i32 %3 to i64
  ret i64 %4
}

define i64 @maxrwxd(ptr %0) {
; CV1-LABEL: maxrwxd:
; CV1:       # %bb.0:
; CV1-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r0r1r2r3 = 32[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    maxwp $r0 = $r4, $r0
; CV1-NEXT:    maxwp $r1 = $r5, $r1
; CV1-NEXT:    maxwp $r2 = $r6, $r2
; CV1-NEXT:    maxwp $r3 = $r7, $r3
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    maxwp $r0 = $r0, $r2
; CV1-NEXT:    maxwp $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srld $r3 = $r1, 32
; CV1-NEXT:    copyd $r4 = $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r4 = $r3, 63, 32
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    maxwp $r0 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    maxwp $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    sxwd $r0 = $r0
; CV1-NEXT:    maxwp $r1 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    maxwp $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 13)
;
; CV2-LABEL: maxrwxd:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r0r1r2r3 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    maxwp $r0 = $r4, $r0
; CV2-NEXT:    maxwp $r1 = $r5, $r1
; CV2-NEXT:    maxwp $r2 = $r6, $r2
; CV2-NEXT:    maxwp $r3 = $r7, $r3
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    maxwp $r0 = $r0, $r2
; CV2-NEXT:    maxwp $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    srld $r3 = $r1, 32
; CV2-NEXT:    copyd $r4 = $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r4 = $r3, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    maxwp $r0 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    maxwp $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    sxwd $r0 = $r0
; CV2-NEXT:    maxwp $r1 = $r1, $r0
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    maxwp $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 13)
  %2 = load <16 x i32>, ptr %0
  %3 = tail call i32 @llvm.vector.reduce.smax.v16i32(<16 x i32> %2)
  %4 = sext i32 %3 to i64
  ret i64 %4
}

define i64 @maxurbod(<8 x i8> %0) {
; CV1-LABEL: maxurbod:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    maxuhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sllhqs $r1 = $r2, 8
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    maxuhq $r0 = $r0, $r2
; CV1-NEXT:    maxuhq $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    srlhqs $r1 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    maxuhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    andd $r0 = $r0, 255
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 22)
;
; CV2-LABEL: maxurbod:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    maxubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    maxubo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    maxubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    andd $r0 = $r0, 255
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 13)
  %2 = tail call i8 @llvm.vector.reduce.umax.v8i8(<8 x i8> %0)
  %3 = zext i8 %2 to i64
  ret i64 %3
}

define i64 @maxurbpd(<2 x i8> %0) {
; CV1-LABEL: maxurbpd:
; CV1:       # %bb.0:
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x20001
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x20001
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x401
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    andd $r0 = $r0, 255
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: maxurbpd:
; CV2:       # %bb.0:
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    maxubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    andd $r0 = $r0, 255
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 4)
  %2 = tail call i8 @llvm.vector.reduce.umax.v2i8(<2 x i8> %0)
  %3 = zext i8 %2 to i64
  ret i64 %3
}

define i64 @maxurbqd(<4 x i8> %0) {
; CV1-LABEL: maxurbqd:
; CV1:       # %bb.0:
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x8000400020001
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sbmm8 $r1 = $r2, 0x8000400020001
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x40100401
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    extfz $r1 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x8000400020001
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    andd $r0 = $r0, 255
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 12)
;
; CV2-LABEL: maxurbqd:
; CV2:       # %bb.0:
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    maxubo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    maxubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    andd $r0 = $r0, 255
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
  %2 = tail call i8 @llvm.vector.reduce.umax.v4i8(<4 x i8> %0)
  %3 = zext i8 %2 to i64
  ret i64 %3
}

define i64 @maxurbvd(<32 x i8> %0) {
; CV1-LABEL: maxurbvd:
; CV1:       # %bb.0:
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r4 = $r2, 8
; CV1-NEXT:    sllhqs $r5 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd $r3 = $r3, 0xff00ff00.@
; CV1-NEXT:    maxuhq $r4 = $r5, $r4
; CV1-NEXT:    sllhqs $r6 = $r3, 8
; CV1-NEXT:    sllhqs $r7 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    maxuhq $r0 = $r0, $r2
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    srlhqs $r2 = $r4, 8
; CV1-NEXT:    maxuhq $r5 = $r7, $r6
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r2
; CV1-NEXT:    maxuhq $r1 = $r1, $r3
; CV1-NEXT:    srlhqs $r3 = $r5, 8
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    iord $r1 = $r1, $r3
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r2 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    maxuhq $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    maxuhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    sllhqs $r1 = $r2, 8
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    maxuhq $r0 = $r0, $r2
; CV1-NEXT:    maxuhq $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    srlhqs $r1 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    maxuhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 27)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 28)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 29)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 30)
; CV1-NEXT:    andd $r0 = $r0, 255
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 31)
;
; CV2-LABEL: maxurbvd:
; CV2:       # %bb.0:
; CV2-NEXT:    maxubo $r0 = $r0, $r2
; CV2-NEXT:    maxubo $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    maxubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    maxubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    maxubo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    maxubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    andd $r0 = $r0, 255
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 15)
  %2 = tail call i8 @llvm.vector.reduce.umax.v32i8(<32 x i8> %0)
  %3 = zext i8 %2 to i64
  ret i64 %3
}

define i64 @maxurbxd(<16 x i8> %0) {
; CV1-LABEL: maxurbxd:
; CV1:       # %bb.0:
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r2 = $r1, 8
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    maxuhq $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    maxuhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    sllhqs $r1 = $r2, 8
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    maxuhq $r0 = $r0, $r2
; CV1-NEXT:    maxuhq $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    srlhqs $r1 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    maxuhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    andd $r0 = $r0, 255
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 26)
;
; CV2-LABEL: maxurbxd:
; CV2:       # %bb.0:
; CV2-NEXT:    maxubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    maxubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    maxubo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    maxubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    andd $r0 = $r0, 255
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 14)
  %2 = tail call i8 @llvm.vector.reduce.umax.v16i8(<16 x i8> %0)
  %3 = zext i8 %2 to i64
  ret i64 %3
}

define i64 @maxurhod(<8 x i16> %0) {
; CV1-LABEL: maxurhod:
; CV1:       # %bb.0:
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    clrf $r0 = $r0, 63, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 8)
;
; CV2-LABEL: maxurhod:
; CV2:       # %bb.0:
; CV2-NEXT:    maxuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    maxuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    maxuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    clrf $r0 = $r0, 63, 16
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
  %2 = tail call i16 @llvm.vector.reduce.umax.v8i16(<8 x i16> %0)
  %3 = zext i16 %2 to i64
  ret i64 %3
}

define i64 @maxurhpd(<2 x i16> %0) {
; CV1-LABEL: maxurhpd:
; CV1:       # %bb.0:
; CV1-NEXT:    srlw $r1 = $r0, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 31, 16
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    clrf $r0 = $r0, 63, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 4)
;
; CV2-LABEL: maxurhpd:
; CV2:       # %bb.0:
; CV2-NEXT:    srlw $r1 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 31, 16
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    maxuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    clrf $r0 = $r0, 63, 16
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 4)
  %2 = tail call i16 @llvm.vector.reduce.umax.v2i16(<2 x i16> %0)
  %3 = zext i16 %2 to i64
  ret i64 %3
}

define i64 @maxurhqd(<4 x i16> %0) {
; CV1-LABEL: maxurhqd:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    clrf $r0 = $r0, 63, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: maxurhqd:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    maxuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    maxuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    clrf $r0 = $r0, 63, 16
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = tail call i16 @llvm.vector.reduce.umax.v4i16(<4 x i16> %0)
  %3 = zext i16 %2 to i64
  ret i64 %3
}

define i64 @maxurhvd(ptr %0) {
; CV1-LABEL: maxurhvd:
; CV1:       # %bb.0:
; CV1-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r0r1r2r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    maxuhq $r0 = $r0, $r4
; CV1-NEXT:    maxuhq $r1 = $r1, $r5
; CV1-NEXT:    maxuhq $r2 = $r2, $r6
; CV1-NEXT:    maxuhq $r3 = $r3, $r7
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    maxuhq $r0 = $r0, $r2
; CV1-NEXT:    maxuhq $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    clrf $r0 = $r0, 63, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 14)
;
; CV2-LABEL: maxurhvd:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r0r1r2r3 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    maxuhq $r0 = $r0, $r4
; CV2-NEXT:    maxuhq $r1 = $r1, $r5
; CV2-NEXT:    maxuhq $r2 = $r2, $r6
; CV2-NEXT:    maxuhq $r3 = $r3, $r7
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    maxuhq $r0 = $r0, $r2
; CV2-NEXT:    maxuhq $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    maxuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    maxuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    maxuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    clrf $r0 = $r0, 63, 16
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 14)
  %2 = load <32 x i16>, ptr %0
  %3 = tail call i16 @llvm.vector.reduce.umax.v32i16(<32 x i16> %2)
  %4 = zext i16 %3 to i64
  ret i64 %4
}

define i64 @maxurhxd(<16 x i16> %0) {
; CV1-LABEL: maxurhxd:
; CV1:       # %bb.0:
; CV1-NEXT:    maxuhq $r0 = $r0, $r2
; CV1-NEXT:    maxuhq $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    clrf $r0 = $r0, 63, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 9)
;
; CV2-LABEL: maxurhxd:
; CV2:       # %bb.0:
; CV2-NEXT:    maxuhq $r0 = $r0, $r2
; CV2-NEXT:    maxuhq $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    maxuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    maxuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    maxuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    clrf $r0 = $r0, 63, 16
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 9)
  %2 = tail call i16 @llvm.vector.reduce.umax.v16i16(<16 x i16> %0)
  %3 = zext i16 %2 to i64
  ret i64 %3
}

define i64 @maxurwod(<8 x i32> %0) {
; CV1-LABEL: maxurwod:
; CV1:       # %bb.0:
; CV1-NEXT:    maxuwp $r0 = $r0, $r2
; CV1-NEXT:    maxuwp $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srld $r3 = $r1, 32
; CV1-NEXT:    copyd $r4 = $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r4 = $r3, 63, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    maxuwp $r0 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    maxuwp $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    maxuwp $r1 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    maxuwp $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 8)
;
; CV2-LABEL: maxurwod:
; CV2:       # %bb.0:
; CV2-NEXT:    maxuwp $r0 = $r0, $r2
; CV2-NEXT:    maxuwp $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srld $r3 = $r1, 32
; CV2-NEXT:    copyd $r4 = $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r4 = $r3, 63, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    maxuwp $r0 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    maxuwp $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    zxwd $r0 = $r0
; CV2-NEXT:    maxuwp $r1 = $r1, $r0
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    maxuwp $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
  %2 = tail call i32 @llvm.vector.reduce.umax.v8i32(<8 x i32> %0)
  %3 = zext i32 %2 to i64
  ret i64 %3
}

define i64 @maxurwpd(<2 x i32> %0) {
; CV1-LABEL: maxurwpd:
; CV1:       # %bb.0:
; CV1-NEXT:    srad $r1 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 63, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    maxuwp $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: maxurwpd:
; CV2:       # %bb.0:
; CV2-NEXT:    srad $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 63, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    maxuwp $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    zxwd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 3)
  %2 = tail call i32 @llvm.vector.reduce.umax.v2i32(<2 x i32> %0)
  %3 = zext i32 %2 to i64
  ret i64 %3
}

define i64 @maxurwqd(<4 x i32> %0) {
; CV1-LABEL: maxurwqd:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r2 = $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    maxuwp $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    maxuwp $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    maxuwp $r1 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    maxuwp $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: maxurwqd:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r2 = $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    maxuwp $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    maxuwp $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    zxwd $r0 = $r0
; CV2-NEXT:    maxuwp $r1 = $r1, $r0
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    maxuwp $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 6)
  %2 = tail call i32 @llvm.vector.reduce.umax.v4i32(<4 x i32> %0)
  %3 = zext i32 %2 to i64
  ret i64 %3
}

define i64 @maxurwvd(ptr %0) {
; CV1-LABEL: maxurwvd:
; CV1:       # %bb.0:
; CV1-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r8r9r10r11 = 96[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    lo $r32r33r34r35 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    lo $r0r1r2r3 = 64[$r0]
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    maxuwp $r4 = $r4, $r8
; CV1-NEXT:    maxuwp $r5 = $r5, $r9
; CV1-NEXT:    maxuwp $r6 = $r6, $r10
; CV1-NEXT:    maxuwp $r7 = $r7, $r11
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    maxuwp $r0 = $r32, $r0
; CV1-NEXT:    maxuwp $r1 = $r33, $r1
; CV1-NEXT:    maxuwp $r2 = $r34, $r2
; CV1-NEXT:    maxuwp $r3 = $r35, $r3
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    maxuwp $r0 = $r0, $r4
; CV1-NEXT:    maxuwp $r1 = $r1, $r5
; CV1-NEXT:    maxuwp $r2 = $r2, $r6
; CV1-NEXT:    maxuwp $r3 = $r3, $r7
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    maxuwp $r0 = $r0, $r2
; CV1-NEXT:    maxuwp $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    srld $r3 = $r1, 32
; CV1-NEXT:    copyd $r4 = $r1
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r4 = $r3, 63, 32
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    maxuwp $r0 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    maxuwp $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    maxuwp $r1 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    maxuwp $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 16)
;
; CV2-LABEL: maxurwvd:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r8r9r10r11 = 96[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    lo $r32r33r34r35 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    lo $r0r1r2r3 = 64[$r0]
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    maxuwp $r4 = $r4, $r8
; CV2-NEXT:    maxuwp $r5 = $r5, $r9
; CV2-NEXT:    maxuwp $r6 = $r6, $r10
; CV2-NEXT:    maxuwp $r7 = $r7, $r11
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    maxuwp $r0 = $r32, $r0
; CV2-NEXT:    maxuwp $r1 = $r33, $r1
; CV2-NEXT:    maxuwp $r2 = $r34, $r2
; CV2-NEXT:    maxuwp $r3 = $r35, $r3
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    maxuwp $r0 = $r0, $r4
; CV2-NEXT:    maxuwp $r1 = $r1, $r5
; CV2-NEXT:    maxuwp $r2 = $r2, $r6
; CV2-NEXT:    maxuwp $r3 = $r3, $r7
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    maxuwp $r0 = $r0, $r2
; CV2-NEXT:    maxuwp $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    srld $r3 = $r1, 32
; CV2-NEXT:    copyd $r4 = $r1
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r4 = $r3, 63, 32
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    maxuwp $r0 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    maxuwp $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    zxwd $r0 = $r0
; CV2-NEXT:    maxuwp $r1 = $r1, $r0
; CV2-NEXT:    ;; # (end cycle 15)
; CV2-NEXT:    maxuwp $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 16)
  %2 = load <32 x i32>, ptr %0
  %3 = tail call i32 @llvm.vector.reduce.umax.v32i32(<32 x i32> %2)
  %4 = zext i32 %3 to i64
  ret i64 %4
}

define i64 @maxurwxd(ptr %0) {
; CV1-LABEL: maxurwxd:
; CV1:       # %bb.0:
; CV1-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r0r1r2r3 = 32[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    maxuwp $r0 = $r4, $r0
; CV1-NEXT:    maxuwp $r1 = $r5, $r1
; CV1-NEXT:    maxuwp $r2 = $r6, $r2
; CV1-NEXT:    maxuwp $r3 = $r7, $r3
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    maxuwp $r0 = $r0, $r2
; CV1-NEXT:    maxuwp $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srld $r3 = $r1, 32
; CV1-NEXT:    copyd $r4 = $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r4 = $r3, 63, 32
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    maxuwp $r0 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    maxuwp $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    maxuwp $r1 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    maxuwp $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 13)
;
; CV2-LABEL: maxurwxd:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r0r1r2r3 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    maxuwp $r0 = $r4, $r0
; CV2-NEXT:    maxuwp $r1 = $r5, $r1
; CV2-NEXT:    maxuwp $r2 = $r6, $r2
; CV2-NEXT:    maxuwp $r3 = $r7, $r3
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    maxuwp $r0 = $r0, $r2
; CV2-NEXT:    maxuwp $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    srld $r3 = $r1, 32
; CV2-NEXT:    copyd $r4 = $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r4 = $r3, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    maxuwp $r0 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    maxuwp $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    zxwd $r0 = $r0
; CV2-NEXT:    maxuwp $r1 = $r1, $r0
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    maxuwp $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 13)
  %2 = load <16 x i32>, ptr %0
  %3 = tail call i32 @llvm.vector.reduce.umax.v16i32(<16 x i32> %2)
  %4 = zext i32 %3 to i64
  ret i64 %4
}

define i64 @minrbod(<8 x i8> %0) {
; CV1-LABEL: minrbod:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    minhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sllhqs $r1 = $r2, 8
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    minhq $r0 = $r0, $r2
; CV1-NEXT:    minhq $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    srlhqs $r1 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    minhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 22)
;
; CV2-LABEL: minrbod:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    minbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    minbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    minbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 13)
  %2 = tail call i8 @llvm.vector.reduce.smin.v8i8(<8 x i8> %0)
  %3 = sext i8 %2 to i64
  ret i64 %3
}

define i64 @minrbpd(<2 x i8> %0) {
; CV1-LABEL: minrbpd:
; CV1:       # %bb.0:
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x401
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: minrbpd:
; CV2:       # %bb.0:
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    minbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 4)
  %2 = tail call i8 @llvm.vector.reduce.smin.v2i8(<2 x i8> %0)
  %3 = sext i8 %2 to i64
  ret i64 %3
}

define i64 @minrbqd(<4 x i8> %0) {
; CV1-LABEL: minrbqd:
; CV1:       # %bb.0:
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sxlbhq $r1 = $r2
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x40100401
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    extfz $r1 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 12)
;
; CV2-LABEL: minrbqd:
; CV2:       # %bb.0:
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    minbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    minbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
  %2 = tail call i8 @llvm.vector.reduce.smin.v4i8(<4 x i8> %0)
  %3 = sext i8 %2 to i64
  ret i64 %3
}

define i64 @minrbvd(<32 x i8> %0) {
; CV1-LABEL: minrbvd:
; CV1:       # %bb.0:
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r4 = $r2, 8
; CV1-NEXT:    sllhqs $r5 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd $r3 = $r3, 0xff00ff00.@
; CV1-NEXT:    minhq $r4 = $r5, $r4
; CV1-NEXT:    sllhqs $r6 = $r3, 8
; CV1-NEXT:    sllhqs $r7 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    minhq $r0 = $r0, $r2
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    srlhqs $r2 = $r4, 8
; CV1-NEXT:    minhq $r5 = $r7, $r6
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r2
; CV1-NEXT:    minhq $r1 = $r1, $r3
; CV1-NEXT:    srlhqs $r3 = $r5, 8
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    iord $r1 = $r1, $r3
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r2 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    minhq $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    minhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    sllhqs $r1 = $r2, 8
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    minhq $r0 = $r0, $r2
; CV1-NEXT:    minhq $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    srlhqs $r1 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    minhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 27)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 28)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 29)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 30)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 31)
;
; CV2-LABEL: minrbvd:
; CV2:       # %bb.0:
; CV2-NEXT:    minbo $r0 = $r0, $r2
; CV2-NEXT:    minbo $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    minbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    minbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    minbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    minbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 15)
  %2 = tail call i8 @llvm.vector.reduce.smin.v32i8(<32 x i8> %0)
  %3 = sext i8 %2 to i64
  ret i64 %3
}

define i64 @minrbxd(<16 x i8> %0) {
; CV1-LABEL: minrbxd:
; CV1:       # %bb.0:
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r2 = $r1, 8
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    minhq $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    minhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    sllhqs $r1 = $r2, 8
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    minhq $r0 = $r0, $r2
; CV1-NEXT:    minhq $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    srlhqs $r1 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    minhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 26)
;
; CV2-LABEL: minrbxd:
; CV2:       # %bb.0:
; CV2-NEXT:    minbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    minbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    minbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    minbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 14)
  %2 = tail call i8 @llvm.vector.reduce.smin.v16i8(<16 x i8> %0)
  %3 = sext i8 %2 to i64
  ret i64 %3
}

define i64 @minrhod(<8 x i16> %0) {
; CV1-LABEL: minrhod:
; CV1:       # %bb.0:
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    sxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 8)
;
; CV2-LABEL: minrhod:
; CV2:       # %bb.0:
; CV2-NEXT:    minhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    minhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    minhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    sxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
  %2 = tail call i16 @llvm.vector.reduce.smin.v8i16(<8 x i16> %0)
  %3 = sext i16 %2 to i64
  ret i64 %3
}

define i64 @minrhpd(<2 x i16> %0) {
; CV1-LABEL: minrhpd:
; CV1:       # %bb.0:
; CV1-NEXT:    srlw $r1 = $r0, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 31, 16
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: minrhpd:
; CV2:       # %bb.0:
; CV2-NEXT:    srlw $r1 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 31, 16
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    minhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 3)
  %2 = tail call i16 @llvm.vector.reduce.smin.v2i16(<2 x i16> %0)
  %3 = sext i16 %2 to i64
  ret i64 %3
}

define i64 @minrhqd(<4 x i16> %0) {
; CV1-LABEL: minrhqd:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    sxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: minrhqd:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    minhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    minhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    sxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = tail call i16 @llvm.vector.reduce.smin.v4i16(<4 x i16> %0)
  %3 = sext i16 %2 to i64
  ret i64 %3
}

define i64 @minrhvd(ptr %0) {
; CV1-LABEL: minrhvd:
; CV1:       # %bb.0:
; CV1-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r0r1r2r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    minhq $r0 = $r0, $r4
; CV1-NEXT:    minhq $r1 = $r1, $r5
; CV1-NEXT:    minhq $r2 = $r2, $r6
; CV1-NEXT:    minhq $r3 = $r3, $r7
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    minhq $r0 = $r0, $r2
; CV1-NEXT:    minhq $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    sxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 14)
;
; CV2-LABEL: minrhvd:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r0r1r2r3 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    minhq $r0 = $r0, $r4
; CV2-NEXT:    minhq $r1 = $r1, $r5
; CV2-NEXT:    minhq $r2 = $r2, $r6
; CV2-NEXT:    minhq $r3 = $r3, $r7
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    minhq $r0 = $r0, $r2
; CV2-NEXT:    minhq $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    minhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    minhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    minhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    sxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 14)
  %2 = load <32 x i16>, ptr %0
  %3 = tail call i16 @llvm.vector.reduce.smin.v32i16(<32 x i16> %2)
  %4 = sext i16 %3 to i64
  ret i64 %4
}

define i64 @minrhxd(<16 x i16> %0) {
; CV1-LABEL: minrhxd:
; CV1:       # %bb.0:
; CV1-NEXT:    minhq $r0 = $r0, $r2
; CV1-NEXT:    minhq $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    sxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 9)
;
; CV2-LABEL: minrhxd:
; CV2:       # %bb.0:
; CV2-NEXT:    minhq $r0 = $r0, $r2
; CV2-NEXT:    minhq $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    minhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    minhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    minhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    sxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 9)
  %2 = tail call i16 @llvm.vector.reduce.smin.v16i16(<16 x i16> %0)
  %3 = sext i16 %2 to i64
  ret i64 %3
}

define i64 @minrwod(<8 x i32> %0) {
; CV1-LABEL: minrwod:
; CV1:       # %bb.0:
; CV1-NEXT:    minwp $r0 = $r0, $r2
; CV1-NEXT:    minwp $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srld $r3 = $r1, 32
; CV1-NEXT:    copyd $r4 = $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r4 = $r3, 63, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    minwp $r0 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    minwp $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    sxwd $r0 = $r0
; CV1-NEXT:    minwp $r1 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    minwp $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 8)
;
; CV2-LABEL: minrwod:
; CV2:       # %bb.0:
; CV2-NEXT:    minwp $r0 = $r0, $r2
; CV2-NEXT:    minwp $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srld $r3 = $r1, 32
; CV2-NEXT:    copyd $r4 = $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r4 = $r3, 63, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    minwp $r0 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    minwp $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    sxwd $r0 = $r0
; CV2-NEXT:    minwp $r1 = $r1, $r0
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    minwp $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
  %2 = tail call i32 @llvm.vector.reduce.smin.v8i32(<8 x i32> %0)
  %3 = sext i32 %2 to i64
  ret i64 %3
}

define i64 @minrwpd(<2 x i32> %0) {
; CV1-LABEL: minrwpd:
; CV1:       # %bb.0:
; CV1-NEXT:    srad $r1 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 63, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    minwp $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sxwd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: minrwpd:
; CV2:       # %bb.0:
; CV2-NEXT:    srad $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 63, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    minwp $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sxwd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 3)
  %2 = tail call i32 @llvm.vector.reduce.smin.v2i32(<2 x i32> %0)
  %3 = sext i32 %2 to i64
  ret i64 %3
}

define i64 @minrwqd(<4 x i32> %0) {
; CV1-LABEL: minrwqd:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r2 = $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    minwp $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    minwp $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sxwd $r0 = $r0
; CV1-NEXT:    minwp $r1 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    minwp $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: minrwqd:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r2 = $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    minwp $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    minwp $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sxwd $r0 = $r0
; CV2-NEXT:    minwp $r1 = $r1, $r0
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    minwp $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 6)
  %2 = tail call i32 @llvm.vector.reduce.smin.v4i32(<4 x i32> %0)
  %3 = sext i32 %2 to i64
  ret i64 %3
}

define i64 @minrwvd(ptr %0) {
; CV1-LABEL: minrwvd:
; CV1:       # %bb.0:
; CV1-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r8r9r10r11 = 96[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    lo $r32r33r34r35 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    lo $r0r1r2r3 = 64[$r0]
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    minwp $r4 = $r4, $r8
; CV1-NEXT:    minwp $r5 = $r5, $r9
; CV1-NEXT:    minwp $r6 = $r6, $r10
; CV1-NEXT:    minwp $r7 = $r7, $r11
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    minwp $r0 = $r32, $r0
; CV1-NEXT:    minwp $r1 = $r33, $r1
; CV1-NEXT:    minwp $r2 = $r34, $r2
; CV1-NEXT:    minwp $r3 = $r35, $r3
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    minwp $r0 = $r0, $r4
; CV1-NEXT:    minwp $r1 = $r1, $r5
; CV1-NEXT:    minwp $r2 = $r2, $r6
; CV1-NEXT:    minwp $r3 = $r3, $r7
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    minwp $r0 = $r0, $r2
; CV1-NEXT:    minwp $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    srld $r3 = $r1, 32
; CV1-NEXT:    copyd $r4 = $r1
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r4 = $r3, 63, 32
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    minwp $r0 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    minwp $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    sxwd $r0 = $r0
; CV1-NEXT:    minwp $r1 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    minwp $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 16)
;
; CV2-LABEL: minrwvd:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r8r9r10r11 = 96[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    lo $r32r33r34r35 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    lo $r0r1r2r3 = 64[$r0]
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    minwp $r4 = $r4, $r8
; CV2-NEXT:    minwp $r5 = $r5, $r9
; CV2-NEXT:    minwp $r6 = $r6, $r10
; CV2-NEXT:    minwp $r7 = $r7, $r11
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    minwp $r0 = $r32, $r0
; CV2-NEXT:    minwp $r1 = $r33, $r1
; CV2-NEXT:    minwp $r2 = $r34, $r2
; CV2-NEXT:    minwp $r3 = $r35, $r3
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    minwp $r0 = $r0, $r4
; CV2-NEXT:    minwp $r1 = $r1, $r5
; CV2-NEXT:    minwp $r2 = $r2, $r6
; CV2-NEXT:    minwp $r3 = $r3, $r7
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    minwp $r0 = $r0, $r2
; CV2-NEXT:    minwp $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    srld $r3 = $r1, 32
; CV2-NEXT:    copyd $r4 = $r1
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r4 = $r3, 63, 32
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    minwp $r0 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    minwp $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    sxwd $r0 = $r0
; CV2-NEXT:    minwp $r1 = $r1, $r0
; CV2-NEXT:    ;; # (end cycle 15)
; CV2-NEXT:    minwp $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 16)
  %2 = load <32 x i32>, ptr %0
  %3 = tail call i32 @llvm.vector.reduce.smin.v32i32(<32 x i32> %2)
  %4 = sext i32 %3 to i64
  ret i64 %4
}

define i64 @minrwxd(ptr %0) {
; CV1-LABEL: minrwxd:
; CV1:       # %bb.0:
; CV1-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r0r1r2r3 = 32[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    minwp $r0 = $r4, $r0
; CV1-NEXT:    minwp $r1 = $r5, $r1
; CV1-NEXT:    minwp $r2 = $r6, $r2
; CV1-NEXT:    minwp $r3 = $r7, $r3
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    minwp $r0 = $r0, $r2
; CV1-NEXT:    minwp $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srld $r3 = $r1, 32
; CV1-NEXT:    copyd $r4 = $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r4 = $r3, 63, 32
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    minwp $r0 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    minwp $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    sxwd $r0 = $r0
; CV1-NEXT:    minwp $r1 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    minwp $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 13)
;
; CV2-LABEL: minrwxd:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r0r1r2r3 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    minwp $r0 = $r4, $r0
; CV2-NEXT:    minwp $r1 = $r5, $r1
; CV2-NEXT:    minwp $r2 = $r6, $r2
; CV2-NEXT:    minwp $r3 = $r7, $r3
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    minwp $r0 = $r0, $r2
; CV2-NEXT:    minwp $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    srld $r3 = $r1, 32
; CV2-NEXT:    copyd $r4 = $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r4 = $r3, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    minwp $r0 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    minwp $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    sxwd $r0 = $r0
; CV2-NEXT:    minwp $r1 = $r1, $r0
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    minwp $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 13)
  %2 = load <16 x i32>, ptr %0
  %3 = tail call i32 @llvm.vector.reduce.smin.v16i32(<16 x i32> %2)
  %4 = sext i32 %3 to i64
  ret i64 %4
}

define i64 @minurbod(<8 x i8> %0) {
; CV1-LABEL: minurbod:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    minuhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sllhqs $r1 = $r2, 8
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    minuhq $r0 = $r0, $r2
; CV1-NEXT:    minuhq $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    srlhqs $r1 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    minuhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    andd $r0 = $r0, 255
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 22)
;
; CV2-LABEL: minurbod:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    minubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    minubo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    minubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    andd $r0 = $r0, 255
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 13)
  %2 = tail call i8 @llvm.vector.reduce.umin.v8i8(<8 x i8> %0)
  %3 = zext i8 %2 to i64
  ret i64 %3
}

define i64 @minurbpd(<2 x i8> %0) {
; CV1-LABEL: minurbpd:
; CV1:       # %bb.0:
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x20001
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x20001
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x401
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    andd $r0 = $r0, 255
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: minurbpd:
; CV2:       # %bb.0:
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    minubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    andd $r0 = $r0, 255
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 4)
  %2 = tail call i8 @llvm.vector.reduce.umin.v2i8(<2 x i8> %0)
  %3 = zext i8 %2 to i64
  ret i64 %3
}

define i64 @minurbqd(<4 x i8> %0) {
; CV1-LABEL: minurbqd:
; CV1:       # %bb.0:
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x8000400020001
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sbmm8 $r1 = $r2, 0x8000400020001
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x40100401
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    extfz $r1 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x8000400020001
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    andd $r0 = $r0, 255
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 12)
;
; CV2-LABEL: minurbqd:
; CV2:       # %bb.0:
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    minubo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    minubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    andd $r0 = $r0, 255
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
  %2 = tail call i8 @llvm.vector.reduce.umin.v4i8(<4 x i8> %0)
  %3 = zext i8 %2 to i64
  ret i64 %3
}

define i64 @minurbvd(<32 x i8> %0) {
; CV1-LABEL: minurbvd:
; CV1:       # %bb.0:
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r4 = $r2, 8
; CV1-NEXT:    sllhqs $r5 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd $r3 = $r3, 0xff00ff00.@
; CV1-NEXT:    minuhq $r4 = $r5, $r4
; CV1-NEXT:    sllhqs $r6 = $r3, 8
; CV1-NEXT:    sllhqs $r7 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    minuhq $r0 = $r0, $r2
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    srlhqs $r2 = $r4, 8
; CV1-NEXT:    minuhq $r5 = $r7, $r6
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r2
; CV1-NEXT:    minuhq $r1 = $r1, $r3
; CV1-NEXT:    srlhqs $r3 = $r5, 8
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    iord $r1 = $r1, $r3
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r2 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    minuhq $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    minuhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    sllhqs $r1 = $r2, 8
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    minuhq $r0 = $r0, $r2
; CV1-NEXT:    minuhq $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    srlhqs $r1 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    minuhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 27)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 28)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 29)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 30)
; CV1-NEXT:    andd $r0 = $r0, 255
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 31)
;
; CV2-LABEL: minurbvd:
; CV2:       # %bb.0:
; CV2-NEXT:    minubo $r0 = $r0, $r2
; CV2-NEXT:    minubo $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    minubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    minubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    minubo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    minubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    andd $r0 = $r0, 255
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 15)
  %2 = tail call i8 @llvm.vector.reduce.umin.v32i8(<32 x i8> %0)
  %3 = zext i8 %2 to i64
  ret i64 %3
}

define i64 @minurbxd(<16 x i8> %0) {
; CV1-LABEL: minurbxd:
; CV1:       # %bb.0:
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r2 = $r1, 8
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    minuhq $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    minuhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    sllhqs $r1 = $r2, 8
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    minuhq $r0 = $r0, $r2
; CV1-NEXT:    minuhq $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    srlhqs $r1 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    minuhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    andd $r0 = $r0, 255
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 26)
;
; CV2-LABEL: minurbxd:
; CV2:       # %bb.0:
; CV2-NEXT:    minubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    minubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    minubo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    minubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    andd $r0 = $r0, 255
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 14)
  %2 = tail call i8 @llvm.vector.reduce.umin.v16i8(<16 x i8> %0)
  %3 = zext i8 %2 to i64
  ret i64 %3
}

define i64 @minurhod(<8 x i16> %0) {
; CV1-LABEL: minurhod:
; CV1:       # %bb.0:
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    clrf $r0 = $r0, 63, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 8)
;
; CV2-LABEL: minurhod:
; CV2:       # %bb.0:
; CV2-NEXT:    minuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    minuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    minuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    clrf $r0 = $r0, 63, 16
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
  %2 = tail call i16 @llvm.vector.reduce.umin.v8i16(<8 x i16> %0)
  %3 = zext i16 %2 to i64
  ret i64 %3
}

define i64 @minurhpd(<2 x i16> %0) {
; CV1-LABEL: minurhpd:
; CV1:       # %bb.0:
; CV1-NEXT:    srlw $r1 = $r0, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 31, 16
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    clrf $r0 = $r0, 63, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 4)
;
; CV2-LABEL: minurhpd:
; CV2:       # %bb.0:
; CV2-NEXT:    srlw $r1 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 31, 16
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    minuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    clrf $r0 = $r0, 63, 16
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 4)
  %2 = tail call i16 @llvm.vector.reduce.umin.v2i16(<2 x i16> %0)
  %3 = zext i16 %2 to i64
  ret i64 %3
}

define i64 @minurhqd(<4 x i16> %0) {
; CV1-LABEL: minurhqd:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    clrf $r0 = $r0, 63, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: minurhqd:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    minuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    minuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    clrf $r0 = $r0, 63, 16
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = tail call i16 @llvm.vector.reduce.umin.v4i16(<4 x i16> %0)
  %3 = zext i16 %2 to i64
  ret i64 %3
}

define i64 @minurhvd(ptr %0) {
; CV1-LABEL: minurhvd:
; CV1:       # %bb.0:
; CV1-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r0r1r2r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    minuhq $r0 = $r0, $r4
; CV1-NEXT:    minuhq $r1 = $r1, $r5
; CV1-NEXT:    minuhq $r2 = $r2, $r6
; CV1-NEXT:    minuhq $r3 = $r3, $r7
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    minuhq $r0 = $r0, $r2
; CV1-NEXT:    minuhq $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    clrf $r0 = $r0, 63, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 14)
;
; CV2-LABEL: minurhvd:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r0r1r2r3 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    minuhq $r0 = $r0, $r4
; CV2-NEXT:    minuhq $r1 = $r1, $r5
; CV2-NEXT:    minuhq $r2 = $r2, $r6
; CV2-NEXT:    minuhq $r3 = $r3, $r7
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    minuhq $r0 = $r0, $r2
; CV2-NEXT:    minuhq $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    minuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    minuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    minuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    clrf $r0 = $r0, 63, 16
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 14)
  %2 = load <32 x i16>, ptr %0
  %3 = tail call i16 @llvm.vector.reduce.umin.v32i16(<32 x i16> %2)
  %4 = zext i16 %3 to i64
  ret i64 %4
}

define i64 @minurhxd(<16 x i16> %0) {
; CV1-LABEL: minurhxd:
; CV1:       # %bb.0:
; CV1-NEXT:    minuhq $r0 = $r0, $r2
; CV1-NEXT:    minuhq $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    clrf $r0 = $r0, 63, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 9)
;
; CV2-LABEL: minurhxd:
; CV2:       # %bb.0:
; CV2-NEXT:    minuhq $r0 = $r0, $r2
; CV2-NEXT:    minuhq $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    minuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    minuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    minuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    clrf $r0 = $r0, 63, 16
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 9)
  %2 = tail call i16 @llvm.vector.reduce.umin.v16i16(<16 x i16> %0)
  %3 = zext i16 %2 to i64
  ret i64 %3
}

define i64 @minurwod(<8 x i32> %0) {
; CV1-LABEL: minurwod:
; CV1:       # %bb.0:
; CV1-NEXT:    minuwp $r0 = $r0, $r2
; CV1-NEXT:    minuwp $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srld $r3 = $r1, 32
; CV1-NEXT:    copyd $r4 = $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r4 = $r3, 63, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    minuwp $r0 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    minuwp $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    minuwp $r1 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    minuwp $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 8)
;
; CV2-LABEL: minurwod:
; CV2:       # %bb.0:
; CV2-NEXT:    minuwp $r0 = $r0, $r2
; CV2-NEXT:    minuwp $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srld $r3 = $r1, 32
; CV2-NEXT:    copyd $r4 = $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r4 = $r3, 63, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    minuwp $r0 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    minuwp $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    zxwd $r0 = $r0
; CV2-NEXT:    minuwp $r1 = $r1, $r0
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    minuwp $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
  %2 = tail call i32 @llvm.vector.reduce.umin.v8i32(<8 x i32> %0)
  %3 = zext i32 %2 to i64
  ret i64 %3
}

define i64 @minurwpd(<2 x i32> %0) {
; CV1-LABEL: minurwpd:
; CV1:       # %bb.0:
; CV1-NEXT:    srad $r1 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 63, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    minuwp $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: minurwpd:
; CV2:       # %bb.0:
; CV2-NEXT:    srad $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 63, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    minuwp $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    zxwd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 3)
  %2 = tail call i32 @llvm.vector.reduce.umin.v2i32(<2 x i32> %0)
  %3 = zext i32 %2 to i64
  ret i64 %3
}

define i64 @minurwqd(<4 x i32> %0) {
; CV1-LABEL: minurwqd:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r2 = $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    minuwp $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    minuwp $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    minuwp $r1 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    minuwp $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: minurwqd:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r2 = $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    minuwp $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    minuwp $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    zxwd $r0 = $r0
; CV2-NEXT:    minuwp $r1 = $r1, $r0
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    minuwp $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 6)
  %2 = tail call i32 @llvm.vector.reduce.umin.v4i32(<4 x i32> %0)
  %3 = zext i32 %2 to i64
  ret i64 %3
}

define i64 @minurwvd(ptr %0) {
; CV1-LABEL: minurwvd:
; CV1:       # %bb.0:
; CV1-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r8r9r10r11 = 96[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    lo $r32r33r34r35 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    lo $r0r1r2r3 = 64[$r0]
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    minuwp $r4 = $r4, $r8
; CV1-NEXT:    minuwp $r5 = $r5, $r9
; CV1-NEXT:    minuwp $r6 = $r6, $r10
; CV1-NEXT:    minuwp $r7 = $r7, $r11
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    minuwp $r0 = $r32, $r0
; CV1-NEXT:    minuwp $r1 = $r33, $r1
; CV1-NEXT:    minuwp $r2 = $r34, $r2
; CV1-NEXT:    minuwp $r3 = $r35, $r3
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    minuwp $r0 = $r0, $r4
; CV1-NEXT:    minuwp $r1 = $r1, $r5
; CV1-NEXT:    minuwp $r2 = $r2, $r6
; CV1-NEXT:    minuwp $r3 = $r3, $r7
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    minuwp $r0 = $r0, $r2
; CV1-NEXT:    minuwp $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    srld $r3 = $r1, 32
; CV1-NEXT:    copyd $r4 = $r1
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r4 = $r3, 63, 32
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    minuwp $r0 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    minuwp $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    minuwp $r1 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    minuwp $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 16)
;
; CV2-LABEL: minurwvd:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r8r9r10r11 = 96[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    lo $r32r33r34r35 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    lo $r0r1r2r3 = 64[$r0]
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    minuwp $r4 = $r4, $r8
; CV2-NEXT:    minuwp $r5 = $r5, $r9
; CV2-NEXT:    minuwp $r6 = $r6, $r10
; CV2-NEXT:    minuwp $r7 = $r7, $r11
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    minuwp $r0 = $r32, $r0
; CV2-NEXT:    minuwp $r1 = $r33, $r1
; CV2-NEXT:    minuwp $r2 = $r34, $r2
; CV2-NEXT:    minuwp $r3 = $r35, $r3
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    minuwp $r0 = $r0, $r4
; CV2-NEXT:    minuwp $r1 = $r1, $r5
; CV2-NEXT:    minuwp $r2 = $r2, $r6
; CV2-NEXT:    minuwp $r3 = $r3, $r7
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    minuwp $r0 = $r0, $r2
; CV2-NEXT:    minuwp $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    srld $r3 = $r1, 32
; CV2-NEXT:    copyd $r4 = $r1
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r4 = $r3, 63, 32
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    minuwp $r0 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    minuwp $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    zxwd $r0 = $r0
; CV2-NEXT:    minuwp $r1 = $r1, $r0
; CV2-NEXT:    ;; # (end cycle 15)
; CV2-NEXT:    minuwp $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 16)
  %2 = load <32 x i32>, ptr %0
  %3 = tail call i32 @llvm.vector.reduce.umin.v32i32(<32 x i32> %2)
  %4 = zext i32 %3 to i64
  ret i64 %4
}

define i64 @minurwxd(ptr %0) {
; CV1-LABEL: minurwxd:
; CV1:       # %bb.0:
; CV1-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r0r1r2r3 = 32[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    minuwp $r0 = $r4, $r0
; CV1-NEXT:    minuwp $r1 = $r5, $r1
; CV1-NEXT:    minuwp $r2 = $r6, $r2
; CV1-NEXT:    minuwp $r3 = $r7, $r3
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    minuwp $r0 = $r0, $r2
; CV1-NEXT:    minuwp $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srld $r3 = $r1, 32
; CV1-NEXT:    copyd $r4 = $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r4 = $r3, 63, 32
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    minuwp $r0 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    minuwp $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    minuwp $r1 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    minuwp $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 13)
;
; CV2-LABEL: minurwxd:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r4r5r6r7 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r0r1r2r3 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    minuwp $r0 = $r4, $r0
; CV2-NEXT:    minuwp $r1 = $r5, $r1
; CV2-NEXT:    minuwp $r2 = $r6, $r2
; CV2-NEXT:    minuwp $r3 = $r7, $r3
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    minuwp $r0 = $r0, $r2
; CV2-NEXT:    minuwp $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    srld $r3 = $r1, 32
; CV2-NEXT:    copyd $r4 = $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r4 = $r3, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    minuwp $r0 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    minuwp $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    zxwd $r0 = $r0
; CV2-NEXT:    minuwp $r1 = $r1, $r0
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    minuwp $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 13)
  %2 = load <16 x i32>, ptr %0
  %3 = tail call i32 @llvm.vector.reduce.umin.v16i32(<16 x i32> %2)
  %4 = zext i32 %3 to i64
  ret i64 %4
}

define i64 @andrbod(<8 x i8> %0) {
; CV1-LABEL: andrbod:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    andd $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    andd $r0 = $r0, 255
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 13)
;
; CV2-LABEL: andrbod:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    andd $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    andd $r0 = $r0, 255
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 13)
  %2 = tail call i8 @llvm.vector.reduce.and.v8i8(<8 x i8> %0)
  %3 = zext i8 %2 to i64
  ret i64 %3
}

define i64 @andrbpd(<2 x i8> %0) {
; CV1-LABEL: andrbpd:
; CV1:       # %bb.0:
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andw $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd $r0 = $r0, 255
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 4)
;
; CV2-LABEL: andrbpd:
; CV2:       # %bb.0:
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    andw $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    andd $r0 = $r0, 255
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 4)
  %2 = tail call i8 @llvm.vector.reduce.and.v2i8(<2 x i8> %0)
  %3 = zext i8 %2 to i64
  ret i64 %3
}

define i64 @andrbqd(<4 x i8> %0) {
; CV1-LABEL: andrbqd:
; CV1:       # %bb.0:
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    andw $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    andw $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    andd $r0 = $r0, 255
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 8)
;
; CV2-LABEL: andrbqd:
; CV2:       # %bb.0:
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    andw $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    andw $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    andd $r0 = $r0, 255
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
  %2 = tail call i8 @llvm.vector.reduce.and.v4i8(<4 x i8> %0)
  %3 = zext i8 %2 to i64
  ret i64 %3
}

define i64 @andrbvd(<32 x i8> %0) {
; CV1-LABEL: andrbvd:
; CV1:       # %bb.0:
; CV1-NEXT:    andd $r0 = $r0, $r2
; CV1-NEXT:    andd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    andd $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    andd $r0 = $r0, 255
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 15)
;
; CV2-LABEL: andrbvd:
; CV2:       # %bb.0:
; CV2-NEXT:    andd $r0 = $r0, $r2
; CV2-NEXT:    andd $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    andd $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    andd $r0 = $r0, 255
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 15)
  %2 = tail call i8 @llvm.vector.reduce.and.v32i8(<32 x i8> %0)
  %3 = zext i8 %2 to i64
  ret i64 %3
}

define i64 @andrbxd(<16 x i8> %0) {
; CV1-LABEL: andrbxd:
; CV1:       # %bb.0:
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    andd $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    andd $r0 = $r0, 255
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 14)
;
; CV2-LABEL: andrbxd:
; CV2:       # %bb.0:
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    andd $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    andd $r0 = $r0, 255
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 14)
  %2 = tail call i8 @llvm.vector.reduce.and.v16i8(<16 x i8> %0)
  %3 = zext i8 %2 to i64
  ret i64 %3
}

define i64 @andrhod(<8 x i16> %0) {
; CV1-LABEL: andrhod:
; CV1:       # %bb.0:
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    clrf $r0 = $r0, 63, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 8)
;
; CV2-LABEL: andrhod:
; CV2:       # %bb.0:
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    clrf $r0 = $r0, 63, 16
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
  %2 = tail call i16 @llvm.vector.reduce.and.v8i16(<8 x i16> %0)
  %3 = zext i16 %2 to i64
  ret i64 %3
}

define i64 @andrhpd(<2 x i16> %0) {
; CV1-LABEL: andrhpd:
; CV1:       # %bb.0:
; CV1-NEXT:    srlw $r1 = $r0, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 31, 16
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andw $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    clrf $r0 = $r0, 63, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 4)
;
; CV2-LABEL: andrhpd:
; CV2:       # %bb.0:
; CV2-NEXT:    srlw $r1 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 31, 16
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    andw $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    clrf $r0 = $r0, 63, 16
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 4)
  %2 = tail call i16 @llvm.vector.reduce.and.v2i16(<2 x i16> %0)
  %3 = zext i16 %2 to i64
  ret i64 %3
}

define i64 @andrhqd(<4 x i16> %0) {
; CV1-LABEL: andrhqd:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    clrf $r0 = $r0, 63, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: andrhqd:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    clrf $r0 = $r0, 63, 16
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = tail call i16 @llvm.vector.reduce.and.v4i16(<4 x i16> %0)
  %3 = zext i16 %2 to i64
  ret i64 %3
}

define i64 @andrhvd(ptr %0) {
; CV1-LABEL: andrhvd:
; CV1:       # %bb.0:
; CV1-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r0r1r2r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andd $r0 = $r0, $r4
; CV1-NEXT:    andd $r1 = $r1, $r5
; CV1-NEXT:    andd $r2 = $r2, $r6
; CV1-NEXT:    andd $r3 = $r3, $r7
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    andd $r0 = $r0, $r2
; CV1-NEXT:    andd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    clrf $r0 = $r0, 63, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 14)
;
; CV2-LABEL: andrhvd:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r0r1r2r3 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    andd $r0 = $r0, $r4
; CV2-NEXT:    andd $r1 = $r1, $r5
; CV2-NEXT:    andd $r2 = $r2, $r6
; CV2-NEXT:    andd $r3 = $r3, $r7
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    andd $r0 = $r0, $r2
; CV2-NEXT:    andd $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    clrf $r0 = $r0, 63, 16
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 14)
  %2 = load <32 x i16>, ptr %0
  %3 = tail call i16 @llvm.vector.reduce.and.v32i16(<32 x i16> %2)
  %4 = zext i16 %3 to i64
  ret i64 %4
}

define i64 @andrhxd(<16 x i16> %0) {
; CV1-LABEL: andrhxd:
; CV1:       # %bb.0:
; CV1-NEXT:    andd $r0 = $r0, $r2
; CV1-NEXT:    andd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    clrf $r0 = $r0, 63, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 9)
;
; CV2-LABEL: andrhxd:
; CV2:       # %bb.0:
; CV2-NEXT:    andd $r0 = $r0, $r2
; CV2-NEXT:    andd $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    clrf $r0 = $r0, 63, 16
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 9)
  %2 = tail call i16 @llvm.vector.reduce.and.v16i16(<16 x i16> %0)
  %3 = zext i16 %2 to i64
  ret i64 %3
}

define i64 @andrwod(<8 x i32> %0) {
; CV1-LABEL: andrwod:
; CV1:       # %bb.0:
; CV1-NEXT:    andd $r0 = $r0, $r2
; CV1-NEXT:    andd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srld $r3 = $r1, 32
; CV1-NEXT:    copyd $r4 = $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r4 = $r3, 63, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    andd $r0 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    andd $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    andd $r1 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    andd $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 8)
;
; CV2-LABEL: andrwod:
; CV2:       # %bb.0:
; CV2-NEXT:    andd $r0 = $r0, $r2
; CV2-NEXT:    andd $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srld $r3 = $r1, 32
; CV2-NEXT:    copyd $r4 = $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r4 = $r3, 63, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    andd $r0 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    andd $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    zxwd $r0 = $r0
; CV2-NEXT:    andd $r1 = $r1, $r0
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    andd $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
  %2 = tail call i32 @llvm.vector.reduce.and.v8i32(<8 x i32> %0)
  %3 = zext i32 %2 to i64
  ret i64 %3
}

define i64 @andrwpd(<2 x i32> %0) {
; CV1-LABEL: andrwpd:
; CV1:       # %bb.0:
; CV1-NEXT:    srad $r1 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 63, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: andrwpd:
; CV2:       # %bb.0:
; CV2-NEXT:    srad $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 63, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    zxwd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 3)
  %2 = tail call i32 @llvm.vector.reduce.and.v2i32(<2 x i32> %0)
  %3 = zext i32 %2 to i64
  ret i64 %3
}

define i64 @andrwqd(<4 x i32> %0) {
; CV1-LABEL: andrwqd:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r2 = $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    andd $r1 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    andd $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: andrwqd:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r2 = $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    andd $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    andd $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    zxwd $r0 = $r0
; CV2-NEXT:    andd $r1 = $r1, $r0
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    andd $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 6)
  %2 = tail call i32 @llvm.vector.reduce.and.v4i32(<4 x i32> %0)
  %3 = zext i32 %2 to i64
  ret i64 %3
}

define i64 @andrwvd(ptr %0) {
; CV1-LABEL: andrwvd:
; CV1:       # %bb.0:
; CV1-NEXT:    lo $r4r5r6r7 = 64[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r8r9r10r11 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    lo $r32r33r34r35 = 96[$r0]
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    lo $r0r1r2r3 = 32[$r0]
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd $r4 = $r8, $r4
; CV1-NEXT:    andd $r5 = $r9, $r5
; CV1-NEXT:    andd $r6 = $r10, $r6
; CV1-NEXT:    andd $r7 = $r11, $r7
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    andd $r0 = $r0, $r32
; CV1-NEXT:    andd $r1 = $r1, $r33
; CV1-NEXT:    andd $r2 = $r2, $r34
; CV1-NEXT:    andd $r3 = $r3, $r35
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    andd $r0 = $r4, $r0
; CV1-NEXT:    andd $r1 = $r5, $r1
; CV1-NEXT:    andd $r2 = $r6, $r2
; CV1-NEXT:    andd $r3 = $r7, $r3
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    andd $r0 = $r0, $r2
; CV1-NEXT:    andd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    make $r1 = 0
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    andd $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    andd $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 13)
;
; CV2-LABEL: andrwvd:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r4r5r6r7 = 64[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r8r9r10r11 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    lo $r32r33r34r35 = 96[$r0]
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    lo $r0r1r2r3 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    andd $r4 = $r8, $r4
; CV2-NEXT:    andd $r5 = $r9, $r5
; CV2-NEXT:    andd $r6 = $r10, $r6
; CV2-NEXT:    andd $r7 = $r11, $r7
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    andd $r0 = $r0, $r32
; CV2-NEXT:    andd $r1 = $r1, $r33
; CV2-NEXT:    andd $r2 = $r2, $r34
; CV2-NEXT:    andd $r3 = $r3, $r35
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    andd $r0 = $r4, $r0
; CV2-NEXT:    andd $r1 = $r5, $r1
; CV2-NEXT:    andd $r2 = $r6, $r2
; CV2-NEXT:    andd $r3 = $r7, $r3
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    andd $r0 = $r0, $r2
; CV2-NEXT:    andd $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    make $r1 = 0
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    andd $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    zxwd $r0 = $r0
; CV2-NEXT:    andd $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 13)
  %2 = load <32 x i32>, ptr %0
  %3 = tail call i32 @llvm.vector.reduce.and.v32i32(<32 x i32> %2)
  %4 = zext i32 %3 to i64
  ret i64 %4
}

define i64 @andrwxd(ptr %0) {
; CV1-LABEL: andrwxd:
; CV1:       # %bb.0:
; CV1-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r0r1r2r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andd $r0 = $r0, $r4
; CV1-NEXT:    andd $r1 = $r1, $r5
; CV1-NEXT:    andd $r2 = $r2, $r6
; CV1-NEXT:    andd $r3 = $r3, $r7
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    andd $r0 = $r0, $r2
; CV1-NEXT:    andd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    make $r1 = 0
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    andd $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    andd $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 10)
;
; CV2-LABEL: andrwxd:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r0r1r2r3 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    andd $r0 = $r0, $r4
; CV2-NEXT:    andd $r1 = $r1, $r5
; CV2-NEXT:    andd $r2 = $r2, $r6
; CV2-NEXT:    andd $r3 = $r3, $r7
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    andd $r0 = $r0, $r2
; CV2-NEXT:    andd $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    make $r1 = 0
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    andd $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    zxwd $r0 = $r0
; CV2-NEXT:    andd $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 10)
  %2 = load <16 x i32>, ptr %0
  %3 = tail call i32 @llvm.vector.reduce.and.v16i32(<16 x i32> %2)
  %4 = zext i32 %3 to i64
  ret i64 %4
}

define i64 @orrbod(<8 x i8> %0) {
; CV1-LABEL: orrbod:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    iord $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    andd $r0 = $r0, 255
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 13)
;
; CV2-LABEL: orrbod:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    iord $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    andd $r0 = $r0, 255
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 13)
  %2 = tail call i8 @llvm.vector.reduce.or.v8i8(<8 x i8> %0)
  %3 = zext i8 %2 to i64
  ret i64 %3
}

define i64 @orrbpd(<2 x i8> %0) {
; CV1-LABEL: orrbpd:
; CV1:       # %bb.0:
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    iorw $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd $r0 = $r0, 255
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 4)
;
; CV2-LABEL: orrbpd:
; CV2:       # %bb.0:
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    iorw $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    andd $r0 = $r0, 255
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 4)
  %2 = tail call i8 @llvm.vector.reduce.or.v2i8(<2 x i8> %0)
  %3 = zext i8 %2 to i64
  ret i64 %3
}

define i64 @orrbqd(<4 x i8> %0) {
; CV1-LABEL: orrbqd:
; CV1:       # %bb.0:
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iorw $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    iorw $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    andd $r0 = $r0, 255
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 8)
;
; CV2-LABEL: orrbqd:
; CV2:       # %bb.0:
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    iorw $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    iorw $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    andd $r0 = $r0, 255
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
  %2 = tail call i8 @llvm.vector.reduce.or.v4i8(<4 x i8> %0)
  %3 = zext i8 %2 to i64
  ret i64 %3
}

define i64 @orrbvd(<32 x i8> %0) {
; CV1-LABEL: orrbvd:
; CV1:       # %bb.0:
; CV1-NEXT:    iord $r0 = $r0, $r2
; CV1-NEXT:    iord $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    iord $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    andd $r0 = $r0, 255
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 15)
;
; CV2-LABEL: orrbvd:
; CV2:       # %bb.0:
; CV2-NEXT:    iord $r0 = $r0, $r2
; CV2-NEXT:    iord $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    iord $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    andd $r0 = $r0, 255
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 15)
  %2 = tail call i8 @llvm.vector.reduce.or.v32i8(<32 x i8> %0)
  %3 = zext i8 %2 to i64
  ret i64 %3
}

define i64 @orrbxd(<16 x i8> %0) {
; CV1-LABEL: orrbxd:
; CV1:       # %bb.0:
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    iord $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    andd $r0 = $r0, 255
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 14)
;
; CV2-LABEL: orrbxd:
; CV2:       # %bb.0:
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    iord $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    andd $r0 = $r0, 255
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 14)
  %2 = tail call i8 @llvm.vector.reduce.or.v16i8(<16 x i8> %0)
  %3 = zext i8 %2 to i64
  ret i64 %3
}

define i64 @orrhod(<8 x i16> %0) {
; CV1-LABEL: orrhod:
; CV1:       # %bb.0:
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    clrf $r0 = $r0, 63, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 8)
;
; CV2-LABEL: orrhod:
; CV2:       # %bb.0:
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    clrf $r0 = $r0, 63, 16
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
  %2 = tail call i16 @llvm.vector.reduce.or.v8i16(<8 x i16> %0)
  %3 = zext i16 %2 to i64
  ret i64 %3
}

define i64 @orrhpd(<2 x i16> %0) {
; CV1-LABEL: orrhpd:
; CV1:       # %bb.0:
; CV1-NEXT:    srlw $r1 = $r0, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 31, 16
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    iorw $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    clrf $r0 = $r0, 63, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 4)
;
; CV2-LABEL: orrhpd:
; CV2:       # %bb.0:
; CV2-NEXT:    srlw $r1 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 31, 16
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    iorw $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    clrf $r0 = $r0, 63, 16
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 4)
  %2 = tail call i16 @llvm.vector.reduce.or.v2i16(<2 x i16> %0)
  %3 = zext i16 %2 to i64
  ret i64 %3
}

define i64 @orrhqd(<4 x i16> %0) {
; CV1-LABEL: orrhqd:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    clrf $r0 = $r0, 63, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: orrhqd:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    clrf $r0 = $r0, 63, 16
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = tail call i16 @llvm.vector.reduce.or.v4i16(<4 x i16> %0)
  %3 = zext i16 %2 to i64
  ret i64 %3
}

define i64 @orrhvd(ptr %0) {
; CV1-LABEL: orrhvd:
; CV1:       # %bb.0:
; CV1-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r0r1r2r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    iord $r0 = $r0, $r4
; CV1-NEXT:    iord $r1 = $r1, $r5
; CV1-NEXT:    iord $r2 = $r2, $r6
; CV1-NEXT:    iord $r3 = $r3, $r7
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    iord $r0 = $r0, $r2
; CV1-NEXT:    iord $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    clrf $r0 = $r0, 63, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 14)
;
; CV2-LABEL: orrhvd:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r0r1r2r3 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    iord $r0 = $r0, $r4
; CV2-NEXT:    iord $r1 = $r1, $r5
; CV2-NEXT:    iord $r2 = $r2, $r6
; CV2-NEXT:    iord $r3 = $r3, $r7
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    iord $r0 = $r0, $r2
; CV2-NEXT:    iord $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    clrf $r0 = $r0, 63, 16
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 14)
  %2 = load <32 x i16>, ptr %0
  %3 = tail call i16 @llvm.vector.reduce.or.v32i16(<32 x i16> %2)
  %4 = zext i16 %3 to i64
  ret i64 %4
}

define i64 @orrhxd(<16 x i16> %0) {
; CV1-LABEL: orrhxd:
; CV1:       # %bb.0:
; CV1-NEXT:    iord $r0 = $r0, $r2
; CV1-NEXT:    iord $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    clrf $r0 = $r0, 63, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 9)
;
; CV2-LABEL: orrhxd:
; CV2:       # %bb.0:
; CV2-NEXT:    iord $r0 = $r0, $r2
; CV2-NEXT:    iord $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    clrf $r0 = $r0, 63, 16
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 9)
  %2 = tail call i16 @llvm.vector.reduce.or.v16i16(<16 x i16> %0)
  %3 = zext i16 %2 to i64
  ret i64 %3
}

define i64 @orrwod(<8 x i32> %0) {
; CV1-LABEL: orrwod:
; CV1:       # %bb.0:
; CV1-NEXT:    iord $r0 = $r0, $r2
; CV1-NEXT:    iord $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srld $r3 = $r1, 32
; CV1-NEXT:    copyd $r4 = $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r4 = $r3, 63, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    iord $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    iord $r1 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    iord $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 8)
;
; CV2-LABEL: orrwod:
; CV2:       # %bb.0:
; CV2-NEXT:    iord $r0 = $r0, $r2
; CV2-NEXT:    iord $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srld $r3 = $r1, 32
; CV2-NEXT:    copyd $r4 = $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r4 = $r3, 63, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    iord $r0 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    iord $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    zxwd $r0 = $r0
; CV2-NEXT:    iord $r1 = $r1, $r0
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    iord $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
  %2 = tail call i32 @llvm.vector.reduce.or.v8i32(<8 x i32> %0)
  %3 = zext i32 %2 to i64
  ret i64 %3
}

define i64 @orrwpd(<2 x i32> %0) {
; CV1-LABEL: orrwpd:
; CV1:       # %bb.0:
; CV1-NEXT:    srad $r1 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 63, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: orrwpd:
; CV2:       # %bb.0:
; CV2-NEXT:    srad $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 63, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    zxwd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 3)
  %2 = tail call i32 @llvm.vector.reduce.or.v2i32(<2 x i32> %0)
  %3 = zext i32 %2 to i64
  ret i64 %3
}

define i64 @orrwqd(<4 x i32> %0) {
; CV1-LABEL: orrwqd:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r2 = $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    iord $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    iord $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    iord $r1 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    iord $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: orrwqd:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r2 = $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    iord $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    iord $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    zxwd $r0 = $r0
; CV2-NEXT:    iord $r1 = $r1, $r0
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    iord $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 6)
  %2 = tail call i32 @llvm.vector.reduce.or.v4i32(<4 x i32> %0)
  %3 = zext i32 %2 to i64
  ret i64 %3
}

define i64 @orrwvd(ptr %0) {
; CV1-LABEL: orrwvd:
; CV1:       # %bb.0:
; CV1-NEXT:    lo $r4r5r6r7 = 64[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r8r9r10r11 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    lo $r32r33r34r35 = 96[$r0]
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    lo $r0r1r2r3 = 32[$r0]
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    iord $r4 = $r8, $r4
; CV1-NEXT:    iord $r5 = $r9, $r5
; CV1-NEXT:    iord $r6 = $r10, $r6
; CV1-NEXT:    iord $r7 = $r11, $r7
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    iord $r0 = $r0, $r32
; CV1-NEXT:    iord $r1 = $r1, $r33
; CV1-NEXT:    iord $r2 = $r2, $r34
; CV1-NEXT:    iord $r3 = $r3, $r35
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    iord $r0 = $r4, $r0
; CV1-NEXT:    iord $r1 = $r5, $r1
; CV1-NEXT:    iord $r2 = $r6, $r2
; CV1-NEXT:    iord $r3 = $r7, $r3
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    iord $r0 = $r0, $r2
; CV1-NEXT:    iord $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    iord $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    iord $r1 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    iord $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 14)
;
; CV2-LABEL: orrwvd:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r4r5r6r7 = 64[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r8r9r10r11 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    lo $r32r33r34r35 = 96[$r0]
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    lo $r0r1r2r3 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    iord $r4 = $r8, $r4
; CV2-NEXT:    iord $r5 = $r9, $r5
; CV2-NEXT:    iord $r6 = $r10, $r6
; CV2-NEXT:    iord $r7 = $r11, $r7
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    iord $r0 = $r0, $r32
; CV2-NEXT:    iord $r1 = $r1, $r33
; CV2-NEXT:    iord $r2 = $r2, $r34
; CV2-NEXT:    iord $r3 = $r3, $r35
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    iord $r0 = $r4, $r0
; CV2-NEXT:    iord $r1 = $r5, $r1
; CV2-NEXT:    iord $r2 = $r6, $r2
; CV2-NEXT:    iord $r3 = $r7, $r3
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    iord $r0 = $r0, $r2
; CV2-NEXT:    iord $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    iord $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    zxwd $r0 = $r0
; CV2-NEXT:    iord $r1 = $r1, $r0
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    iord $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 14)
  %2 = load <32 x i32>, ptr %0
  %3 = tail call i32 @llvm.vector.reduce.or.v32i32(<32 x i32> %2)
  %4 = zext i32 %3 to i64
  ret i64 %4
}

define i64 @orrwxd(ptr %0) {
; CV1-LABEL: orrwxd:
; CV1:       # %bb.0:
; CV1-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r0r1r2r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    iord $r0 = $r0, $r4
; CV1-NEXT:    iord $r1 = $r1, $r5
; CV1-NEXT:    iord $r2 = $r2, $r6
; CV1-NEXT:    iord $r3 = $r3, $r7
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    iord $r0 = $r0, $r2
; CV1-NEXT:    iord $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    iord $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    iord $r1 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    iord $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 11)
;
; CV2-LABEL: orrwxd:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r0r1r2r3 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    iord $r0 = $r0, $r4
; CV2-NEXT:    iord $r1 = $r1, $r5
; CV2-NEXT:    iord $r2 = $r2, $r6
; CV2-NEXT:    iord $r3 = $r3, $r7
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    iord $r0 = $r0, $r2
; CV2-NEXT:    iord $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    iord $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    zxwd $r0 = $r0
; CV2-NEXT:    iord $r1 = $r1, $r0
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    iord $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 11)
  %2 = load <16 x i32>, ptr %0
  %3 = tail call i32 @llvm.vector.reduce.or.v16i32(<16 x i32> %2)
  %4 = zext i32 %3 to i64
  ret i64 %4
}

define i64 @xorrbod(<8 x i8> %0) {
; CV1-LABEL: xorrbod:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    andd $r0 = $r0, 255
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 13)
;
; CV2-LABEL: xorrbod:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    eord $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    andd $r0 = $r0, 255
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 13)
  %2 = tail call i8 @llvm.vector.reduce.xor.v8i8(<8 x i8> %0)
  %3 = zext i8 %2 to i64
  ret i64 %3
}

define i64 @xorrbpd(<2 x i8> %0) {
; CV1-LABEL: xorrbpd:
; CV1:       # %bb.0:
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    eorw $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd $r0 = $r0, 255
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 4)
;
; CV2-LABEL: xorrbpd:
; CV2:       # %bb.0:
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    eorw $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    andd $r0 = $r0, 255
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 4)
  %2 = tail call i8 @llvm.vector.reduce.xor.v2i8(<2 x i8> %0)
  %3 = zext i8 %2 to i64
  ret i64 %3
}

define i64 @xorrbqd(<4 x i8> %0) {
; CV1-LABEL: xorrbqd:
; CV1:       # %bb.0:
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    eorw $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    eorw $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    andd $r0 = $r0, 255
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 8)
;
; CV2-LABEL: xorrbqd:
; CV2:       # %bb.0:
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    eorw $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    eorw $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    andd $r0 = $r0, 255
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
  %2 = tail call i8 @llvm.vector.reduce.xor.v4i8(<4 x i8> %0)
  %3 = zext i8 %2 to i64
  ret i64 %3
}

define i64 @xorrbvd(<32 x i8> %0) {
; CV1-LABEL: xorrbvd:
; CV1:       # %bb.0:
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    eord $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    andd $r0 = $r0, 255
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 15)
;
; CV2-LABEL: xorrbvd:
; CV2:       # %bb.0:
; CV2-NEXT:    eord $r0 = $r0, $r2
; CV2-NEXT:    eord $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    eord $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    andd $r0 = $r0, 255
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 15)
  %2 = tail call i8 @llvm.vector.reduce.xor.v32i8(<32 x i8> %0)
  %3 = zext i8 %2 to i64
  ret i64 %3
}

define i64 @xorrbxd(<16 x i8> %0) {
; CV1-LABEL: xorrbxd:
; CV1:       # %bb.0:
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    andd $r0 = $r0, 255
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 14)
;
; CV2-LABEL: xorrbxd:
; CV2:       # %bb.0:
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    eord $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    andd $r0 = $r0, 255
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 14)
  %2 = tail call i8 @llvm.vector.reduce.xor.v16i8(<16 x i8> %0)
  %3 = zext i8 %2 to i64
  ret i64 %3
}

define i64 @xorrhod(<8 x i16> %0) {
; CV1-LABEL: xorrhod:
; CV1:       # %bb.0:
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    clrf $r0 = $r0, 63, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 8)
;
; CV2-LABEL: xorrhod:
; CV2:       # %bb.0:
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    clrf $r0 = $r0, 63, 16
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
  %2 = tail call i16 @llvm.vector.reduce.xor.v8i16(<8 x i16> %0)
  %3 = zext i16 %2 to i64
  ret i64 %3
}

define i64 @xorrhpd(<2 x i16> %0) {
; CV1-LABEL: xorrhpd:
; CV1:       # %bb.0:
; CV1-NEXT:    srlw $r1 = $r0, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 31, 16
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    eorw $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    clrf $r0 = $r0, 63, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 4)
;
; CV2-LABEL: xorrhpd:
; CV2:       # %bb.0:
; CV2-NEXT:    srlw $r1 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 31, 16
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    eorw $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    clrf $r0 = $r0, 63, 16
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 4)
  %2 = tail call i16 @llvm.vector.reduce.xor.v2i16(<2 x i16> %0)
  %3 = zext i16 %2 to i64
  ret i64 %3
}

define i64 @xorrhqd(<4 x i16> %0) {
; CV1-LABEL: xorrhqd:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    clrf $r0 = $r0, 63, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: xorrhqd:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    clrf $r0 = $r0, 63, 16
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = tail call i16 @llvm.vector.reduce.xor.v4i16(<4 x i16> %0)
  %3 = zext i16 %2 to i64
  ret i64 %3
}

define i64 @xorrhvd(ptr %0) {
; CV1-LABEL: xorrhvd:
; CV1:       # %bb.0:
; CV1-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r0r1r2r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    eord $r0 = $r0, $r4
; CV1-NEXT:    eord $r1 = $r1, $r5
; CV1-NEXT:    eord $r2 = $r2, $r6
; CV1-NEXT:    eord $r3 = $r3, $r7
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    eord $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    clrf $r0 = $r0, 63, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 14)
;
; CV2-LABEL: xorrhvd:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r0r1r2r3 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    eord $r0 = $r0, $r4
; CV2-NEXT:    eord $r1 = $r1, $r5
; CV2-NEXT:    eord $r2 = $r2, $r6
; CV2-NEXT:    eord $r3 = $r3, $r7
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    eord $r0 = $r0, $r2
; CV2-NEXT:    eord $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    clrf $r0 = $r0, 63, 16
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 14)
  %2 = load <32 x i16>, ptr %0
  %3 = tail call i16 @llvm.vector.reduce.xor.v32i16(<32 x i16> %2)
  %4 = zext i16 %3 to i64
  ret i64 %4
}

define i64 @xorrhxd(<16 x i16> %0) {
; CV1-LABEL: xorrhxd:
; CV1:       # %bb.0:
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    eord $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    clrf $r0 = $r0, 63, 16
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 9)
;
; CV2-LABEL: xorrhxd:
; CV2:       # %bb.0:
; CV2-NEXT:    eord $r0 = $r0, $r2
; CV2-NEXT:    eord $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    clrf $r0 = $r0, 63, 16
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 9)
  %2 = tail call i16 @llvm.vector.reduce.xor.v16i16(<16 x i16> %0)
  %3 = zext i16 %2 to i64
  ret i64 %3
}

define i64 @xorrwod(<8 x i32> %0) {
; CV1-LABEL: xorrwod:
; CV1:       # %bb.0:
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    eord $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srld $r3 = $r1, 32
; CV1-NEXT:    copyd $r4 = $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r4 = $r3, 63, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    eord $r0 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    eord $r1 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    eord $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 8)
;
; CV2-LABEL: xorrwod:
; CV2:       # %bb.0:
; CV2-NEXT:    eord $r0 = $r0, $r2
; CV2-NEXT:    eord $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srld $r3 = $r1, 32
; CV2-NEXT:    copyd $r4 = $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r4 = $r3, 63, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    eord $r0 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    eord $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    zxwd $r0 = $r0
; CV2-NEXT:    eord $r1 = $r1, $r0
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    eord $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
  %2 = tail call i32 @llvm.vector.reduce.xor.v8i32(<8 x i32> %0)
  %3 = zext i32 %2 to i64
  ret i64 %3
}

define i64 @xorrwpd(<2 x i32> %0) {
; CV1-LABEL: xorrwpd:
; CV1:       # %bb.0:
; CV1-NEXT:    srad $r1 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 63, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: xorrwpd:
; CV2:       # %bb.0:
; CV2-NEXT:    srad $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 63, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    zxwd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 3)
  %2 = tail call i32 @llvm.vector.reduce.xor.v2i32(<2 x i32> %0)
  %3 = zext i32 %2 to i64
  ret i64 %3
}

define i64 @xorrwqd(<4 x i32> %0) {
; CV1-LABEL: xorrwqd:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r2 = $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    eord $r1 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    eord $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: xorrwqd:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r2 = $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    eord $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    eord $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    zxwd $r0 = $r0
; CV2-NEXT:    eord $r1 = $r1, $r0
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    eord $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 6)
  %2 = tail call i32 @llvm.vector.reduce.xor.v4i32(<4 x i32> %0)
  %3 = zext i32 %2 to i64
  ret i64 %3
}

define i64 @xorrwvd(ptr %0) {
; CV1-LABEL: xorrwvd:
; CV1:       # %bb.0:
; CV1-NEXT:    lo $r4r5r6r7 = 64[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r8r9r10r11 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    lo $r32r33r34r35 = 96[$r0]
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    lo $r0r1r2r3 = 32[$r0]
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    eord $r4 = $r8, $r4
; CV1-NEXT:    eord $r5 = $r9, $r5
; CV1-NEXT:    eord $r6 = $r10, $r6
; CV1-NEXT:    eord $r7 = $r11, $r7
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    eord $r0 = $r0, $r32
; CV1-NEXT:    eord $r1 = $r1, $r33
; CV1-NEXT:    eord $r2 = $r2, $r34
; CV1-NEXT:    eord $r3 = $r3, $r35
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    eord $r0 = $r4, $r0
; CV1-NEXT:    eord $r1 = $r5, $r1
; CV1-NEXT:    eord $r2 = $r6, $r2
; CV1-NEXT:    eord $r3 = $r7, $r3
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    eord $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    copyd $r1 = $r2
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    eord $r1 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 14)
;
; CV2-LABEL: xorrwvd:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r4r5r6r7 = 64[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r8r9r10r11 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    lo $r32r33r34r35 = 96[$r0]
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    lo $r0r1r2r3 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    eord $r4 = $r8, $r4
; CV2-NEXT:    eord $r5 = $r9, $r5
; CV2-NEXT:    eord $r6 = $r10, $r6
; CV2-NEXT:    eord $r7 = $r11, $r7
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    eord $r0 = $r0, $r32
; CV2-NEXT:    eord $r1 = $r1, $r33
; CV2-NEXT:    eord $r2 = $r2, $r34
; CV2-NEXT:    eord $r3 = $r3, $r35
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    eord $r0 = $r4, $r0
; CV2-NEXT:    eord $r1 = $r5, $r1
; CV2-NEXT:    eord $r2 = $r6, $r2
; CV2-NEXT:    eord $r3 = $r7, $r3
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    eord $r0 = $r0, $r2
; CV2-NEXT:    eord $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    eord $r0 = $r0, $r2
; CV2-NEXT:    copyd $r1 = $r2
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    zxwd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    eord $r1 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 14)
  %2 = load <32 x i32>, ptr %0
  %3 = tail call i32 @llvm.vector.reduce.xor.v32i32(<32 x i32> %2)
  %4 = zext i32 %3 to i64
  ret i64 %4
}

define i64 @xorrwxd(ptr %0) {
; CV1-LABEL: xorrwxd:
; CV1:       # %bb.0:
; CV1-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    lo $r0r1r2r3 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    eord $r0 = $r0, $r4
; CV1-NEXT:    eord $r1 = $r1, $r5
; CV1-NEXT:    eord $r2 = $r2, $r6
; CV1-NEXT:    eord $r3 = $r3, $r7
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    eord $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    copyd $r1 = $r2
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    zxwd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    eord $r1 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 11)
;
; CV2-LABEL: xorrwxd:
; CV2:       # %bb.0:
; CV2-NEXT:    lo $r4r5r6r7 = 32[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    lo $r0r1r2r3 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    eord $r0 = $r0, $r4
; CV2-NEXT:    eord $r1 = $r1, $r5
; CV2-NEXT:    eord $r2 = $r2, $r6
; CV2-NEXT:    eord $r3 = $r3, $r7
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    eord $r0 = $r0, $r2
; CV2-NEXT:    eord $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    eord $r0 = $r0, $r2
; CV2-NEXT:    copyd $r1 = $r2
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    zxwd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    eord $r1 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 11)
  %2 = load <16 x i32>, ptr %0
  %3 = tail call i32 @llvm.vector.reduce.xor.v16i32(<16 x i32> %2)
  %4 = zext i32 %3 to i64
  ret i64 %4
}

declare i8 @llvm.vector.reduce.smax.v8i8(<8 x i8>)

declare i8 @llvm.vector.reduce.smax.v2i8(<2 x i8>)

declare i8 @llvm.vector.reduce.smax.v4i8(<4 x i8>)

declare i8 @llvm.vector.reduce.smax.v32i8(<32 x i8>)

declare i8 @llvm.vector.reduce.smax.v16i8(<16 x i8>)

declare i16 @llvm.vector.reduce.smax.v8i16(<8 x i16>)

declare i16 @llvm.vector.reduce.smax.v2i16(<2 x i16>)

declare i16 @llvm.vector.reduce.smax.v4i16(<4 x i16>)

declare i16 @llvm.vector.reduce.smax.v32i16(<32 x i16>)

declare i16 @llvm.vector.reduce.smax.v16i16(<16 x i16>)

declare i32 @llvm.vector.reduce.smax.v8i32(<8 x i32>)

declare i32 @llvm.vector.reduce.smax.v2i32(<2 x i32>)

declare i32 @llvm.vector.reduce.smax.v4i32(<4 x i32>)

declare i32 @llvm.vector.reduce.smax.v32i32(<32 x i32>)

declare i32 @llvm.vector.reduce.smax.v16i32(<16 x i32>)

declare i8 @llvm.vector.reduce.umax.v8i8(<8 x i8>)

declare i8 @llvm.vector.reduce.umax.v2i8(<2 x i8>)

declare i8 @llvm.vector.reduce.umax.v4i8(<4 x i8>)

declare i8 @llvm.vector.reduce.umax.v32i8(<32 x i8>)

declare i8 @llvm.vector.reduce.umax.v16i8(<16 x i8>)

declare i16 @llvm.vector.reduce.umax.v8i16(<8 x i16>)

declare i16 @llvm.vector.reduce.umax.v2i16(<2 x i16>)

declare i16 @llvm.vector.reduce.umax.v4i16(<4 x i16>)

declare i16 @llvm.vector.reduce.umax.v32i16(<32 x i16>)

declare i16 @llvm.vector.reduce.umax.v16i16(<16 x i16>)

declare i32 @llvm.vector.reduce.umax.v8i32(<8 x i32>)

declare i32 @llvm.vector.reduce.umax.v2i32(<2 x i32>)

declare i32 @llvm.vector.reduce.umax.v4i32(<4 x i32>)

declare i32 @llvm.vector.reduce.umax.v32i32(<32 x i32>)

declare i32 @llvm.vector.reduce.umax.v16i32(<16 x i32>)

declare i8 @llvm.vector.reduce.smin.v8i8(<8 x i8>)

declare i8 @llvm.vector.reduce.smin.v2i8(<2 x i8>)

declare i8 @llvm.vector.reduce.smin.v4i8(<4 x i8>)

declare i8 @llvm.vector.reduce.smin.v32i8(<32 x i8>)

declare i8 @llvm.vector.reduce.smin.v16i8(<16 x i8>)

declare i16 @llvm.vector.reduce.smin.v8i16(<8 x i16>)

declare i16 @llvm.vector.reduce.smin.v2i16(<2 x i16>)

declare i16 @llvm.vector.reduce.smin.v4i16(<4 x i16>)

declare i16 @llvm.vector.reduce.smin.v32i16(<32 x i16>)

declare i16 @llvm.vector.reduce.smin.v16i16(<16 x i16>)

declare i32 @llvm.vector.reduce.smin.v8i32(<8 x i32>)

declare i32 @llvm.vector.reduce.smin.v2i32(<2 x i32>)

declare i32 @llvm.vector.reduce.smin.v4i32(<4 x i32>)

declare i32 @llvm.vector.reduce.smin.v32i32(<32 x i32>)

declare i32 @llvm.vector.reduce.smin.v16i32(<16 x i32>)

declare i8 @llvm.vector.reduce.umin.v8i8(<8 x i8>)

declare i8 @llvm.vector.reduce.umin.v2i8(<2 x i8>)

declare i8 @llvm.vector.reduce.umin.v4i8(<4 x i8>)

declare i8 @llvm.vector.reduce.umin.v32i8(<32 x i8>)

declare i8 @llvm.vector.reduce.umin.v16i8(<16 x i8>)

declare i16 @llvm.vector.reduce.umin.v8i16(<8 x i16>)

declare i16 @llvm.vector.reduce.umin.v2i16(<2 x i16>)

declare i16 @llvm.vector.reduce.umin.v4i16(<4 x i16>)

declare i16 @llvm.vector.reduce.umin.v32i16(<32 x i16>)

declare i16 @llvm.vector.reduce.umin.v16i16(<16 x i16>)

declare i32 @llvm.vector.reduce.umin.v8i32(<8 x i32>)

declare i32 @llvm.vector.reduce.umin.v2i32(<2 x i32>)

declare i32 @llvm.vector.reduce.umin.v4i32(<4 x i32>)

declare i32 @llvm.vector.reduce.umin.v32i32(<32 x i32>)

declare i32 @llvm.vector.reduce.umin.v16i32(<16 x i32>)

declare i8 @llvm.vector.reduce.and.v8i8(<8 x i8>)

declare i8 @llvm.vector.reduce.and.v2i8(<2 x i8>)

declare i8 @llvm.vector.reduce.and.v4i8(<4 x i8>)

declare i8 @llvm.vector.reduce.and.v32i8(<32 x i8>)

declare i8 @llvm.vector.reduce.and.v16i8(<16 x i8>)

declare i16 @llvm.vector.reduce.and.v8i16(<8 x i16>)

declare i16 @llvm.vector.reduce.and.v2i16(<2 x i16>)

declare i16 @llvm.vector.reduce.and.v4i16(<4 x i16>)

declare i16 @llvm.vector.reduce.and.v32i16(<32 x i16>)

declare i16 @llvm.vector.reduce.and.v16i16(<16 x i16>)

declare i32 @llvm.vector.reduce.and.v8i32(<8 x i32>)

declare i32 @llvm.vector.reduce.and.v2i32(<2 x i32>)

declare i32 @llvm.vector.reduce.and.v4i32(<4 x i32>)

declare i32 @llvm.vector.reduce.and.v32i32(<32 x i32>)

declare i32 @llvm.vector.reduce.and.v16i32(<16 x i32>)

declare i8 @llvm.vector.reduce.or.v8i8(<8 x i8>)

declare i8 @llvm.vector.reduce.or.v2i8(<2 x i8>)

declare i8 @llvm.vector.reduce.or.v4i8(<4 x i8>)

declare i8 @llvm.vector.reduce.or.v32i8(<32 x i8>)

declare i8 @llvm.vector.reduce.or.v16i8(<16 x i8>)

declare i16 @llvm.vector.reduce.or.v8i16(<8 x i16>)

declare i16 @llvm.vector.reduce.or.v2i16(<2 x i16>)

declare i16 @llvm.vector.reduce.or.v4i16(<4 x i16>)

declare i16 @llvm.vector.reduce.or.v32i16(<32 x i16>)

declare i16 @llvm.vector.reduce.or.v16i16(<16 x i16>)

declare i32 @llvm.vector.reduce.or.v8i32(<8 x i32>)

declare i32 @llvm.vector.reduce.or.v2i32(<2 x i32>)

declare i32 @llvm.vector.reduce.or.v4i32(<4 x i32>)

declare i32 @llvm.vector.reduce.or.v32i32(<32 x i32>)

declare i32 @llvm.vector.reduce.or.v16i32(<16 x i32>)

declare i8 @llvm.vector.reduce.xor.v8i8(<8 x i8>)

declare i8 @llvm.vector.reduce.xor.v2i8(<2 x i8>)

declare i8 @llvm.vector.reduce.xor.v4i8(<4 x i8>)

declare i8 @llvm.vector.reduce.xor.v32i8(<32 x i8>)

declare i8 @llvm.vector.reduce.xor.v16i8(<16 x i8>)

declare i16 @llvm.vector.reduce.xor.v8i16(<8 x i16>)

declare i16 @llvm.vector.reduce.xor.v2i16(<2 x i16>)

declare i16 @llvm.vector.reduce.xor.v4i16(<4 x i16>)

declare i16 @llvm.vector.reduce.xor.v32i16(<32 x i16>)

declare i16 @llvm.vector.reduce.xor.v16i16(<16 x i16>)

declare i32 @llvm.vector.reduce.xor.v8i32(<8 x i32>)

declare i32 @llvm.vector.reduce.xor.v2i32(<2 x i32>)

declare i32 @llvm.vector.reduce.xor.v4i32(<4 x i32>)

declare i32 @llvm.vector.reduce.xor.v32i32(<32 x i32>)

declare i32 @llvm.vector.reduce.xor.v16i32(<16 x i32>)

