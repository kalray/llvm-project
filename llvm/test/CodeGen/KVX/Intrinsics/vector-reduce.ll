; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mcpu=kv3-1 -O2 -o - %s | FileCheck %s --check-prefix=CV1
; RUN: llc -mcpu=kv3-2 -O2 -o - %s | FileCheck %s --check-prefix=CV2
; RUN: clang -O2 -march=kv3-1 -c -o /dev/null %s
; RUN: clang -O2 -march=kv3-2 -c -o /dev/null %s

target triple = "kvx-kalray-cos"

define i64 @longSextAdd(<8 x i8> %0) {
; CV1-LABEL: longSextAdd:
; CV1:       # %bb.0:
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    srlw $r3 = $r0, 24
; CV1-NEXT:    zxbd $r6 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    extfz $r0 = $r0, 23, 16
; CV1-NEXT:    extfz $r4 = $r2, 15, 8
; CV1-NEXT:    srlw $r5 = $r2, 24
; CV1-NEXT:    zxbd $r7 = $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    extfz $r2 = $r2, 23, 16
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sxbd $r7 = $r7
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    addwd $r3 = $r5, $r3
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    addwd $r0 = $r2, $r0
; CV1-NEXT:    addwd $r2 = $r7, $r6
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    addd $r0 = $r2, $r0
; CV1-NEXT:    addwd $r1 = $r4, $r1
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    addd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    addd $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 10)
;
; CV2-LABEL: longSextAdd:
; CV2:       # %bb.0:
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    srlw $r3 = $r0, 24
; CV2-NEXT:    zxbd $r6 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    extfz $r0 = $r0, 23, 16
; CV2-NEXT:    extfz $r4 = $r2, 15, 8
; CV2-NEXT:    srlw $r5 = $r2, 24
; CV2-NEXT:    zxbd $r7 = $r2
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sxbd $r1 = $r1
; CV2-NEXT:    extfz $r2 = $r2, 23, 16
; CV2-NEXT:    sxbd $r3 = $r3
; CV2-NEXT:    sxbd $r4 = $r4
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    sxbd $r5 = $r5
; CV2-NEXT:    sxbd $r6 = $r6
; CV2-NEXT:    sxbd $r7 = $r7
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    addwd $r1 = $r4, $r1
; CV2-NEXT:    sxbd $r2 = $r2
; CV2-NEXT:    addwd $r3 = $r5, $r3
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    addwd $r0 = $r2, $r0
; CV2-NEXT:    addd $r1 = $r1, $r3
; CV2-NEXT:    addwd $r2 = $r7, $r6
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    addd $r0 = $r2, $r0
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    addd $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = sext <8 x i8> %0 to <8 x i64>
  %3 = tail call i64 @llvm.vector.reduce.add.v8i64(<8 x i64> %2)
  ret i64 %3
}

define i64 @longAddSext(<8 x i8> %0) {
; CV1-LABEL: longAddSext:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    andd.@ $r3 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    xord $r0 = $r0, $r1
; CV1-NEXT:    andd.@ $r2 = $r1, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    insf $r3 = $r0, 15, 8
; CV1-NEXT:    andd.@ $r5 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    copyd $r1 = $r3
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r4 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r2 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    xord $r0 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    extfz $r2 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    insf $r2 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    andd.@ $r1 = $r0, 0x7f7f7f7f
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r3 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 21)
;
; CV2-LABEL: longAddSext:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    insf $r3 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    copyd $r1 = $r3
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r3, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r2 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r2 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    insf $r2 = $r3, 31, 16
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 15)
  %2 = tail call i8 @llvm.vector.reduce.add.v8i8(<8 x i8> %0)
  %3 = sext i8 %2 to i64
  ret i64 %3
}

define i64 @longReduceSext(<8 x i8> %0) {
; CV1-LABEL: longReduceSext:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    andd.@ $r3 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    xord $r0 = $r0, $r1
; CV1-NEXT:    andd.@ $r2 = $r1, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    insf $r3 = $r0, 15, 8
; CV1-NEXT:    andd.@ $r5 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    copyd $r1 = $r3
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r4 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r2 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    xord $r0 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    extfz $r2 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    insf $r2 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    andd.@ $r1 = $r0, 0x7f7f7f7f
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r3 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 21)
;
; CV2-LABEL: longReduceSext:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    insf $r3 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    copyd $r1 = $r3
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r3, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r2 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r2 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    insf $r2 = $r3, 31, 16
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 15)
  %2 = tail call i8 @llvm.vector.reduce.add.v8i8(<8 x i8> %0)
  %3 = sext i8 %2 to i64
  ret i64 %3
}

declare i8 @llvm.vector.reduce.add.v8i8(<8 x i8>)

define i64 @longSextReduce(<8 x i8> %0) {
; CV1-LABEL: longSextReduce:
; CV1:       # %bb.0:
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    srlw $r3 = $r0, 24
; CV1-NEXT:    zxbd $r6 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    extfz $r0 = $r0, 23, 16
; CV1-NEXT:    extfz $r4 = $r2, 15, 8
; CV1-NEXT:    srlw $r5 = $r2, 24
; CV1-NEXT:    zxbd $r7 = $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    extfz $r2 = $r2, 23, 16
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sxbd $r7 = $r7
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    addwd $r3 = $r5, $r3
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    addwd $r0 = $r2, $r0
; CV1-NEXT:    addwd $r2 = $r7, $r6
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    addd $r0 = $r2, $r0
; CV1-NEXT:    addwd $r1 = $r4, $r1
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    addd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    addd $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 10)
;
; CV2-LABEL: longSextReduce:
; CV2:       # %bb.0:
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    srlw $r3 = $r0, 24
; CV2-NEXT:    zxbd $r6 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    extfz $r0 = $r0, 23, 16
; CV2-NEXT:    extfz $r4 = $r2, 15, 8
; CV2-NEXT:    srlw $r5 = $r2, 24
; CV2-NEXT:    zxbd $r7 = $r2
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sxbd $r1 = $r1
; CV2-NEXT:    extfz $r2 = $r2, 23, 16
; CV2-NEXT:    sxbd $r3 = $r3
; CV2-NEXT:    sxbd $r4 = $r4
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    sxbd $r5 = $r5
; CV2-NEXT:    sxbd $r6 = $r6
; CV2-NEXT:    sxbd $r7 = $r7
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    addwd $r1 = $r4, $r1
; CV2-NEXT:    sxbd $r2 = $r2
; CV2-NEXT:    addwd $r3 = $r5, $r3
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    addwd $r0 = $r2, $r0
; CV2-NEXT:    addd $r1 = $r1, $r3
; CV2-NEXT:    addwd $r2 = $r7, $r6
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    addd $r0 = $r2, $r0
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    addd $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = sext <8 x i8> %0 to <8 x i64>
  %3 = tail call i64 @llvm.vector.reduce.add.v8i64(<8 x i64> %2)
  ret i64 %3
}

declare i64 @llvm.vector.reduce.add.v8i64(<8 x i64>)

define i64 @longLoopSextReduceVector(<8 x i8> %0) {
; CV1-LABEL: longLoopSextReduceVector:
; CV1:       # %bb.0:
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    srlw $r3 = $r0, 24
; CV1-NEXT:    zxbd $r6 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    extfz $r0 = $r0, 23, 16
; CV1-NEXT:    extfz $r4 = $r2, 15, 8
; CV1-NEXT:    srlw $r5 = $r2, 24
; CV1-NEXT:    zxbd $r7 = $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    extfz $r2 = $r2, 23, 16
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sxbd $r7 = $r7
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    addwd $r3 = $r5, $r3
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    addwd $r0 = $r2, $r0
; CV1-NEXT:    addwd $r2 = $r7, $r6
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    addd $r0 = $r2, $r0
; CV1-NEXT:    addwd $r1 = $r4, $r1
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    addd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    addd $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 10)
;
; CV2-LABEL: longLoopSextReduceVector:
; CV2:       # %bb.0:
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    srlw $r3 = $r0, 24
; CV2-NEXT:    zxbd $r6 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    extfz $r0 = $r0, 23, 16
; CV2-NEXT:    extfz $r4 = $r2, 15, 8
; CV2-NEXT:    srlw $r5 = $r2, 24
; CV2-NEXT:    zxbd $r7 = $r2
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sxbd $r1 = $r1
; CV2-NEXT:    extfz $r2 = $r2, 23, 16
; CV2-NEXT:    sxbd $r3 = $r3
; CV2-NEXT:    sxbd $r4 = $r4
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    sxbd $r5 = $r5
; CV2-NEXT:    sxbd $r6 = $r6
; CV2-NEXT:    sxbd $r7 = $r7
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    addwd $r1 = $r4, $r1
; CV2-NEXT:    sxbd $r2 = $r2
; CV2-NEXT:    addwd $r3 = $r5, $r3
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    addwd $r0 = $r2, $r0
; CV2-NEXT:    addd $r1 = $r1, $r3
; CV2-NEXT:    addwd $r2 = $r7, $r6
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    addd $r0 = $r2, $r0
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    addd $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = sext <8 x i8> %0 to <8 x i64>
  %3 = tail call i64 @llvm.vector.reduce.add.v8i64(<8 x i64> %2)
  ret i64 %3
}

define i64 @longLoopReduceSextVector(<8 x i8> %0) {
; CV1-LABEL: longLoopReduceSextVector:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    andd.@ $r3 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    xord $r0 = $r0, $r1
; CV1-NEXT:    andd.@ $r2 = $r1, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    insf $r3 = $r0, 15, 8
; CV1-NEXT:    andd.@ $r5 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    copyd $r1 = $r3
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r4 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r2 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    xord $r0 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    extfz $r2 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    insf $r2 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    andd.@ $r1 = $r0, 0x7f7f7f7f
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r3 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 21)
;
; CV2-LABEL: longLoopReduceSextVector:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    insf $r3 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    copyd $r1 = $r3
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r3, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r2 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r2 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    insf $r2 = $r3, 31, 16
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 15)
  %2 = tail call i8 @llvm.vector.reduce.add.v8i8(<8 x i8> %0)
  %3 = sext i8 %2 to i64
  ret i64 %3
}

define i64 @longLoopSextReduceArray(ptr %0) {
; CV1-LABEL: longLoopSextReduceArray:
; CV1:       # %bb.0:
; CV1-NEXT:    ld $r0 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    extfz $r3 = $r0, 23, 16
; CV1-NEXT:    srlw $r4 = $r0, 24
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    extfz $r5 = $r2, 15, 8
; CV1-NEXT:    srlw $r6 = $r2, 24
; CV1-NEXT:    extfz $r7 = $r2, 23, 16
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sxbd $r1 = $r1
; CV1-NEXT:    zxbd $r2 = $r2
; CV1-NEXT:    sxbd $r4 = $r4
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    sxbd $r3 = $r3
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    sxbd $r5 = $r5
; CV1-NEXT:    sxbd $r6 = $r6
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    sxbd $r2 = $r2
; CV1-NEXT:    sxbd $r7 = $r7
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    addwd $r0 = $r2, $r0
; CV1-NEXT:    addwd $r3 = $r7, $r3
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    addd $r0 = $r0, $r3
; CV1-NEXT:    addwd $r1 = $r5, $r1
; CV1-NEXT:    addwd $r2 = $r6, $r4
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    addd $r1 = $r1, $r2
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    addd $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 12)
;
; CV2-LABEL: longLoopSextReduceArray:
; CV2:       # %bb.0:
; CV2-NEXT:    ld $r0 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    srlw $r3 = $r0, 24
; CV2-NEXT:    zxbd $r4 = $r0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    extfz $r0 = $r0, 23, 16
; CV2-NEXT:    extfz $r5 = $r2, 15, 8
; CV2-NEXT:    srlw $r6 = $r2, 24
; CV2-NEXT:    zxbd $r7 = $r2
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sxbd $r1 = $r1
; CV2-NEXT:    extfz $r2 = $r2, 23, 16
; CV2-NEXT:    sxbd $r3 = $r3
; CV2-NEXT:    sxbd $r5 = $r5
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    sxbd $r4 = $r4
; CV2-NEXT:    sxbd $r6 = $r6
; CV2-NEXT:    sxbd $r7 = $r7
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    addwd $r1 = $r5, $r1
; CV2-NEXT:    sxbd $r2 = $r2
; CV2-NEXT:    addwd $r3 = $r6, $r3
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addwd $r0 = $r2, $r0
; CV2-NEXT:    addd $r1 = $r1, $r3
; CV2-NEXT:    addwd $r2 = $r7, $r4
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    addd $r0 = $r2, $r0
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    addd $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 10)
  %2 = load <8 x i8>, ptr %0
  %3 = sext <8 x i8> %2 to <8 x i64>
  %4 = tail call i64 @llvm.vector.reduce.add.v8i64(<8 x i64> %3)
  ret i64 %4
}

define i64 @longLoopReduceSextArray(ptr %0) {
; CV1-LABEL: longLoopReduceSextArray:
; CV1:       # %bb.0:
; CV1-NEXT:    ld $r0 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd.@ $r1 = $r0, 0x7f7f7f7f
; CV1-NEXT:    copyd $r2 = $r0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    insf $r2 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r2 = $r2, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r3 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    insf $r3 = $r0, 15, 8
; CV1-NEXT:    andd.@ $r5 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    copyd $r1 = $r3
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r4 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r2 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    xord $r0 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    extfz $r2 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    insf $r2 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    andd.@ $r1 = $r0, 0x7f7f7f7f
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r3 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 24)
;
; CV2-LABEL: longLoopReduceSextArray:
; CV2:       # %bb.0:
; CV2-NEXT:    ld $r0 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    insf $r3 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    copyd $r1 = $r3
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r3, 31, 16
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    extfz $r2 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    insf $r2 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    insf $r2 = $r3, 31, 16
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 15)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 16)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 17)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 18)
  %2 = load <8 x i8>, ptr %0
  %3 = tail call i8 @llvm.vector.reduce.add.v8i8(<8 x i8> %2)
  %4 = sext i8 %3 to i64
  ret i64 %4
}

define i32 @intSextAdd(<8 x i8> %0) {
; CV1-LABEL: intSextAdd:
; CV1:       # %bb.0:
; CV1-NEXT:    sxmbhq $r0 = $r0
; CV1-NEXT:    sxlbhq $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxmhwp $r3 = $r1
; CV1-NEXT:    sxmhwp $r5 = $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sxlhwp $r0 = $r0
; CV1-NEXT:    sxlhwp $r2 = $r1
; CV1-NEXT:    addwp $r3 = $r3, $r5
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    addwp $r0 = $r2, $r0
; CV1-NEXT:    srld $r1 = $r3, 32
; CV1-NEXT:    copyd $r4 = $r3
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    insf $r4 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    addwp $r0 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    addwp $r1 = $r3, $r0
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    addwp $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    addwp $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 9)
;
; CV2-LABEL: intSextAdd:
; CV2:       # %bb.0:
; CV2-NEXT:    sxmbhq $r0 = $r0
; CV2-NEXT:    sxlbhq $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sxlhwp $r0 = $r0
; CV2-NEXT:    sxlhwp $r2 = $r1
; CV2-NEXT:    sxmhwp $r3 = $r1
; CV2-NEXT:    sxmhwp $r5 = $r0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    addwp $r0 = $r2, $r0
; CV2-NEXT:    addwp $r3 = $r3, $r5
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    srld $r1 = $r3, 32
; CV2-NEXT:    copyd $r4 = $r3
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    insf $r4 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    addwp $r0 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    addwp $r1 = $r3, $r0
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addwp $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    addwp $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 9)
  %2 = sext <8 x i8> %0 to <8 x i32>
  %3 = tail call i32 @llvm.vector.reduce.add.v8i32(<8 x i32> %2)
  ret i32 %3
}

define i32 @intAddSext(<8 x i8> %0) {
; CV1-LABEL: intAddSext:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    andd.@ $r3 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    xord $r0 = $r0, $r1
; CV1-NEXT:    andd.@ $r2 = $r1, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    insf $r3 = $r0, 15, 8
; CV1-NEXT:    andd.@ $r5 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    copyd $r1 = $r3
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r4 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r2 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    xord $r0 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    extfz $r2 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    insf $r2 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    andd.@ $r1 = $r0, 0x7f7f7f7f
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r3 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 21)
;
; CV2-LABEL: intAddSext:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    insf $r3 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    copyd $r1 = $r3
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r3, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r2 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r2 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    insf $r2 = $r3, 31, 16
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 15)
  %2 = tail call i8 @llvm.vector.reduce.add.v8i8(<8 x i8> %0)
  %3 = sext i8 %2 to i32
  ret i32 %3
}

define i32 @intReduceSext(<8 x i8> %0) {
; CV1-LABEL: intReduceSext:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    andd.@ $r3 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    xord $r0 = $r0, $r1
; CV1-NEXT:    andd.@ $r2 = $r1, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    insf $r3 = $r0, 15, 8
; CV1-NEXT:    andd.@ $r5 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    copyd $r1 = $r3
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r4 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r2 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    xord $r0 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    extfz $r2 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    insf $r2 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    andd.@ $r1 = $r0, 0x7f7f7f7f
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r3 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 21)
;
; CV2-LABEL: intReduceSext:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    insf $r3 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    copyd $r1 = $r3
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r3, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r2 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r2 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    insf $r2 = $r3, 31, 16
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 15)
  %2 = tail call i8 @llvm.vector.reduce.add.v8i8(<8 x i8> %0)
  %3 = sext i8 %2 to i32
  ret i32 %3
}

define i32 @intSextReduce(<8 x i8> %0) {
; CV1-LABEL: intSextReduce:
; CV1:       # %bb.0:
; CV1-NEXT:    sxmbhq $r0 = $r0
; CV1-NEXT:    sxlbhq $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxmhwp $r3 = $r1
; CV1-NEXT:    sxmhwp $r5 = $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sxlhwp $r0 = $r0
; CV1-NEXT:    sxlhwp $r2 = $r1
; CV1-NEXT:    addwp $r3 = $r3, $r5
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    addwp $r0 = $r2, $r0
; CV1-NEXT:    srld $r1 = $r3, 32
; CV1-NEXT:    copyd $r4 = $r3
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    insf $r4 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    addwp $r0 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    addwp $r1 = $r3, $r0
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    addwp $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    addwp $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 9)
;
; CV2-LABEL: intSextReduce:
; CV2:       # %bb.0:
; CV2-NEXT:    sxmbhq $r0 = $r0
; CV2-NEXT:    sxlbhq $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sxlhwp $r0 = $r0
; CV2-NEXT:    sxlhwp $r2 = $r1
; CV2-NEXT:    sxmhwp $r3 = $r1
; CV2-NEXT:    sxmhwp $r5 = $r0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    addwp $r0 = $r2, $r0
; CV2-NEXT:    addwp $r3 = $r3, $r5
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    srld $r1 = $r3, 32
; CV2-NEXT:    copyd $r4 = $r3
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    insf $r4 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    addwp $r0 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    addwp $r1 = $r3, $r0
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addwp $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    addwp $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 9)
  %2 = sext <8 x i8> %0 to <8 x i32>
  %3 = tail call i32 @llvm.vector.reduce.add.v8i32(<8 x i32> %2)
  ret i32 %3
}

declare i32 @llvm.vector.reduce.add.v8i32(<8 x i32>)

define i32 @intLoopSextReduceVector(<8 x i8> %0) {
; CV1-LABEL: intLoopSextReduceVector:
; CV1:       # %bb.0:
; CV1-NEXT:    sxmbhq $r0 = $r0
; CV1-NEXT:    sxlbhq $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxmhwp $r3 = $r1
; CV1-NEXT:    sxmhwp $r5 = $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sxlhwp $r0 = $r0
; CV1-NEXT:    sxlhwp $r2 = $r1
; CV1-NEXT:    addwp $r3 = $r3, $r5
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    addwp $r0 = $r2, $r0
; CV1-NEXT:    srld $r1 = $r3, 32
; CV1-NEXT:    copyd $r4 = $r3
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    insf $r4 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    addwp $r0 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    addwp $r1 = $r3, $r0
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    addwp $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    addwp $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 9)
;
; CV2-LABEL: intLoopSextReduceVector:
; CV2:       # %bb.0:
; CV2-NEXT:    sxmbhq $r0 = $r0
; CV2-NEXT:    sxlbhq $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sxlhwp $r0 = $r0
; CV2-NEXT:    sxlhwp $r2 = $r1
; CV2-NEXT:    sxmhwp $r3 = $r1
; CV2-NEXT:    sxmhwp $r5 = $r0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    addwp $r0 = $r2, $r0
; CV2-NEXT:    addwp $r3 = $r3, $r5
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    srld $r1 = $r3, 32
; CV2-NEXT:    copyd $r4 = $r3
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    insf $r4 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    addwp $r0 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    addwp $r1 = $r3, $r0
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addwp $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    addwp $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 9)
  %2 = sext <8 x i8> %0 to <8 x i32>
  %3 = tail call i32 @llvm.vector.reduce.add.v8i32(<8 x i32> %2)
  ret i32 %3
}

define i32 @intLoopReduceSextVector(<8 x i8> %0) {
; CV1-LABEL: intLoopReduceSextVector:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    andd.@ $r3 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    xord $r0 = $r0, $r1
; CV1-NEXT:    andd.@ $r2 = $r1, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    insf $r3 = $r0, 15, 8
; CV1-NEXT:    andd.@ $r5 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    copyd $r1 = $r3
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r4 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r2 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    xord $r0 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    extfz $r2 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    insf $r2 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    andd.@ $r1 = $r0, 0x7f7f7f7f
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r3 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 21)
;
; CV2-LABEL: intLoopReduceSextVector:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    insf $r3 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    copyd $r1 = $r3
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r3, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r2 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r2 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    insf $r2 = $r3, 31, 16
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 15)
  %2 = tail call i8 @llvm.vector.reduce.add.v8i8(<8 x i8> %0)
  %3 = sext i8 %2 to i32
  ret i32 %3
}

define i32 @intLoopSextReduceArray(ptr %0) {
; CV1-LABEL: intLoopSextReduceArray:
; CV1:       # %bb.0:
; CV1-NEXT:    ld $r0 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxmbhq $r0 = $r0
; CV1-NEXT:    sxlbhq $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sxmhwp $r3 = $r1
; CV1-NEXT:    sxmhwp $r5 = $r0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sxlhwp $r0 = $r0
; CV1-NEXT:    sxlhwp $r2 = $r1
; CV1-NEXT:    addwp $r3 = $r3, $r5
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    addwp $r0 = $r2, $r0
; CV1-NEXT:    srld $r1 = $r3, 32
; CV1-NEXT:    copyd $r4 = $r3
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r4 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    addwp $r0 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    addwp $r1 = $r3, $r0
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    addwp $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    addwp $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 12)
;
; CV2-LABEL: intLoopSextReduceArray:
; CV2:       # %bb.0:
; CV2-NEXT:    ld $r0 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sxmbhq $r0 = $r0
; CV2-NEXT:    sxlbhq $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sxlhwp $r0 = $r0
; CV2-NEXT:    sxlhwp $r2 = $r1
; CV2-NEXT:    sxmhwp $r3 = $r1
; CV2-NEXT:    sxmhwp $r5 = $r0
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    addwp $r0 = $r2, $r0
; CV2-NEXT:    addwp $r3 = $r3, $r5
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    srld $r1 = $r3, 32
; CV2-NEXT:    copyd $r4 = $r3
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r4 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addwp $r0 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    addwp $r1 = $r3, $r0
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    addwp $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    addwp $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 12)
  %2 = load <8 x i8>, ptr %0
  %3 = sext <8 x i8> %2 to <8 x i32>
  %4 = tail call i32 @llvm.vector.reduce.add.v8i32(<8 x i32> %3)
  ret i32 %4
}

define i32 @intLoopReduceSextArray(ptr %0) {
; CV1-LABEL: intLoopReduceSextArray:
; CV1:       # %bb.0:
; CV1-NEXT:    ld $r0 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd.@ $r1 = $r0, 0x7f7f7f7f
; CV1-NEXT:    copyd $r2 = $r0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    insf $r2 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r2 = $r2, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r3 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    insf $r3 = $r0, 15, 8
; CV1-NEXT:    andd.@ $r5 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    copyd $r1 = $r3
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r4 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r2 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    xord $r0 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    extfz $r2 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    insf $r2 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    andd.@ $r1 = $r0, 0x7f7f7f7f
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r3 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 24)
;
; CV2-LABEL: intLoopReduceSextArray:
; CV2:       # %bb.0:
; CV2-NEXT:    ld $r0 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    insf $r3 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    copyd $r1 = $r3
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r3, 31, 16
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    extfz $r2 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    insf $r2 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    insf $r2 = $r3, 31, 16
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 15)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 16)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 17)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 18)
  %2 = load <8 x i8>, ptr %0
  %3 = tail call i8 @llvm.vector.reduce.add.v8i8(<8 x i8> %2)
  %4 = sext i8 %3 to i32
  ret i32 %4
}

define i16 @shortSextAdd(<8 x i8> %0) {
; CV1-LABEL: shortSextAdd:
; CV1:       # %bb.0:
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxmbhq $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r1 = $r0, 16
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r1 = $r0, 31, 16
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r0, 47, 32
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 10)
;
; CV2-LABEL: shortSextAdd:
; CV2:       # %bb.0:
; CV2-NEXT:    sxlbhq $r0 = $r0
; CV2-NEXT:    sxmbhq $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    addhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    addhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    srlw $r1 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r1 = $r0, 31, 16
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r1 = $r0, 47, 32
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    addhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 10)
  %2 = sext <8 x i8> %0 to <8 x i16>
  %3 = tail call i16 @llvm.vector.reduce.add.v8i16(<8 x i16> %2)
  ret i16 %3
}

define i16 @shortAddSext(<8 x i8> %0) {
; CV1-LABEL: shortAddSext:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    andd.@ $r3 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    xord $r0 = $r0, $r1
; CV1-NEXT:    andd.@ $r2 = $r1, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    insf $r3 = $r0, 15, 8
; CV1-NEXT:    andd.@ $r5 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    copyd $r1 = $r3
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r4 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r2 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    xord $r0 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    extfz $r2 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    insf $r2 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    andd.@ $r1 = $r0, 0x7f7f7f7f
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r3 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 21)
;
; CV2-LABEL: shortAddSext:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    insf $r3 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    copyd $r1 = $r3
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r3, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r2 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r2 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    insf $r2 = $r3, 31, 16
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 15)
  %2 = tail call i8 @llvm.vector.reduce.add.v8i8(<8 x i8> %0)
  %3 = sext i8 %2 to i16
  ret i16 %3
}

define i16 @shortReduceSext(<8 x i8> %0) {
; CV1-LABEL: shortReduceSext:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    andd.@ $r3 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    xord $r0 = $r0, $r1
; CV1-NEXT:    andd.@ $r2 = $r1, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    insf $r3 = $r0, 15, 8
; CV1-NEXT:    andd.@ $r5 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    copyd $r1 = $r3
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r4 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r2 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    xord $r0 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    extfz $r2 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    insf $r2 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    andd.@ $r1 = $r0, 0x7f7f7f7f
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r3 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 21)
;
; CV2-LABEL: shortReduceSext:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    insf $r3 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    copyd $r1 = $r3
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r3, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r2 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r2 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    insf $r2 = $r3, 31, 16
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 15)
  %2 = tail call i8 @llvm.vector.reduce.add.v8i8(<8 x i8> %0)
  %3 = sext i8 %2 to i16
  ret i16 %3
}

define i16 @shortSextReduce(<8 x i8> %0) {
; CV1-LABEL: shortSextReduce:
; CV1:       # %bb.0:
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxmbhq $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r1 = $r0, 16
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r1 = $r0, 31, 16
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r0, 47, 32
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 10)
;
; CV2-LABEL: shortSextReduce:
; CV2:       # %bb.0:
; CV2-NEXT:    sxlbhq $r0 = $r0
; CV2-NEXT:    sxmbhq $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    addhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    addhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    srlw $r1 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r1 = $r0, 31, 16
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r1 = $r0, 47, 32
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    addhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 10)
  %2 = sext <8 x i8> %0 to <8 x i16>
  %3 = tail call i16 @llvm.vector.reduce.add.v8i16(<8 x i16> %2)
  ret i16 %3
}

declare i16 @llvm.vector.reduce.add.v8i16(<8 x i16>)

define i16 @shortLoopSextReduceVector(<8 x i8> %0) {
; CV1-LABEL: shortLoopSextReduceVector:
; CV1:       # %bb.0:
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxmbhq $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r1 = $r0, 16
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r1 = $r0, 31, 16
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r0, 47, 32
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 10)
;
; CV2-LABEL: shortLoopSextReduceVector:
; CV2:       # %bb.0:
; CV2-NEXT:    sxlbhq $r0 = $r0
; CV2-NEXT:    sxmbhq $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    addhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    addhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    srlw $r1 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r1 = $r0, 31, 16
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r1 = $r0, 47, 32
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    addhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 10)
  %2 = sext <8 x i8> %0 to <8 x i16>
  %3 = tail call i16 @llvm.vector.reduce.add.v8i16(<8 x i16> %2)
  ret i16 %3
}

define i16 @shortLoopReduceSextVector(<8 x i8> %0) {
; CV1-LABEL: shortLoopReduceSextVector:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    andd.@ $r3 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    xord $r0 = $r0, $r1
; CV1-NEXT:    andd.@ $r2 = $r1, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    insf $r3 = $r0, 15, 8
; CV1-NEXT:    andd.@ $r5 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    copyd $r1 = $r3
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r4 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r2 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    xord $r0 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    extfz $r2 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    insf $r2 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    andd.@ $r1 = $r0, 0x7f7f7f7f
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r3 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 21)
;
; CV2-LABEL: shortLoopReduceSextVector:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    insf $r3 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    copyd $r1 = $r3
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r3, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r2 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r2 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    insf $r2 = $r3, 31, 16
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 15)
  %2 = tail call i8 @llvm.vector.reduce.add.v8i8(<8 x i8> %0)
  %3 = sext i8 %2 to i16
  ret i16 %3
}

define i16 @shortLoopSextReduceArray(ptr %0) {
; CV1-LABEL: shortLoopSextReduceArray:
; CV1:       # %bb.0:
; CV1-NEXT:    ld $r0 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    sxmbhq $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    srlw $r1 = $r0, 16
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r1 = $r0, 31, 16
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    insf $r1 = $r0, 47, 32
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    addhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 13)
;
; CV2-LABEL: shortLoopSextReduceArray:
; CV2:       # %bb.0:
; CV2-NEXT:    ld $r0 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    sxlbhq $r0 = $r0
; CV2-NEXT:    sxmbhq $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    addhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    srlw $r1 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r1 = $r0, 31, 16
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    insf $r1 = $r0, 47, 32
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    addhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 13)
  %2 = load <8 x i8>, ptr %0
  %3 = sext <8 x i8> %2 to <8 x i16>
  %4 = tail call i16 @llvm.vector.reduce.add.v8i16(<8 x i16> %3)
  ret i16 %4
}

define i16 @shortLoopReduceSextArray(ptr %0) {
; CV1-LABEL: shortLoopReduceSextArray:
; CV1:       # %bb.0:
; CV1-NEXT:    ld $r0 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd.@ $r1 = $r0, 0x7f7f7f7f
; CV1-NEXT:    copyd $r2 = $r0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    insf $r2 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r2 = $r2, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r3 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    insf $r3 = $r0, 15, 8
; CV1-NEXT:    andd.@ $r5 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    copyd $r1 = $r3
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r4 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r2 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    xord $r0 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    extfz $r2 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    insf $r2 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    andd.@ $r1 = $r0, 0x7f7f7f7f
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r3 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    sxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 24)
;
; CV2-LABEL: shortLoopReduceSextArray:
; CV2:       # %bb.0:
; CV2-NEXT:    ld $r0 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    insf $r3 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    copyd $r1 = $r3
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r3, 31, 16
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    extfz $r2 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    insf $r2 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    insf $r2 = $r3, 31, 16
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 15)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 16)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ;; # (end cycle 17)
; CV2-NEXT:    sxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 18)
  %2 = load <8 x i8>, ptr %0
  %3 = tail call i8 @llvm.vector.reduce.add.v8i8(<8 x i8> %2)
  %4 = sext i8 %3 to i16
  ret i16 %4
}

define i8 @charAdd(<8 x i8> %0) {
; CV1-LABEL: charAdd:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    andd.@ $r3 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    xord $r0 = $r0, $r1
; CV1-NEXT:    andd.@ $r2 = $r1, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    insf $r3 = $r0, 15, 8
; CV1-NEXT:    andd.@ $r5 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    copyd $r1 = $r3
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r4 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r2 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    xord $r0 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    extfz $r2 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    insf $r2 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    andd.@ $r1 = $r0, 0x7f7f7f7f
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r3 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 20)
;
; CV2-LABEL: charAdd:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    insf $r3 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    copyd $r1 = $r3
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r3, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r2 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r2 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    insf $r2 = $r3, 31, 16
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 14)
  %2 = tail call i8 @llvm.vector.reduce.add.v8i8(<8 x i8> %0)
  ret i8 %2
}

define i8 @charReduce(<8 x i8> %0) {
; CV1-LABEL: charReduce:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    andd.@ $r3 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    xord $r0 = $r0, $r1
; CV1-NEXT:    andd.@ $r2 = $r1, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    insf $r3 = $r0, 15, 8
; CV1-NEXT:    andd.@ $r5 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    copyd $r1 = $r3
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r4 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r2 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    xord $r0 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    extfz $r2 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    insf $r2 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    andd.@ $r1 = $r0, 0x7f7f7f7f
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r3 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 20)
;
; CV2-LABEL: charReduce:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    insf $r3 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    copyd $r1 = $r3
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r3, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r2 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r2 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    insf $r2 = $r3, 31, 16
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 14)
  %2 = tail call i8 @llvm.vector.reduce.add.v8i8(<8 x i8> %0)
  ret i8 %2
}

define i8 @charLoopReduceVector(<8 x i8> %0) {
; CV1-LABEL: charLoopReduceVector:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    andd.@ $r3 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    xord $r0 = $r0, $r1
; CV1-NEXT:    andd.@ $r2 = $r1, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    insf $r3 = $r0, 15, 8
; CV1-NEXT:    andd.@ $r5 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    copyd $r1 = $r3
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r4 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r2 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    xord $r0 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    extfz $r2 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    insf $r2 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    andd.@ $r1 = $r0, 0x7f7f7f7f
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r3 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 20)
;
; CV2-LABEL: charLoopReduceVector:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    insf $r3 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    copyd $r1 = $r3
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r3, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r2 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r2 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    insf $r2 = $r3, 31, 16
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 14)
  %2 = tail call i8 @llvm.vector.reduce.add.v8i8(<8 x i8> %0)
  ret i8 %2
}

define i8 @charLoopReduceArray(ptr %0) {
; CV1-LABEL: charLoopReduceArray:
; CV1:       # %bb.0:
; CV1-NEXT:    ld $r0 = 0[$r0]
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd.@ $r1 = $r0, 0x7f7f7f7f
; CV1-NEXT:    copyd $r2 = $r0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    insf $r2 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r2 = $r2, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r3 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    insf $r3 = $r0, 15, 8
; CV1-NEXT:    andd.@ $r5 = $r0, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    copyd $r1 = $r3
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r4 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r2 = $r5, $r4
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    xord $r0 = $r2, $r0
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    extfz $r2 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    insf $r2 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    insf $r2 = $r3, 31, 16
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    andd.@ $r1 = $r0, 0x7f7f7f7f
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    xord $r0 = $r0, $r2
; CV1-NEXT:    andd.@ $r3 = $r2, 0x7f7f7f7f
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    andd.@ $r0 = $r0, 0x80808080
; CV1-NEXT:    addd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    xord $r0 = $r1, $r0
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 23)
;
; CV2-LABEL: charLoopReduceArray:
; CV2:       # %bb.0:
; CV2-NEXT:    ld $r0 = 0[$r0]
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    addbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    insf $r3 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    copyd $r1 = $r3
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r3, 31, 16
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    extfz $r2 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    insf $r2 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    insf $r2 = $r3, 31, 16
; CV2-NEXT:    ;; # (end cycle 14)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 15)
; CV2-NEXT:    addbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 16)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 17)
  %2 = load <8 x i8>, ptr %0
  %3 = tail call i8 @llvm.vector.reduce.add.v8i8(<8 x i8> %2)
  ret i8 %3
}

