; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mcpu=kv3-1 -O2 -o - %s | FileCheck %s --check-prefix=CV1
; RUN: llc -mcpu=kv3-2 -O2 -o - %s | FileCheck %s --check-prefix=CV2
; RUN: clang -O2 -march=kv3-1 -c -o /dev/null %s
; RUN: clang -O2 -march=kv3-2 -c -o /dev/null %s

target triple = "kvx-kalray-cos"

define i8 @maxrbod(<8 x i8> %0) {
; CV1-LABEL: maxrbod:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    maxhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sllhqs $r1 = $r2, 8
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    maxhq $r0 = $r0, $r2
; CV1-NEXT:    maxhq $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    srlhqs $r1 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    maxhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 21)
;
; CV2-LABEL: maxrbod:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    maxbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    maxbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    maxbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 12)
  %2 = tail call i8 @llvm.vector.reduce.smax.v8i8(<8 x i8> %0)
  ret i8 %2
}

define i8 @maxrbpd(<2 x i8> %0) {
; CV1-LABEL: maxrbpd:
; CV1:       # %bb.0:
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x401
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 5)
;
; CV2-LABEL: maxrbpd:
; CV2:       # %bb.0:
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    maxbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 3)
  %2 = tail call i8 @llvm.vector.reduce.smax.v2i8(<2 x i8> %0)
  ret i8 %2
}

define i8 @maxrbqd(<4 x i8> %0) {
; CV1-LABEL: maxrbqd:
; CV1:       # %bb.0:
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sxlbhq $r1 = $r2
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x40100401
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    extfz $r1 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 11)
;
; CV2-LABEL: maxrbqd:
; CV2:       # %bb.0:
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    maxbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    maxbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = tail call i8 @llvm.vector.reduce.smax.v4i8(<4 x i8> %0)
  ret i8 %2
}

define i8 @maxrbvd(<32 x i8> %0) {
; CV1-LABEL: maxrbvd:
; CV1:       # %bb.0:
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r4 = $r2, 8
; CV1-NEXT:    sllhqs $r5 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd $r3 = $r3, 0xff00ff00.@
; CV1-NEXT:    maxhq $r4 = $r5, $r4
; CV1-NEXT:    sllhqs $r6 = $r3, 8
; CV1-NEXT:    sllhqs $r7 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    maxhq $r0 = $r0, $r2
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    srlhqs $r2 = $r4, 8
; CV1-NEXT:    maxhq $r5 = $r7, $r6
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r2
; CV1-NEXT:    maxhq $r1 = $r1, $r3
; CV1-NEXT:    srlhqs $r3 = $r5, 8
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    iord $r1 = $r1, $r3
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r2 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    maxhq $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    maxhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    sllhqs $r1 = $r2, 8
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    maxhq $r0 = $r0, $r2
; CV1-NEXT:    maxhq $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    srlhqs $r1 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    maxhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 27)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 28)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 29)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 30)
;
; CV2-LABEL: maxrbvd:
; CV2:       # %bb.0:
; CV2-NEXT:    maxbo $r0 = $r0, $r2
; CV2-NEXT:    maxbo $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    maxbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    maxbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    maxbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    maxbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 14)
  %2 = tail call i8 @llvm.vector.reduce.smax.v32i8(<32 x i8> %0)
  ret i8 %2
}

define i8 @maxrbxd(<16 x i8> %0) {
; CV1-LABEL: maxrbxd:
; CV1:       # %bb.0:
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r2 = $r1, 8
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    maxhq $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    maxhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    sllhqs $r1 = $r2, 8
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    maxhq $r0 = $r0, $r2
; CV1-NEXT:    maxhq $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    srlhqs $r1 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    maxhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 25)
;
; CV2-LABEL: maxrbxd:
; CV2:       # %bb.0:
; CV2-NEXT:    maxbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    maxbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    maxbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    maxbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 13)
  %2 = tail call i8 @llvm.vector.reduce.smax.v16i8(<16 x i8> %0)
  ret i8 %2
}

define i16 @maxrhod(<8 x i16> %0) {
; CV1-LABEL: maxrhod:
; CV1:       # %bb.0:
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: maxrhod:
; CV2:       # %bb.0:
; CV2-NEXT:    maxhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    maxhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    maxhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = tail call i16 @llvm.vector.reduce.smax.v8i16(<8 x i16> %0)
  ret i16 %2
}

define i16 @maxrhpd(<2 x i16> %0) {
; CV1-LABEL: maxrhpd:
; CV1:       # %bb.0:
; CV1-NEXT:    srlw $r1 = $r0, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 31, 16
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: maxrhpd:
; CV2:       # %bb.0:
; CV2-NEXT:    srlw $r1 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 31, 16
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    maxhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 3)
  %2 = tail call i16 @llvm.vector.reduce.smax.v2i16(<2 x i16> %0)
  ret i16 %2
}

define i16 @maxrhqd(<4 x i16> %0) {
; CV1-LABEL: maxrhqd:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: maxrhqd:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    maxhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    maxhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 6)
  %2 = tail call i16 @llvm.vector.reduce.smax.v4i16(<4 x i16> %0)
  ret i16 %2
}

define i16 @maxrhxd(<16 x i16> %0) {
; CV1-LABEL: maxrhxd:
; CV1:       # %bb.0:
; CV1-NEXT:    maxhq $r0 = $r0, $r2
; CV1-NEXT:    maxhq $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    maxhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 8)
;
; CV2-LABEL: maxrhxd:
; CV2:       # %bb.0:
; CV2-NEXT:    maxhq $r0 = $r0, $r2
; CV2-NEXT:    maxhq $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    maxhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    maxhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    maxhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
  %2 = tail call i16 @llvm.vector.reduce.smax.v16i16(<16 x i16> %0)
  ret i16 %2
}

define i32 @maxrwod(<8 x i32> %0) {
; CV1-LABEL: maxrwod:
; CV1:       # %bb.0:
; CV1-NEXT:    maxwp $r0 = $r0, $r2
; CV1-NEXT:    maxwp $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srld $r3 = $r1, 32
; CV1-NEXT:    copyd $r4 = $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r4 = $r3, 63, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    maxwp $r0 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    maxwp $r1 = $r1, $r0
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    maxwp $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    maxwp $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: maxrwod:
; CV2:       # %bb.0:
; CV2-NEXT:    maxwp $r0 = $r0, $r2
; CV2-NEXT:    maxwp $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srld $r3 = $r1, 32
; CV2-NEXT:    copyd $r4 = $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r4 = $r3, 63, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    maxwp $r0 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    maxwp $r1 = $r1, $r0
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    maxwp $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    maxwp $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = tail call i32 @llvm.vector.reduce.smax.v8i32(<8 x i32> %0)
  ret i32 %2
}

define i32 @maxrwpd(<2 x i32> %0) {
; CV1-LABEL: maxrwpd:
; CV1:       # %bb.0:
; CV1-NEXT:    srad $r1 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 63, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    maxwp $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 2)
;
; CV2-LABEL: maxrwpd:
; CV2:       # %bb.0:
; CV2-NEXT:    srad $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 63, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    maxwp $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 2)
  %2 = tail call i32 @llvm.vector.reduce.smax.v2i32(<2 x i32> %0)
  ret i32 %2
}

define i32 @maxrwqd(<4 x i32> %0) {
; CV1-LABEL: maxrwqd:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r2 = $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    maxwp $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    maxwp $r1 = $r1, $r0
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    maxwp $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    maxwp $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 5)
;
; CV2-LABEL: maxrwqd:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r2 = $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    maxwp $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    maxwp $r1 = $r1, $r0
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    maxwp $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    maxwp $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 5)
  %2 = tail call i32 @llvm.vector.reduce.smax.v4i32(<4 x i32> %0)
  ret i32 %2
}

define i8 @maxurbod(<8 x i8> %0) {
; CV1-LABEL: maxurbod:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    maxuhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sllhqs $r1 = $r2, 8
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    maxuhq $r0 = $r0, $r2
; CV1-NEXT:    maxuhq $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    srlhqs $r1 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    maxuhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 21)
;
; CV2-LABEL: maxurbod:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    maxubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    maxubo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    maxubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 12)
  %2 = tail call i8 @llvm.vector.reduce.umax.v8i8(<8 x i8> %0)
  ret i8 %2
}

define i8 @maxurbpd(<2 x i8> %0) {
; CV1-LABEL: maxurbpd:
; CV1:       # %bb.0:
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x20001
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x20001
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x401
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 5)
;
; CV2-LABEL: maxurbpd:
; CV2:       # %bb.0:
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    maxubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 3)
  %2 = tail call i8 @llvm.vector.reduce.umax.v2i8(<2 x i8> %0)
  ret i8 %2
}

define i8 @maxurbqd(<4 x i8> %0) {
; CV1-LABEL: maxurbqd:
; CV1:       # %bb.0:
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x8000400020001
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sbmm8 $r1 = $r2, 0x8000400020001
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x40100401
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    extfz $r1 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x8000400020001
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 11)
;
; CV2-LABEL: maxurbqd:
; CV2:       # %bb.0:
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    maxubo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    maxubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = tail call i8 @llvm.vector.reduce.umax.v4i8(<4 x i8> %0)
  ret i8 %2
}

define i8 @maxurbvd(<32 x i8> %0) {
; CV1-LABEL: maxurbvd:
; CV1:       # %bb.0:
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r4 = $r2, 8
; CV1-NEXT:    sllhqs $r5 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd $r3 = $r3, 0xff00ff00.@
; CV1-NEXT:    maxuhq $r4 = $r5, $r4
; CV1-NEXT:    sllhqs $r6 = $r3, 8
; CV1-NEXT:    sllhqs $r7 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    maxuhq $r0 = $r0, $r2
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    srlhqs $r2 = $r4, 8
; CV1-NEXT:    maxuhq $r5 = $r7, $r6
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r2
; CV1-NEXT:    maxuhq $r1 = $r1, $r3
; CV1-NEXT:    srlhqs $r3 = $r5, 8
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    iord $r1 = $r1, $r3
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r2 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    maxuhq $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    maxuhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    sllhqs $r1 = $r2, 8
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    maxuhq $r0 = $r0, $r2
; CV1-NEXT:    maxuhq $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    srlhqs $r1 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    maxuhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 27)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 28)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 29)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 30)
;
; CV2-LABEL: maxurbvd:
; CV2:       # %bb.0:
; CV2-NEXT:    maxubo $r0 = $r0, $r2
; CV2-NEXT:    maxubo $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    maxubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    maxubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    maxubo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    maxubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 14)
  %2 = tail call i8 @llvm.vector.reduce.umax.v32i8(<32 x i8> %0)
  ret i8 %2
}

define i8 @maxurbxd(<16 x i8> %0) {
; CV1-LABEL: maxurbxd:
; CV1:       # %bb.0:
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r2 = $r1, 8
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    maxuhq $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    maxuhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    sllhqs $r1 = $r2, 8
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    maxuhq $r0 = $r0, $r2
; CV1-NEXT:    maxuhq $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    srlhqs $r1 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    maxuhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 25)
;
; CV2-LABEL: maxurbxd:
; CV2:       # %bb.0:
; CV2-NEXT:    maxubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    maxubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    maxubo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    maxubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 13)
  %2 = tail call i8 @llvm.vector.reduce.umax.v16i8(<16 x i8> %0)
  ret i8 %2
}

define i16 @maxurhod(<8 x i16> %0) {
; CV1-LABEL: maxurhod:
; CV1:       # %bb.0:
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: maxurhod:
; CV2:       # %bb.0:
; CV2-NEXT:    maxuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    maxuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    maxuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = tail call i16 @llvm.vector.reduce.umax.v8i16(<8 x i16> %0)
  ret i16 %2
}

define i16 @maxurhpd(<2 x i16> %0) {
; CV1-LABEL: maxurhpd:
; CV1:       # %bb.0:
; CV1-NEXT:    srlw $r1 = $r0, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 31, 16
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: maxurhpd:
; CV2:       # %bb.0:
; CV2-NEXT:    srlw $r1 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 31, 16
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    maxuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 3)
  %2 = tail call i16 @llvm.vector.reduce.umax.v2i16(<2 x i16> %0)
  ret i16 %2
}

define i16 @maxurhqd(<4 x i16> %0) {
; CV1-LABEL: maxurhqd:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: maxurhqd:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    maxuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    maxuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 6)
  %2 = tail call i16 @llvm.vector.reduce.umax.v4i16(<4 x i16> %0)
  ret i16 %2
}

define i16 @maxurhxd(<16 x i16> %0) {
; CV1-LABEL: maxurhxd:
; CV1:       # %bb.0:
; CV1-NEXT:    maxuhq $r0 = $r0, $r2
; CV1-NEXT:    maxuhq $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    maxuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 8)
;
; CV2-LABEL: maxurhxd:
; CV2:       # %bb.0:
; CV2-NEXT:    maxuhq $r0 = $r0, $r2
; CV2-NEXT:    maxuhq $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    maxuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    maxuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    maxuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
  %2 = tail call i16 @llvm.vector.reduce.umax.v16i16(<16 x i16> %0)
  ret i16 %2
}

define i32 @maxurwod(<8 x i32> %0) {
; CV1-LABEL: maxurwod:
; CV1:       # %bb.0:
; CV1-NEXT:    maxuwp $r0 = $r0, $r2
; CV1-NEXT:    maxuwp $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srld $r3 = $r1, 32
; CV1-NEXT:    copyd $r4 = $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r4 = $r3, 63, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    maxuwp $r0 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    maxuwp $r1 = $r1, $r0
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    maxuwp $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    maxuwp $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: maxurwod:
; CV2:       # %bb.0:
; CV2-NEXT:    maxuwp $r0 = $r0, $r2
; CV2-NEXT:    maxuwp $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srld $r3 = $r1, 32
; CV2-NEXT:    copyd $r4 = $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r4 = $r3, 63, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    maxuwp $r0 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    maxuwp $r1 = $r1, $r0
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    maxuwp $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    maxuwp $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = tail call i32 @llvm.vector.reduce.umax.v8i32(<8 x i32> %0)
  ret i32 %2
}

define i32 @maxurwpd(<2 x i32> %0) {
; CV1-LABEL: maxurwpd:
; CV1:       # %bb.0:
; CV1-NEXT:    srad $r1 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 63, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    maxuwp $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 2)
;
; CV2-LABEL: maxurwpd:
; CV2:       # %bb.0:
; CV2-NEXT:    srad $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 63, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    maxuwp $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 2)
  %2 = tail call i32 @llvm.vector.reduce.umax.v2i32(<2 x i32> %0)
  ret i32 %2
}

define i32 @maxurwqd(<4 x i32> %0) {
; CV1-LABEL: maxurwqd:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r2 = $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    maxuwp $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    maxuwp $r1 = $r1, $r0
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    maxuwp $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    maxuwp $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 5)
;
; CV2-LABEL: maxurwqd:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r2 = $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    maxuwp $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    maxuwp $r1 = $r1, $r0
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    maxuwp $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    maxuwp $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 5)
  %2 = tail call i32 @llvm.vector.reduce.umax.v4i32(<4 x i32> %0)
  ret i32 %2
}

define i8 @minrbod(<8 x i8> %0) {
; CV1-LABEL: minrbod:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    minhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sllhqs $r1 = $r2, 8
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    minhq $r0 = $r0, $r2
; CV1-NEXT:    minhq $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    srlhqs $r1 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    minhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 21)
;
; CV2-LABEL: minrbod:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    minbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    minbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    minbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 12)
  %2 = tail call i8 @llvm.vector.reduce.smin.v8i8(<8 x i8> %0)
  ret i8 %2
}

define i8 @minrbpd(<2 x i8> %0) {
; CV1-LABEL: minrbpd:
; CV1:       # %bb.0:
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x401
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 5)
;
; CV2-LABEL: minrbpd:
; CV2:       # %bb.0:
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    minbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 3)
  %2 = tail call i8 @llvm.vector.reduce.smin.v2i8(<2 x i8> %0)
  ret i8 %2
}

define i8 @minrbqd(<4 x i8> %0) {
; CV1-LABEL: minrbqd:
; CV1:       # %bb.0:
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sxlbhq $r0 = $r0
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sxlbhq $r1 = $r2
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x40100401
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    extfz $r1 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    sxlbhq $r1 = $r1
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 11)
;
; CV2-LABEL: minrbqd:
; CV2:       # %bb.0:
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    minbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    minbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = tail call i8 @llvm.vector.reduce.smin.v4i8(<4 x i8> %0)
  ret i8 %2
}

define i8 @minrbvd(<32 x i8> %0) {
; CV1-LABEL: minrbvd:
; CV1:       # %bb.0:
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r4 = $r2, 8
; CV1-NEXT:    sllhqs $r5 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd $r3 = $r3, 0xff00ff00.@
; CV1-NEXT:    minhq $r4 = $r5, $r4
; CV1-NEXT:    sllhqs $r6 = $r3, 8
; CV1-NEXT:    sllhqs $r7 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    minhq $r0 = $r0, $r2
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    srlhqs $r2 = $r4, 8
; CV1-NEXT:    minhq $r5 = $r7, $r6
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r2
; CV1-NEXT:    minhq $r1 = $r1, $r3
; CV1-NEXT:    srlhqs $r3 = $r5, 8
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    iord $r1 = $r1, $r3
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r2 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    minhq $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    minhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    sllhqs $r1 = $r2, 8
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    minhq $r0 = $r0, $r2
; CV1-NEXT:    minhq $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    srlhqs $r1 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    minhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 27)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 28)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 29)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 30)
;
; CV2-LABEL: minrbvd:
; CV2:       # %bb.0:
; CV2-NEXT:    minbo $r0 = $r0, $r2
; CV2-NEXT:    minbo $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    minbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    minbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    minbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    minbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 14)
  %2 = tail call i8 @llvm.vector.reduce.smin.v32i8(<32 x i8> %0)
  ret i8 %2
}

define i8 @minrbxd(<16 x i8> %0) {
; CV1-LABEL: minrbxd:
; CV1:       # %bb.0:
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r2 = $r1, 8
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    minhq $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    minhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    sllhqs $r1 = $r2, 8
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    minhq $r0 = $r0, $r2
; CV1-NEXT:    minhq $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    srlhqs $r1 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    minhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 25)
;
; CV2-LABEL: minrbxd:
; CV2:       # %bb.0:
; CV2-NEXT:    minbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    minbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    minbo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    minbo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 13)
  %2 = tail call i8 @llvm.vector.reduce.smin.v16i8(<16 x i8> %0)
  ret i8 %2
}

define i16 @minrhod(<8 x i16> %0) {
; CV1-LABEL: minrhod:
; CV1:       # %bb.0:
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: minrhod:
; CV2:       # %bb.0:
; CV2-NEXT:    minhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    minhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    minhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = tail call i16 @llvm.vector.reduce.smin.v8i16(<8 x i16> %0)
  ret i16 %2
}

define i16 @minrhpd(<2 x i16> %0) {
; CV1-LABEL: minrhpd:
; CV1:       # %bb.0:
; CV1-NEXT:    srlw $r1 = $r0, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 31, 16
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: minrhpd:
; CV2:       # %bb.0:
; CV2-NEXT:    srlw $r1 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 31, 16
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    minhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 3)
  %2 = tail call i16 @llvm.vector.reduce.smin.v2i16(<2 x i16> %0)
  ret i16 %2
}

define i16 @minrhqd(<4 x i16> %0) {
; CV1-LABEL: minrhqd:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: minrhqd:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    minhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    minhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 6)
  %2 = tail call i16 @llvm.vector.reduce.smin.v4i16(<4 x i16> %0)
  ret i16 %2
}

define i16 @minrhxd(<16 x i16> %0) {
; CV1-LABEL: minrhxd:
; CV1:       # %bb.0:
; CV1-NEXT:    minhq $r0 = $r0, $r2
; CV1-NEXT:    minhq $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    minhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 8)
;
; CV2-LABEL: minrhxd:
; CV2:       # %bb.0:
; CV2-NEXT:    minhq $r0 = $r0, $r2
; CV2-NEXT:    minhq $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    minhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    minhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    minhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
  %2 = tail call i16 @llvm.vector.reduce.smin.v16i16(<16 x i16> %0)
  ret i16 %2
}

define i32 @minrwod(<8 x i32> %0) {
; CV1-LABEL: minrwod:
; CV1:       # %bb.0:
; CV1-NEXT:    minwp $r0 = $r0, $r2
; CV1-NEXT:    minwp $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srld $r3 = $r1, 32
; CV1-NEXT:    copyd $r4 = $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r4 = $r3, 63, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    minwp $r0 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    minwp $r1 = $r1, $r0
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    minwp $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    minwp $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: minrwod:
; CV2:       # %bb.0:
; CV2-NEXT:    minwp $r0 = $r0, $r2
; CV2-NEXT:    minwp $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srld $r3 = $r1, 32
; CV2-NEXT:    copyd $r4 = $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r4 = $r3, 63, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    minwp $r0 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    minwp $r1 = $r1, $r0
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    minwp $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    minwp $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = tail call i32 @llvm.vector.reduce.smin.v8i32(<8 x i32> %0)
  ret i32 %2
}

define i32 @minrwpd(<2 x i32> %0) {
; CV1-LABEL: minrwpd:
; CV1:       # %bb.0:
; CV1-NEXT:    srad $r1 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 63, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    minwp $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 2)
;
; CV2-LABEL: minrwpd:
; CV2:       # %bb.0:
; CV2-NEXT:    srad $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 63, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    minwp $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 2)
  %2 = tail call i32 @llvm.vector.reduce.smin.v2i32(<2 x i32> %0)
  ret i32 %2
}

define i32 @minrwqd(<4 x i32> %0) {
; CV1-LABEL: minrwqd:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r2 = $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    minwp $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    minwp $r1 = $r1, $r0
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    minwp $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    minwp $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 5)
;
; CV2-LABEL: minrwqd:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r2 = $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    minwp $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    minwp $r1 = $r1, $r0
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    minwp $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    minwp $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 5)
  %2 = tail call i32 @llvm.vector.reduce.smin.v4i32(<4 x i32> %0)
  ret i32 %2
}

define i8 @minurbod(<8 x i8> %0) {
; CV1-LABEL: minurbod:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    minuhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sllhqs $r1 = $r2, 8
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    minuhq $r0 = $r0, $r2
; CV1-NEXT:    minuhq $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    srlhqs $r1 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    minuhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 21)
;
; CV2-LABEL: minurbod:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    minubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    minubo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    minubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 12)
  %2 = tail call i8 @llvm.vector.reduce.umin.v8i8(<8 x i8> %0)
  ret i8 %2
}

define i8 @minurbpd(<2 x i8> %0) {
; CV1-LABEL: minurbpd:
; CV1:       # %bb.0:
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x20001
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x20001
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x401
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 5)
;
; CV2-LABEL: minurbpd:
; CV2:       # %bb.0:
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    minubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 3)
  %2 = tail call i8 @llvm.vector.reduce.umin.v2i8(<2 x i8> %0)
  ret i8 %2
}

define i8 @minurbqd(<4 x i8> %0) {
; CV1-LABEL: minurbqd:
; CV1:       # %bb.0:
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x8000400020001
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sbmm8 $r1 = $r2, 0x8000400020001
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x40100401
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    extfz $r1 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x8000400020001
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    sbmm8 $r0 = $r0, 0x40100401
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 11)
;
; CV2-LABEL: minurbqd:
; CV2:       # %bb.0:
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    minubo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    minubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = tail call i8 @llvm.vector.reduce.umin.v4i8(<4 x i8> %0)
  ret i8 %2
}

define i8 @minurbvd(<32 x i8> %0) {
; CV1-LABEL: minurbvd:
; CV1:       # %bb.0:
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r4 = $r2, 8
; CV1-NEXT:    sllhqs $r5 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd $r3 = $r3, 0xff00ff00.@
; CV1-NEXT:    minuhq $r4 = $r5, $r4
; CV1-NEXT:    sllhqs $r6 = $r3, 8
; CV1-NEXT:    sllhqs $r7 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    minuhq $r0 = $r0, $r2
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    srlhqs $r2 = $r4, 8
; CV1-NEXT:    minuhq $r5 = $r7, $r6
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r2
; CV1-NEXT:    minuhq $r1 = $r1, $r3
; CV1-NEXT:    srlhqs $r3 = $r5, 8
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    iord $r1 = $r1, $r3
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r2 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    minuhq $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    minuhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    sllhqs $r1 = $r2, 8
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    minuhq $r0 = $r0, $r2
; CV1-NEXT:    minuhq $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    srlhqs $r1 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 25)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 26)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    minuhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 27)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 28)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 29)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 30)
;
; CV2-LABEL: minurbvd:
; CV2:       # %bb.0:
; CV2-NEXT:    minubo $r0 = $r0, $r2
; CV2-NEXT:    minubo $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    minubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    minubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    minubo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    minubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 14)
  %2 = tail call i8 @llvm.vector.reduce.umin.v32i8(<32 x i8> %0)
  ret i8 %2
}

define i8 @minurbxd(<16 x i8> %0) {
; CV1-LABEL: minurbxd:
; CV1:       # %bb.0:
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r2 = $r1, 8
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    minuhq $r2 = $r3, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    minuhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    sllhqs $r3 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 14)
; CV1-NEXT:    sllhqs $r1 = $r2, 8
; CV1-NEXT:    andd $r2 = $r2, 0xff00ff00.@
; CV1-NEXT:    ;; # (end cycle 15)
; CV1-NEXT:    minuhq $r0 = $r0, $r2
; CV1-NEXT:    minuhq $r1 = $r3, $r1
; CV1-NEXT:    ;; # (end cycle 16)
; CV1-NEXT:    srlhqs $r1 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 17)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 18)
; CV1-NEXT:    andd $r0 = $r0, 0xff00ff00.@
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    sllhqs $r2 = $r0, 8
; CV1-NEXT:    ;; # (end cycle 19)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 20)
; CV1-NEXT:    andd $r1 = $r1, 0xff00ff00.@
; CV1-NEXT:    sllhqs $r3 = $r1, 8
; CV1-NEXT:    ;; # (end cycle 21)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    minuhq $r2 = $r2, $r3
; CV1-NEXT:    ;; # (end cycle 22)
; CV1-NEXT:    srlhqs $r1 = $r2, 8
; CV1-NEXT:    ;; # (end cycle 23)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 24)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 25)
;
; CV2-LABEL: minurbxd:
; CV2:       # %bb.0:
; CV2-NEXT:    minubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    minubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    minubo $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    minubo $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 13)
  %2 = tail call i8 @llvm.vector.reduce.umin.v16i8(<16 x i8> %0)
  ret i8 %2
}

define i16 @minurhod(<8 x i16> %0) {
; CV1-LABEL: minurhod:
; CV1:       # %bb.0:
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: minurhod:
; CV2:       # %bb.0:
; CV2-NEXT:    minuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    minuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    minuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = tail call i16 @llvm.vector.reduce.umin.v8i16(<8 x i16> %0)
  ret i16 %2
}

define i16 @minurhpd(<2 x i16> %0) {
; CV1-LABEL: minurhpd:
; CV1:       # %bb.0:
; CV1-NEXT:    srlw $r1 = $r0, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 31, 16
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: minurhpd:
; CV2:       # %bb.0:
; CV2-NEXT:    srlw $r1 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 31, 16
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    minuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 3)
  %2 = tail call i16 @llvm.vector.reduce.umin.v2i16(<2 x i16> %0)
  ret i16 %2
}

define i16 @minurhqd(<4 x i16> %0) {
; CV1-LABEL: minurhqd:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: minurhqd:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    minuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    minuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 6)
  %2 = tail call i16 @llvm.vector.reduce.umin.v4i16(<4 x i16> %0)
  ret i16 %2
}

define i16 @minurhxd(<16 x i16> %0) {
; CV1-LABEL: minurhxd:
; CV1:       # %bb.0:
; CV1-NEXT:    minuhq $r0 = $r0, $r2
; CV1-NEXT:    minuhq $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    minuhq $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 8)
;
; CV2-LABEL: minurhxd:
; CV2:       # %bb.0:
; CV2-NEXT:    minuhq $r0 = $r0, $r2
; CV2-NEXT:    minuhq $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    minuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    minuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    minuhq $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
  %2 = tail call i16 @llvm.vector.reduce.umin.v16i16(<16 x i16> %0)
  ret i16 %2
}

define i32 @minurwod(<8 x i32> %0) {
; CV1-LABEL: minurwod:
; CV1:       # %bb.0:
; CV1-NEXT:    minuwp $r0 = $r0, $r2
; CV1-NEXT:    minuwp $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srld $r3 = $r1, 32
; CV1-NEXT:    copyd $r4 = $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r4 = $r3, 63, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    minuwp $r0 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    minuwp $r1 = $r1, $r0
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    minuwp $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    minuwp $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: minurwod:
; CV2:       # %bb.0:
; CV2-NEXT:    minuwp $r0 = $r0, $r2
; CV2-NEXT:    minuwp $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srld $r3 = $r1, 32
; CV2-NEXT:    copyd $r4 = $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r4 = $r3, 63, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    minuwp $r0 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    minuwp $r1 = $r1, $r0
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    minuwp $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    minuwp $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = tail call i32 @llvm.vector.reduce.umin.v8i32(<8 x i32> %0)
  ret i32 %2
}

define i32 @minurwpd(<2 x i32> %0) {
; CV1-LABEL: minurwpd:
; CV1:       # %bb.0:
; CV1-NEXT:    srad $r1 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 63, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    minuwp $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 2)
;
; CV2-LABEL: minurwpd:
; CV2:       # %bb.0:
; CV2-NEXT:    srad $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 63, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    minuwp $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 2)
  %2 = tail call i32 @llvm.vector.reduce.umin.v2i32(<2 x i32> %0)
  ret i32 %2
}

define i32 @minurwqd(<4 x i32> %0) {
; CV1-LABEL: minurwqd:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r2 = $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    minuwp $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    minuwp $r1 = $r1, $r0
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    minuwp $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    minuwp $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 5)
;
; CV2-LABEL: minurwqd:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r2 = $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    minuwp $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    minuwp $r1 = $r1, $r0
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    minuwp $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    minuwp $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 5)
  %2 = tail call i32 @llvm.vector.reduce.umin.v4i32(<4 x i32> %0)
  ret i32 %2
}

define i8 @andrbod(<8 x i8> %0) {
; CV1-LABEL: andrbod:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    andd $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 12)
;
; CV2-LABEL: andrbod:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    andd $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 12)
  %2 = tail call i8 @llvm.vector.reduce.and.v8i8(<8 x i8> %0)
  ret i8 %2
}

define i8 @andrbpd(<2 x i8> %0) {
; CV1-LABEL: andrbpd:
; CV1:       # %bb.0:
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andw $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: andrbpd:
; CV2:       # %bb.0:
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    andw $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 3)
  %2 = tail call i8 @llvm.vector.reduce.and.v2i8(<2 x i8> %0)
  ret i8 %2
}

define i8 @andrbqd(<4 x i8> %0) {
; CV1-LABEL: andrbqd:
; CV1:       # %bb.0:
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    andw $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    andw $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: andrbqd:
; CV2:       # %bb.0:
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    andw $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    andw $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = tail call i8 @llvm.vector.reduce.and.v4i8(<4 x i8> %0)
  ret i8 %2
}

define i8 @andrbvd(<32 x i8> %0) {
; CV1-LABEL: andrbvd:
; CV1:       # %bb.0:
; CV1-NEXT:    andd $r0 = $r0, $r2
; CV1-NEXT:    andd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    andd $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 14)
;
; CV2-LABEL: andrbvd:
; CV2:       # %bb.0:
; CV2-NEXT:    andd $r0 = $r0, $r2
; CV2-NEXT:    andd $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    andd $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 14)
  %2 = tail call i8 @llvm.vector.reduce.and.v32i8(<32 x i8> %0)
  ret i8 %2
}

define i8 @andrbxd(<16 x i8> %0) {
; CV1-LABEL: andrbxd:
; CV1:       # %bb.0:
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    andd $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 13)
;
; CV2-LABEL: andrbxd:
; CV2:       # %bb.0:
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    andd $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 13)
  %2 = tail call i8 @llvm.vector.reduce.and.v16i8(<16 x i8> %0)
  ret i8 %2
}

define i16 @andrhod(<8 x i16> %0) {
; CV1-LABEL: andrhod:
; CV1:       # %bb.0:
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: andrhod:
; CV2:       # %bb.0:
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = tail call i16 @llvm.vector.reduce.and.v8i16(<8 x i16> %0)
  ret i16 %2
}

define i16 @andrhpd(<2 x i16> %0) {
; CV1-LABEL: andrhpd:
; CV1:       # %bb.0:
; CV1-NEXT:    srlw $r1 = $r0, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 31, 16
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andw $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: andrhpd:
; CV2:       # %bb.0:
; CV2-NEXT:    srlw $r1 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 31, 16
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    andw $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 3)
  %2 = tail call i16 @llvm.vector.reduce.and.v2i16(<2 x i16> %0)
  ret i16 %2
}

define i16 @andrhqd(<4 x i16> %0) {
; CV1-LABEL: andrhqd:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: andrhqd:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 6)
  %2 = tail call i16 @llvm.vector.reduce.and.v4i16(<4 x i16> %0)
  ret i16 %2
}

define i16 @andrhxd(<16 x i16> %0) {
; CV1-LABEL: andrhxd:
; CV1:       # %bb.0:
; CV1-NEXT:    andd $r0 = $r0, $r2
; CV1-NEXT:    andd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 8)
;
; CV2-LABEL: andrhxd:
; CV2:       # %bb.0:
; CV2-NEXT:    andd $r0 = $r0, $r2
; CV2-NEXT:    andd $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
  %2 = tail call i16 @llvm.vector.reduce.and.v16i16(<16 x i16> %0)
  ret i16 %2
}

define i32 @andrwod(<8 x i32> %0) {
; CV1-LABEL: andrwod:
; CV1:       # %bb.0:
; CV1-NEXT:    andd $r0 = $r0, $r2
; CV1-NEXT:    andd $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srld $r3 = $r1, 32
; CV1-NEXT:    copyd $r4 = $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r4 = $r3, 63, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    andd $r0 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd $r1 = $r1, $r0
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    andd $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    andd $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: andrwod:
; CV2:       # %bb.0:
; CV2-NEXT:    andd $r0 = $r0, $r2
; CV2-NEXT:    andd $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srld $r3 = $r1, 32
; CV2-NEXT:    copyd $r4 = $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r4 = $r3, 63, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    andd $r0 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    andd $r1 = $r1, $r0
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    andd $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    andd $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = tail call i32 @llvm.vector.reduce.and.v8i32(<8 x i32> %0)
  ret i32 %2
}

define i32 @andrwpd(<2 x i32> %0) {
; CV1-LABEL: andrwpd:
; CV1:       # %bb.0:
; CV1-NEXT:    srad $r1 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 63, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andd $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 2)
;
; CV2-LABEL: andrwpd:
; CV2:       # %bb.0:
; CV2-NEXT:    srad $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 63, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    andd $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 2)
  %2 = tail call i32 @llvm.vector.reduce.and.v2i32(<2 x i32> %0)
  ret i32 %2
}

define i32 @andrwqd(<4 x i32> %0) {
; CV1-LABEL: andrwqd:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r2 = $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    andd $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    andd $r1 = $r1, $r0
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    andd $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    andd $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 5)
;
; CV2-LABEL: andrwqd:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r2 = $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    andd $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    andd $r1 = $r1, $r0
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    andd $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    andd $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 5)
  %2 = tail call i32 @llvm.vector.reduce.and.v4i32(<4 x i32> %0)
  ret i32 %2
}

define i8 @orrbod(<8 x i8> %0) {
; CV1-LABEL: orrbod:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    iord $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 12)
;
; CV2-LABEL: orrbod:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    iord $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 12)
  %2 = tail call i8 @llvm.vector.reduce.or.v8i8(<8 x i8> %0)
  ret i8 %2
}

define i8 @orrbpd(<2 x i8> %0) {
; CV1-LABEL: orrbpd:
; CV1:       # %bb.0:
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    iorw $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: orrbpd:
; CV2:       # %bb.0:
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    iorw $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 3)
  %2 = tail call i8 @llvm.vector.reduce.or.v2i8(<2 x i8> %0)
  ret i8 %2
}

define i8 @orrbqd(<4 x i8> %0) {
; CV1-LABEL: orrbqd:
; CV1:       # %bb.0:
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iorw $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    iorw $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: orrbqd:
; CV2:       # %bb.0:
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    iorw $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    iorw $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = tail call i8 @llvm.vector.reduce.or.v4i8(<4 x i8> %0)
  ret i8 %2
}

define i8 @orrbvd(<32 x i8> %0) {
; CV1-LABEL: orrbvd:
; CV1:       # %bb.0:
; CV1-NEXT:    iord $r0 = $r0, $r2
; CV1-NEXT:    iord $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    iord $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 14)
;
; CV2-LABEL: orrbvd:
; CV2:       # %bb.0:
; CV2-NEXT:    iord $r0 = $r0, $r2
; CV2-NEXT:    iord $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    iord $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 14)
  %2 = tail call i8 @llvm.vector.reduce.or.v32i8(<32 x i8> %0)
  ret i8 %2
}

define i8 @orrbxd(<16 x i8> %0) {
; CV1-LABEL: orrbxd:
; CV1:       # %bb.0:
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    iord $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 13)
;
; CV2-LABEL: orrbxd:
; CV2:       # %bb.0:
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    iord $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 13)
  %2 = tail call i8 @llvm.vector.reduce.or.v16i8(<16 x i8> %0)
  ret i8 %2
}

define i16 @orrhod(<8 x i16> %0) {
; CV1-LABEL: orrhod:
; CV1:       # %bb.0:
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: orrhod:
; CV2:       # %bb.0:
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = tail call i16 @llvm.vector.reduce.or.v8i16(<8 x i16> %0)
  ret i16 %2
}

define i16 @orrhpd(<2 x i16> %0) {
; CV1-LABEL: orrhpd:
; CV1:       # %bb.0:
; CV1-NEXT:    srlw $r1 = $r0, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 31, 16
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    iorw $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: orrhpd:
; CV2:       # %bb.0:
; CV2-NEXT:    srlw $r1 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 31, 16
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    iorw $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 3)
  %2 = tail call i16 @llvm.vector.reduce.or.v2i16(<2 x i16> %0)
  ret i16 %2
}

define i16 @orrhqd(<4 x i16> %0) {
; CV1-LABEL: orrhqd:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: orrhqd:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 6)
  %2 = tail call i16 @llvm.vector.reduce.or.v4i16(<4 x i16> %0)
  ret i16 %2
}

define i16 @orrhxd(<16 x i16> %0) {
; CV1-LABEL: orrhxd:
; CV1:       # %bb.0:
; CV1-NEXT:    iord $r0 = $r0, $r2
; CV1-NEXT:    iord $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 8)
;
; CV2-LABEL: orrhxd:
; CV2:       # %bb.0:
; CV2-NEXT:    iord $r0 = $r0, $r2
; CV2-NEXT:    iord $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
  %2 = tail call i16 @llvm.vector.reduce.or.v16i16(<16 x i16> %0)
  ret i16 %2
}

define i32 @orrwod(<8 x i32> %0) {
; CV1-LABEL: orrwod:
; CV1:       # %bb.0:
; CV1-NEXT:    iord $r0 = $r0, $r2
; CV1-NEXT:    iord $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srld $r3 = $r1, 32
; CV1-NEXT:    copyd $r4 = $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r4 = $r3, 63, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    iord $r0 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    iord $r1 = $r1, $r0
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    iord $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    iord $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: orrwod:
; CV2:       # %bb.0:
; CV2-NEXT:    iord $r0 = $r0, $r2
; CV2-NEXT:    iord $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srld $r3 = $r1, 32
; CV2-NEXT:    copyd $r4 = $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r4 = $r3, 63, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    iord $r0 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    iord $r1 = $r1, $r0
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    iord $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    iord $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = tail call i32 @llvm.vector.reduce.or.v8i32(<8 x i32> %0)
  ret i32 %2
}

define i32 @orrwpd(<2 x i32> %0) {
; CV1-LABEL: orrwpd:
; CV1:       # %bb.0:
; CV1-NEXT:    srad $r1 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 63, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    iord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 2)
;
; CV2-LABEL: orrwpd:
; CV2:       # %bb.0:
; CV2-NEXT:    srad $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 63, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    iord $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 2)
  %2 = tail call i32 @llvm.vector.reduce.or.v2i32(<2 x i32> %0)
  ret i32 %2
}

define i32 @orrwqd(<4 x i32> %0) {
; CV1-LABEL: orrwqd:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r2 = $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    iord $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    iord $r1 = $r1, $r0
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    iord $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    iord $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 5)
;
; CV2-LABEL: orrwqd:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r2 = $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    iord $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    iord $r1 = $r1, $r0
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    iord $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    iord $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 5)
  %2 = tail call i32 @llvm.vector.reduce.or.v4i32(<4 x i32> %0)
  ret i32 %2
}

define i8 @xorrbod(<8 x i8> %0) {
; CV1-LABEL: xorrbod:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 12)
;
; CV2-LABEL: xorrbod:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    eord $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 12)
  %2 = tail call i8 @llvm.vector.reduce.xor.v8i8(<8 x i8> %0)
  ret i8 %2
}

define i8 @xorrbpd(<2 x i8> %0) {
; CV1-LABEL: xorrbpd:
; CV1:       # %bb.0:
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    eorw $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: xorrbpd:
; CV2:       # %bb.0:
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    eorw $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 3)
  %2 = tail call i8 @llvm.vector.reduce.xor.v2i8(<2 x i8> %0)
  ret i8 %2
}

define i8 @xorrbqd(<4 x i8> %0) {
; CV1-LABEL: xorrbqd:
; CV1:       # %bb.0:
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    eorw $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    eorw $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: xorrbqd:
; CV2:       # %bb.0:
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    eorw $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    eorw $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = tail call i8 @llvm.vector.reduce.xor.v4i8(<4 x i8> %0)
  ret i8 %2
}

define i8 @xorrbvd(<32 x i8> %0) {
; CV1-LABEL: xorrbvd:
; CV1:       # %bb.0:
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    eord $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 13)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 14)
;
; CV2-LABEL: xorrbvd:
; CV2:       # %bb.0:
; CV2-NEXT:    eord $r0 = $r0, $r2
; CV2-NEXT:    eord $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    eord $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 13)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 14)
  %2 = tail call i8 @llvm.vector.reduce.xor.v32i8(<32 x i8> %0)
  ret i8 %2
}

define i8 @xorrbxd(<16 x i8> %0) {
; CV1-LABEL: xorrbxd:
; CV1:       # %bb.0:
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r1 = $r0, 7, 0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    srlw $r1 = $r0, 24
; CV1-NEXT:    extfz $r2 = $r0, 23, 16
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    insf $r1 = $r0, 15, 8
; CV1-NEXT:    insf $r2 = $r1, 15, 8
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    insf $r1 = $r1, 31, 16
; CV1-NEXT:    insf $r2 = $r1, 31, 16
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    insf $r2 = $r1, 63, 32
; CV1-NEXT:    ;; # (end cycle 8)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 9)
; CV1-NEXT:    extfz $r1 = $r0, 15, 8
; CV1-NEXT:    ;; # (end cycle 10)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV1-NEXT:    ;; # (end cycle 11)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 12)
; CV1-NEXT:    zxbd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 13)
;
; CV2-LABEL: xorrbxd:
; CV2:       # %bb.0:
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r1 = $r0, 7, 0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x101010180402010
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    srlw $r1 = $r0, 24
; CV2-NEXT:    extfz $r2 = $r0, 23, 16
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    insf $r1 = $r0, 15, 8
; CV2-NEXT:    insf $r2 = $r1, 15, 8
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    insf $r1 = $r1, 31, 16
; CV2-NEXT:    insf $r2 = $r1, 31, 16
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    insf $r2 = $r1, 63, 32
; CV2-NEXT:    ;; # (end cycle 8)
; CV2-NEXT:    eord $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 9)
; CV2-NEXT:    extfz $r1 = $r0, 15, 8
; CV2-NEXT:    ;; # (end cycle 10)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x1010101.@
; CV2-NEXT:    ;; # (end cycle 11)
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 12)
; CV2-NEXT:    zxbd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 13)
  %2 = tail call i8 @llvm.vector.reduce.xor.v16i8(<16 x i8> %0)
  ret i8 %2
}

define i16 @xorrhod(<8 x i16> %0) {
; CV1-LABEL: xorrhod:
; CV1:       # %bb.0:
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: xorrhod:
; CV2:       # %bb.0:
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = tail call i16 @llvm.vector.reduce.xor.v8i16(<8 x i16> %0)
  ret i16 %2
}

define i16 @xorrhpd(<2 x i16> %0) {
; CV1-LABEL: xorrhpd:
; CV1:       # %bb.0:
; CV1-NEXT:    srlw $r1 = $r0, 16
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 31, 16
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    eorw $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 3)
;
; CV2-LABEL: xorrhpd:
; CV2:       # %bb.0:
; CV2-NEXT:    srlw $r1 = $r0, 16
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 31, 16
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    eorw $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 3)
  %2 = tail call i16 @llvm.vector.reduce.xor.v2i16(<2 x i16> %0)
  ret i16 %2
}

define i16 @xorrhqd(<4 x i16> %0) {
; CV1-LABEL: xorrhqd:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 6)
;
; CV2-LABEL: xorrhqd:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 6)
  %2 = tail call i16 @llvm.vector.reduce.xor.v4i16(<4 x i16> %0)
  ret i16 %2
}

define i16 @xorrhxd(<16 x i16> %0) {
; CV1-LABEL: xorrhxd:
; CV1:       # %bb.0:
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    eord $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    copyd $r1 = $r0
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r1 = $r0, 15, 0
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ;; # (end cycle 7)
; CV1-NEXT:    zxhd $r0 = $r0
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 8)
;
; CV2-LABEL: xorrhxd:
; CV2:       # %bb.0:
; CV2-NEXT:    eord $r0 = $r0, $r2
; CV2-NEXT:    eord $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    copyd $r1 = $r0
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r1 = $r0, 15, 0
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    sbmm8 $r1 = $r1, 0x201020180402010
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    sbmm8 $r1 = $r0, 0x8040804.@
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ;; # (end cycle 7)
; CV2-NEXT:    zxhd $r0 = $r0
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 8)
  %2 = tail call i16 @llvm.vector.reduce.xor.v16i16(<16 x i16> %0)
  ret i16 %2
}

define i32 @xorrwod(<8 x i32> %0) {
; CV1-LABEL: xorrwod:
; CV1:       # %bb.0:
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    eord $r1 = $r1, $r3
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    srld $r3 = $r1, 32
; CV1-NEXT:    copyd $r4 = $r1
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    insf $r4 = $r3, 63, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    eord $r0 = $r0, $r4
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    eord $r1 = $r1, $r0
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 5)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 6)
; CV1-NEXT:    eord $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 7)
;
; CV2-LABEL: xorrwod:
; CV2:       # %bb.0:
; CV2-NEXT:    eord $r0 = $r0, $r2
; CV2-NEXT:    eord $r1 = $r1, $r3
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    srld $r3 = $r1, 32
; CV2-NEXT:    copyd $r4 = $r1
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    insf $r4 = $r3, 63, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    eord $r0 = $r0, $r4
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    eord $r1 = $r1, $r0
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 5)
; CV2-NEXT:    eord $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 6)
; CV2-NEXT:    eord $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 7)
  %2 = tail call i32 @llvm.vector.reduce.xor.v8i32(<8 x i32> %0)
  ret i32 %2
}

define i32 @xorrwpd(<2 x i32> %0) {
; CV1-LABEL: xorrwpd:
; CV1:       # %bb.0:
; CV1-NEXT:    srad $r1 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    insf $r1 = $r0, 63, 32
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    eord $r0 = $r0, $r1
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 2)
;
; CV2-LABEL: xorrwpd:
; CV2:       # %bb.0:
; CV2-NEXT:    srad $r1 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    insf $r1 = $r0, 63, 32
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    eord $r0 = $r0, $r1
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 2)
  %2 = tail call i32 @llvm.vector.reduce.xor.v2i32(<2 x i32> %0)
  ret i32 %2
}

define i32 @xorrwqd(<4 x i32> %0) {
; CV1-LABEL: xorrwqd:
; CV1:       # %bb.0:
; CV1-NEXT:    copyd $r2 = $r1
; CV1-NEXT:    ;; # (end cycle 0)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    ;; # (end cycle 1)
; CV1-NEXT:    eord $r1 = $r1, $r0
; CV1-NEXT:    srld $r2 = $r0, 32
; CV1-NEXT:    ;; # (end cycle 2)
; CV1-NEXT:    insf $r2 = $r2, 63, 32
; CV1-NEXT:    ;; # (end cycle 3)
; CV1-NEXT:    eord $r0 = $r0, $r2
; CV1-NEXT:    copyd $r3 = $r2
; CV1-NEXT:    ;; # (end cycle 4)
; CV1-NEXT:    eord $r1 = $r1, $r3
; CV1-NEXT:    ret
; CV1-NEXT:    ;; # (end cycle 5)
;
; CV2-LABEL: xorrwqd:
; CV2:       # %bb.0:
; CV2-NEXT:    copyd $r2 = $r1
; CV2-NEXT:    ;; # (end cycle 0)
; CV2-NEXT:    eord $r0 = $r0, $r2
; CV2-NEXT:    ;; # (end cycle 1)
; CV2-NEXT:    eord $r1 = $r1, $r0
; CV2-NEXT:    srld $r2 = $r0, 32
; CV2-NEXT:    ;; # (end cycle 2)
; CV2-NEXT:    insf $r2 = $r2, 63, 32
; CV2-NEXT:    ;; # (end cycle 3)
; CV2-NEXT:    eord $r0 = $r0, $r2
; CV2-NEXT:    copyd $r3 = $r2
; CV2-NEXT:    ;; # (end cycle 4)
; CV2-NEXT:    eord $r1 = $r1, $r3
; CV2-NEXT:    ret
; CV2-NEXT:    ;; # (end cycle 5)
  %2 = tail call i32 @llvm.vector.reduce.xor.v4i32(<4 x i32> %0)
  ret i32 %2
}

declare i8 @llvm.vector.reduce.smax.v8i8(<8 x i8>)

declare i8 @llvm.vector.reduce.smax.v2i8(<2 x i8>)

declare i8 @llvm.vector.reduce.smax.v4i8(<4 x i8>)

declare i8 @llvm.vector.reduce.smax.v32i8(<32 x i8>)

declare i8 @llvm.vector.reduce.smax.v16i8(<16 x i8>)

declare i16 @llvm.vector.reduce.smax.v8i16(<8 x i16>)

declare i16 @llvm.vector.reduce.smax.v2i16(<2 x i16>)

declare i16 @llvm.vector.reduce.smax.v4i16(<4 x i16>)

declare i16 @llvm.vector.reduce.smax.v32i16(<32 x i16>)

declare i16 @llvm.vector.reduce.smax.v16i16(<16 x i16>)

declare i32 @llvm.vector.reduce.smax.v8i32(<8 x i32>)

declare i32 @llvm.vector.reduce.smax.v2i32(<2 x i32>)

declare i32 @llvm.vector.reduce.smax.v4i32(<4 x i32>)

declare i32 @llvm.vector.reduce.smax.v32i32(<32 x i32>)

declare i32 @llvm.vector.reduce.smax.v16i32(<16 x i32>)

declare i8 @llvm.vector.reduce.umax.v8i8(<8 x i8>)

declare i8 @llvm.vector.reduce.umax.v2i8(<2 x i8>)

declare i8 @llvm.vector.reduce.umax.v4i8(<4 x i8>)

declare i8 @llvm.vector.reduce.umax.v32i8(<32 x i8>)

declare i8 @llvm.vector.reduce.umax.v16i8(<16 x i8>)

declare i16 @llvm.vector.reduce.umax.v8i16(<8 x i16>)

declare i16 @llvm.vector.reduce.umax.v2i16(<2 x i16>)

declare i16 @llvm.vector.reduce.umax.v4i16(<4 x i16>)

declare i16 @llvm.vector.reduce.umax.v32i16(<32 x i16>)

declare i16 @llvm.vector.reduce.umax.v16i16(<16 x i16>)

declare i32 @llvm.vector.reduce.umax.v8i32(<8 x i32>)

declare i32 @llvm.vector.reduce.umax.v2i32(<2 x i32>)

declare i32 @llvm.vector.reduce.umax.v4i32(<4 x i32>)

declare i32 @llvm.vector.reduce.umax.v32i32(<32 x i32>)

declare i32 @llvm.vector.reduce.umax.v16i32(<16 x i32>)

declare i8 @llvm.vector.reduce.smin.v8i8(<8 x i8>)

declare i8 @llvm.vector.reduce.smin.v2i8(<2 x i8>)

declare i8 @llvm.vector.reduce.smin.v4i8(<4 x i8>)

declare i8 @llvm.vector.reduce.smin.v32i8(<32 x i8>)

declare i8 @llvm.vector.reduce.smin.v16i8(<16 x i8>)

declare i16 @llvm.vector.reduce.smin.v8i16(<8 x i16>)

declare i16 @llvm.vector.reduce.smin.v2i16(<2 x i16>)

declare i16 @llvm.vector.reduce.smin.v4i16(<4 x i16>)

declare i16 @llvm.vector.reduce.smin.v32i16(<32 x i16>)

declare i16 @llvm.vector.reduce.smin.v16i16(<16 x i16>)

declare i32 @llvm.vector.reduce.smin.v8i32(<8 x i32>)

declare i32 @llvm.vector.reduce.smin.v2i32(<2 x i32>)

declare i32 @llvm.vector.reduce.smin.v4i32(<4 x i32>)

declare i32 @llvm.vector.reduce.smin.v32i32(<32 x i32>)

declare i32 @llvm.vector.reduce.smin.v16i32(<16 x i32>)

declare i8 @llvm.vector.reduce.umin.v8i8(<8 x i8>)

declare i8 @llvm.vector.reduce.umin.v2i8(<2 x i8>)

declare i8 @llvm.vector.reduce.umin.v4i8(<4 x i8>)

declare i8 @llvm.vector.reduce.umin.v32i8(<32 x i8>)

declare i8 @llvm.vector.reduce.umin.v16i8(<16 x i8>)

declare i16 @llvm.vector.reduce.umin.v8i16(<8 x i16>)

declare i16 @llvm.vector.reduce.umin.v2i16(<2 x i16>)

declare i16 @llvm.vector.reduce.umin.v4i16(<4 x i16>)

declare i16 @llvm.vector.reduce.umin.v32i16(<32 x i16>)

declare i16 @llvm.vector.reduce.umin.v16i16(<16 x i16>)

declare i32 @llvm.vector.reduce.umin.v8i32(<8 x i32>)

declare i32 @llvm.vector.reduce.umin.v2i32(<2 x i32>)

declare i32 @llvm.vector.reduce.umin.v4i32(<4 x i32>)

declare i32 @llvm.vector.reduce.umin.v32i32(<32 x i32>)

declare i32 @llvm.vector.reduce.umin.v16i32(<16 x i32>)

declare i8 @llvm.vector.reduce.and.v8i8(<8 x i8>)

declare i8 @llvm.vector.reduce.and.v2i8(<2 x i8>)

declare i8 @llvm.vector.reduce.and.v4i8(<4 x i8>)

declare i8 @llvm.vector.reduce.and.v32i8(<32 x i8>)

declare i8 @llvm.vector.reduce.and.v16i8(<16 x i8>)

declare i16 @llvm.vector.reduce.and.v8i16(<8 x i16>)

declare i16 @llvm.vector.reduce.and.v2i16(<2 x i16>)

declare i16 @llvm.vector.reduce.and.v4i16(<4 x i16>)

declare i16 @llvm.vector.reduce.and.v32i16(<32 x i16>)

declare i16 @llvm.vector.reduce.and.v16i16(<16 x i16>)

declare i32 @llvm.vector.reduce.and.v8i32(<8 x i32>)

declare i32 @llvm.vector.reduce.and.v2i32(<2 x i32>)

declare i32 @llvm.vector.reduce.and.v4i32(<4 x i32>)

declare i32 @llvm.vector.reduce.and.v32i32(<32 x i32>)

declare i32 @llvm.vector.reduce.and.v16i32(<16 x i32>)

declare i8 @llvm.vector.reduce.or.v8i8(<8 x i8>)

declare i8 @llvm.vector.reduce.or.v2i8(<2 x i8>)

declare i8 @llvm.vector.reduce.or.v4i8(<4 x i8>)

declare i8 @llvm.vector.reduce.or.v32i8(<32 x i8>)

declare i8 @llvm.vector.reduce.or.v16i8(<16 x i8>)

declare i16 @llvm.vector.reduce.or.v8i16(<8 x i16>)

declare i16 @llvm.vector.reduce.or.v2i16(<2 x i16>)

declare i16 @llvm.vector.reduce.or.v4i16(<4 x i16>)

declare i16 @llvm.vector.reduce.or.v32i16(<32 x i16>)

declare i16 @llvm.vector.reduce.or.v16i16(<16 x i16>)

declare i32 @llvm.vector.reduce.or.v8i32(<8 x i32>)

declare i32 @llvm.vector.reduce.or.v2i32(<2 x i32>)

declare i32 @llvm.vector.reduce.or.v4i32(<4 x i32>)

declare i32 @llvm.vector.reduce.or.v32i32(<32 x i32>)

declare i32 @llvm.vector.reduce.or.v16i32(<16 x i32>)

declare i8 @llvm.vector.reduce.xor.v8i8(<8 x i8>)

declare i8 @llvm.vector.reduce.xor.v2i8(<2 x i8>)

declare i8 @llvm.vector.reduce.xor.v4i8(<4 x i8>)

declare i8 @llvm.vector.reduce.xor.v32i8(<32 x i8>)

declare i8 @llvm.vector.reduce.xor.v16i8(<16 x i8>)

declare i16 @llvm.vector.reduce.xor.v8i16(<8 x i16>)

declare i16 @llvm.vector.reduce.xor.v2i16(<2 x i16>)

declare i16 @llvm.vector.reduce.xor.v4i16(<4 x i16>)

declare i16 @llvm.vector.reduce.xor.v32i16(<32 x i16>)

declare i16 @llvm.vector.reduce.xor.v16i16(<16 x i16>)

declare i32 @llvm.vector.reduce.xor.v8i32(<8 x i32>)

declare i32 @llvm.vector.reduce.xor.v2i32(<2 x i32>)

declare i32 @llvm.vector.reduce.xor.v4i32(<4 x i32>)

declare i32 @llvm.vector.reduce.xor.v32i32(<32 x i32>)

declare i32 @llvm.vector.reduce.xor.v16i32(<16 x i32>)

